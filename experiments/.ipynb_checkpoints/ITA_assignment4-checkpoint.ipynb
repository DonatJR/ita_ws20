{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hp0qzvF5quM"
   },
   "source": [
    "# Assignment 4 : \"RNN/CNN and keyphrase extraction \"\n",
    "Due: Monday 2pm, Feburary 15, 2021, via Moodle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsXIFFvQ5quX"
   },
   "source": [
    "### Submission guidelines\n",
    "\n",
    "- Solutions need to be uploaded as a single Jupyter notebook. You will find many provided codes in the notebook, your task is to fill in the missing cells.\n",
    "- For the written solution, use LaTeX in markdown inside the same notebook. Do *not* hand in a separate file for it.\n",
    "- Download the .zip file containing the dataset but do *not* upload it with your solution.\n",
    "- It is sufficient if one person per group uploads the solution to Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ur58Ru825quZ"
   },
   "source": [
    "## Task 1: Spam Detection (3 + 3 + 2 + 4 + 1) = 13 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmPrBmWZ5quZ"
   },
   "source": [
    "In this task we tackle the task of Spam Detection (Text Classification) using RNNs and CNNs. To start download the `spam.csv` file from Moodle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WjdVHmVe5qua"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Bidirectional, Conv1D,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Jj3aNGEz5qub",
    "outputId": "0ab0dd32-fb30-4b0d-e7d7-11a9180b9e0b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8eiJUrD5quf"
   },
   "source": [
    "### Sub Task 1: Pre-processsing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hG1Fq4nP5qug"
   },
   "source": [
    "There are some unnecessary columns in the data `Unnamed:2 to 4`. Remove those and rename the remaining columns to `labels` and `text`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzC6DrBl5qui"
   },
   "outputs": [],
   "source": [
    "### your code ###\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7mkzcTp5quj"
   },
   "source": [
    "Notice that the labels are string values. We need to convert them to integers of `0` and `1` for Tensorflow to understand. Create a new column called `binary_labels` and save the binary values in there. Then split the dataset into train and test set using `sklearn`, where the test data is 20 precent of the entire set. Then use 20 percent of the training set for your validation set. Convert them into a `tensorflow.Dataset` for training and test, shuffle the `dataset_train` and set the batch_size of both dataset to `16`. You can change the batch size to a smaller value if your device does not support it. Set the random_state for both to `12`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G0sO5Yi5qul"
   },
   "outputs": [],
   "source": [
    "df['binary_labels'] = ### your code ###\n",
    "Y = ### your code ###\n",
    "# split up the data\n",
    "X_train, X_test, Ytrain, Ytest = ### your code ###\n",
    "X_train, X_val, Ytrain, Yval = ### your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUpzwr9O5qun"
   },
   "outputs": [],
   "source": [
    "dataset_train =  ### your code ###\n",
    "dataset_train =  ### shuffle, set batch to 16 ###\n",
    "\n",
    "dataset_val = ### your code ###\n",
    "dataset_val = ### set batch to 16 ###\n",
    "\n",
    "dataset_test =  ### your code ###\n",
    "dataset_test = ### set batch to 16 ###\n",
    "\n",
    "for feat, targ in dataset_train.take(1):\n",
    "    print ('Features: {}, Target: {}'.format(feat, targ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUgbolxu5quo"
   },
   "source": [
    "Set the vocabulary size to `10000` and use the `TextVectorization` class to preprocess the textual data and turn it into integer vectors. Set the lenght of the output sequence to `128`. Any sequence with more tokens will be truncated and shorter sequences will be padded to fit the size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf3o2L_t5quo"
   },
   "outputs": [],
   "source": [
    "encoder = ### your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OgHYbTYz5qup",
    "outputId": "6b59901a-89ca-42a8-8caa-6b61ab6d924b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded [2153   24 1003   10 6369   23  116 1528   61  228   11  191  853    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "Original:  b\"Yunny i'm walking in citylink now \\xc3\\x8c_ faster come down... Me very hungry...\"\n",
      "After vectorization:  yunny im walking in citylink now ÃŒ faster come down me very hungry                                                                                                                   \n"
     ]
    }
   ],
   "source": [
    "# run this cell to make sure your output makes sense \n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "print(\"Encoded\",encoder(feat).numpy()[0]) # this should output the encoded vector with integer values to the size of 128 \n",
    "print(\"Original: \", feat.numpy()[0]) # the orginal text should be retrieved \n",
    "print(\"After vectorization: \", \" \".join(vocab[encoder(feat).numpy()[0]])) #text with preprocessing (lowercase and normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNecc5_65quq"
   },
   "source": [
    "## Sub Task 2: Create the LSTM Model and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "su9blgeI5quq"
   },
   "source": [
    "In this sub task you will use a RNN with LSTM cells for text classification. We use the Keras functional API for our model building. Below you will see the summary output of our model. Use this as a basis to model your own network with the same layers: \n",
    "```\n",
    "Model: \"model_63\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_88 (InputLayer)        [(None, 1)]               0         \n",
    "_________________________________________________________________\n",
    "text_vectorization_9 (TextVe (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "embedding_73 (Embedding)     (None, 128, 64)           640064    \n",
    "_________________________________________________________________\n",
    "lstm_41 (LSTM)               (None, 128, 32)           12416     \n",
    "_________________________________________________________________\n",
    "global_max_pooling1d_39 (Glo (None, 32)                0         \n",
    "_________________________________________________________________\n",
    "dense_49 (Dense)             (None, 1)                 33        \n",
    "=================================================================\n",
    "Total params: 652,513\n",
    "Trainable params: 652,513\n",
    "Non-trainable params: 0\n",
    "```\n",
    "Train the model for 6 epochs with the `fit` function, using the validation set for intermediate testing. Use the Adam optimizer with default parameters and binary cross entropy loss. Also keep track of the accuracy during training so we can plot it later. You can specify the metric you would like to track, when compiling your model. The value of the loss and the callback function for accuracy will be saved in the `history` of the returned object for `fit` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6RkwYNw5quq"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "embedding_dim = 64\n",
    "hidden_dim = 32\n",
    "\n",
    "i = Input(shape=(1,), dtype=\"string\")\n",
    "x = ### pre-processing ### \n",
    "x =  ### embedding ###\n",
    "x =  ### lstm - keep in mind the output shape is (None, 128, 32) meaning that ALL the hidden states are returned and not just the final one ###\n",
    "x = ### max pool over everything ### \n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "lstm_model = ### build model ### \n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nxTuy7d5qur"
   },
   "outputs": [],
   "source": [
    "# Compile and fit\n",
    "lstm_model.compile ### your code ### \n",
    "print('Training model...')\n",
    "r = lstm_model.fit ### your code ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKUEbBor5qut"
   },
   "outputs": [],
   "source": [
    "# Plot loss per iteration\n",
    "\n",
    "### your code ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "446Xdczb5qut"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy per iteration\n",
    "\n",
    "### your code ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9aKKpMM5qut"
   },
   "source": [
    "## Sub Task 3: Create the CNN Model and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaRCqxMs5quu"
   },
   "source": [
    "We want to use the same data but this time with a CNN architecture to classify the spam emails. Once again, use this model summary to create a similiar network: \n",
    "```\n",
    "Model: \"model_64\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_89 (InputLayer)        [(None, 1)]               0         \n",
    "_________________________________________________________________\n",
    "text_vectorization_9 (TextVe (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "embedding_74 (Embedding)     (None, 128, 64)           640064    \n",
    "_________________________________________________________________\n",
    "conv1d_72 (Conv1D)           (None, 126, 32)           6176      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_67 (MaxPooling (None, 42, 32)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_73 (Conv1D)           (None, 40, 64)            6208      \n",
    "_________________________________________________________________\n",
    "max_pooling1d_68 (MaxPooling (None, 13, 64)            0         \n",
    "_________________________________________________________________\n",
    "dense_50 (Dense)             (None, 13, 1)             65        \n",
    "=================================================================\n",
    "Total params: 652,513\n",
    "Trainable params: 652,513\n",
    "Non-trainable params: 0\n",
    "```\n",
    "Train the model for 6 epochs using the validation set for intermediate testing. Use the Adam optimizer with default parameters and binary cross entropy loss, keep track of the accuracy during training so we can plot it later. Note that the CNN trains much quicker than the LSTM network since it can be parallelized easier on a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtH8z2WZ5quv"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "embedding_dim = 64\n",
    "\n",
    "i = Input(shape=(1,), dtype=\"string\")\n",
    "x = ### pre-processing ### \n",
    "x = ### embedding ### \n",
    "x = ### first conv, activation='relu', filter_size= can you guess? ### \n",
    "x = ### first maxpool, filter_size= can you guess?   ### \n",
    "x = ### second conv, activation='relu', filter_size= can you guess?  ### \n",
    "x = ### second maxpool, filter_size= can you guess?   ### \n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "cnn_model = ### build model ### \n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--AyqxtW5quv"
   },
   "outputs": [],
   "source": [
    "# Compile and fit\n",
    "cnn_model.compile ### your code ### \n",
    "print('Training model...')\n",
    "r = cnn_model.fit ### your code ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYnknsRP5qux"
   },
   "outputs": [],
   "source": [
    "# Plot loss per iteration\n",
    "\n",
    "### your code ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlWTqE-45qux"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy per iteration\n",
    "\n",
    "### your code ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IOP2DHL5qux"
   },
   "source": [
    "## Sub Task 4: Mix and Match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xhtum7s5qux"
   },
   "source": [
    "Let's use the power of the functional API to stack our two models together. Our final model takes as input two parallel branches and combine them to get a final output. One input passage will go through our LSTM network and the other through our CNN network. Keep in mind that you need to remove the last sigmoid layer of the both models for it to work properly. The output of two models are concatenated and passed through two additional `Dense` layers. Here is the model summary of the model you need to replicate: \n",
    "\n",
    "```\n",
    "Model: \"model_61\"\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_85 (InputLayer)           [(None, 1)]          0                                            \n",
    "__________________________________________________________________________________________________\n",
    "input_86 (InputLayer)           [(None, 1)]          0                                            \n",
    "__________________________________________________________________________________________________\n",
    "text_vectorization_9 (TextVecto (None, 128)          0           input_85[0][0]                   \n",
    "                                                                 input_86[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "embedding_71 (Embedding)        (None, 128, 20)      200020      text_vectorization_9[51][0]      \n",
    "__________________________________________________________________________________________________\n",
    "conv1d_70 (Conv1D)              (None, 126, 32)      1952        embedding_71[0][0]               \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_65 (MaxPooling1D) (None, 42, 32)       0           conv1d_70[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "embedding_70 (Embedding)        (None, 128, 20)      200020      text_vectorization_9[50][0]      \n",
    "__________________________________________________________________________________________________\n",
    "conv1d_71 (Conv1D)              (None, 40, 64)       6208        max_pooling1d_65[0][0]           \n",
    "__________________________________________________________________________________________________\n",
    "lstm_39 (LSTM)                  (None, 128, 15)      2160        embedding_70[0][0]               \n",
    "__________________________________________________________________________________________________\n",
    "max_pooling1d_66 (MaxPooling1D) (None, 13, 64)       0           conv1d_71[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "global_max_pooling1d_37 (Global (None, 15)           0           lstm_39[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "flatten_6 (Flatten)             (None, 832)          0           max_pooling1d_66[0][0]           \n",
    "__________________________________________________________________________________________________\n",
    "tf.concat_10 (TFOpLambda)       (None, 847)          0           global_max_pooling1d_37[0][0]    \n",
    "                                                                 flatten_6[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "dense_46 (Dense)                (None, 128)          108544      tf.concat_10[0][0]               \n",
    "__________________________________________________________________________________________________\n",
    "dense_47 (Dense)                (None, 1)            129         dense_46[0][0]                   \n",
    "==================================================================================================\n",
    "Total params: 519,033\n",
    "Trainable params: 519,033\n",
    "Non-trainable params: 0\n",
    "_____________________________\n",
    "```\n",
    "Train the model for 4 epochs using the validation set for intermediate testing. Use the Adam optimizer with default parameters and binary cross entropy loss, keep track of the accuracy during training so we can plot it later. Note that the input should be a list containing your training features (twice), the same applies for the validation set and later the test set. This is due to the fact that your model has two input branches, however, since the output branch is still one, no changes are needed for the loss function or target values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eITYuuK25quy"
   },
   "outputs": [],
   "source": [
    "# define two sets of inputs\n",
    "inputA = Input(shape=(1,), dtype=\"string\")\n",
    "inputB = Input(shape=(1,), dtype=\"string\")\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x = ### pre-processing ### \n",
    "x = ### Embedding ### \n",
    "x = ### lstm_model ###\n",
    "x = ### lstm_model ###\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "\n",
    "# the second branch operates on the second input\n",
    "y = ### pre processing ### \n",
    "y = ### Embedding ### \n",
    "y = ### cnn_model ###\n",
    "y = ### cnn_model ###\n",
    "y = ### cnn_model ###\n",
    "y = ### cnn_model ###\n",
    "y = Flatten()(y) ### we need to flatten the data to remove the additional dimension here\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "\n",
    "# combine the output of the two branches\n",
    "combined = tf.concat ### your code ### \n",
    "\n",
    "# combined outputs\n",
    "z = ### your code ### \n",
    "z = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "# our model will accept the inputs of the two branches and then output a single value\n",
    "mix_model = Model(inputs=[x.input, y.input], outputs=z)\n",
    "mix_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMUMLO2R5qu0"
   },
   "outputs": [],
   "source": [
    "# Compile and fit\n",
    "mix_model.compile(### your code ### \n",
    "print('Training model...')\n",
    "r = mix_model.fit(### your code ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAY1r0005qu1"
   },
   "outputs": [],
   "source": [
    "# Plot loss per iteration\n",
    "### your code ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSHM242x5qu2"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy per iteration\n",
    "### your code ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7WVr_Fx5qu2"
   },
   "source": [
    "## Sub Task 5: Comparision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QncqSQGY5qu3"
   },
   "source": [
    "Use the `evaluate` function to compare the performance of the three models on the test set. Which one works better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3_gdKr25qu4"
   },
   "outputs": [],
   "source": [
    "print(\"Test score lstm:\",### your code ### )\n",
    "print(\"Test score cnn:\", ### your code ### )\n",
    "print(\"Test score mix:\", ### your code ### )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr8VogsJ5qu4"
   },
   "source": [
    "## Task 2: Theoretical Questions (1+1+1+1+1+1+1+1) = 8 Points  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lSTMz2f5qu5"
   },
   "source": [
    "### Sub Task 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cebFt4UW5qu5"
   },
   "source": [
    "While training an RNN at the *t*-th time-step, what probability is the RNN computing? ($w_t$ is the word at time $t$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3iYzCh_5qu6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1Q6i4RS5qu6"
   },
   "source": [
    "### Sub Task 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u12qUQWD5qu6"
   },
   "source": [
    "Consider the following two scenarios for the language model:  \n",
    "(i) During training,  \n",
    "(ii) After training, while using the language model to sample.  \n",
    "**For both cases answer the following:** How is the next word chosen? What value is passed on to the next time step? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIdkTPwG5qu7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCF0hNoD5qu7"
   },
   "source": [
    "### Sub Task 3: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtOvQuLm5qu7"
   },
   "source": [
    "While training your RNN, you realise that your weights and activations have taken the value of `NaN` (not a number), what can be the cause of this? Name one method to avoid this problem.  (short answer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56NqUKw45qu7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcAmrnWr5qu7"
   },
   "source": [
    "### Sub Task 4: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otFcTN6A5qu8"
   },
   "source": [
    "You have a pet dragon, whose mood is heavily dependent on the weather on the current and past few days. You have collected data of the weather for the past 365 days as $w_1,...,w_{365}$, where $w_1$ is the first day of your recording, and $w_{365}$ the most recent day. You also recorded the mood of your dragon as $y_1,...,y_{365}$ corresponding to the individual days. To avoid any \"dragon accidents\", you want to predict her mood for the next days. Would you use a unidirectional or bidirectional RNN? Justify your answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIHGqn5OwKEV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeTeI2GE5qu8"
   },
   "source": [
    "### Sub Task 5: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGinHwKR5qu8"
   },
   "source": [
    "You have an input of shape `(84x3)` and you convolve (1D) it with `16` filters of size `7`, using a stride of `2` and no padding. What is the output dimension?  \n",
    "What if you had an image of shape `(84x84x3)` (last dimension is the RGB channel) and you convolve it with `32` filters of shape `(7x7)`, using a stride of `2` and no padding. What is the shape of the output volume?\n",
    "\n",
    "The question is independent of the batch size, imagine a single example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_R_Ui6OB5qu9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OV7ExHyP5qu-"
   },
   "source": [
    "## Sub Task 6: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0AisEM25qu-"
   },
   "source": [
    "What is the effect of pooling layers in terms of the derivative for backpropagation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1suQkseG5qu-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iyUGEDY5qu-"
   },
   "source": [
    "### Sub Task 7: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3aByyOY5qu_"
   },
   "source": [
    "You have an input volume of shape `(32x32x64)` and you apply *max pooling* with stride `4` and a filter size of `2`. What is the size of the output volume? The question is independent of the batch size, imagine a single example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGViPD4NwKEZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NC53qV1h98hS"
   },
   "source": [
    "## Sub Task 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVcu82ep-PR4"
   },
   "source": [
    "You trained a classifier using a convolutional neural network that stacked 4 convolutional layers to differentiate images of animals. There are 100 different classes of animals the network classifies. Now you want to find a hidden unit that strongly indicates that the image is a dragon, i.e., a hidden unit that is strongly activated when shown an image of a dragon. In which of the four layers are you most likely to find such a unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Nsei_lQwKEa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "315kqIiW2Zhr"
   },
   "source": [
    "## Task 3: Unsupervised Keyphrase Extraction (4+3+2) = 9 Points\n",
    "\n",
    "In this task, we will implement a simple unsupervised keyphrase extraction module utilizing a simple grammatical ruling system, which we apply to a Sherlock Holmes novel. To generate TF-IDF-weighted phrases, we will be using the entire collection from Sir Arthur Donan Coyle to calculate document frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joME4y0pC-bq"
   },
   "source": [
    "### Sub Task 1: Extract Keyphrase Candidates\n",
    "We will generate a set of suitable candidate phrases by using spaCy's rule-based `Matcher` class.\n",
    "The pattern should satisfy the following rules for a valid candidate:\n",
    "\n",
    "1. An optional adjective, noun, proper noun\n",
    "2. An optional adjective, noun, proper noun, or adposition\n",
    "3. A mandatory noun or proper noun.\n",
    "\n",
    "This will match phrases of an length of 1-3 tokens, which is a suitable rule of thumb to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "j7yKhziiDkzJ"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlY8UveeHmQH"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load( # load the spaCy model \"en\", and disable named entity recognition #\n",
    "matcher = # Instantiate a Matcher object with the vocabulary of the \"en\" module #\n",
    "\n",
    "pattern = ### your code ###\n",
    "matcher.add( # Append your pattern to the matcher class #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71JZGbIlHfxx",
    "outputId": "7b650ae4-99bd-4226-ab95-fe0b8cc2b09e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Run to verify correctness of your rule-based matcher. Should return \"11\" if correctly implemented.\n",
    "doc = nlp(\"This is a simple test. It should return 'simple', and 'test'. Maybe we can also see if it can recognize the art of war. Would it recognize integer linear programming, too?\")\n",
    "matches = matcher(doc)\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x2TRsI-GeLq"
   },
   "source": [
    "Once you have matched the correct number of keyphrase candidates on the above example, apply your rule-based matcher to an actual data sample. We are going to use the Sherlock Holmes novel \"Hounds of Baskervilles\". You can find the raw text file at the following URL:\n",
    "\n",
    "https://sherlock-holm.es/stories/plain-text/houn.txt\n",
    "\n",
    "Download the text from this URL and apply your spacy model and matcher on it.  \n",
    "**Hint:** Make sure you properly decode your input, since some libraries return binary strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNgRWVB5JmTf"
   },
   "outputs": [],
   "source": [
    "def load_txt_from_url(url=\"https://sherlock-holm.es/stories/plain-text/houn.txt\"):\n",
    "  ### your code here ###\n",
    "  return text\n",
    "\n",
    "text = load_txt_from_url()\n",
    "\n",
    "doc = ### your code here ###\n",
    "matches = ### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4U96ELPKtIA"
   },
   "source": [
    "We will now investigate which phrase candidates are the most frequently appearing in this novel, simply based on the phrase frequency. Therefore, convert your abstract match objects into actual strings, lowercase them, and return the 10 most frequently occurring phrase candidates and their respective frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rizDaIubNMfx"
   },
   "outputs": [],
   "source": [
    "### your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYQt8278NA7v"
   },
   "source": [
    "#### Briefly summarize the quality of your top 10 candidates:\n",
    "\n",
    "YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aucrN3TPJqw1"
   },
   "source": [
    "### Sub Task 2: Generating Document Frequency Values\n",
    "\n",
    "To compare the previously generated terms with a more refined model, we are going to extract document frequencies from the collection of all Sherlock Holmes works. Since the books are relatively long documents, we are instead going to split based on a simple heuristic in the input document, which should allow a decent approximation by taking into account individual chapters of each novel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfp7rUN9N218"
   },
   "source": [
    "Start by loading the Sherlock Holmes canon from https://sherlock-holm.es/stories/plain-text/cnus.txt\n",
    "\n",
    "Afterwards, split the full document into individual chapters. For this, use three consecutive line breaks `\\n\\n\\n` as a splitting condition to approximate the chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWlFYasYRW8k"
   },
   "outputs": [],
   "source": [
    "### your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRcMyiKkRU7d"
   },
   "source": [
    "After splitting, you should have 353 individual \"documents\" to work with. Now, create a dictionary containing each phrase encountered in the larger corpus, and its associated document frequency. \n",
    "\n",
    "**Hint:** Since the processing of 353 documents might take a while, you can incorporate tools such as `tqdm.tqdm` to visualize progress on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLLyxoqOWXbl"
   },
   "outputs": [],
   "source": [
    "def return_occurring_phrases(doc_text):\n",
    "  # should return the candidate phrases which occur in a single \"document\"\n",
    "\n",
    "# iterate over all generated documents\n",
    "\n",
    "# Afterwards, count the number of individual documents a phrase occurred in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUTXGZ83WqEx"
   },
   "outputs": [],
   "source": [
    "# Print the 10 most frequent phrases according to the df-counts, to verify your result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR9FmhTpS_TB"
   },
   "source": [
    "### Sub Task 3: Generating Weighted Keyphrases\n",
    "\n",
    "We can now incorporate the extracted keyphrases to calculate tf_idf scores, and return a hopefully improved version of our keyphrases for the original novel.\n",
    "Iterate over all phrases occurring in the novel, and weigh them according to the definition of TF-IDF. Use the smoothed definition of idf:\n",
    "\n",
    "$ idf(t, D) = \\log \\frac{N}{|\\{d \\in D : t \\in d\\}| + 1} + 1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PC6XIhtYV0nK"
   },
   "outputs": [],
   "source": [
    "def tf_idf\n",
    "  return ### your code ###\n",
    "\n",
    "### calculate the weights for each candidate phrase from the \"Hounds of Baskervilles\" ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSfTPsukVoEF"
   },
   "source": [
    "Now print the top 10 candidate phrases by TF-IDF weight, and compare the results to your previous output. Can you give reasons as to why some of the phrases might be occurring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDahNju-Vv58"
   },
   "outputs": [],
   "source": [
    "### your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdmGgOThVmyz"
   },
   "source": [
    "COMPARE YOUR RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2y8eaRPNWHut"
   },
   "source": [
    "Give two examples of how you could further improve the list of keyphrase values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yfMZ0etWNj_"
   },
   "source": [
    "\n",
    "YOUR ANSWER\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LSTM_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
