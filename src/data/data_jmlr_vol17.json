{
"papers": [
    {
        "title": "On the Complexity of Best-Arm Identification in Multi-Armed Bandit Models",
        "abstract": "The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the  best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when  under general assumptions. In the specific case of two armed- bandits, we derive refined lower bounds in both the fixed- confidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed- budget setting may be smaller than the complexity of the fixed- confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest : a deviation lemma for self-normalized sums (Lemma 7) and a novel change of measure inequality for bandit models (Lemma 1).",
        "keywords": [
            "multi-armed bandit",
            "best-arm identification",
            "pure exploration",
            "information-     theoretic divergences",
            ""
        ],
        "author": [
            "Emilie Kaufmann",
            "Olivier Cappé",
            "Aurélien Garivier"
        ],
        "ref": "http://jmlr.org/papers/volume17/kaufman16a/kaufman16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Multiscale Dictionary Learning: Non-Asymptotic Bounds and Robustness",
        "abstract": "",
        "keywords": [
            "dictionary learning",
            "multi-resolution analysis",
            "manifold learning",
            "robustness",
            "sparsityc 2016 Mauro Maggioni",
            "Stanislav Minsker",
            ""
        ],
        "author": [
            "Mauro Maggioni",
            "Stanislav Minsker",
            "Nate Strawn"
        ],
        "ref": "http://jmlr.org/papers/volume17/maggioni16a/maggioni16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Consistent Algorithms for Clustering Time Series",
        "abstract": "The problem of clustering is considered for the case where every point is a time series. The time series are either given in one batch (offline setting), or they are allowed to grow with time and new time series can be added along the way (online setting). We propose a natural notion of consistency for this problem, and show that there are simple, computationally efficient algorithms that are asymptotically consistent under extremely weak assumptions on the distributions that generate the data. The notion of consistency is as follows. A clustering algorithm is called consistent if it places two time series into the same cluster if and only if the distribution that generates them is the same. In the considered framework the time series are allowed to be highly dependent, and the dependence can have arbitrary form. If the number of clusters is known, the only assumption we make is that the (marginal) distribution of each time series is stationary ergodic. No parametric, memory or mixing assumptions are made. When the number of clusters is unknown, stronger assumptions are provably necessary, but it is still possible to devise nonparametric algorithms that are consistent under very general conditions. The theoretical findings of this work are illustrated with experiments on both synthetic and real data.",
        "keywords": [
            "clustering",
            "time series",
            "ergodicity",
            ""
        ],
        "author": [
            "Azadeh Khaleghi",
            "Daniil Ryabko",
            "Jérémie Mary",
            "Philippe Preux"
        ],
        "ref": "http://jmlr.org/papers/volume17/khaleghi16a/khaleghi16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Random Rotation Ensembles",
        "abstract": "In machine learning, ensemble methods combine the predictions of multiple base learners to construct more accurate aggregate predictions. Established supervised learning algorithms inject randomness into the construction of the individual base learners in an effort to promote diversity within the resulting ensembles. An undesirable side effect of this approach is that it generally also reduces the accuracy of the base learners. In this paper, we introduce a method that is simple to implement yet general and effective in improving ensemble diversity with only modest impact on the accuracy of the individual base learners. By randomly rotating the feature space prior to inducing the base learners, we achieve favorable aggregate predictions on standard data sets compared to state of the art ensemble methods, most notably for tree-based ensembles, which are particularly sensitive to rotation.",
        "keywords": [
            "feature rotation",
            "ensemble diversity",
            ""
        ],
        "author": [
            "Rico Blaser",
            "Piotr Fryzlewicz"
        ],
        "ref": "http://jmlr.org/papers/volume17/blaser16a/blaser16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Should We Really Use Post-Hoc Tests Based on Mean-Ranks?",
        "abstract": "The statistical comparison of multiple algorithms over multiple data sets is fundamental in machine learning. This is typically carried out by the Friedman test. When the Friedman test rejects the null hypothesis, multiple comparisons are carried out to establish which are the significant differences among algorithms. The multiple comparisons are usually performed using the mean-ranks test. The aim of this technical note is to discuss the inconsistencies of the mean-ranks post-hoc test with the goal of discouraging its use in machine learning as well as in medicine, psychology, etc.. We show that the outcome of the mean-ranks test depends on the pool of algorithms originally included in the experiment. In other words, the outcome of the comparison between algorithms  and  depends also on the performance of the other algorithms included in the original experiment. This can lead to paradoxical situations. For instance the difference between  and  could be declared significant if the pool comprises algorithms  and not significant if the pool comprises algorithms . To overcome these issues, we suggest instead to perform the multiple comparison using a test whose outcome only depends on the two algorithms being compared, such as the sign-test or the Wilcoxon signed-rank test.",
        "keywords": [
            "statistical comparison",
            "Friedman test",
            ""
        ],
        "author": [
            "Alessio Benavoli",
            "Giorgio Corani",
            "Francesca Mangili"
        ],
        "ref": "http://jmlr.org/papers/volume17/benavoli16a/benavoli16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Minimax Rates in Permutation Estimation for Feature Matching",
        "abstract": "The problem of matching two sets of features appears in various tasks of computer vision and can be often formalized as a problem of permutation estimation. We address this problem from a statistical point of view and provide a theoretical analysis of the accuracy of several natural estimators. To this end, the minimax rate of separation is investigated and its expression is obtained as a function of the sample size, noise level and dimension of the features. We consider the cases of homoscedastic and heteroscedastic noise and establish, in each case, tight upper bounds on the separation distance of several estimators. These upper bounds are shown to be unimprovable both in the homoscedastic and heteroscedastic settings. Interestingly, these bounds demonstrate that a phase transition occurs when the dimension  of the features is of the order of the logarithm of the number of features . For , the rate is dimension free and equals , where  is the noise level. In contrast, when  is larger than  for some constant , the minimax rate increases with  and is of the order of . We also discuss the computational aspects of the estimators and provide empirical evidence of their consistency on synthetic data. Finally, we show that our results extend to more general matching criteria.",
        "keywords": [
            "permutation estimation",
            "minimax rate of separation",
            ""
        ],
        "author": [
            "Olivier Collier",
            "Arnak S. Dalalyan"
        ],
        "ref": "http://jmlr.org/papers/volume17/collier16a/collier16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics",
        "abstract": "Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data sets is computationally expensive. Both the calculation of the acceptance probability and the creation of informed proposals usually require an iteration through the whole data set. The recently proposed stochastic gradient Langevin dynamics (SGLD) method circumvents this problem by generating proposals which are only based on a subset of the data, by skipping the accept-reject step and by using decreasing step-sizes sequence . We provide in this article a rigorous mathematical framework for analysing this algorithm. We prove that, under verifiable assumptions, the algorithm is consistent, satisfies a central limit theorem (CLT) and its asymptotic bias-variance decomposition can be characterized by an explicit functional of the step-sizes sequence . We leverage this analysis to give practical recommendations for the notoriously difficult tuning of this algorithm: it is asymptotically optimal to use a step-size sequence of the type , leading to an algorithm whose mean squared error (MSE) decreases at rate .",
        "keywords": [
            "Markov chain Monte Carlo",
            "Langevin dynamics",
            ""
        ],
        "author": [
            "Yee Whye Teh",
            "Alexandre H. Thiery",
            "Sebastian J. Vollmer"
        ],
        "ref": "http://jmlr.org/papers/volume17/teh16a/teh16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Knowledge Matters: Importance of Prior Information for Optimization",
        "abstract": "We explored the effect of introducing prior knowledge into the intermediate level of deep supervised neural networks on two tasks. On a task we designed, all black-box state-of-the-art machine learning algorithms which we tested, failed to generalize well. We motivate our work from the hypothesis that, there is a training barrier involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals by using a form of supervision or guidance using a curriculum. Our results provide a positive evidence in favor of this hypothesis. In our experiments, we trained a two- tiered MLP architecture on a dataset for which each input image contains three sprites, and the binary target class is  if all of three shapes belong to the same category and otherwise the class is . In terms of generalization, black-box machine learning algorithms could not perform better than chance on this task. Standard deep supervised neural networks also failed to generalize. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allowed us to solve the task efficiently. We obtained much better than chance, but imperfect results by exploring different architectures and optimization variants. This observation might be an indication of optimization difficulty when the neural network trained without hints on this task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with the hypotheses on cultural learning inspired by the observations of training of neural networks sometimes getting stuck, even though good solutions exist, both in terms of training and generalization error.",
        "keywords": [
            "deep learning",
            "neural networks",
            "optimization",
            "evolution of culture",
            "curriculum     learning",
            ""
        ],
        "author": [
            "Çağlar Gülçehre",
            "Yoshua Bengio"
        ],
        "ref": "http://jmlr.org/papers/volume17/gulchere16a/gulchere16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Harry: A Tool for Measuring String Similarity",
        "abstract": "Comparing strings and assessing their similarity is a basic operation in many application domains of machine learning, such as in information retrieval, natural language processing and bioinformatics. The practitioner can choose from a large variety of available similarity measures for this task, each emphasizing different aspects of the string data. In this article, we present Harry, a small tool specifically designed for measuring the similarity of strings. Harry implements over 20 similarity measures, including common string distances and string kernels, such as the Levenshtein distance and the Subsequence kernel. The tool has been designed with efficiency in mind and allows for multi-threaded as well as distributed computing, enabling the analysis of large data sets of strings. Harry supports common data formats and thus can interface with analysis environments, such as Matlab, Pylab and Weka.",
        "keywords": [
            "string kernels",
            "string distances",
            ""
        ],
        "author": [
            "Konrad Rieck",
            "Christian Wressnegger"
        ],
        "ref": "http://jmlr.org/papers/volume17/rieck16a/rieck16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Herded Gibbs Sampling",
        "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an  convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.",
        "keywords": [
            "Gibbs sampling",
            "herding",
            ""
        ],
        "author": [
            "Yutian Chen",
            "Luke Bornn",
            "Nando de Freitas",
            "Mareija Eskelin",
            "Jing Fang",
            "Max Welling"
        ],
        "ref": "http://jmlr.org/papers/volume17/chen16a/chen16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Complexity of Representation and Inference in Compositional Models with Part Sharing",
        "abstract": "This paper performs a complexity analysis of a class of serial and parallel compositional models of multiple objects and shows that they enable efficient representation and rapid inference. Compositional models are generative and represent objects in a hierarchically distributed manner in terms of parts and subparts, which are constructed recursively by part-subpart compositions. Parts are represented more coarsely at higher level of the hierarchy, so that the upper levels give coarse summary descriptions (e.g., there is a horse in the image) while the lower levels represents the details (e.g., the positions of the legs of the horse). This hierarchically distributed representation obeys the executive summary principle, meaning that a high level executive only requires a coarse summary description and can, if necessary, get more details by consulting lower level executives. The parts and subparts are organized in terms of hierarchical dictionaries which enables part sharing between different objects allowing efficient representation of many objects. The first main contribution of this paper is to show that compositional models can be mapped onto a parallel visual architecture similar to that used by bio- inspired visual models such as deep convolutional networks but more explicit in terms of representation, hence enabling part detection as well as object detection, and suitable for complexity analysis. Inference algorithms can be run on this architecture to exploit the gains caused by part sharing and executive summary. Effectively, this compositional architecture enables us to perform exact inference simultaneously over a large class of generative models of objects. The second contribution is an analysis of the complexity of compositional models in terms of computation time (for serial computers) and numbers of nodes (e.g., \"neurons\") for parallel computers. In particular, we compute the complexity gains by part sharing and executive summary and their dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level of the hierarchy, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can enable linear processing on parallel computers.",
        "keywords": [],
        "author": [
            "Alan Yuille",
            "Roozbeh Mottaghi"
        ],
        "ref": "http://jmlr.org/papers/volume17/yuille16a/yuille16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Noisy Sparse Subspace Clustering",
        "abstract": "This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabeled input data points, which are assumed to be in a union of low-dimensional subspaces. We show that a modified version of SSC is provably effective in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to more practical settings and provides justification to the success of SSC in a class of real applications.",
        "keywords": [
            "Subspace clustering",
            "robustness",
            "stability",
            "compressive sensing",
            ""
        ],
        "author": [
            "Yu-Xiang Wang",
            "Huan Xu"
        ],
        "ref": "http://jmlr.org/papers/volume17/13-354/13-354.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Learning the Variance of the Reward-To-Go",
        "abstract": "In Markov decision processes (MDPs), the variance of the reward- to-go is a natural measure of uncertainty about the long term performance of a policy, and is important in domains such as finance, resource allocation, and process control. Currently however, there is no tractable procedure for calculating it in large scale MDPs. This is in contrast to the case of the expected reward-to-go, also known as the value function, for which effective simulation-based algorithms are known, and have been used successfully in various domains. In this paper we extend temporal difference (TD) learning algorithms to estimating the variance of the reward-to- go for a fixed policy. We propose variants of both TD(0) and LSTD() with linear function approximation, prove their convergence, and demonstrate their utility in an option pricing problem. Our results show a dramatic improvement in terms of sample efficiency over standard Monte-Carlo methods, which are currently the state-of-the-art.",
        "keywords": [
            "Reinforcement learning",
            "Markov decision processes",
            "variance estimation",
            "simulation",
            ""
        ],
        "author": [
            "Aviv Tamar",
            "Dotan Di Castro",
            "Shie Mannor"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-335/14-335.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Convex Calibration Dimension for Multiclass Loss Matrices",
        "abstract": "We study consistency properties of surrogate loss functions for general multiclass learning problems, defined by a general multiclass loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be calibrated with respect to a loss matrix in this setting. We then introduce the notion of convex calibration dimension of a multiclass loss matrix, which measures the smallest \"size\" of a prediction space in which it is possible to design a convex surrogate that is calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, we apply our framework to study various subset ranking losses, and use the convex calibration dimension as a tool to show both the existence and non-existence of various types of convex calibrated surrogates for these losses. Our results strengthen recent results of Duchi et al. (2010) and CalauzÃ¨nes et al. (2012) on the non-existence of certain types of convex calibrated surrogates in subset ranking. We anticipate the convex calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems.",
        "keywords": [
            "Statistical consistency",
            "multiclass loss",
            "loss matrix",
            "surrogate loss",
            "convex     surrogates",
            "calibrated surrogates",
            "classification calibration",
            ""
        ],
        "author": [
            "Harish G. Ramaswamy",
            "Shivani Agarwal"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-316/14-316.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "LLORMA: Local Low-Rank Matrix Approximation",
        "abstract": "Matrix approximation is a common tool in recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is low-rank. In this paper, we propose, analyze, and experiment with two procedures, one parallel and the other global, for constructing local matrix approximations. The two approaches approximate the observed matrix as a weighted sum of low-rank matrices. These matrices are limited to a local region of the observed matrix. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks.",
        "keywords": [
            "Matrix approximation",
            "non-parametric methods",
            "kernel smoothing",
            "collabo-     rative Filtering",
            ""
        ],
        "author": [
            "Joonseok Lee",
            "Seungyeon Kim",
            "Guy Lebanon",
            "Yoram Singer",
            "Samy Bengio"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-301/14-301.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Consistent Information Criterion for Support Vector Machines in Diverging Model Spaces",
        "abstract": "Information criteria have been popularly used in model selection and proved to possess nice theoretical properties. For classification, Claeskens et al. (2880) proposed support vector machine information criterion for feature selection and provided encouraging numerical evidence. Yet no theoretical justification was given there. This work aims to fill the gap and to provide some theoretical justifications for support vector machine information criterion in both fixed and diverging model spaces. We first derive a uniform convergence rate for the support vector machine solution and then show that a modification of the support vector machine information criterion achieves model selection consistency even when the number of features diverges at an exponential rate of the sample size. This consistency result can be further applied to selecting the optimal tuning parameter for various penalized support vector machine methods. Finite-sample performance of the proposed information criterion is investigated using Monte Carlo studies and one real-world gene selection problem.",
        "keywords": [
            "Bayesian Information Criterion",
            "Diverging Model Spaces",
            "Feature Selection",
            ""
        ],
        "author": [
            "Xiang Zhang",
            "Yichao Wu",
            "Lan Wang",
            "Runze Li"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-231/14-231.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Extremal Mechanisms for Local Differential Privacy",
        "abstract": "Local differential privacy has recently surfaced as a strong measure of privacy in contexts where personal information remains private even from data analysts. Working in a setting where both the data providers and data analysts want to maximize the utility of statistical analyses performed on the released data, we study the fundamental trade-off between local differential privacy and utility. This trade-off is formulated as a constrained optimization problem: maximize utility subject to local differential privacy constraints. We introduce a combinatorial family of extremal privatization mechanisms, which we call staircase mechanisms, and show that it contains the optimal privatization mechanisms for a broad class of information theoretic utilities such as mutual information and -divergences. We further prove that for any utility function and any privacy level, solving the privacy-utility maximization problem is equivalent to solving a finite-dimensional linear program, the outcome of which is the optimal staircase mechanism. However, solving this linear program can be computationally expensive since it has a number of variables that is exponential in the size of the alphabet the data lives in. To account for this, we show that two simple privatization mechanisms, the binary and randomized response mechanisms, are universally optimal in the low and high privacy regimes, and well approximate the intermediate regime.",
        "keywords": [
            "local differential privacy",
            "privacy-preserving machine learning algorithms",
            "information theoretic utilities",
            "f -divergences",
            "mutual information",
            "statistical inference",
            "hy-     pothesis testing",
            ""
        ],
        "author": [
            "Peter Kairouz",
            "Sewoong Oh",
            "Pramod Viswanath"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-135/15-135.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Loss Minimization and Parameter Estimation with Heavy Tails",
        "abstract": "This work studies applications and generalizations of a simple estimation technique that provides exponential concentration under heavy-tailed distributions, assuming only bounded low- order moments. We show that the technique can be used for approximate minimization of smooth and strongly convex losses, and specifically for least squares linear regression. For instance, our -dimensional estimator requires just  random samples to obtain a constant factor approximation to the optimal least squares loss with probability , without requiring the covariates or noise to be bounded or subgaussian. We provide further applications to sparse linear regression and low-rank covariance matrix estimation with similar allowances on the noise and covariate distributions. The core technique is a generalization of the median-of-means estimator to arbitrary metric spaces.",
        "keywords": [
            "Heavy-tailed distributions",
            "unbounded losses",
            "linear regression",
            ""
        ],
        "author": [
            "Daniel Hsu",
            "Sivan Sabato"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-273/14-273.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Analysis of Classification-based Policy Iteration Algorithms",
        "abstract": "We introduce a variant of the classification-based approach to policy iteration which uses a cost-sensitive loss function weighting each classification mistake by its actual regret, that is, the difference between the action- value of the greedy action and of the action chosen by the classifier. For this algorithm, we provide a full finite-sample analysis. Our results state a performance bound in terms of the number of policy improvement steps, the number of rollouts used in each iteration, the capacity of the considered policy space (classifier), and a capacity measure which indicates how well the policy space can approximate policies that are greedy with respect to any of its members. The analysis reveals a tradeoff between the estimation and approximation errors in this classification-based policy iteration setting. Furthermore it confirms the intuition that classification-based policy iteration algorithms could be favorably compared to value-based approaches when the policies can be approximated more easily than their corresponding value functions. We also study the consistency of the algorithm when there exists a sequence of policy spaces with increasing capacity.",
        "keywords": [
            "reinforcement learning",
            "policy iteration",
            "classification-based approach to      policy iteration",
            ""
        ],
        "author": [
            "Alessandro Lazaric",
            "Mohammad Ghavamzadeh",
            "R{\\'e}mi Munos"
        ],
        "ref": "http://jmlr.org/papers/volume17/10-364/10-364.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Operator-valued Kernels for Learning from Functional Response Data",
        "abstract": "In this paper (This is a combined and expanded version of previous conference papers Kadri et al., 2010, 2011c) we consider the problems of supervised classification and regression in the case where attributes and labels are functions: a data is represented by a set of functions, and the label is also a function. We focus on the use of reproducing kernel Hilbert space theory to learn from such functional data. Basic concepts and properties of kernel-based learning are extended to include the estimation of function-valued functions. In this setting, the representer theorem is restated, a set of rigorously defined infinite-dimensional operator-valued kernels that can be valuably applied when the data are functions is described, and a learning algorithm for nonlinear functional data analysis is introduced. The methodology is illustrated through speech and audio signal processing experiments.",
        "keywords": [
            "nonlinear functional data analysis",
            "operator-valued kernels",
            "function-valued     reproducing kernel Hilbert spaces",
            ""
        ],
        "author": [
            "Hachem Kadri",
            "Emmanuel Duflos",
            "Philippe Preux",
            "Stéphane Canu",
            "Alain Rakotomamonjy",
            "Julien Audiffren"
        ],
        "ref": "http://jmlr.org/papers/volume17/11-315/11-315.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "MEKA: A Multi-label/Multi-target Extension to WEKA",
        "abstract": "Multi-label classification has rapidly attracted interest in the machine learning literature, and there are now a large number and considerable variety of methods for this type of learning. We present MEKA: an open-source Java framework based on the well-known WEKA library. MEKA provides interfaces to facilitate practical application, and a wealth of multi-label classifiers, evaluation metrics, and tools for multi-label experiments and development. It supports multi-label and multi-target data, including in incremental and semi- supervised contexts.",
        "keywords": [
            "classification",
            "learning",
            "multi-label",
            "multi-target",
            ""
        ],
        "author": [
            "Jesse Read",
            "Peter Reutemann",
            "Bernhard Pfahringer",
            "Geoff Holmes"
        ],
        "ref": "http://jmlr.org/papers/volume17/12-164/12-164.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Gradients Weights improve Regression and Classification",
        "abstract": "In regression problems over , the unknown function  often varies more in some coordinates than in others. We show that weighting each coordinate  according to an estimate of the variation of  along coordinate  -- e.g. the  norm of the th-directional derivative of  -- is an efficient way to significantly improve the performance of distance-based regressors such as kernel and -NN regressors. The approach, termed Gradient Weighting (GW), consists of a first pass regression estimate  which serves to evaluate the directional derivatives of , and a second-pass regression estimate on the re-weighted data. The GW approach can be instantiated for both regression and classification, and is grounded in strong theoretical principles having to do with the way regression bias and variance are affected by a generic feature-weighting scheme. These theoretical principles provide further technical foundation for some existing feature-weighting heuristics that have proved successful in practice. We propose a simple estimator of these derivative norms and prove its consistency. The proposed estimator computes efficiently and easily extends to run online. We then derive a classification version of the GW approach which evaluates on real-worlds datasets with as much success as its regression counterpart.",
        "keywords": [
            "Nonparametric learning",
            "feature selection",
            "feature weighting",
            "nonparametric sparsity",
            ""
        ],
        "author": [
            "Samory Kpotufe",
            "Abdeslam Boularias",
            "Thomas Schultz",
            "Kyoungok Kim"
        ],
        "ref": "http://jmlr.org/papers/volume17/13-351/13-351.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Closer Look at Adaptive Regret",
        "abstract": "For the prediction with expert advice setting, we consider methods to construct algorithms that have low adaptive regret. The adaptive regret of an algorithm on a time interval  is the loss of the algorithm minus the loss of the best expert over that interval. Adaptive regret measures how well the algorithm approximates the best expert locally, and so is different from, although closely related to, both the classical regret, measured over an initial time interval , and the tracking regret, where the algorithm is compared to a good sequence of experts over . We investigate two existing intuitive methods for deriving algorithms with low adaptive regret, one based on specialist experts and the other based on restarts. Quite surprisingly, we show that both methods lead to the same algorithm, namely Fixed Share, which is known for its tracking regret. We provide a thorough analysis of the adaptive regret of Fixed Share. We obtain the exact worst-case adaptive regret for Fixed Share, from which the classical tracking bounds follow. We prove that Fixed Share is optimal for adaptive regret: the worst-case adaptive regret of any algorithm is at least that of an instance of Fixed Share.",
        "keywords": [
            "online learning",
            "adaptive regret",
            "Fixed Share",
            ""
        ],
        "author": [
            "Dmitry Adamskiy",
            "Wouter M. Koolen",
            "Alexey Chernov",
            "Vladimir Vovk"
        ],
        "ref": "http://jmlr.org/papers/volume17/13-533/13-533.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Learning Using Anti-Training with Sacrificial Data",
        "abstract": "Traditionally the machine-learning community has viewed the No Free Lunch (NFL) theorems for search and optimization as a limitation. We review, analyze, and unify the NFL theorem with the perspectives of \"blind\" search and meta-learning to arrive at necessary conditions for improving black-box optimization. We survey meta-learning literature to determine when and how meta- learning can benefit machine learning. Then, we generalize meta- learning in the context of the NFL theorems, to arrive at a novel technique called anti-training with sacrificial data (ATSD). Our technique applies at the meta level to arrive at domain specific algorithms. We also show how to generate sacrificial data. An extensive case study is presented along with simulated annealing results to demonstrate the efficacy of the ATSD method.",
        "keywords": [
            "Machine Learning",
            "Optimization",
            "Meta Optimization",
            "No Free Lunch",
            "Anti-                                     Training",
            ""
        ],
        "author": [
            "Michael L. Valenzuela",
            "Jerzy W. Rozenblit"
        ],
        "ref": "http://jmlr.org/papers/volume17/13-589/13-589.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Unifying Framework in Vector-valued Reproducing Kernel Hilbert Spaces for Manifold Regularization and Co-Regularized Multi-view Learning",
        "abstract": "This paper presents a general vector-valued reproducing kernel Hilbert spaces (RKHS) framework for the problem of learning an unknown functional dependency between a structured input space and a structured output space. Our formulation encompasses both Vector-valued Manifold Regularization and Co-regularized Multi- view Learning, providing in particular a unifying framework linking these two important learning approaches. In the case of the least square loss function, we provide a closed form solution, which is obtained by solving a system of linear equations. In the case of Support Vector Machine (SVM) classification, our formulation generalizes in particular both the binary Laplacian SVM to the multi-class, multi-view settings and the multi-class Simplex Cone SVM to the semi-supervised, multi-view settings. The solution is obtained by solving a single quadratic optimization problem, as in standard SVM, via the Sequential Minimal Optimization (SMO) approach. Empirical results obtained on the task of object recognition, using several challenging data sets, demonstrate the competitiveness of our algorithms compared with other state-of-the-art methods.",
        "keywords": [
            "kernel methods",
            "vector-valued RKHS",
            "multi-view learning",
            "multi-modality     learning",
            "multi-kernel learning",
            "manifold regularization",
            ""
        ],
        "author": [
            "Hà Quang Minh",
            "Loris Bazzani",
            "Vittorio Murino"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-036/14-036.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Quantifying Uncertainty in Random Forests via Confidence Intervals and Hypothesis Tests",
        "abstract": "This work develops formal statistical inference procedures for predictions generated by supervised learning ensembles. Ensemble methods based on bootstrapping, such as bagging and random forests, have improved the predictive accuracy of individual trees, but fail to provide a framework in which distributional results can be easily determined. Instead of aggregating full bootstrap samples, we consider predicting by averaging over trees built on subsamples of the training set and demonstrate that the resulting estimator takes the form of a U-statistic. As such, predictions for individual feature vectors are asymptotically normal, allowing for confidence intervals to accompany predictions. In practice, a subset of subsamples is used for computational speed; here our estimators take the form of incomplete U-statistics and equivalent results are derived. We further demonstrate that this setup provides a framework for testing the significance of features. Moreover, the internal estimation method we develop allows us to estimate the variance parameters and perform these inference procedures at no additional computational cost. Simulations and illustrations on a real data set are provided.",
        "keywords": [
            "trees",
            "u-statistics",
            "bagging",
            "subbagging",
            ""
        ],
        "author": [
            "Lucas Mentch",
            "Giles Hooker"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-168/14-168.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Statistical-Computational Tradeoffs in Planted Problems and Submatrix Localization with a Growing Number of Clusters and Submatrices",
        "abstract": "",
        "keywords": [
            "planted partition",
            "planted clique",
            "planted coloring",
            "submatrix localization",
            "graph clus-     tering",
            "bi-clustering",
            "minimax recovery",
            "computational hardness",
            ""
        ],
        "author": [
            "Yudong Chen",
            "Jiaming Xu"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-330/14-330.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Non-linear Causal Inference using Gaussianity Measures",
        "abstract": "We provide theoretical and empirical evidence for a type of asymmetry between causes and effects that is present when these are related via linear models contaminated with additive non- Gaussian noise. Assuming that the causes and the effects have the same distribution, we show that the distribution of the residuals of a linear fit in the anti-causal direction is closer to a Gaussian than the distribution of the residuals in the causal direction. This Gaussianization effect is characterized by reduction of the magnitude of the high-order cumulants and by an increment of the differential entropy of the residuals. The problem of non-linear causal inference is addressed by performing an embedding in an expanded feature space, in which the relation between causes and effects can be assumed to be linear. The effectiveness of a method to discriminate between causes and effects based on this type of asymmetry is illustrated in a variety of experiments using different measures of Gaussianity. The proposed method is shown to be competitive with state-of-the-art techniques for causal inference.",
        "keywords": [
            "causal inference",
            "Gaussianity of the residuals",
            ""
        ],
        "author": [
            "Daniel Hern{\\'a}ndez-Lobato",
            "Pablo Morales-Mombiela",
            "David Lopez-Paz",
            "Alberto Su{\\'a}rez"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-375/14-375.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Consistent Distribution-Free $K$-Sample and Independence Tests for Univariate Random Variables",
        "abstract": "A popular approach for testing if two univariate random variables are statistically independent consists of partitioning the sample space into bins, and evaluating a test statistic on the binned data. The partition size matters, and the optimal partition size is data dependent. While for detecting simple relationships coarse partitions may be best, for detecting complex relationships a great gain in power can be achieved by considering finer partitions. We suggest novel consistent distribution-free tests that are based on summation or maximization aggregation of scores over all partitions of a fixed size. We show that our test statistics based on summation can serve as good estimators of the mutual information. Moreover, we suggest regularized tests that aggregate over all partition sizes, and prove those are consistent too. We provide polynomial-time algorithms, which are critical for computing the suggested test statistics efficiently. We show that the power of the regularized tests is excellent compared to existing tests, and almost as powerful as the tests based on the optimal (yet unknown in practice) partition size, in simulations as well as on a real data example.",
        "keywords": [
            "bivariate distribution",
            "nonparametric test",
            "statistical independence",
            "mutual informa-     tion",
            "two-sample test",
            ""
        ],
        "author": [
            "Ruth Heller",
            "Yair Heller",
            "Shachar Kaufman",
            "Barak Brill",
            "Malka Gorfine"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-441/14-441.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Gibbs Sampler for Learning DAGs",
        "abstract": "We propose a Gibbs sampler for structure learning in directed acyclic graph (DAG) models. The standard Markov chain Monte Carlo algorithms used for learning DAGs are random-walk Metropolis-Hastings samplers. These samplers are guaranteed to converge asymptotically but often mix slowly when exploring the large graph spaces that arise in structure learning. In each step, the sampler we propose draws entire sets of parents for multiple nodes from the appropriate conditional distribution. This provides an efficient way to make large moves in graph space, permitting faster mixing whilst retaining asymptotic guarantees of convergence. The conditional distribution is related to variable selection with candidate parents playing the role of covariates or inputs. We empirically examine the performance of the sampler using several simulated and real data examples. The proposed method gives robust results in diverse settings, outperforming several existing Bayesian and frequentist methods. In addition, our empirical results shed some light on the relative merits of Bayesian and constraint- based methods for structure learning.",
        "keywords": [
            "structure learning",
            "DAGs",
            "Bayesian networks",
            "Gibbs sampling",
            "Markov chain     Monte Carlo",
            ""
        ],
        "author": [
            "Robert J. B. Goudie",
            "Sach Mukherjee"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-486/14-486.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning",
        "abstract": "Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution . These methods rely on a singular value decomposition of a matrix , called the empirical Hankel matrix, that records the frequencies of (some of) the observed strings . The accuracy of the learned distribution depends both on the quantity of information embedded in  and on the distance between  and its mean . Existing concentration bounds seem to indicate that the concentration over  gets looser with its dimensions, suggesting that it might be necessary to bound the dimensions of  for learning. We prove new dimension-free concentration bounds for classical Hankel matrices and several variants, based on prefixes or factors of strings, that are useful for learning. Experiments demonstrate that these bounds are tight and that they significantly improve existing (dimension-dependent) bounds. One consequence of these results is that the spectral learning approach remains consistent even if all the observations are recorded within the empirical matrix.",
        "keywords": [
            "Hankel matrices",
            "Matrix Bernstein bounds",
            "Probabilistic Grammatical In-     ference",
            "Rational series",
            ""
        ],
        "author": [
            "François Denis",
            "Mattias Gybels",
            "Amaury Habrard"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-501/14-501.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Distinguishing Cause from Effect Using Observational Data: Methods and Benchmarks",
        "abstract": "The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether  causes  or, alternatively,  causes , given joint observations of two variables . An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of no confounding, no feedback loops, and no selection bias, such bivariate causal discovery problems are challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: methods based on Additive Noise Models (ANMs) and Information Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs that consists of data for 100 different cause-effect pairs selected from 37 data sets from various domains (e.g., meteorology, biology, medicine, engineering, economy, etc.) and motivate our decisions regarding the ground truth causal directions of all pairs. We evaluate the performance of several bivariate causal discovery methods on these real-world benchmark data and in addition on artificially simulated data. Our empirical results on real-world data indicate that certain methods are indeed able to distinguish cause from effect using only purely observational data, although more benchmark data would be needed to obtain statistically significant conclusions. One of the best performing methods overall is the method based on Additive Noise Models that has originally been proposed by Hoyer et al. (2009), which obtains an accuracy of 63  10 % and an AUC of 0.74  0.05 on the real-world benchmark. As the main theoretical contribution of this work we prove the consistency of that method.",
        "keywords": [],
        "author": [
            "Joris M. Mooij",
            "Jonas Peters",
            "Dominik Janzing",
            "Jakob Zscheischler",
            "Bernhard Schölkopf"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-518/14-518.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Multi-task Sparse Structure Learning with Gaussian Copula Models",
        "abstract": "Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously. While sometimes the underlying task relationship structure is known, often the structure needs to be estimated from data at hand. In this paper, we present a novel family of models for MTL, applicable to regression and classification problems, capable of learning the structure of tasks relationship. In particular, we consider a joint estimation problem of the tasks relationship structure and the individual task parameters, which is solved using alternating minimization. The task relationship revealed by structure learning is founded on recent advances in Gaussian graphical models endowed with sparse estimators of the precision (inverse covariance) matrix. An extension to include flexible Gaussian copula models that relaxes the Gaussian marginal assumption is also proposed. We illustrate the effectiveness of the proposed model on a variety of synthetic and benchmark data sets for regression and classification. We also consider the problem of combining Earth System Model (ESM) outputs for better projections of future climate, with focus on projections of temperature by combining ESMs in South and North America, and show that the proposed model outperforms several existing methods for the problem.",
        "keywords": [
            "multi-task learning",
            "structure learning",
            "Gaussian copula",
            "probabilistic graph-     ical model",
            ""
        ],
        "author": [
            "André R. Gonçalves",
            "Fernando J. Von Zuben",
            "Arindam Banerjee"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-215/15-215.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "MLlib: Machine Learning in Apache Spark",
        "abstract": "Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open- source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.",
        "keywords": [],
        "author": [
            "Xiangrui Meng",
            "Joseph Bradley",
            "Burak Yavuz",
            "Evan Sparks",
            "Shivaram Venkataraman",
            "Davies Liu",
            "Jeremy Freeman",
            "DB Tsai",
            "Manish Amde",
            "Sean Owen",
            "Doris Xin",
            "Reynold Xin",
            "Michael J. Franklin",
            "Reza Zadeh",
            "Matei Zaharia",
            "Ameet Talwalkar"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-237/15-237.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "OLPS: A Toolbox for On-Line Portfolio Selection",
        "abstract": "On-line portfolio selection is a practical financial engineering problem, which aims to sequentially allocate capital among a set of assets in order to maximize long-term return. In recent years, a variety of machine learning algorithms have been proposed to address this challenging problem, but no comprehensive open-source toolbox has been released for various reasons. This article presents the first open-source toolbox for \"On-Line Portfolio Selection\" (OLPS), which implements a collection of classical and state-of-the-art strategies powered by machine learning algorithms. We hope that OLPS can facilitate the development of new learning methods and enable the performance benchmarking and comparisons of different strategies. OLPS is an open-source project released under Apache License (version 2.0), which is available at github.com/OLPS/OLPS or OLPS.stevenhoi.org.",
        "keywords": [
            "On-line portfolio selection",
            "online learning",
            "trading system",
            ""
        ],
        "author": [
            "Bin Li",
            "Doyen Sahoo",
            "Steven C.H. Hoi"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-317/15-317.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Bounded p-norm Approximation of Max-Convolution for Sub-Quadratic Bayesian Inference on Additive Factors",
        "abstract": "Max-convolution is an important problem closely resembling standard convolution; as such, max-convolution occurs frequently across many fields. Here we extend the method with fastest known worst-case runtime, which can be applied to nonnegative vectors by numerically approximating the Chebyshev norm , and use this approach to derive two numerically stable methods based on the idea of computing -norms via fast convolution: The first method proposed, with runtime in  (which is less than  for any vectors that can be practically realized), uses the -norm as a direct approximation of the Chebyshev norm. The second approach proposed, with runtime in  (although in practice both perform similarly), uses a novel null space projection method, which extracts information from a sequence of -norms to estimate the maximum value in the vector (this is equivalent to querying a small number of moments from a distribution of bounded support in order to estimate the maximum). The -norm approaches are compared to one another and are shown to compute an approximation of the Viterbi path in a hidden Markov model where the transition matrix is a Toeplitz matrix; the runtime of approximating the Viterbi path is thus reduced from  steps to  steps in practice, and is demonstrated by inferring the U.S. unemployment rate from the S&P 500 stock index.",
        "keywords": [
            "Bayesian inference",
            "maximum a posteriori",
            "fast Fourier transform",
            "max-convolution",
            "p-norm",
            "Lp space",
            "hidden Markov model",
            "null space projection",
            ""
        ],
        "author": [
            "Julianus Pfeuffer",
            "Oliver Serang"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-319/15-319.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Learn Neural Networks",
        "abstract": "In this paper, we propose a novel model for high-dimensional data, called the Hybrid Orthogonal Projection and Estimation (HOPE) model, which combines a linear orthogonal projection and a finite mixture model under a unified generative modeling framework. The HOPE model itself can be learned unsupervised from unlabelled data based on the maximum likelihood estimation as well as discriminatively from labelled data. More interestingly, we have shown the proposed HOPE models are closely related to neural networks (NNs) in a sense that each hidden layer can be reformulated as a HOPE model. As a result, the HOPE framework can be used as a novel tool to probe why and how NNs work, more importantly, to learn NNs in either supervised or unsupervised ways. In this work, we have investigated the HOPE framework to learn NNs for several standard tasks, including image recognition on MNIST and speech recognition on TIMIT. Experimental results have shown that the HOPE framework yields significant performance gains over the current state-of-the-art methods in various types of NN learning problems, including unsupervised feature learning, supervised or semi-supervised learning.",
        "keywords": [
            "Orthogonal Projection",
            "PCA",
            "Mixture Models",
            "Neural Networks",
            "Unsuper-     vised Learning",
            ""
        ],
        "author": [
            "Shiliang Zhang",
            "Hui Jiang",
            "Lirong Dai"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-335/15-335.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "The Optimal Sample Complexity of PAC Learning",
        "abstract": "This work establishes a new upper bound on the number of samples sufficient for PAC learning in the realizable case. The bound matches known lower bounds up to numerical constant factors. This solves a long-standing open problem on the sample complexity of PAC learning. The technique and analysis build on a recent breakthrough by Hans Simon.",
        "keywords": [
            "sample complexity",
            "PAC learning",
            "statistical learning theory",
            "minimax anal-     ysis",
            ""
        ],
        "author": [
            "Steve Hanneke"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-389/15-389.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "End-to-End Training of Deep Visuomotor Policies",
        "abstract": "Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.",
        "keywords": [
            "Reinforcement Learning",
            "Optimal Control",
            "Vision",
            ""
        ],
        "author": [
            "Sergey Levine",
            "Chelsea Finn",
            "Trevor Darrell",
            "Pieter Abbeel"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-522/15-522.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "On Quantile Regression in Reproducing Kernel Hilbert Spaces with the Data Sparsity Constraint",
        "abstract": "For spline regressions, it is well known that the choice of knots is crucial for the performance of the estimator. As a general learning framework covering the smoothing splines, learning in a Reproducing Kernel Hilbert Space (RKHS) has a similar issue. However, the selection of training data points for kernel functions in the RKHS representation has not been carefully studied in the literature. In this paper we study quantile regression as an example of learning in a RKHS. In this case, the regular squared norm penalty does not perform training data selection. We propose a data sparsity constraint that imposes thresholding on the kernel function coefficients to achieve a sparse kernel function representation. We demonstrate that the proposed data sparsity method can have competitive prediction performance for certain situations, and have comparable performance in other cases compared to that of the traditional squared norm penalty. Therefore, the data sparsity method can serve as a competitive alternative to the squared norm penalty method. Some theoretical properties of our proposed method using the data sparsity constraint are obtained. Both simulated and real data sets are used to demonstrate the usefulness of our data sparsity constraint.",
        "keywords": [
            "kernel learning",
            "Rademacher complexity",
            "regression",
            "smoothing",
            ""
        ],
        "author": [
            "Chong Zhang",
            "Yufeng Liu",
            "Yichao Wu"
        ],
        "ref": "http://jmlr.org/papers/volume17/zhang16a/zhang16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "BayesPy: Variational Bayesian Inference in Python",
        "abstract": "BayesPy is an open-source Python software package for performing variational Bayesian inference. It is based on the variational message passing framework and supports conjugate exponential family models. By removing the tedious task of implementing the variational Bayesian update equations, the user can construct models faster and in a less error-prone way. Simple syntax, flexible model construction and efficient inference make BayesPy suitable for both average and expert Bayesian users. It also supports some advanced methods such as stochastic and collapsed variational inference.",
        "keywords": [
            "variational Bayes",
            "probabilistic programming",
            ""
        ],
        "author": [
            "Jaakko Luttinen"
        ],
        "ref": "http://jmlr.org/papers/volume17/luttinen16a/luttinen16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes",
        "abstract": "The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximised over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximising an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from i.i.d. observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the non-linear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain or partially missing inputs. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data.",
        "keywords": [
            "Gaussian processes",
            "variational inference",
            "latent variable models",
            "dynamical     systems",
            ""
        ],
        "author": [
            "Andreas C. Damianou",
            "Michalis K. Titsias",
            "Neil D. Lawrence"
        ],
        "ref": "http://jmlr.org/papers/volume17/damianou16a/damianou16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "On the Estimation of the Gradient Lines of a Density and the Consistency of the Mean-Shift Algorithm",
        "abstract": "We consider the problem of estimating the gradient lines of a density, which can be used to cluster points sampled from that density, for example via the mean-shift algorithm of Fukunaga and Hostetler (1975). We prove general convergence bounds that we then specialize to kernel density estimation.",
        "keywords": [
            "mean-shift",
            "gradient lines",
            "density estimation",
            ""
        ],
        "author": [
            "Ery Arias-Castro",
            "David Mason",
            "Bruno Pelletier"
        ],
        "ref": "http://jmlr.org/papers/volume17/ariascastro16a/ariascastro16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Scalable Learning of Bayesian Network Classifiers",
        "abstract": "Ever increasing data quantity makes ever more urgent the need for highly scalable learners that have good classification performance. Therefore, an out-of-core learner with excellent time and space complexity, along with high expressivity (that is, capacity to learn very complex multivariate probability distributions) is extremely desirable. This paper presents such a learner. We propose an extension to the -dependence Bayesian classifier (KDB) that discriminatively selects a sub- model of a full KDB classifier. It requires only one additional pass through the training data, making it a three-pass learner. Our extensive experimental evaluation on  large data sets reveals that this out-of-core algorithm achieves competitive classification performance, and substantially better training and classification time than state-of-the-art in-core learners such as random forest and linear and non-linear logistic regression.",
        "keywords": [
            "scalable Bayesian classification",
            "feature selection",
            "out-of-core learning",
            ""
        ],
        "author": [
            "Ana M. Martínez",
            "Geoffrey I. Webb",
            "Shenglei Chen",
            "Nayyar A. Zaidi"
        ],
        "ref": "http://jmlr.org/papers/volume17/martinez16a/martinez16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Unified View on Multi-class Support Vector Classification",
        "abstract": "",
        "keywords": [
            "support vector machines",
            "multi-class classification",
            ""
        ],
        "author": [
            "{\\\"U}rün Do\\u{g}an",
            "Tobias Glasmachers",
            "Christian Igel"
        ],
        "ref": "http://jmlr.org/papers/volume17/11-229/11-229.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Addressing Environment Non-Stationarity by Repeating Q-learning Updates",
        "abstract": "",
        "keywords": [
            "reinforcement learning",
            "Q-learning",
            "multi-agent learning",
            ""
        ],
        "author": [
            "Sherief Abdallah",
            "Michael Kaisers"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-037/14-037.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Large Scale Online Kernel Learning",
        "abstract": "In this paper, we present a new framework for large scale online kernel learning, making kernel methods efficient and scalable for large-scale online learning applications. Unlike the regular budget online kernel learning scheme that usually uses some budget maintenance strategies to bound the number of support vectors, our framework explores a completely different approach of kernel functional approximation techniques to make the subsequent online learning task efficient and scalable. Specifically, we present two different online kernel machine learning algorithms: (i) Fourier Online Gradient Descent (FOGD) algorithm that applies the random Fourier features for approximating kernel functions; and (ii) NystrÃ¶m Online Gradient Descent (NOGD) algorithm that applies the NystrÃ¶m method to approximate large kernel matrices. We explore these two approaches to tackle three online learning tasks: binary classification, multi-class classification, and regression. The encouraging results of our experiments on large-scale datasets validate the effectiveness and efficiency of the proposed algorithms, making them potentially more practical than the family of existing budget online kernel learning approaches.",
        "keywords": [
            "online learning",
            "kernel approximation",
            ""
        ],
        "author": [
            "Jing Lu",
            "Steven C.H. Hoi",
            "Jialei Wang",
            "Peilin Zhao",
            "Zhi-Yong Liu"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-148/14-148.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Kernel Mean Shrinkage Estimators",
        "abstract": "A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel mean, is central to kernel methods in that it is used by many classical algorithms such as kernel principal component analysis, and it also forms the core inference step of modern kernel methods that rely on embedding probability distributions in RKHSs. Given a finite sample, an empirical average has been used commonly as a standard estimator of the true kernel mean. Despite a widespread use of this estimator, we show that it can be improved thanks to the well-known Stein phenomenon. We propose a new family of estimators called kernel mean shrinkage estimators (KMSEs), which benefit from both theoretical justifications and good empirical performance. The results demonstrate that the proposed estimators outperform the standard one, especially in a \"large , small \" paradigm.",
        "keywords": [
            "covariance operator",
            "James-Stein estimators",
            "kernel methods",
            "kernel mean",
            "shrinkage estimators",
            "Stein effect",
            ""
        ],
        "author": [
            "Krikamol Mu",
            "et",
            "Bharath Sriperumbudur",
            "Kenji Fukumizu",
            "Arthur Gretton",
            "Bernhard Schölkopf"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-195/14-195.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "SPSD Matrix Approximation vis Column Selection: Theories, Algorithms, and Extensions",
        "abstract": "Symmetric positive semidefinite (SPSD) matrix approximation is an important problem with applications in kernel methods. However, existing SPSD matrix approximation methods such as the NystrÃ¶m method only have weak error bounds. In this paper we conduct in-depth studies of an SPSD matrix approximation model and establish strong relative-error bounds. We call it the prototype model for it has more efficient and effective extensions, and some of its extensions have high scalability. Though the prototype model itself is not suitable for large- scale data, it is still useful to study its properties, on which the analysis of its extensions relies. This paper offers novel theoretical analysis, efficient algorithms, and a highly accurate extension. First, we establish a lower error bound for the prototype model and improve the error bound of an existing column selection algorithm to match the lower bound. In this way, we obtain the first optimal column selection algorithm for the prototype model. We also prove that the prototype model is exact under certain conditions. Second, we develop a simple column selection algorithm with a provable error bound. Third, we propose a so-called spectral shifting model to make the approximation more accurate when the eigenvalues of the matrix decay slowly, and the improvement is theoretically quantified. The spectral shifting method can also be applied to improve other SPSD matrix approximation models.",
        "keywords": [
            "Matrix approximation",
            "matrix factorization",
            "kernel methods",
            ""
        ],
        "author": [
            "Shusen Wang",
            "Luo Luo",
            "Zhihua Zhang"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-199/14-199.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms",
        "abstract": "We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where subsets of base arms with unknown distributions form super arms. In each round, a super arm is played and the base arms contained in the super arm are played and their outcomes are observed. We further consider the extension in which more base arms could be probabilistically triggered based on the outcomes of already triggered arms. The reward of the super arm depends on the outcomes of all played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an offline -approximation oracle that takes the means of the outcome distributions of arms and outputs a super arm that with probability  generates an  fraction of the optimal expected reward. The objective of an online learning algorithm for CMAB is to minimize -approximation regret, which is the difference in total expected reward between the  fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves  distribution-dependent regret, where  is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound of UCB1 algorithm (up to a constant factor) for the classical MAB problem, and it significantly improves the regret bound in an earlier paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage (PMC) for online advertising and social influence maximization for viral marketing, both having nonlinear reward structures. In particular, application to social influence maximization requires our extension on probabilistically triggered arms.",
        "keywords": [],
        "author": [
            "Wei Chen",
            "Yajun Wang",
            "Yang Yuan",
            "Qinshi Wang"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-298/14-298.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Differentially Private Data Releasing for Smooth Queries",
        "abstract": "In the past few years, differential privacy has become a standard concept in the area of privacy. One of the most important problems in this field is to answer queries while preserving differential privacy. In spite of extensive studies, most existing work on differentially private query answering assumes the data are discrete (i.e., in ) and focuses on queries induced by \\emph{Boolean} functions. In real applications however, continuous data are at least as common as binary data. Thus, in this work we explore a less studied topic, namely, differential privately query answering for continuous data with continuous function. As a first step towards the continuous case, we study a natural class of linear queries on continuous data which we refer to as smooth queries. A linear query is said to be -smooth if it is specified by a function defined on  whose partial derivatives up to order  are all bounded. We develop two -differentially private mechanisms which are able to answer all smooth queries. The first mechanism outputs a summary of the database and can then give answers to the queries. The second mechanism is an improvement of the first one and it outputs a synthetic database. The two mechanisms both achieve an accuracy of . Here we assume that the dimension  is a constant. It turns out that even in this parameter setting (which is almost trivial in the discrete case), using existing discrete mechanisms to answer the smooth queries is difficult and requires more noise. Our mechanisms are based on -approximation of (transformed) smooth functions by low-degree even trigonometric polynomials with uniformly bounded coefficients. We also develop practically efficient variants of the mechanisms with promising experimental results.",
        "keywords": [],
        "author": [
            "Ziteng Wang",
            "Chi Jin",
            "Kai Fan",
            "Jiaqi Zhang",
            "Junliang Huang",
            "Yiqiao Zhong",
            "Liwei Wang"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-388/14-388.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Subspace Learning with Partial Information",
        "abstract": "The goal of subspace learning is to find a -dimensional subspace of , such that the expected squared distance between instance vectors and the subspace is as small as possible. In this paper we study subspace learning in a partial information setting, in which the learner can only observe  attributes from each instance vector. We propose several efficient algorithms for this task, and analyze their sample complexity.",
        "keywords": [
            "principal components analysis",
            "budgeted learning",
            "statistical learning",
            "learning with     partial information",
            ""
        ],
        "author": [
            "Alon Gonen",
            "Dan Rosenbaum",
            "Yonina C. Eldar",
            "Shai Shalev-Shwartz"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-443/14-443.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Iterative Hessian Sketch: Fast and Accurate Solution Approximation for Constrained Least-Squares",
        "abstract": "We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including -regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment.",
        "keywords": [
            "Convex optimization",
            "Random Projection",
            "Lasso",
            "Low-rank Approximation",
            ""
        ],
        "author": [
            "Mert Pilanci",
            "Martin J. Wainwright"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-460/14-460.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Estimating Causal Structure Using Conditional DAG Models",
        "abstract": "This paper considers inference of causal structure in a class of graphical models called conditional DAGs. These are directed acyclic graph (DAG) models with two kinds of variables, primary and secondary. The secondary variables are used to aid in the estimation of the structure of causal relationships between the primary variables. We prove that, under certain assumptions, such causal structure is identifiable from the joint observational distribution of the primary and secondary variables. We give causal semantics for the model class, put forward a score-based approach for estimation and establish consistency results. Empirical results demonstrate gains compared with formulations that treat all variables on an equal footing, or that ignore secondary variables. The methodology is motivated by applications in biology that involve multiple data types and is illustrated here using simulated data and in an analysis of molecular data from the Cancer Genome Atlas.",
        "keywords": [
            "Graphical Models",
            "Causal Inference",
            "Directed Acyclic Graphs",
            "Instrumental     Variables",
            ""
        ],
        "author": [
            "Chris. J. Oates",
            "Jim Q. Smith",
            "Sach Mukherjee"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-479/14-479.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Adaptive Lasso and group-Lasso for functional Poisson regression",
        "abstract": "High dimensional Poisson regression has become a standard framework for the analysis of massive counts datasets. In this work we estimate the intensity function of the Poisson regression model by using a dictionary approach, which generalizes the classical basis approach, combined with a Lasso or a group-Lasso procedure. Selection depends on penalty weights that need to be calibrated. Standard methodologies developed in the Gaussian framework can not be directly applied to Poisson models due to heteroscedasticity. Here we provide data-driven weights for the Lasso and the group-Lasso derived from concentration inequalities adapted to the Poisson case. We show that the associated Lasso and group-Lasso procedures satisfy fast and slow oracle inequalities. Simulations are used to assess the empirical performance of our procedure, and an original application to the analysis of Next Generation Sequencing data is provided.",
        "keywords": [
            "Functional Poisson regression",
            "adaptive lasso",
            "adaptive group-lasso",
            "calibra-     tion",
            "concentrationIntroductionPoisson functional regression has become a standard framework for image or spectra anal-ysis",
            ""
        ],
        "author": [
            "Stéphane Ivanoff",
            "Franck Picard",
            "Vincent Rivoirard"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-021/15-021.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Causal Inference through a Witness Protection Program",
        "abstract": "One of the most fundamental problems in causal inference is the estimation of a causal effect when treatment and outcome are confounded. This is difficult in an observational study, because one has no direct evidence that all confounders have been adjusted for. We introduce a novel approach for estimating causal effects that exploits observational conditional independencies to suggest âweakâ paths in an unknown causal graph. The widely used faithfulness condition of Spirtes et al. is relaxed to allow for varying degrees of âpath cancellationsâ that imply conditional independencies but do not rule out the existence of confounding causal paths. The output is a posterior distribution over bounds on the average causal effect via a linear programming approach and Bayesian inference. We claim this approach should be used in regular practice as a complement to other tools in observational studies.",
        "keywords": [
            "Causal inference",
            "instrumental variables",
            "Bayesian inference",
            ""
        ],
        "author": [
            "Ricardo Silva",
            "Robin Evans"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-130/15-130.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Structure Discovery in Bayesian Networks by Sampling Partial Orders",
        "abstract": "We present methods based on Metropolis-coupled Markov chain Monte Carlo (MC3) and annealed importance sampling (AIS) for estimating the posterior distribution of Bayesian networks. The methods draw samples from an appropriate distribution of partial orders on the nodes, continued by sampling directed acyclic graphs (DAGs) conditionally on the sampled partial orders. We show that the computations needed for the sampling algorithms are feasible as long as the encountered partial orders have relatively few down-sets. While the algorithms assume suitable modularity properties of the priors, arbitrary priors can be handled by dividing the importance weight of each sampled DAG by the number of topological sorts it has---we give a practical dynamic programming algorithm to compute these numbers. Our empirical results demonstrate that the presented partial-order- based samplers are superior to previous Markov chain Monte Carlo methods, which sample DAGs either directly or via linear orders on the nodes. The results also suggest that the convergence rate of the estimators based on AIS are competitive to those of MC3. Thus AIS is the preferred method, as it enables easier large- scale parallelization and, in addition, supplies good probabilistic lower bound guarantees for the marginal likelihood of the model.",
        "keywords": [
            "annealed importance sampling",
            "directed acyclic graph",
            "fast zeta transform",
            "linear extension",
            ""
        ],
        "author": [
            "Teppo Niinim\\\"{a}ki",
            "Pekka Parviainen",
            "Mikko Koivisto"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-140/15-140.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence",
        "abstract": "Data in the form of pairwise comparisons arises in many domains, including preference elicitation, sporting competitions, and peer grading among others. We consider parametric ordinal models for such pairwise comparison data involving a latent vector  that represents the âqualitiesâ of the  items being compared; this class of models includes the two most widely used parametric models---the Bradley-Terry-Luce (BTL) and the Thurstone models. Working within a standard minimax framework, we provide tight upper and lower bounds on the optimal error in estimating the quality score vector  under this class of models. The bounds depend on the topology of the comparison graph induced by the subset of pairs being compared, via the spectrum of the Laplacian of the comparison graph. Thus, in settings where the subset of pairs may be chosen, our results provide principled guidelines for making this choice. Finally, we compare these error rates to those under cardinal measurement models and show that the error rates in the ordinal and cardinal settings have identical scalings apart from constant pre- factors.",
        "keywords": [],
        "author": [
            "Nihar B. Shah",
            "Sivaraman Balakrishnan",
            "Joseph Bradley",
            "Abhay Parekh",
            "Kannan Ramch",
            "ran",
            "Martin J. Wainwright"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-189/15-189.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Domain-Adversarial Training of Neural Networks",
        "abstract": "",
        "keywords": [
            "domain adaptation",
            "neural network",
            "representation learning",
            "deep learning",
            "synthetic data",
            "image classification",
            "sentiment analysis",
            "person re-identificationc 2016 Yaroslav Ganin",
            "Evgeniya Ustinova",
            "Hana Ajakan",
            "Pascal Germain",
            "Hugo Larochelle",
            ""
        ],
        "author": [
            "Yaroslav Ganin",
            "Evgeniya Ustinova",
            "Hana Ajakan",
            "Pascal Germain",
            "Hugo Larochelle",
            "François Laviolette",
            "Mario March",
            "Victor Lempitsky"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-239/15-239.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Probabilistic Low-Rank Matrix Completion from Quantized Measurements",
        "abstract": "We consider the recovery of a low rank real-valued matrix  given a subset of noisy discrete (or quantized) measurements. Such problems arise in several applications such as collaborative filtering, learning and content analytics, and sensor network localization. We consider constrained maximum likelihood estimation of , under a constraint on the entry- wise infinity-norm of  and an exact rank constraint. We provide upper bounds on the Frobenius norm of matrix estimation error under this model. Previous theoretical investigations have focused on binary (1-bit) quantizers, and been based on convex relaxation of the rank. Compared to the existing binary results, our performance upper bound has faster convergence rate with matrix dimensions when the fraction of revealed observations is fixed. We also propose a globally convergent optimization algorithm based on low rank factorization of  and validate the method on synthetic and real data, with improved performance over previous methods.",
        "keywords": [
            "constrained maximum likelihood",
            "quantization",
            "matrix completion",
            "collabo-     rative filtering",
            ""
        ],
        "author": [
            "Sonia A. Bhaskar"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-273/15-273.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "DSA: Decentralized Double Stochastic Averaging Gradient Algorithm",
        "abstract": "This paper considers optimization problems where nodes of a network have access to summands of a global objective. Each of these local objectives is further assumed to be an average of a finite set of functions. The motivation for this setup is to solve large scale machine learning problems where elements of the training set are distributed to multiple computational elements. The decentralized double stochastic averaging gradient (DSA) algorithm is proposed as a solution alternative that relies on: (i) The use of local stochastic averaging gradients. (ii) Determination of descent steps as differences of consecutive stochastic averaging gradients. Strong convexity of local functions and Lipschitz continuity of local gradients is shown to guarantee linear convergence of the sequence generated by DSA in expectation. Local iterates are further shown to approach the optimal argument for almost all realizations. The expected linear convergence of DSA is in contrast to the sublinear rate characteristic of existing methods for decentralized stochastic optimization. Numerical experiments on a logistic regression problem illustrate reductions in convergence time and number of feature vectors processed until convergence relative to these other alternatives.",
        "keywords": [
            "decentralized optimization",
            "stochastic optimization",
            "stochastic averaging gradient",
            "linear convergence",
            "large-scale optimization",
            ""
        ],
        "author": [
            "Aryan Mokhtari",
            "Alejandro Ribeiro"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-292/15-292.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "The Statistical Performance of Collaborative Inference",
        "abstract": "The statistical analysis of massive and complex data sets will require the development of algorithms that depend on distributed computing and collaborative inference. Inspired by this, we propose a collaborative framework that aims to estimate the unknown mean  of a random variable . In the model we present, a certain number of calculation units, distributed across a communication network represented by a graph, participate in the estimation of  by sequentially receiving independent data from  while exchanging messages via a stochastic matrix  defined over the graph. We give precise conditions on the matrix  under which the statistical precision of the individual units is comparable to that of a (gold standard) virtual centralized estimate, even though each unit does not have access to all of the data. We show in particular the fundamental role played by both the non-trivial eigenvalues of  and the Ramanujan class of expander graphs, which provide remarkable performance for moderate algorithmic cost.",
        "keywords": [
            "distributed computing",
            "collaborative estimation",
            "stochastic matrix",
            "graph     theory",
            "complexity",
            ""
        ],
        "author": [
            "Gérard Biau",
            "Kevin Bleakley",
            "Benoît Cadre"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-346/15-346.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Convergence of an Alternating Maximization Procedure",
        "abstract": "We derive two convergence results for a sequential alternating maximization procedure to approximate the maximizer of random functionals such as the realized log likelihood in MLE estimation. We manage to show that the sequence attains the same deviation properties as shown for the profile M-estimator by Andresen and Spokoiny (2013), that means a finite sample Wilks and Fisher theorem. Further under slightly stronger smoothness constraints on the random functional we can show nearly linear convergence to the global maximizer if the starting point for the procedure is well chosen.",
        "keywords": [
            "alternating maximization",
            "alternating minimization",
            "profile maximum likeli-     hood",
            "EM-algorithm",
            "M-estimation",
            "local linear approximation",
            "local concentration",
            "semi-     parametricc 2016 Andreas Andresen",
            ""
        ],
        "author": [
            "Andreas Andresen",
            "Vladimir Spokoiny"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-392/15-392.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "StructED: Risk Minimization in Structured Prediction",
        "abstract": "Structured tasks are distinctive: each task has its own measure of performance, such as the word error rate in speech recognition, the BLEU score in machine translation, the NDCG score in information retrieval, or the intersection-over-union score in visual object segmentation. This paper presents StructED, a software package for learning structured prediction models with training methods that aimed at optimizing the task measure of performance. The package was written in Java and released under the MIT license. It can be downloaded from  adiyoss.github.io/StructED.",
        "keywords": [
            "structured prediction",
            "structural SVM",
            "CRF",
            ""
        ],
        "author": [
            "Yossi Adi",
            "Joseph Keshet"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-531/15-531.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches",
        "abstract": "We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.",
        "keywords": [
            "stereo",
            "matching cost",
            "similarity learning",
            "supervised learning",
            ""
        ],
        "author": [
            "Jure Žbontar",
            "Yann LeCun"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-535/15-535.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Bayesian Policy Gradient and Actor-Critic Algorithms",
        "abstract": "Policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate. Many conventional policy gradient methods use Monte-Carlo techniques to estimate this gradient. The policy is improved by adjusting the parameters in the direction of the gradient estimate. Since Monte-Carlo methods tend to have high variance, a large number of samples is required to attain accurate estimates, resulting in slow convergence. In this paper, we first propose a Bayesian framework for policy gradient, based on modeling the policy gradient as a Gaussian process. This reduces the number of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates, namely, the gradient covariance, are provided at little extra cost. Since the proposed Bayesian framework considers system trajectories as its basic observable unit, it does not require the dynamics within trajectories to be of any particular form, and thus, can be easily extended to partially observable problems. On the downside, it cannot take advantage of the Markov property when the system is Markovian. To address this issue, we proceed to supplement our Bayesian policy gradient framework with a new actor-critic learning model in which a Bayesian class of non- parametric critics, based on Gaussian process temporal difference learning, is used. Such critics model the action- value function as a Gaussian process, allowing Bayes' rule to be used in computing the posterior distribution over action-value functions, conditioned on the observed data. Appropriate choices of the policy parameterization and of the prior covariance (kernel) between action-values allow us to obtain closed-form expressions for the posterior distribution of the gradient of the expected return with respect to the policy parameters. We perform detailed experimental comparisons of the proposed Bayesian policy gradient and actor-critic algorithms with classic Monte-Carlo based policy gradient methods, as well as with each other, on a number of reinforcement learning problems.",
        "keywords": [
            "reinforcement learning",
            "policy gradient methods",
            "actor-critic algorithms",
            "Bayesian inference",
            ""
        ],
        "author": [
            "Mohammad Ghavamzadeh",
            "Yaakov Engel",
            "Michal Valko"
        ],
        "ref": "http://jmlr.org/papers/volume17/10-245/10-245.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Practical Kernel-Based Reinforcement Learning",
        "abstract": "Kernel-based reinforcement learning (KBRL) stands out among approximate reinforcement learning algorithms for its strong theoretical guarantees. By casting the learning problem as a local kernel approximation, KBRL provides a way of computing a decision policy which converges to a unique solution and is statistically consistent. Unfortunately, the model constructed by KBRL grows with the number of sample transitions, resulting in a computational cost that precludes its application to large-scale or on-line domains. In this paper we introduce an algorithm that turns KBRL into a practical reinforcement learning tool. Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a transition probability matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix, potentially much smaller than the original, which retains some fundamental properties of its precursor. KBSF exploits such an insight to compress the information contained in KBRL's model into an approximator of fixed size. This makes it possible to build an approximation considering both the difficulty of the problem and the associated computational cost. KBSF's computational complexity is linear in the number of sample transitions, which is the best one can do without discarding data. Moreover, the algorithm's simple mechanics allow for a fully incremental implementation that makes the amount of memory used independent of the number of sample transitions. The result is a kernel-based reinforcement learning algorithm that can be applied to large-scale problems in both off-line and on-line regimes. We derive upper bounds for the distance between the value functions computed by KBRL and KBSF using the same data. We also prove that it is possible to control the magnitude of the variables appearing in our bounds, which means that, given enough computational resources, we can make KBSF's value function as close as desired to the value function that would be computed by KBRL using the same set of sample transitions. The potential of our algorithm is demonstrated in an extensive empirical study in which KBSF is applied to difficult tasks based on real-world data. Not only does KBSF solve problems that had never been solved before, but it also significantly outperforms other state-of-the-art reinforcement learning algorithms on the tasks studied.",
        "keywords": [
            "reinforcement learning",
            "dynamic programming",
            "Markov decision processes",
            "kernel-based approximation",
            ""
        ],
        "author": [
            "André M.S. Barreto",
            "Doina Precup",
            "Joelle Pineau"
        ],
        "ref": "http://jmlr.org/papers/volume17/13-134/13-134.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "An Information-Theoretic Analysis of Thompson Sampling",
        "abstract": "We provide an information-theoretic analysis of Thompson sampling that applies across a broad range of online optimization problems in which a decision-maker must learn from partial feedback. This analysis inherits the simplicity and elegance of information theory and leads to regret bounds that scale with the entropy of the optimal-action distribution. This strengthens preexisting results and yields new insight into how information improves performance.",
        "keywords": [
            "Thompson sampling",
            "online optimization",
            ""
        ],
        "author": [
            "Daniel Russo",
            "Benjamin Van Roy"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-087/14-087.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Compressed Gaussian Process for Manifold Regression",
        "abstract": "Nonparametric regression for large numbers of features () is an increasingly important problem. If the sample size  is massive, a common strategy is to partition the feature space, and then separately apply simple models to each partition set. This is not ideal when  is modest relative to , and we propose an alternative approach relying on random compression of the feature vector combined with Gaussian process regression. The proposed approach is particularly motivated by the setting in which the response is conditionally independent of the features given the projection to a low dimensional manifold. Conditionally on the random compression matrix and a smoothness parameter, the posterior distribution for the regression surface and posterior predictive distributions are available analytically. Running the analysis in parallel for many random compression matrices and smoothness parameters, model averaging is used to combine the results. The algorithm can be implemented rapidly even in very large  and moderately large  nonparametric regression, has strong theoretical justification, and is found to yield state of the art predictive performance.",
        "keywords": [
            ""
        ],
        "author": [
            "Rajarshi Guhaniyogi",
            "David B. Dunson"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-230/14-230.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "On the Characterization of a Class of Fisher-Consistent Loss Functions and its Application to Boosting",
        "abstract": "Accurate classification of categorical outcomes is essential in a wide range of applications. Due to computational issues with minimizing the empirical 0/1 loss, Fisher consistent losses have been proposed as viable proxies. However, even with smooth losses, direct minimization remains a daunting task. To approximate such a minimizer, various boosting algorithms have been suggested. For example, with exponential loss, the AdaBoost algorithm (Freund and Schapire, 1995) is widely used for two- class problems and has been extended to the multi-class setting (Zhu et al., 2009). Alternative loss functions, such as the logistic and the hinge losses, and their corresponding boosting algorithms have also been proposed (Zou et al., 2008; Wang, 2012).  In this paper we demonstrate that a broad class of losses, including non-convex functions, achieve Fisher consistency, and in addition can be used for explicit estimation of the conditional class probabilities. Furthermore, we provide a generic boosting algorithm that is not loss-specific. Extensive simulation results suggest that the proposed boosting algorithms could outperform existing methods with properly chosen losses and bags of weak learners.",
        "keywords": [
            "Boosting",
            "Fisher-Consistency",
            "Multiclass Classification",
            ""
        ],
        "author": [
            "Matey Neykov",
            "Jun S. Liu",
            "Tianxi Cai"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-306/14-306.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Exact Inference on Gaussian Graphical Models of Arbitrary Topology using Path-Sums",
        "abstract": "We present the path-sum formulation for exact statistical inference of marginals on Gaussian graphical models of arbitrary topology. The path-sum formulation gives the covariance between each pair of variables as a branched continued fraction of finite depth and breadth. Our method originates from the closed- form resummation of infinite families of terms of the walk-sum representation of the covariance matrix. We prove that the path- sum formulation always exists for models whose covariance matrix is positive definite: i.e. it is valid for both walk-summable and non-walk-summable graphical models of arbitrary topology. We show that for graphical models on trees the path-sum formulation is equivalent to Gaussian belief propagation. We also recover, as a corollary, an existing result that uses determinants to calculate the covariance matrix. We show that the path-sum formulation formulation is valid for arbitrary partitions of the inverse covariance matrix. We give detailed examples demonstrating our results.",
        "keywords": [
            "Gaussian graphical models",
            "belief propagation",
            "path-sum",
            "walk-sum",
            "graphs     of arbitrary topology",
            "block matricesc 2016 Pierre-Louis Giscard",
            "Zheng Choo",
            ""
        ],
        "author": [
            "P.-L. Giscard",
            "Z. Choo",
            "S. J. Thwaite",
            "D. Jaksch"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-445/14-445.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Challenges in multimodal gesture recognition",
        "abstract": "This paper surveys the state of the art on multimodal gesture recognition and introduces the JMLR special topic on gesture recognition 2011-2015. We began right at the start of the \\kinect revolution when inexpensive infrared cameras providing image depth recordings became available. We published papers using this technology and other more conventional methods, including regular video cameras, to record data, thus providing a good overview of uses of machine learning and computer vision using multimodal data in this area of application. Notably, we organized a series of challenges and made available several datasets we recorded for that purpose, including tens of thousands of videos, which are available to conduct further research. We also overview recent state of the art works on gesture recognition based on a proposed taxonomy for gesture recognition, discussing challenges and future lines of research.",
        "keywords": [
            "Gesture Recognition",
            "Time Series Analysis",
            "Multimodal Data Analysis",
            "Computer     Vision",
            "Pattern Recognition",
            "Wearable sensors",
            "Infrared Cameras",
            ""
        ],
        "author": [
            "Sergio Escalera",
            "Vassilis Athitsos",
            "Isabelle Guyon"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-468/14-468.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
        "abstract": "In this paper we introduce the idea of improving the performance of parametric temporal-difference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, we show that varying the emphasis of linear TD()'s updates in a particular way causes its expected update to become stable under off-policy training. The only prior model-free TD methods to achieve this with per- step computation linear in the number of function approximation parameters are the gradient-TD family of methods including TDC, GTD(), and GQ). Compared to these methods, our emphatic TD() is simpler and easier to use; it has only one learned parameter vector and one step-size parameter. Our treatment includes general state- dependent discounting and bootstrapping functions, and a way of specifying varying degrees of interest in accurately valuing different states.",
        "keywords": [
            "Temporal-difference learning",
            "Off-policy learning",
            "Function approximation",
            "Stability",
            ""
        ],
        "author": [
            "Richard S. Sutton",
            "A. Rupam Mahmood",
            "Martha White"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-488/14-488.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Learning Algorithms for Second-Price Auctions with Reserve",
        "abstract": "Second-price auctions with reserve play a critical role in the revenue of modern search engine and popular online sites since the revenue of these companies often directly depends on the outcome of such auctions. The choice of the reserve price is the main mechanism through which the auction revenue can be influenced in these electronic markets. We cast the problem of selecting the reserve price to optimize revenue as a learning problem and present a full theoretical analysis dealing with the complex properties of the corresponding loss function. We further give novel algorithms for solving this problem and report the results of several experiments in both synthetic and real-world data demonstrating their effectiveness.",
        "keywords": [
            "Learning Theory",
            "Auctions",
            ""
        ],
        "author": [
            "Mehryar Mohri",
            "Andres Munoz Medina"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-499/14-499.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Distributed Coordinate Descent Method for Learning with Big Data",
        "abstract": "In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent method for solving loss minimization problems with big data. We initially partition the coordinates (features) and assign each partition to a different node of a cluster. At every iteration, each node picks a random subset of the coordinates from those it owns, independently from the other computers, and in parallel computes and applies updates to the selected coordinates based on a simple closed-form formula. We give bounds on the number of iterations sufficient to approximately solve the problem with high probability, and show how it depends on the data and on the partitioning. We perform numerical experiments with a LASSO instance described by a 3TB matrix.",
        "keywords": [
            "stochastic methods",
            "parallel coordinate descent",
            "distributed algorithms",
            ""
        ],
        "author": [
            "Peter Richtárik",
            "Martin Takáč"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-001/15-001.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Scaling-up Empirical Risk Minimization: Optimization of Incomplete $U$-statistics",
        "abstract": "In a wide range of statistical learning problems such as ranking, clustering or metric learning among others, the risk is accurately estimated by -statistics of degree , i.e. functionals of the training data with low variance that take the form of averages over -tuples. From a computational perspective, the calculation of such statistics is highly expensive even for a moderate sample size , as it requires averaging  terms. This makes learning procedures relying on the optimization of such data functionals hardly feasible in practice. It is the major goal of this paper to show that, strikingly, such empirical risks can be replaced by drastically computationally simpler Monte-Carlo estimates based on  terms only, usually referred to as incomplete -statistics, without damaging the  learning rate of Empirical Risk Minimization (ERM) procedures. For this purpose, we establish uniform deviation results describing the error made when approximating a -process by its incomplete version under appropriate complexity assumptions. Extensions to model selection, fast rate situations and various sampling techniques are also considered, as well as an application to stochastic gradient descent for ERM. Finally, numerical examples are displayed in order to provide strong empirical evidence that the approach we promote largely surpasses more naive subsampling techniques.",
        "keywords": [
            "big data",
            "empirical risk minimization",
            "U-processes",
            "rate bound analysis",
            "sampling design",
            ""
        ],
        "author": [
            "Stephan Clémençon",
            "Igor Colin",
            "Aurélien Bellet"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-012/15-012.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Iterative Regularization for Learning with Convex Loss Functions",
        "abstract": "We consider the problem of supervised learning with convex loss functions and propose a new form of iterative regularization based on the subgradient method. Unlike other regularization approaches, in iterative regularization no constraint or penalization is considered, and generalization is achieved by (early) stopping an empirical iteration. We consider a nonparametric setting, in the framework of reproducing kernel Hilbert spaces, and prove consistency and finite sample bounds on the excess risk under general regularity conditions. Our study provides a new class of efficient regularized learning algorithms and gives insights on the interplay between statistics and optimization in machine learning.",
        "keywords": [],
        "author": [
            "Junhong Lin",
            "Lorenzo Rosasco",
            "Ding-Xuan Zhou"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-115/15-115.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Latent Space Inference of Internet-Scale Networks",
        "abstract": "The rise of Internet-scale networks, such as web graphs and social media with hundreds of millions to billions of nodes, presents new scientific opportunities, such as overlapping community detection to discover the structure of the Internet, or to analyze trends in online social behavior. However, many existing probabilistic network models are difficult or impossible to deploy at these massive scales. We propose a scalable approach for modeling and inferring latent spaces in Internet-scale networks, with an eye towards overlapping community detection as a key application. By applying a succinct representation of networks as a bag of triangular motifs, developing a parsimonious statistical model, deriving an efficient stochastic variational inference algorithm, and implementing it as a distributed cluster program via the Petuum parameter server system, we demonstrate overlapping community detection on real networks with up to 100 million nodes and 1000 communities on 5 machines in under 40 hours. Compared to other state-of-the-art probabilistic network approaches, our method is several orders of magnitude faster, with competitive or improved accuracy at overlapping community detection.",
        "keywords": [
            "probabilistic network models",
            "triangular modeling",
            "stochastic variational     inference",
            "distributed computation",
            ""
        ],
        "author": [
            "Qirong Ho",
            "Junming Yin",
            "Eric P. Xing"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-142/15-142.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Patient Risk Stratification with Time-Varying Parameters: A Multitask Learning Approach",
        "abstract": "The proliferation of electronic health records (EHRs) frames opportunities for using machine learning to build models that help healthcare providers improve patient outcomes. However, building useful risk stratification models presents many technical challenges including the large number of factors (both intrinsic and extrinsic) influencing a patient's risk of an adverse outcome and the inherent evolution of that risk over time. We address these challenges in the context of learning a risk stratification model for predicting which patients are at risk of acquiring a Clostridium difficile infection (CDI). We take a novel data-centric approach, leveraging the contents of EHRs from nearly 50,000 hospital admissions. We show how, by adapting techniques from multitask learning, we can learn models for patient risk stratification with unprecedented classification performance. Our model, based on thousands of variables, both time-varying and time-invariant, changes over the course of a patient admission. Applied to a held out set of approximately 25,000 patient admissions, we achieve an area under the receiver operating characteristic curve of 0.81 (95% CI 0.78-0.84). The model has been integrated into the health record system at a large hospital in the US, and can be used to produce daily risk estimates for each inpatient. While more complex than traditional risk stratification methods, the widespread development and use of such data-driven models could ultimately enable cost-effective, targeted prevention strategies that lead to better patient outcomes.",
        "keywords": [
            "risk stratification",
            "time-varying coefficients",
            "multitask learning",
            "Clostridium     difficile",
            ""
        ],
        "author": [
            "Jenna Wiens",
            "John Guttag",
            "Eric Horvitz"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-177/15-177.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Multiplicative Multitask Feature Learning",
        "abstract": "We investigate a general framework of multiplicative multitask feature learning which decomposes individual task's model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods can be proved to be special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the task- specific component for all these regularizers, leading to a better understanding of the shrinkage effects of different regularizers. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. An efficient blockwise coordinate descent algorithm is developed suitable for solving the entire family of formulations with rigorous convergence analysis. Simulation studies have identified the statistical properties of data that would be in favor of the new formulations. Extensive empirical studies on various classification and regression benchmark data sets have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks.",
        "keywords": [],
        "author": [
            "Xin Wang",
            "Jinbo Bi",
            "Shipeng Yu",
            "Jiangwen Sun",
            "Minghu Song"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-234/15-234.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "The Benefit of Multitask Representation Learning",
        "abstract": "We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks.",
        "keywords": [
            "learning-to-learn",
            "multitask learning",
            "representation learning",
            "statistical     learning theory",
            ""
        ],
        "author": [
            "Andreas Maurer",
            "Massimiliano Pontil",
            "Bernardino Romera-Paredes"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-242/15-242.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Model-free Variable Selection in Reproducing Kernel Hilbert Space",
        "abstract": "Variable selection is popular in high-dimensional data analysis to identify the truly informative variables. Many variable selection methods have been developed under various model assumptions. Whereas success has been widely reported in literature, their performances largely depend on validity of the assumed models, such as the linear or additive models. This article introduces a model-free variable selection method via learning the gradient functions. The idea is based on the equivalence between whether a variable is informative and whether its corresponding gradient function is substantially non-zero. The proposed variable selection method is then formulated in a framework of learning gradients in a flexible reproducing kernel Hilbert space. The key advantage of the proposed method is that it requires no explicit model assumption and allows for general variable effects. Its asymptotic estimation and selection consistencies are studied, which establish the convergence rate of the estimated sparse gradients and assure that the truly informative variables are correctly identified in probability. The effectiveness of the proposed method is also supported by a variety of simulated examples and two real-life examples.",
        "keywords": [
            "group Lasso",
            "high-dimensional data",
            "kernel regression",
            "learning gradients",
            ""
        ],
        "author": [
            "Lei Yang",
            "Shaogao Lv",
            "Junhui Wang"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-390/15-390.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "CVXPY: A Python-Embedded Modeling Language for Convex Optimization",
        "abstract": "CVXPY is a domain-specific language for convex optimization embedded in Python. It allows the user to express convex optimization problems in a natural syntax that follows the math, rather than in the restrictive standard form required by solvers. CVXPY makes it easy to combine convex optimization with high-level features of Python such as parallelism and object- oriented design. CVXPY is available at www.cvxpy.org under the GPL license, along with documentation and examples.",
        "keywords": [
            "convex optimization",
            "domain-specific languages",
            "Python",
            "conic programming",
            ""
        ],
        "author": [
            "Steven Diamond",
            "Stephen Boyd"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-408/15-408.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Lenient Learning in Independent-Learner Stochastic Cooperative Games",
        "abstract": "We introduce the Lenient Multiagent Reinforcement Learning 2 (LMRL2) algorithm for independent-learner stochastic cooperative games. LMRL2 is designed to overcome a pathology called relative overgeneralization, and to do so while still performing well in games with stochastic transitions, stochastic rewards, and miscoordination. We discuss the existing literature, then compare LMRL2 against other algorithms drawn from the literature which can be used for games of this kind: traditional (âDistributedâ) Q-learning, Hysteretic Q-learning, WoLF-PHC, SOoN, and (for repeated games only) FMQ. The results show that LMRL2 is very effective in both of our measures (complete and correct policies), and is found in the top rank more often than any other technique. LMRL2 is also easy to tune: though it has many available parameters, almost all of them stay at default settings. Generally the algorithm is optimally tuned with a single parameter, if any. We then examine and discuss a number of side-issues and options for LMRL2.",
        "keywords": [
            "multiagent learning",
            "reinforcement learning",
            "game theory",
            "lenient learning",
            ""
        ],
        "author": [
            "Ermo Wei",
            "Sean Luke"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-417/15-417.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Structure-Leveraged Methods in Breast Cancer Risk Prediction",
        "abstract": "Predicting breast cancer risk has long been a goal of medical research in the pursuit of precision medicine. The goal of this study is to develop novel penalized methods to improve breast cancer risk prediction by leveraging structure information in electronic health records. We conducted a retrospective case- control study, garnering 49 mammography descriptors and 77 high- frequency/low-penetrance single-nucleotide polymorphisms (SNPs) from an existing personalized medicine data repository. Structured mammography reports and breast imaging features have long been part of a standard electronic health record (EHR), and genetic markers likely will be in the near future. Lasso and its variants are widely used approaches to integrated learning and feature selection, and our methodological contribution is to incorporate the dependence structure among the features into these approaches. More specifically, we propose a new methodology by combining group penalty and  () fusion penalty to improve breast cancer risk prediction, taking into account structure information in mammography descriptors and SNPs. We demonstrate that our method provides benefits that are both statistically significant and potentially significant to people's lives.",
        "keywords": [],
        "author": [
            "Jun Fan",
            "Yirong Wu",
            "Ming Yuan",
            "David Page",
            "Jie Liu",
            "Irene M. Ong",
            "Peggy Peissig",
            "Elizabeth Burnside"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-444/15-444.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "LIBMF: A Library for Parallel Matrix Factorization in Shared-memory Systems",
        "abstract": "Matrix factorization (MF) plays a key role in many applications such as recommender systems and computer vision, but MF may take long running time for handling large matrices commonly seen in the big data era. Many parallel techniques have been proposed to reduce the running time, but few parallel MF packages are available. Therefore, we present an open source library, LIBMF, based on recent advances of parallel MF for shared-memory systems. LIBMF includes easy-to-use command-line tools, interfaces to C/C++ languages, and comprehensive documentation. Our experiments demonstrate that LIBMF outperforms state of the art packages. LIBMF is BSD-licensed, so users can freely use, modify, and redistribute the code.",
        "keywords": [
            "Matrix factorization",
            "non-negative matrix factorization",
            "binary matrix fac-      torization",
            "logistic matrix factorization",
            "one-class matrix factorization",
            "stochastic gradient      method",
            "adaptive learning rate",
            ""
        ],
        "author": [
            "Wei-Sheng Chin",
            "Bo-Wen Yuan",
            "Meng-Yuan Yang",
            "Yong Zhuang",
            "Yu-Chin Juan",
            "Chih-Jen Lin"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-471/15-471.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "L1-Regularized Least Squares for Support Recovery of High Dimensional Single Index Models with Gaussian Designs",
        "abstract": "It is known that for a certain class of single index models (SIMs) , support recovery is impossible when  and a model complexity adjusted sample size is below a critical threshold. Recently, optimal algorithms based on Sliced Inverse Regression (SIR) were suggested. These algorithms work provably under the assumption that the design  comes from an i.i.d. Gaussian distribution. In the present paper we analyze algorithms based on covariance screening and least squares with  penalization (i.e. LASSO) and demonstrate that they can also enjoy optimal (up to a scalar) rescaled sample size in terms of support recovery, albeit under slightly different assumptions on  and  compared to the SIR based algorithms. Furthermore, we show more generally, that LASSO succeeds in recovering the signed support of  if , and the covariance  satisfies the irrepresentable condition. Our work extends existing results on the support recovery of LASSO for the linear model, to a more general class of SIMs.",
        "keywords": [
            "Single index models",
            "Sparsity",
            "Support recovery",
            "High-dimensional statistics",
            ""
        ],
        "author": [
            "Matey Neykov",
            "Jun S. Liu",
            "Tianxi Cai"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-006/16-006.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Spectral Ranking using Seriation",
        "abstract": "We describe a seriation algorithm for ranking a set of items given pairwise comparisons between these items. Intuitively, the algorithm assigns similar rankings to items that compare similarly with all others. It does so by constructing a similarity matrix from pairwise comparisons, using seriation methods to reorder this matrix and construct a ranking. We first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order. We then show that ranking reconstruction is still exact when some pairwise comparisons are corrupted or missing, and that seriation based spectral ranking is more robust to noise than classical scoring methods. Finally, we bound the ranking error when only a random subset of the comparions are observed. An additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems. Experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods.",
        "keywords": [
            "Ranking",
            "seriation",
            ""
        ],
        "author": [
            "Fajwel Fogel",
            "Alexandre d'Aspremont",
            "Milan Vojnovic"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-035/16-035.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Sparsity and Error Analysis of Empirical Feature-Based Regularization Schemes",
        "abstract": "We consider a learning algorithm generated by a regularization scheme with a concave regularizer for the purpose of achieving sparsity and good learning rates in a least squares regression setting. The regularization is induced for linear combinations of empirical features, constructed in the literatures of kernel principal component analysis and kernel projection machines, based on kernels and samples. In addition to the separability of the involved optimization problem caused by the empirical features, we carry out sparsity and error analysis, giving bounds in the norm of the reproducing kernel Hilbert space, based on a priori conditions which do not require assumptions on sparsity in terms of any basis or system. In particular, we show that as the concave exponent  of the concave regularizer increases to , the learning ability of the algorithm improves. Some numerical simulations for both artificial and real MHC-peptide binding data involving the  regularizer and the SCAD penalty are presented to demonstrate the sparsity and error analysis.",
        "keywords": [
            "Sparsity",
            "concave regularizer",
            "reproducing kernel Hilbert space",
            "regularization     with empirical features",
            ""
        ],
        "author": [
            "Xin Guo",
            "Jun Fan",
            "Ding-Xuan Zhou"
        ],
        "ref": "http://jmlr.org/papers/volume17/11-207/11-207.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Estimating Diffusion Networks: Recovery Conditions, Sample Complexity and Soft-thresholding Algorithm",
        "abstract": "Information spreads across social and technological networks, but often the network structures are hidden from us and we only observe the traces left by the diffusion  processes, called cascades. Can we recover the hidden network structures from these observed cascades? What kind of cascades and how many cascades  do we need? Are there some network structures which are more difficult than others to recover? Can we design efficient inference algorithms with provable  guarantees? Despite the increasing availability of cascade data and methods for inferring networks from these data, a thorough theoretical understanding of the above questions  remains largely unexplored in the literature. In this paper, we investigate the network structure inference problem for a general family of continuous- time diffusion models  using an -regularized likelihood maximization framework.   We show that, as long as the cascade sampling process satisfies a natural incoherence condition, our framework can recover the correct network structure with high probability if we observe  cascades, where  is the maximum number of parents of a node and  is the total number of nodes. Moreover, we develop a  simple and efficient soft-thresholding network inference algorithm which demonstrate the match between our theoretical prediction and empirical results. In practice, this new algorithm also outperforms other alternatives in terms of the accuracy of recovering hidden diffusion networks.",
        "keywords": [],
        "author": [
            "Manuel Gomez-Rodriguez",
            "Le Song",
            "Hadi Daneshm",
            "Bernhard Schölkopf"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-430/14-430.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Rounding-based Moves for Semi-Metric Labeling",
        "abstract": "Semi-metric labeling is a special case of energy minimization for pairwise Markov random fields. The energy function consists of arbitrary unary potentials, and pairwise potentials that are proportional to a given semi-metric distance function over the label set. Popular methods for solving semi-metric labeling include (i) move-making algorithms, which iteratively solve a minimum -cut problem; and (ii) the linear programming ( LP) relaxation based approach. In order to convert the fractional solution of the LP relaxation to an integer solution, several randomized rounding procedures have been developed in the literature. We consider a large class of parallel rounding procedures, and design move-making algorithms that closely mimic them. We prove that the multiplicative bound of a move-making algorithm exactly matches the approximation factor of the corresponding rounding procedure for any arbitrary distance function. Our analysis includes all known results for move- making algorithms as special cases.",
        "keywords": [
            "semi-metric labeling",
            "move-making algorithms",
            "linear programming relaxation",
            ""
        ],
        "author": [
            "M. Pawan Kumar",
            "Puneet K. Dokania"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-454/14-454.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Rate Optimal Denoising of Simultaneously Sparse and Low Rank Matrices",
        "abstract": "We study minimax rates for denoising simultaneously sparse and low rank matrices in high dimensions. We show that an iterative thresholding algorithm achieves (near) optimal rates adaptively under mild conditions for a large class of loss functions. Numerical experiments on synthetic datasets also demonstrate the competitive performance of the proposed method.",
        "keywords": [
            "Denoising",
            "High dimensionality",
            "Low rank matrices",
            "Minimax rates",
            "Simul-     taneously structured matrices",
            "Sparse SVD",
            ""
        ],
        "author": [
            "Dan Yang",
            "Zongming Ma",
            "Andreas Buja"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-134/15-134.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Hierarchical Relative Entropy Policy Search",
        "abstract": "Many reinforcement learning (RL) tasks, especially in robotics, consist of multiple sub-tasks that are strongly structured. Such task structures can be exploited by incorporating hierarchical policies that consist of gating networks and sub-policies. However, this concept has only been partially explored for real world settings and complete methods, derived from first principles, are needed. Real world settings are challenging due to large and continuous state-action spaces that are prohibitive for exhaustive sampling methods. We define the problem of learning sub-policies in continuous state action spaces as finding a hierarchical policy that is composed of a high-level gating policy to select the low-level sub-policies for execution by the agent. In order to efficiently share experience with all sub-policies, also called inter-policy learning, we treat these sub-policies as latent variables which allows for distribution of the update information between the sub-policies. We present three different variants of our algorithm, designed to be suitable for a wide variety of real world robot learning tasks and evaluate our algorithms in two real robot learning scenarios as well as several simulations and comparisons.",
        "keywords": [
            "Reinforcement Learning",
            "Policy Search",
            "Hierarchical Learning",
            "Robot Learning",
            "Mo-      tor Skill Learning",
            "Robust Learning",
            "Structured Learning",
            "Temporal Correlation",
            "HiREPS",
            ""
        ],
        "author": [
            "Christian Daniel",
            "Gerhard Neumann",
            "Oliver Kroemer",
            "Jan Peters"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-188/15-188.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Convex Regression with Interpretable Sharp Partitions",
        "abstract": "We consider the problem of predicting an outcome variable on the basis of a small number of covariates, using an interpretable yet non-additive model. We propose convex regression with interpretable sharp partitions (CRISP) for this task. CRISP partitions the covariate space into blocks in a data- adaptive way, and fits a mean model within each block. Unlike other partitioning methods, CRISP is fit using a non-greedy approach by solving a convex optimization problem, resulting in low- variance fits. We explore the properties of CRISP, and evaluate its performance in a simulation study and on a housing price data set.",
        "keywords": [
            "convex optimization",
            "interpretability",
            "non-additivity",
            "non-parametric regres-     sion",
            ""
        ],
        "author": [
            "Ashley Petersen",
            "Noah Simon",
            "Daniela Witten"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-344/15-344.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "JCLAL: A Java Framework for Active Learning",
        "abstract": "Active Learning has become an important area of research owing to the increasing number of real-world problems which contain labelled and unlabelled examples at the same time. JCLAL is a Java Class Library for Active Learning which has an architecture that follows strong principles of object-oriented design. It is easy to use, and it allows the developers to adapt, modify and extend the framework according to their needs. The library offers a variety of active learning methods that have been proposed in the literature. The software is available under the GPL license.",
        "keywords": [
            "active learning",
            "framework",
            "java language",
            ""
        ],
        "author": [
            "Oscar Reyes",
            "Eduardo Pérez",
            "María del Carmen Rodríguez-Hernández",
            "Habib M. Fardoun",
            "Sebastián  Ventura"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-347/15-347.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Integrated Common Sense Learning and Planning in POMDPs",
        "abstract": "We formulate a new variant of the problem of planning in an unknown environment, for which we can provide algorithms with reasonable theoretical guarantees in spite of large state spaces and time horizons, partial observability, and complex dynamics. In this variant, an agent is given a collection of example traces produced by a reference policy, which may, for example, capture the agent's past behavior. The agent is (only) asked to find policies that are supported by regularities in the dynamics that are observable on these example traces. We describe an efficient algorithm that uses such common sense knowledge reflected in the example traces to construct decision tree policies for goal-oriented factored POMDPs. More precisely, our algorithm (provably) succeeds at finding a policy for a given input goal when (1) there is a CNF that is almost always observed satisfied on the traces of the POMDP, capturing a sufficient approximation of its dynamics and (2) for a decision tree policy of bounded complexity, there exist small- space resolution proofs that the goal is achieved on each branch using the aforementioned CNF capturing the common sense rules. Such a CNF always exists for noisy STRIPS domains, for example. Our results thus essentially establish that the possession of a suitable exploration policy for collecting the necessary examples is the fundamental obstacle to learning to act in such environments.",
        "keywords": [
            "Partially Observed Markov Decision Process",
            "Decision Tree Policies",
            "PAC-     Semantics",
            "Noisy STRIPS",
            ""
        ],
        "author": [
            "Brendan Juba"
        ],
        "ref": "http://jmlr.org/papers/volume17/13-584/13-584.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Cells in Multidimensional Recurrent Neural Networks",
        "abstract": "The transcription of handwritten text on images is one task in machine learning and one solution to solve it is using multi- dimensional recurrent neural networks (MDRNN) with connectionist temporal classification (CTC). The RNNs can contain special units, the long short-term memory (LSTM) cells. They are able to learn long term dependencies but they get unstable when the dimension is chosen greater than one. We defined some useful and necessary properties for the one-dimensional LSTM cell and extend them in the multi-dimensional case. Thereby we introduce several new cells with better stability. We present a method to design cells using the theory of linear shift invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT and Rimes database, where we can improve the recognition rate compared to the LSTM cell. So each application where the LSTM cells in MDRNNs are used could be improved by substituting them by the new developed cells.",
        "keywords": [],
        "author": [
            "Gundram Leifert",
            "Tobias Strau{\\ss}",
            "Tobias Gr{ü}ning",
            "Welf Wustlich",
            "Roger Labahn"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-203/14-203.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Learning Taxonomy Adaptation in Large-scale Classification",
        "abstract": "In this paper, we study flat and hierarchical classification strategies in the context of large-scale taxonomies. Addressing the problem from a learning-theoretic point of view, we first propose a multi-class, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. Based on this bound, we also propose a technique for modifying a given taxonomy through pruning, that leads to a lower value of the upper bound as compared to the original taxonomy. We then present another method for hierarchy pruning by studying approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.",
        "keywords": [
            "Large-scale classification",
            "Hierarchical classification",
            "Taxonomy adaptation",
            "Rademacher complexity",
            ""
        ],
        "author": [
            "Rohit Babbar",
            "Ioannis Partalas",
            "Eric Gaussier",
            "Massih-Reza Amini",
            "Cécile Amblard"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-207/14-207.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "How to Center Deep Boltzmann Machines",
        "abstract": "This work analyzes centered Restricted Boltzmann Machines (RBMs) and centered Deep Boltzmann Machines (DBMs), where centering is done by subtracting offset values from visible and hidden variables. We show analytically that (i) centered and normal Boltzmann Machines (BMs) and thus RBMs and DBMs are different parameterizations of the same model class, such that any normal BM/RBM/DBM can be transformed to an equivalent centered BM/RBM/DBM and vice versa, and that this equivalence generalizes to artificial neural networks in general, (ii) the expected performance of centered binary BMs/RBMs/DBMs is invariant under simultaneous flip of data and offsets, for any offset value in the range of zero to one, (iii) centering can be reformulated as a different update rule for normal BMs/RBMs/DBMs, and (iv) using the enhanced gradient is equivalent to setting the offset values to the average over model and data mean. Furthermore, we present numerical simulations suggesting that (i) optimal generative performance is achieved by subtracting mean values from visible as well as hidden variables, (ii) centered binary RBMs/DBMs reach significantly higher log-likelihood values than normal binary RBMs/DBMs, (iii) centering variants whose offsets depend on the model mean, like the enhanced gradient, suffer from severe divergence problems, (iv) learning is stabilized if an exponentially moving average over the batch means is used for the offset values instead of the current batch mean, which also prevents the enhanced gradient from severe divergence, (v) on a similar level of log-likelihood values centered binary RBMs/DBMs have smaller weights and bigger bias parameters than normal binary RBMs/DBMs, (vi) centering leads to an update direction that is closer to the natural gradient, which is extremely efficient for training as we show for small binary RBMs, (vii) centering eliminates the need for greedy layer-wise pre-training of DBMs, which often even deteriorates the results independently of whether centering is used or not, and (ix) centering is also beneficial for auto encoders.",
        "keywords": [
            "centering",
            "restricted Boltzmann machine",
            "deep Boltzmann machine",
            "gener-     ative model",
            "artificial neural network",
            "auto encoder",
            "enhanced gradient",
            "natural gradient",
            "stochastic maximum likelihood",
            "contrastive divergence",
            ""
        ],
        "author": [
            "Jan Melchior",
            "Asja Fischer",
            "Laurenz Wiskott"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-237/14-237.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Control Function Instrumental Variable Estimation of Nonlinear Causal Effect Models",
        "abstract": "The instrumental variable method consistently estimates the effect of a treatment when there is unmeasured confounding and a valid instrumental variable. A valid instrumental variable is a variable that is independent of unmeasured confounders and affects the treatment but does not have a direct effect on the outcome beyond its effect on the treatment. Two commonly used estimators for using an instrumental variable to estimate a treatment effect are the two stage least squares estimator and the control function estimator. For linear causal effect models, these two estimators are equivalent, but for nonlinear causal effect models, the estimators are different. We provide a systematic comparison of these two estimators for nonlinear causal effect models and develop an approach to combing the two estimators that generally performs better than either one alone. We show that the control function estimator is a two stage least squares estimator with an augmented set of instrumental variables. If these augmented instrumental variables are valid, then the control function estimator can be much more efficient than usual two stage least squares without the augmented instrumental variables while if the augmented instrumental variables are not valid, then the control function estimator may be inconsistent while the usual two stage least squares remains consistent. We apply the Hausman test to test whether the augmented instrumental variables are valid and construct a pretest estimator based on this test. The pretest estimator is shown to work well in a simulation study. An application to the effect of exposure to violence on time preference is considered.",
        "keywords": [
            "Causal Inference",
            "Control Function Estimator",
            "Endogenous Variable",
            "Instru-     mental Variable Method",
            "Two Stage Least Squares Estimator",
            ""
        ],
        "author": [
            "Zijian Guo",
            "Dylan S. Small"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-379/14-379.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Structure Learning in Bayesian Networks of a Moderate Size by Efficient Sampling",
        "abstract": "We study the Bayesian model averaging approach to learning Bayesian network structures (DAGs) from data. We develop new algorithms including the first algorithm that is able to efficiently sample DAGs of a moderate size (with up to about 25 variables) according to the exact structure posterior. The DAG samples can then be used to construct estimators for the posterior of any feature. We theoretically prove good properties of our estimators and empirically show that our estimators considerably outperform the estimators from the previous state- of-the-art methods.",
        "keywords": [
            "Bayesian model averaging",
            "Bayesian networks",
            "DAG sampling",
            "dynamic program-     ming",
            "order sampling",
            ""
        ],
        "author": [
            "Ru He",
            "Jin Tian",
            "Huaiqing Wu"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-497/14-497.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Spectral Methods Meet EM: A Provably Optimal Algorithm for Crowdsourcing",
        "abstract": "Crowdsourcing is a popular paradigm for effectively collecting labels at low cost. The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.",
        "keywords": [
            "crowdsourcing",
            "spectral methods",
            "EM",
            "Dawid-Skene model",
            "non-convex op-     timization",
            ""
        ],
        "author": [
            "Yuchen Zhang",
            "Xi Chen",
            "Dengyong Zhou",
            "Michael I. Jordan"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-511/14-511.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Bayesian Leave-One-Out Cross-Validation Approximations for Gaussian Latent Variable Models",
        "abstract": "The future predictive performance of a Bayesian model can be estimated using Bayesian cross-validation. In this article, we consider Gaussian latent variable models where the integration over the latent values is approximated using the Laplace method or expectation propagation (EP). We study the properties of several Bayesian leave-one-out (LOO) cross-validation approximations that in most cases can be computed with a small additional cost after forming the posterior approximation given the full data. Our main objective is to assess the accuracy of the approximative LOO cross-validation estimators. That is, for each method (Laplace and EP) we compare the approximate fast computation with the exact brute force LOO computation. Secondarily, we evaluate the accuracy of the Laplace and EP approximations themselves against a ground truth established through extensive Markov chain Monte Carlo simulation. Our empirical results show that the approach based upon a Gaussian approximation to the LOO marginal distribution (the so-called cavity distribution) gives the most accurate and reliable results among the fast methods.",
        "keywords": [
            "predictive performance",
            "leave-one-out cross-validation",
            "Gaussian latent vari-     able model",
            "Laplace approximation",
            ""
        ],
        "author": [
            "Aki Vehtari",
            "Tommi Mononen",
            "Ville Tolvanen",
            "Tuomas Sivula",
            "Ole Winther"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-540/14-540.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "e-PAL: An Active Learning Approach to the Multi-Objective Optimization Problem",
        "abstract": "In many fields one encounters the challenge of identifying out of a pool of possible designs those that simultaneously optimize multiple objectives. In many applications an exhaustive search for the Pareto-optimal set is infeasible. To address this challenge, we propose the -Pareto Active Learning (-PAL) algorithm which adaptively samples the design space to predict a set of Pareto-optimal solutions that cover the true Pareto front of the design space with some granularity regulated by a parameter . Key features of -PAL include (1) modeling the objectives as draws from a Gaussian process distribution to capture structure and accommodate noisy evaluation; (2) a method to carefully choose the next design to evaluate to maximize progress; and (3) the ability to control prediction accuracy and sampling cost. We provide theoretical bounds on -PAL's sampling cost required to achieve a desired accuracy. Further, we perform an experimental evaluation on three real-world data sets that demonstrate -PAL's effectiveness; in comparison to the state-of-the-art active learning algorithm PAL, -PAL reduces the amount of computations and the number of samples from the design space required to meet the user's desired level of accuracy. In addition, we show that -PAL improves significantly over a state-of-the-art multi- objective optimization method, saving in most cases 30\\% to 70\\% evaluations to achieve the same accuracy.",
        "keywords": [
            "multi-objective optimization",
            "active learning",
            "pareto optimality",
            "Bayesian     optimization",
            ""
        ],
        "author": [
            "Marcela Zuluaga",
            "Andreas Krause",
            "Markus P{ü}schel"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-047/15-047.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Trend Filtering on Graphs",
        "abstract": "We introduce a family of adaptive estimators on graphs, based on penalizing the  norm of discrete graph differences. This generalizes the idea of trend filtering (Kim et al., 2009; Tibshirani, 2014), used for univariate nonparametric regression, to graphs. Analogous to the univariate case, graph trend filtering exhibits a level of local adaptivity unmatched by the usual -based graph smoothers. It is also defined by a convex minimization problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We demonstrate the merits of graph trend filtering through both examples and theory.",
        "keywords": [
            "trend filtering",
            "graph smoothing",
            "total variation denoising",
            "fused lasso",
            ""
        ],
        "author": [
            "Yu-Xiang Wang",
            "James Sharpnack",
            "Alexander J. Smola",
            "Ryan J. Tibshirani"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-147/15-147.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Multi-Task Learning for Straggler Avoiding Predictive Job Scheduling",
        "abstract": "Parallel processing frameworks (Dean and Ghemawat, 2004) accelerate jobs by breaking them into tasks that execute in parallel. However, slow running or straggler tasks can run up to 8 times slower than the median task on a production cluster (Ananthanarayanan et al., 2013), leading to delayed job completion and inefficient use of resources. Existing straggler mitigation techniques wait to detect stragglers and then relaunch them, delaying straggler detection and wasting resources. We built Wrangler (Yadwadkar et al., 2014), a system that predicts when stragglers are going to occur and makes scheduling decisions to avoid such situations. To capture node and workload variability, Wrangler built separate models for every node and workload, requiring the time-consuming collection of substantial training data. In this paper, we propose multi- task learning formulations that share information between the various models, allowing us to use less training data and bring training time down from 4 hours to 40 minutes. Unlike naive multi-task learning formulations, our formulations capture the shared structure in our data, improving generalization performance on limited data. Finally, we extend these formulations using group sparsity inducing norms to automatically discover the similarities between tasks and improve interpretability.",
        "keywords": [],
        "author": [
            "Neeraja J. Yadwadkar",
            "Bharath Hariharan",
            "Joseph E. Gonzalez",
            "R",
            "y Katz"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-149/15-149.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation",
        "abstract": "Despite tremendous progress in computer vision, there has not been an attempt to apply machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's Picture Archiving and Communication System. With natural language processing, we mine a collection of 216K representative two-dimensional images selected by clinicians for diagnostic reference and match the images with their descriptions in an automated manner. We then employ a weakly supervised approach using all of our available data to build models for generating approximate interpretations of patient images. Finally, we demonstrate a more strictly supervised approach to detect the presence and absence of a number of frequent disease types, providing more specific interpretations of patient scans. A relatively small amount of data is used for this part, due to the challenge in gathering quality labels from large raw text data. Our work shows the feasibility of large-scale learning and prediction in electronic patient records available in most modern clinical institutions. It also demonstrates the trade-offs to consider in designing machine learning systems for analyzing large medical data.",
        "keywords": [
            "Deep learning",
            "Convolutional Neural Networks",
            "Topic Models",
            "Natural Lan-     guage Processing",
            ""
        ],
        "author": [
            "Hoo-Chang Shin",
            "Le Lu",
            "Lauren Kim",
            "Ari Seff",
            "Jianhua Yao",
            "Ronald M. Summers"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-176/15-176.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Distribution-Matching Embedding for Visual Domain Adaptation",
        "abstract": "Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Distribution-Matching Embedding approach: An unsupervised domain adaptation method that overcomes this issue by mapping the data to a latent space where the distance between the empirical distributions of the source and target examples is minimized. In other words, we seek to extract the information that is invariant across the source and target data. In particular, we study two different distances to compare the source and target distributions: the Maximum Mean Discrepancy and the Hellinger distance. Furthermore, we show that our approach allows us to learn either a linear embedding, or a nonlinear one. We demonstrate the benefits of our approach on the tasks of visual object recognition, text categorization, and WiFi localization.",
        "keywords": [
            "Domain Adaptation",
            "Maximum Mean Discrepancy",
            "Hellinger Distance",
            "Distribution     Matching",
            ""
        ],
        "author": [
            "Mahsa Baktashmotlagh",
            "Mehrtash Har",
            "i",
            "Mathieu Salzmann"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-207/15-207.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Monotonic Calibrated Interpolated Look-Up Tables",
        "abstract": "Real-world machine learning applications may have requirements beyond accuracy, such as fast evaluation times and interpretability. In particular, guaranteed monotonicity of the learned function with respect to some of the inputs can be critical for user confidence. We propose meeting these goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables. We extend the structural risk minimization framework of lattice regression to monotonic functions by adding linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, at the cost of making the objective non-convex. We address large- scale learning through parallelization, mini-batching, and random sampling of additive regularizer terms. Case studies on real-world problems with up to sixteen features and up to hundreds of millions of training samples demonstrate the proposed monotonic functions can achieve state-of-the-art accuracy in practice while providing greater transparency to users.",
        "keywords": [
            "interpretability",
            "interpolation",
            "look-up tables",
            ""
        ],
        "author": [
            "Maya Gupta",
            "Andrew Cotter",
            "Jan Pfeifer",
            "Konstantin Voevodski",
            "Kevin Canini",
            "Alexander Mangylov",
            "Wojciech Moczydlowski",
            "Alexander van Esbroeck"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-243/15-243.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Are Random Forests Truly the Best Classifiers?",
        "abstract": "The JMLR study Do we need hundreds of classifiers to solve real world classification problems? benchmarks 179 classifiers in 17 families on 121 data sets from the UCI repository and claims that âthe random forest is clearly the best family of classifierâ. In this response, we show that the study's results are biased by the lack of a held-out test set and the exclusion of trials with errors. Further, the study's own statistical tests indicate that random forests do not have significantly higher percent accuracy than support vector machines and neural networks, calling into question the conclusion that random forests are the best classifiers.",
        "keywords": [
            "classification",
            "benchmarking",
            "random forests",
            "support vector machines",
            ""
        ],
        "author": [
            "Michael Wainberg",
            "Babak Alipanahi",
            "Brendan J. Frey"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-374/15-374.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Minimax Adaptive Estimation of Nonparametric Hidden Markov Models",
        "abstract": "We consider stationary hidden Markov models with finite state space and nonparametric modeling of the emission distributions. It has remained unknown until very recently that such models are identifiable. In this paper, we propose a new penalized least- squares estimator for the emission distributions which is statistically optimal and practically tractable. We prove a non asymptotic oracle inequality for our nonparametric estimator of the emission distributions. A consequence is that this new estimator is rate minimax adaptive up to a logarithmic term. Our methodology is based on projections of the emission distributions onto nested subspaces of increasing complexity. The popular spectral estimators are unable to achieve the optimal rate but may be used as initial points in our procedure. Simulations are given that show the improvement obtained when applying the least-squares minimization consecutively to the spectral estimation.",
        "keywords": [
            "nonparametric estimation",
            "hidden Markov models",
            "minimax adaptive esti-     mation",
            "oracle inequality",
            ""
        ],
        "author": [
            "Yohann De Castro",
            "{\\'E}lisabeth Gassiat",
            "Claire Lacour"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-381/15-381.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Decrypting “Cryptogenic” Epilepsy: Semi-supervised Hierarchical Conditional Random Fields For Detecting Cortical Lesions In MRI-Negative Patients",
        "abstract": "Focal cortical dysplasia (FCD) is the most common cause of pediatric epilepsy and the third most common cause in adults with treatment-resistant epilepsy. Surgical resection of the lesion is the most effective treatment to stop seizures. Technical advances in MRI have revolutionized the diagnosis of FCD, leading to high success rates for resective surgery. However, 45% of histologically confirmed FCD patients have normal MRIs (MRI-negative). Without a visible lesion, the success rate of surgery drops from 66% to 29%. In this work, we cast the problem of detecting potential FCD lesions using MRI scans of MRI-negative patients in an image segmentation framework based on hierarchical conditional random fields (HCRF). We use surface based morphometry to model the cortical surface as a two-dimensional surface which is then segmented at multiple scales to extract superpixels of different sizes. Each superpixel is assigned an outlier score by comparing it to a control population. The lesion is detected by fusing the outlier probabilities across multiple scales using a tree- structured HCRF. The proposed method achieves a higher detection rate, with superior recall and precision on a sample of twenty MRI-negative FCD patients as compared to a baseline across four morphological features and their combinations.",
        "keywords": [],
        "author": [
            "Bilal Ahmed",
            "Thomas Thesen",
            "Karen E. Blackmon",
            "Ruben Kuzniekcy",
            "Orrin Devinsky",
            "Carla E. Brodley"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-428/15-428.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Fused Lasso Approach in Regression Coefficients Clustering -- Learning Parameter Heterogeneity in Data Integration",
        "abstract": "As data sets of related studies become more easily accessible, combining data sets of similar studies is often undertaken in practice to achieve a larger sample size and higher power. A major challenge arising from data integration pertains to data heterogeneity in terms of study population, study design, or study coordination. Ignoring such heterogeneity in data analysis may result in biased estimation and misleading inference. Traditional techniques of remedy to data heterogeneity include the use of interactions and random effects, which are inferior to achieving desirable statistical power or providing a meaningful interpretation, especially when a large number of smaller data sets are combined. In this paper, we propose a regularized fusion method that allows us to identify and merge inter-study homogeneous parameter clusters in regression analysis, without the use of hypothesis testing approach. Using the fused lasso, we establish a computationally efficient procedure to deal with large-scale integrated data. Incorporating the estimated parameter ordering in the fused lasso facilitates computing speed with no loss of statistical power. We conduct extensive simulation studies and provide an application example to demonstrate the performance of the new method with a comparison to the conventional methods.",
        "keywords": [
            "Fused lasso",
            "Data integration",
            "Extended BIC",
            ""
        ],
        "author": [
            "Lu Tang",
            "Peter X.K. Song"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-598/15-598.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "The LRP Toolbox for Artificial Neural Networks",
        "abstract": "The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. With the LRP Toolbox we provide platform-agnostic implementations for explaining the predictions of pre-trained state of the art Caffe networks and stand-alone implementations for fully connected Neural Network models. The implementations for Matlab and python shall serve as a playing field to familiarize oneself with the LRP algorithm and are implemented with readability and transparency in mind. Models and data can be imported and exported using raw text formats, Matlab's .mat files and the .npy format for numpy or plain text.",
        "keywords": [
            "layer-wise relevance propagation",
            "explaining classifiers",
            "deep learning",
            "artifi-     cial neural networks",
            ""
        ],
        "author": [
            "Sebastian Lapuschkin",
            "Alexander Binder",
            "Grégoire Montavon",
            "Klaus-Robert Müller",
            "Wojciech Samek"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-618/15-618.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Equivalence of Graphical Lasso and Thresholding for Sparse Graphs",
        "abstract": "This paper is concerned with the problem of finding a sparse graph capturing the conditional dependence between the entries of a Gaussian random vector, where the only available information is a sample correlation matrix. A popular approach to address this problem is the graphical lasso technique, which employs a sparsity-promoting regularization term. This paper derives a simple condition under which the computationally- expensive graphical lasso behaves the same as the simple heuristic method of thresholding. This condition depends only on the solution of graphical lasso and makes no direct use of the sample correlation matrix or the regularization coefficient. It is proved that this condition is always satisfied if the solution of graphical lasso is close to its first-order Taylor approximation or equivalently the regularization term is relatively large. This condition is tested on several random problems, and it is shown that graphical lasso and the thresholding method lead to highly similar results in the case where a sparse graph is sought. We also conduct two case studies on brain connectivity networks of twenty subjects based on fMRI data and the topology identification of electrical circuits to support the findings of this work on the similarity of graphical lasso and thresholding.",
        "keywords": [
            "Graphical Lasso",
            "Graphical Models",
            "Sparse Graphs",
            "Brain Connectivity     Networks",
            ""
        ],
        "author": [
            "Somayeh Sojoudi"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-013/16-013.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Network That Learns Strassen Multiplication",
        "abstract": "We study neural networks whose only non-linear components are multipliers, to test a new training rule in a context where the precise representation of data is paramount. These networks are challenged to discover the rules of matrix multiplication, given many examples. By limiting the number of multipliers, the network is forced to discover the Strassen multiplication rules. This is the mathematical equivalent of finding low rank decompositions of the  matrix multiplication tensor, . We train these networks with the conservative learning rule, which makes minimal changes to the weights so as to give the correct output for each input at the time the input-output pair is received. Conservative learning needs a few thousand examples to find the rank 7 decomposition of , and  for the rank 23 decomposition of  (the lowest known). High precision is critical, especially for , to discriminate between true decompositions and âborder approximations\".",
        "keywords": [
            "sum-product networks",
            "Strassen multiplication",
            ""
        ],
        "author": [
            "Veit Elser"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-074/16-074.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Revisiting the Nyström Method for Improved Large-scale Machine Learning",
        "abstract": "We reconsider randomized algorithms for the low-rank approximation of symmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods; they characterize the effects of common data preprocessing steps on the performance of these algorithms; and they point to important differences between uniform sampling and nonuniform sampling methods based on leverage scores. In addition, our empirical results illustrate that existing theory is so weak that it does not provide even a qualitative guide to practice. Thus, we complement our empirical results with a suite of worst- case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds---e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error---and they point to future directions to make these algorithms useful in even larger-scale machine learning applications.",
        "keywords": [
            ""
        ],
        "author": [
            "Alex Gittens",
            "Michael W. Mahoney"
        ],
        "ref": "http://jmlr.org/papers/volume17/gittens16a/gittens16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Improving Structure MCMC for Bayesian Networks through Markov Blanket Resampling",
        "abstract": "Algorithms for inferring the structure of Bayesian networks from data have become an increasingly popular method for uncovering the direct and indirect influences among variables in complex systems. A Bayesian approach to structure learning uses posterior probabilities to quantify the strength with which the data and prior knowledge jointly support each possible graph feature. Existing Markov Chain Monte Carlo (MCMC) algorithms for estimating these posterior probabilities are slow in mixing and convergence, especially for large networks. We present a novel Markov blanket resampling (MBR) scheme that intermittently reconstructs the Markov blanket of nodes, thus allowing the sampler to more effectively traverse low-probability regions between local maxima. As we can derive the complementary forward and backward directions of the MBR proposal distribution, the Metropolis-Hastings algorithm can be used to account for any asymmetries in these proposals. Experiments across a range of network sizes show that the MBR scheme outperforms other state- of-the-art algorithms, both in terms of learning performance and convergence rate. In particular, MBR achieves better learning performance than the other algorithms when the number of observations is relatively small and faster convergence when the number of variables in the network is large.",
        "keywords": [
            "probabilistic graphical models",
            "directed acyclic graph",
            "Bayesian inference",
            ""
        ],
        "author": [
            "Chengwei Su",
            "Mark E. Borsuk"
        ],
        "ref": "http://jmlr.org/papers/volume17/su16a/su16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Volumetric Spanners: An Efficient Exploration Basis for Learning",
        "abstract": "Numerous learning problems that contain exploration, such as experiment design, multi-arm bandits, online routing, search result aggregation and many more, have been studied extensively in isolation. In this paper we consider a generic and efficiently computable method for action space exploration based on convex geometry. We define a novel geometric notion of an exploration mechanism with low variance called volumetric spanners, and give efficient algorithms to construct such spanners. We describe applications of this mechanism to the problem of optimal experiment design and the general framework for decision making under uncertainty of bandit linear optimization. For the latter we give efficient and near-optimal regret algorithm over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self- concordant barrier for the underlying set.",
        "keywords": [
            "barycentric spanner",
            "volumetric spanner",
            "linear bandits",
            ""
        ],
        "author": [
            "Elad Hazan",
            "Zohar Karnin"
        ],
        "ref": "http://jmlr.org/papers/volume17/hazan16a/hazan16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels",
        "abstract": "We consider the problem of improving the efficiency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large data sets. These approximate feature maps arise as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use  Quasi-Monte Carlo (QMC) approximations instead, where the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy based on theoretical characterizations of the integration error with respect to a given sequence. We then propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization. Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of classical and adaptive QMC techniques for this problem.",
        "keywords": [],
        "author": [
            "Haim Avron",
            "Vikas Sindhwani",
            "Jiyan Yang",
            "Michael W. Mahoney"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-538/14-538.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Variational Dependent Multi-output Gaussian Process Dynamical Systems",
        "abstract": "This paper presents a dependent multi-output Gaussian process (GP) for modeling complex dynamical systems. The outputs are dependent in this model, which is largely different from previous GP dynamical systems. We adopt convolved multi-output GPs to model the outputs, which are provided with a flexible multi-output covariance function. We adapt the variational inference method with inducing points for learning the model. Conjugate gradient based optimization is used to solve parameters involved by maximizing the variational lower bound of the marginal likelihood. The proposed model has superiority on modeling dynamical systems under the more reasonable assumption and the fully Bayesian learning framework. Further, it can be flexibly extended to handle regression problems. We evaluate the model on both synthetic and real-world data including motion capture data, traffic flow data and robot inverse dynamics data. Various evaluation methods are taken on the experiments to demonstrate the effectiveness of our model, and encouraging results are observed.",
        "keywords": [
            "Gaussian process",
            "variational inference",
            "dynamical system",
            ""
        ],
        "author": [
            "Jing Zhao",
            "Shiliang Sun"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-423/14-423.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Multiple Output Regression with Latent Noise",
        "abstract": "In high-dimensional data, structured noise caused by observed and unobserved factors affecting multiple target variables simultaneously, imposes a serious challenge for modeling, by masking the often weak signal. Therefore, (1) explaining away the structured noise in multiple-output regression is of paramount importance. Additionally, (2) assumptions about the correlation structure of the regression weights are needed. We note that both can be formulated in a natural way in a latent variable model, in which both the interesting signal and the noise are mediated through the same latent factors. Under this assumption, the signal model then borrows strength from the noise model by encouraging similar effects on correlated targets. We introduce a hyperparameter for the latent signal-to-noise ratio which turns out to be important for modelling weak signals, and an ordered infinite-dimensional shrinkage prior that resolves the rotational unidentifiability in reduced-rank regression models. Simulations and prediction experiments with metabolite, gene expression, FMRI measurement, and macroeconomic time series data show that our model equals or exceeds the state-of-the-art performance and, in particular, outperforms the standard approach of assuming independent noise and signal models.",
        "keywords": [],
        "author": [
            "Jussi Gillberg",
            "Pekka Marttinen",
            "Matti Pirinen",
            "Antti J. Kangas",
            "Pasi Soininen",
            "Mehreen Ali",
            "Aki S. Havulinna",
            "Marjo-Riitta Järvelin",
            "Mika Ala-Korpela",
            "Samuel Kaski"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-436/14-436.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "The Constrained Dantzig Selector with Enhanced Consistency",
        "abstract": "The Dantzig selector has received popularity for many applications such as compressed sensing and sparse modeling, thanks to its computational efficiency as a linear programming problem and its nice sampling properties. Existing results show that it can recover sparse signals mimicking the accuracy of the ideal procedure, up to a logarithmic factor of the dimensionality. Such a factor has been shown to hold for many regularization methods. An important question is whether this factor can be reduced to a logarithmic factor of the sample size in ultra-high dimensions under mild regularity conditions. To provide an affirmative answer, in this paper we suggest the constrained Dantzig selector, which has more flexible constraints and parameter space. We prove that the suggested method can achieve convergence rates within a logarithmic factor of the sample size of the oracle rates and improved sparsity, under a fairly weak assumption on the signal strength. Such improvement is significant in ultra-high dimensions. This method can be implemented efficiently through sequential linear programming. Numerical studies confirm that the sample size needed for a certain level of accuracy in these problems can be much reduced.",
        "keywords": [
            "Sparse Modeling",
            "Compressed Sensing",
            "Ultra-high Dimensionality",
            "Dantzig     Selector",
            "Regularization Methods",
            ""
        ],
        "author": [
            "Yinfei Kong",
            "Zemin Zheng",
            "Jinchi Lv"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-513/14-513.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Bootstrap-Based Regularization for Low-Rank Matrix Estimation",
        "abstract": "We develop a flexible framework for low-rank matrix estimation that allows us to transform noise models into regularization schemes via a simple bootstrap algorithm. Effectively, our procedure seeks an autoencoding basis for the observed matrix that is stable with respect to the specified noise model; we call the resulting procedure a stable autoencoder. In the simplest case, with an isotropic noise model, our method is equivalent to a classical singular value shrinkage estimator. For non-isotropic noise models---e.g., Poisson noise---the method does not reduce to singular value shrinkage, and instead yields new estimators that perform well in experiments. Moreover, by iterating our stable autoencoding scheme, we can automatically generate low-rank estimates without specifying the target rank as a tuning parameter.",
        "keywords": [
            "Correspondence analysis",
            "empirical Bayes",
            "Lévy bootstrap",
            ""
        ],
        "author": [
            "Julie Josse",
            "Stefan Wager"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-534/14-534.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models",
        "abstract": "Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.",
        "keywords": [
            "intractable likelihood",
            "latent variables",
            "Bayesian inference",
            "approximate Bayesian     computation",
            ""
        ],
        "author": [
            "Michael U. Gutmann",
            "Jukka Cor",
            "er"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-017/15-017.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "On Lower and Upper Bounds in Smooth and Strongly Convex Optimization",
        "abstract": "We develop a novel framework to study smooth and strongly convex optimization algorithms. Focusing on quadratic functions we are able to examine optimization algorithms as a recursive application of linear operators. This, in turn, reveals a powerful connection between a class of optimization algorithms and the analytic theory of polynomials whereby new lower and upper bounds are derived. Whereas existing lower bounds for this setting are only valid when the dimensionality scales with the number of iterations, our lower bound holds in the natural regime where the dimensionality is fixed. Lastly, expressing it as an optimal solution for the corresponding optimization problem over polynomials, as formulated by our framework, we present a novel systematic derivation of Nesterov's well-known Accelerated Gradient Descent method. This rather natural interpretation of AGD contrasts with earlier ones which lacked a simple, yet solid, motivation.",
        "keywords": [
            "smooth and strongly convex optimization",
            "full gradient descent",
            "accelerated     gradient descent",
            ""
        ],
        "author": [
            "Yossi Arjevani",
            "Shai Shalev-Shwartz",
            "Ohad Shamir"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-106/15-106.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Dual Control for Approximate Bayesian Reinforcement Learning",
        "abstract": "Control of non-episodic, finite-horizon dynamical systems with uncertain dynamics poses a tough and elementary case of the exploration-exploitation trade-off. Bayesian reinforcement learning, reasoning about the effect of actions and future observations, offers a principled solution, but is intractable. We review, then extend an old approximate approach from control theory---where the problem is known as dual control---in the context of modern regression methods, specifically generalized linear regression. Experiments on simulated systems show that this framework offers a useful approximation to the intractable aspects of Bayesian RL, producing structured exploration strategies that differ from standard RL approaches. We provide simple examples for the use of this framework in (approximate) Gaussian process regression and feedforward neural networks for the control of exploration.",
        "keywords": [
            "reinforcement learning",
            "control",
            "Gaussian processes",
            "filtering",
            ""
        ],
        "author": [
            "Edgar D. Klenske",
            "Philipp Hennig"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-162/15-162.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Multiple-Instance Learning from Distributions",
        "abstract": "We propose a new theoretical framework for analyzing the multiple-instance learning (MIL) setting. In MIL, training examples are provided to a learning algorithm in the form of labeled sets, or \"bags,\" of instances. Applications of MIL include 3-D quantitative structure--activity relationship prediction for drug discovery and content-based image retrieval for web search. The goal of an algorithm is to learn a function that correctly labels new bags or a function that correctly labels new instances. We propose that bags should be treated as latent distributions from which samples are observed. We show that it is possible to learn accurate instance- and bag-labeling functions in this setting as well as functions that correctly rank bags or instances under weak assumptions. Additionally, our theoretical results suggest that it is possible to learn to rank efficiently using traditional, well-studied \"supervised\" learning approaches. We perform an extensive empirical evaluation that supports the theoretical predictions entailed by the new framework. The proposed theoretical framework leads to a better understanding of the relationship between the MI and standard supervised learning settings, and it provides new methods for learning from MI data that are more accurate, more efficient, and have better understood theoretical properties than existing MI-specific algorithms.",
        "keywords": [
            "multiple-instance learning",
            "learning theory",
            "ranking",
            ""
        ],
        "author": [
            "Gary Doran",
            "Soumya Ray"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-171/15-171.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "An Online Convex Optimization Approach to Blackwell's Approachability",
        "abstract": "The problem of approachability in repeated games with vector payoffs was introduced by Blackwell in the 1950s, along with geometric conditions and corresponding approachability strategies that rely on computing a sequence of direction vectors in the payoff space. For convex target sets, these vectors are obtained as projections from the current average payoff vector to the set. A recent paper by Abernethy, Batlett and Hazan (2011) proposed a class of approachability algorithms that rely on Online Linear Programming for obtaining alternative sequences of direction vectors. This is first implemented for target sets that are convex cones, and then generalized to any convex set by embedding it in a higher-dimensional convex cone. In this paper we present a more direct formulation that relies on general Online Convex Optimization (OCO) algorithms, along with basic properties of the support function of convex sets. This leads to a general class of approachability algorithms, depending on the choice of the OCO algorithm and the used norms. Blackwell's original algorithm and its convergence are recovered when Follow The Leader (or a regularized version thereof) is used for the OCO algorithm.",
        "keywords": [
            "approachability",
            "online convex optimization",
            ""
        ],
        "author": [
            "Nahum Shimkin"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-339/15-339.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Well-Conditioned and Sparse Estimation of Covariance and Inverse Covariance Matrices Using a Joint Penalty",
        "abstract": "We develop a method for estimating well-conditioned and sparse covariance and inverse covariance matrices from a sample of vectors drawn from a sub-Gaussian distribution in high dimensional setting. The proposed estimators are obtained by minimizing the quadratic loss function and joint penalty of  norm and variance of its eigenvalues. In contrast to some of the existing methods of covariance and inverse covariance matrix estimation, where often the interest is to estimate a sparse matrix, the proposed method is flexible in estimating both a sparse and well-conditioned covariance matrix simultaneously. The proposed estimators are optimal in the sense that they achieve the mini-max rate of estimation in operator norm for the underlying class of covariance and inverse covariance matrices. We give a very fast algorithm for computation of these covariance and inverse covariance matrices which is easily scalable to large scale data analysis problems. The simulation study for varying sample sizes and variables shows that the proposed estimators performs better than several other estimators for various choices of structured covariance and inverse covariance matrices. We also use our proposed estimator for tumor tissues classification using gene expression data and compare its performance with some other classification methods.",
        "keywords": [
            "Sparsity",
            "Eigenvalue Penalty",
            ""
        ],
        "author": [
            "Ashwini Maurya"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-345/15-345.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "String and Membrane Gaussian Processes",
        "abstract": "In this paper we introduce a novel framework for making exact nonparametric Bayesian inference on latent functions that is particularly suitable for Big Data tasks. Firstly, we introduce a class of stochastic processes we refer to as string Gaussian processes (string GPs which are not to be mistaken for Gaussian processes operating on text). We construct string GPs so that their finite- dimensional marginals exhibit suitable local conditional independence structures, which allow for scalable, distributed, and flexible nonparametric Bayesian inference, without resorting to approximations, and while ensuring some mild global regularity constraints. Furthermore, string GP priors naturally cope with heterogeneous input data, and the gradient of the learned latent function is readily available for explanatory analysis. Secondly, we provide some theoretical results relating our approach to the standard GP paradigm. In particular, we prove that some string GPs are Gaussian processes, which provides a complementary global perspective on our framework. Finally, we derive a scalable and distributed MCMC scheme for supervised learning tasks under string GP priors. The proposed MCMC scheme has computational time complexity  and memory requirement , where  is the data size and  the dimension of the input space. We illustrate the efficacy of the proposed approach on several synthetic and real-world data sets, including a data set with  millions input points and  attributes.",
        "keywords": [
            "String Gaussian processes",
            "scalable Bayesian nonparametrics",
            "Gaussian processes",
            "nonstationary kernels",
            "reversible-jump MCMC",
            ""
        ],
        "author": [
            "Yves-Laurent Kom Samo",
            "Stephen J. Roberts"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-382/15-382.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Extracting PICO Sentences from Clinical Trial Reports using Supervised Distant Supervision",
        "abstract": "Systematic reviews underpin Evidence Based Medicine (EBM) by addressing precise clinical questions via comprehensive synthesis of all relevant published evidence. Authors of systematic reviews typically define a Population/Problem, Intervention, Comparator, and Outcome (a PICO criteria) of interest, and then retrieve, appraise and synthesize results from all reports of clinical trials that meet these criteria. Identifying PICO elements in the full-texts of trial reports is thus a critical yet time-consuming step in the systematic review process. We seek to expedite evidence synthesis by developing machine learning models to automatically extract sentences from articles relevant to PICO elements. Collecting a large corpus of training data for this task would be prohibitively expensive. Therefore, we derive distant supervision (DS) with which to train models using previously conducted reviews. DS entails heuristically deriving 'soft' labels from an available structured resource. However, we have access only to unstructured, free-text summaries of PICO elements for corresponding articles; we must derive from these the desired sentence-level annotations. To this end, we propose a novel method -- supervised distant supervision (SDS) -- that uses a small amount of direct supervision to better exploit a large corpus of distantly labeled instances by learning to pseudo-annotate articles using the available DS. We show that this approach tends to outperform existing methods with respect to automated PICO extraction.",
        "keywords": [],
        "author": [
            "Byron C. Wallace",
            "Joël Kuiper",
            "Aakash Sharma",
            "Mingxi (Brian) Zhu",
            "Iain J. Marshall"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-404/15-404.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Cross-Corpora Unsupervised Learning of Trajectories in Autism Spectrum Disorders",
        "abstract": "",
        "keywords": [
            "Disease progression model",
            ""
        ],
        "author": [
            "Huseyin Melih Elibol",
            "Vincent Nguyen",
            "Scott Linderman",
            "Matthew Johnson",
            "Amna Hashmi",
            "Finale Doshi-Velez"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-431/15-431.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Adjusting for Chance Clustering Comparison Measures",
        "abstract": "Adjusted for chance measures are widely used to compare partitions/clusterings of the same data set. In particular, the Adjusted Rand Index (ARI) based on pair-counting, and the Adjusted Mutual Information (AMI) based on Shannon information theory are very popular in the clustering community. Nonetheless it is an open problem as to what are the best application scenarios for each measure and guidelines in the literature for their usage are sparse, with the result that users often resort to using both. Generalized Information Theoretic (IT) measures based on the Tsallis entropy have been shown to link pair- counting and Shannon IT measures. In this paper, we aim to bridge the gap between adjustment of measures based on pair- counting and measures based on information theory. We solve the key technical challenge of analytically computing the expected value and variance of generalized IT measures. This allows us to propose adjustments of generalized IT measures, which reduce to well known adjusted clustering comparison measures as special cases. Using the theory of generalized IT measures, we are able to propose the following guidelines for using ARI and AMI as external validation indices: ARI should be used when the reference clustering has large equal sized clusters; AMI should be used when the reference clustering is unbalanced and there exist small clusters.",
        "keywords": [
            "Clustering Comparison",
            "Clustering Validation",
            "Adjustment for Chance",
            "Gen-     eralized Information Theoretic Measures",
            ""
        ],
        "author": [
            "Simone Romano",
            "Nguyen Xuan Vinh",
            "James Bailey",
            "Karin Verspoor"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-627/15-627.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Refined Error Bounds for Several Learning Algorithms",
        "abstract": "This article studies the achievable guarantees on the error rates of certain learning algorithms, with particular focus on refining logarithmic factors. Many of the results are based on a general technique for obtaining bounds on the error rates of sample-consistent classifiers with monotonic error regions, in the realizable case. We prove bounds of this type expressed in terms of either the VC dimension or the sample compression size. This general technique also enables us to derive several new bounds on the error rates of general sample-consistent learning algorithms, as well as refined bounds on the label complexity of the CAL active learning algorithm. Additionally, we establish a simple necessary and sufficient condition for the existence of a distribution-free bound on the error rates of all sample- consistent learning rules, converging at a rate inversely proportional to the sample size. We also study learning in the presence of classification noise, deriving a new excess error rate guarantee for general VC classes under Tsybakov's noise condition, and establishing a simple and general necessary and sufficient condition for the minimax excess risk under bounded noise to converge at a rate inversely proportional to the sample size.",
        "keywords": [
            "sample complexity",
            "PAC learning",
            "statistical learning theory",
            "active learning",
            ""
        ],
        "author": [
            "Steve Hanneke"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-655/15-655.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Synergy of Monotonic Rules",
        "abstract": "",
        "keywords": [
            "conditional probability",
            "synergy",
            "ensemble learning",
            "intelligent teacher",
            "priv-     ileged information",
            "knowledge transfer",
            "support vector machines",
            ""
        ],
        "author": [
            "Vladimir Vapnik",
            "Rauf Izmailov"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-137/16-137.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic Differentiation",
        "abstract": "",
        "keywords": [
            "Riemannian optimization",
            "non-convex optimization",
            "manifold optimization",
            "projec-     tion matrices",
            "symmetric matrices",
            "rotation matrices",
            ""
        ],
        "author": [
            "James Townsend",
            "Niklas Koep",
            "Sebastian Weichwald"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-177/16-177.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "CrossCat: A Fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data",
        "abstract": "There is a widespread need for statistical methods that can analyze high-dimensional datasets without imposing restrictive or opaque modeling assumptions. This paper describes a domain- general data analysis method called CrossCat. CrossCat infers multiple non-overlapping views of the data, each consisting of a subset of the variables, and uses a separate nonparametric mixture to model each view. CrossCat is based on approximately Bayesian inference in a hierarchical, nonparametric model for data tables. This model consists of a Dirichlet process mixture over the columns of a data table in which each mixture component is itself an independent Dirichlet process mixture over the rows; the inner mixture components are simple parametric models whose form depends on the types of data in the table. CrossCat combines strengths of mixture modeling and Bayesian network structure learning. Like mixture modeling, CrossCat can model a broad class of distributions by positing latent variables, and produces representations that can be efficiently conditioned and sampled from for prediction. Like Bayesian networks, CrossCat represents the dependencies and independencies between variables, and thus remains accurate when there are multiple statistical signals. Inference is done via a scalable Gibbs sampling scheme; this paper shows that it works well in practice. This paper also includes empirical results on heterogeneous tabular data of up to 10 million cells, such as hospital cost and quality measures, voting records, unemployment rates, gene expression measurements, and images of handwritten digits. CrossCat infers structure that is consistent with accepted findings and common-sense knowledge in multiple domains and yields predictive accuracy competitive with generative, discriminative, and model-free alternatives.",
        "keywords": [
            "Bayesian nonparametrics",
            "Dirichlet processes",
            "Markov chain Monte Carlo",
            "multivari-     ate analysis",
            "structure learning",
            "unsupervised learning",
            ""
        ],
        "author": [
            "Vikash Mansinghka",
            "Patrick Shafto",
            "Eric Jonas",
            "Cap Petschulat",
            "Max Gasner",
            "Joshua B. Tenenbaum"
        ],
        "ref": "http://jmlr.org/papers/volume17/11-392/11-392.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Regularized Policy Iteration with Nonparametric Function Spaces",
        "abstract": "We study two regularization-based approximate policy iteration algorithms, namely REG-LSPI and REG-BRM, to solve reinforcement learning and planning problems in discounted Markov Decision Processes with large state and finite action spaces. The core of these algorithms are the regularized extensions of the Least- Squares Temporal Difference (LSTD) learning and Bellman Residual Minimization (BRM), which are used in the algorithms' policy evaluation steps. Regularization provides a convenient way to control the complexity of the function space to which the estimated value function belongs and as a result enables us to work with rich nonparametric function spaces. We derive efficient implementations of our methods when the function space is a reproducing kernel Hilbert space. We analyze the statistical properties of REG-LSPI and provide an upper bound on the policy evaluation error and the performance loss of the policy returned by this method. Our bound shows the dependence of the loss on the number of samples, the capacity of the function space, and some intrinsic properties of the underlying Markov Decision Process. The dependence of the policy evaluation bound on the number of samples is minimax optimal. This is the first work that provides such a strong guarantee for a nonparametric approximate policy iteration algorithm. (This work is an extension of the NIPS 2008 conference paper by Farahmand et al. (2009b).)",
        "keywords": [
            "reinforcement learning",
            "approximate policy iteration",
            "regularization",
            "non-     parametric method",
            ""
        ],
        "author": [
            "Amir-massoud Farahm",
            "",
            "Mohammad Ghavamzadeh",
            "Csaba Szepesvári",
            "Shie Mannor"
        ],
        "ref": "http://jmlr.org/papers/volume17/13-016/13-016.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Multiscale Adaptive Representation of Signals: I. The Basic Framework",
        "abstract": "We introduce a framework for designing multi-scale, adaptive, shift-invariant frames and bi-frames for representing signals. The new framework, called AdaFrame, improves over dictionary learning-based techniques in terms of computational efficiency at inference time. It improves classical multi-scale basis such as wavelet frames in terms of coding efficiency. It provides an attractive alternative to dictionary learning-based techniques for low level signal processing tasks, such as compression and denoising, as well as high level tasks, such as feature extraction for object recognition. Connections with deep convolutional networks are also discussed. In particular, the proposed framework reveals a drawback in the commonly used approach for visualizing the activations of the intermediate layers in convolutional networks, and suggests a natural alternative.",
        "keywords": [
            "AdaFrame",
            "Dictionary Learning",
            ""
        ],
        "author": [
            "Cheng Tai",
            "Weinan E"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-072/15-072.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Sparse PCA via Covariance Thresholding",
        "abstract": "",
        "keywords": [],
        "author": [
            "Yash Deshp",
            "e",
            "Andrea Montanari"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-160/15-160.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Large Scale Visual Recognition through Adaptation using Joint Representation and Multiple Instance Learning",
        "abstract": "A major barrier towards scaling visual recognition systems is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) trained used 1.2M+ labeled images have emerged as clear winners on object classification benchmarks. Unfortunately, only a small fraction of those labels are available with bounding box localization for training the detection task and even fewer pixel level annotations are available for semantic segmentation. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect scene-centric images with precisely localized labels. We develop methods for learning large scale recognition models which exploit joint training over both weak (image-level) and strong (bounding box) labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. We provide a novel formulation of a joint multiple instance learning method that includes examples from object-centric data with image-level labels when available, and also performs domain transfer learning to improve the underlying detector representation. We then show how to use our large scale detectors to produce pixel level annotations. Using our method, we produce a 7.6K category detector and release code and models at lsda.berkeley vision.org.",
        "keywords": [
            "Computer Vision",
            "Deep Learning",
            "Transfer Learning",
            ""
        ],
        "author": [
            "Judy Hoffman",
            "Deepak Pathak",
            "Eric Tzeng",
            "Jonathan Long",
            "Sergio Guadarrama",
            "Trevor Darrell",
            "Kate Saenko"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-223/15-223.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Covariance-based Clustering in Multivariate and Functional Data Analysis",
        "abstract": "In this paper we propose a new algorithm to perform clustering of multivariate and functional data. We study the case of two populations different in their covariances, rather than in their means. The algorithm relies on a proper quantification of distance between the estimated covariance operators of the populations, and subdivides data in two groups maximising the distance between their induced covariances. The naive implementation of such an algorithm is computationally forbidding, so we propose a heuristic formulation with a much lighter complexity and we study its convergence properties, along with its computational cost. We also propose to use an enhanced estimator for the estimation of discrete covariances of functional data, namely a linear shrinkage estimator, in order to improve the precision of the clustering. We establish the effectiveness of our algorithm through applications to both synthetic data and a real data set coming from a biomedical context, showing also how the use of shrinkage estimation may lead to substantially better results.",
        "keywords": [
            "Clustering",
            "covariance operator",
            "operator distance",
            "shrinkage estimation",
            ""
        ],
        "author": [
            "Francesca Ieva",
            "Anna Maria Paganoni",
            "Nicholas Tarabelloni"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-568/15-568.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "MOCCA: Mirrored Convex/Concave Optimization for Nonconvex Composite Functions",
        "abstract": "Many optimization problems arising in high-dimensional statistics decompose naturally into a sum of several terms, where the individual terms are relatively simple but the composite objective function can only be optimized with iterative algorithms. In this paper, we are interested in optimization problems of the form , where  is a fixed linear transformation, while  and  are functions that may be nonconvex and/or nondifferentiable. In particular, if either of the terms are nonconvex, existing alternating minimization techniques may fail to converge; other types of existing approaches may instead be unable to handle nondifferentiability. We propose the MOCCA (mirrored convex/concave) algorithm, a primal/dual optimization approach that takes a local convex approximation to each term at every iteration. Inspired by optimization problems arising in computed tomography (CT) imaging, this algorithm can handle a range of nonconvex composite optimization problems, and offers theoretical guarantees for convergence when the overall problem is approximately convex (that is, any concavity in one term is balanced out by convexity in the other term). Empirical results show fast convergence for several structured signal recovery problems.",
        "keywords": [
            "MOCCA",
            "ADMM",
            "nonconvex",
            "penalized likelihood",
            "total variation",
            ""
        ],
        "author": [
            "Rina Foygel Barber",
            "Emil Y. Sidky"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-583/15-583.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "True Online Temporal-Difference Learning",
        "abstract": "The temporal-difference methods TD() and Sarsa() form a core part of modern reinforcement learning. Their appeal comes from their good performance, low computational cost, and their simple interpretation, given by their forward view. Recently, new versions of these methods were introduced, called true online TD() and true online Sarsa(), respectively (van Seijen & Sutton, 2014). Algorithmically, these true online methods only make two small changes to the update rules of the regular methods, and the extra computational cost is negligible in most cases. However, they follow the ideas underlying the forward view much more closely. In particular, they maintain an exact equivalence with the forward view at all times, whereas the traditional versions only approximate it for small step-sizes. We hypothesize that these true online methods not only have better theoretical properties, but also dominate the regular methods empirically. In this article, we put this hypothesis to the test by performing an extensive empirical comparison. Specifically, we compare the performance of true online TD()/Sarsa() with regular TD()/Sarsa() on random MRPs, a real-world myoelectric prosthetic arm, and a domain from the Arcade Learning Environment. We use linear function approximation with tabular, binary, and non-binary features. Our results suggest that the true online methods indeed dominate the regular methods. Across all domains/representations the learning speed of the true online methods are often better, but never worse than that of the regular methods. An additional advantage is that no choice between traces has to be made for the true online methods. Besides the empirical results, we provide an in-dept analysis of the theory behind true online temporal-difference learning. In addition, we show that new true online temporal- difference methods can be derived by making changes to the online forward view and then rewriting the update equations.",
        "keywords": [
            "temporal-difference learning",
            "eligibility traces",
            "forward-view equivalencec 2016 Harm van Seijen",
            ""
        ],
        "author": [
            "Harm van Seijen",
            "A. Rupam Mahmood",
            "Patrick M. Pilarski",
            "Marlos C. Machado",
            "Richard S. Sutton"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-599/15-599.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Penalized Maximum Likelihood Estimation of Multi-layered Gaussian Graphical Models",
        "abstract": "Analyzing multi-layered graphical models provides insight into understanding the conditional relationships among nodes within layers after adjusting for and quantifying the effects of nodes from other layers. We obtain the penalized maximum likelihood estimator for Gaussian multi-layered graphical models, based on a computational approach involving screening of variables, iterative estimation of the directed edges between layers and undirected edges within layers and a final refitting and stability selection step that provides improved performance in finite sample settings. We establish the consistency of the estimator in a high-dimensional setting. To obtain this result, we develop a strategy that leverages the biconvexity of the likelihood function to ensure convergence of the developed iterative algorithm to a stationary point, as well as careful uniform error control of the estimates over iterations. The performance of the maximum likelihood estimator is illustrated on synthetic data.",
        "keywords": [
            "graphical models",
            "penalized likelihood",
            "block coordinate descent",
            "conver-     gence",
            ""
        ],
        "author": [
            "Jiahe Lin",
            "Sumanta Basu",
            "Moulinath Banerjee",
            "George Michailidis"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-004/16-004.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Local Network Community Detection with Continuous Optimization of Conductance and Weighted Kernel K-Means",
        "abstract": "Local network community detection is the task of finding a single community of nodes concentrated around few given seed nodes in a localized way. Conductance is a popular objective function used in many algorithms for local community detection. This paper studies a continuous relaxation of conductance. We show that continuous optimization of this objective still leads to discrete communities. We investigate the relation of conductance with weighted kernel k-means for a single community, which leads to the introduction of a new objective function, -conductance. Conductance is obtained by setting  to . Two algorithms, EMc and PGDc, are proposed to locally optimize -conductance and automatically tune the parameter . They are based on expectation maximization and projected gradient descent, respectively. We prove locality and give performance guarantees for EMc and PGDc for a class of dense and well separated communities centered around the seeds. Experiments are conducted on networks with ground-truth communities, comparing to state-of-the-art graph diffusion algorithms for conductance optimization. On large graphs, results indicate that EMc and PGDc stay localized and produce communities most similar to the ground, while graph diffusion algorithms generate large communities of lower quality. (Source code of the algorithms used in the paper is available   online.)",
        "keywords": [
            "community detection",
            "conductance",
            ""
        ],
        "author": [
            "Twan van Laarhoven",
            "Elena Marchiori"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-043/16-043.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Megaman: Scalable Manifold Learning in Python",
        "abstract": "Manifold Learning (ML) is a class of algorithms seeking a low-dimensional non-linear representation of high-dimensional data. Thus, ML algorithms are most applicable to high- dimensional data and require large sample sizes to accurately estimate the manifold. Despite this, most existing manifold learning implementations are not particularly scalable. Here we present a Python package that implements a variety of manifold learning algorithms in a modular and scalable fashion, using fast approximate neighbors searches and fast sparse eigendecompositions. The package incorporates theoretical advances in manifold learning, such as the unbiased Laplacian estimator introduced by Coifman and Lafon (2006) and the estimation of the embedding distortion by the Riemannian metric method introduced by Perrault-Joncas and Meila (2013). In benchmarks, even on a single-core desktop computer, our code embeds millions of data points in minutes, and takes just 200 minutes to embed the main sample of galaxy spectra from the Sloan Digital Sky Survey--- consisting of 0.6 million samples in 3750-dimensions---a task which has not previously been possible.",
        "keywords": [
            "manifold learning",
            "dimension reduction",
            "Riemannian metric",
            "graph embed-     ding",
            "scalable methods",
            ""
        ],
        "author": [
            "James McQueen",
            "Marina Meilă",
            "Jacob VanderPlas",
            "Zhongyue Zhang"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-109/16-109.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Kernel Estimation and Model Combination in A Bandit Problem with Covariates",
        "abstract": "Multi-armed bandit problem is an important optimization game that requires an exploration-exploitation tradeoff to achieve optimal total reward. Motivated from industrial applications such as online advertising and clinical research, we consider a setting where the rewards of bandit machines are associated with covariates, and the accurate estimation of the corresponding mean reward functions plays an important role in the performance of allocation rules. Under a flexible problem setup, we establish asymptotic strong consistency and perform a finite- time regret analysis for a sequential randomized allocation strategy based on kernel estimation. In addition, since many nonparametric and parametric methods in supervised learning may be applied to estimating the mean reward functions but guidance on how to choose among them is generally unavailable, we propose a model combining allocation strategy for adaptive performance. Simulations and a real data evaluation are conducted to illustrate the performance of the proposed allocation strategy.",
        "keywords": [
            "contextual bandit problem",
            "exploration-exploitation tradeoff",
            "nonparametric     regression",
            "regret bound",
            ""
        ],
        "author": [
            "Wei Qian",
            "Yuhong Yang"
        ],
        "ref": "http://jmlr.org/papers/volume17/13-210/13-210.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A General Framework for Consistency of Principal Component Analysis",
        "abstract": "A general asymptotic framework is developed for studying consistency properties of principal component analysis (PCA). Our framework includes several previously studied domains of asymptotics as special cases and allows one to investigate interesting connections and transitions among the various domains. More importantly, it enables us to investigate asymptotic scenarios that have not been considered before, and gain new insights into the consistency, subspace consistency and strong inconsistency regions of PCA and the boundaries among them. We also establish the corresponding convergence rate within each region. Under general spike covariance models, the dimension (or number of variables) discourages the consistency of PCA, while the sample size and spike information (the relative size of the population eigenvalues) encourage PCA consistency. Our framework nicely illustrates the relationship among these three types of information in terms of dimension, sample size and spike size, and rigorously characterizes how their relationships affect PCA consistency.",
        "keywords": [
            "High dimension low sample size",
            "PCA",
            "Random matrix",
            ""
        ],
        "author": [
            "Dan Shen",
            "Haipeng Shen",
            "J. S. Marron"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-229/14-229.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Conditional Independencies under the Algorithmic Independence of Conditionals",
        "abstract": "In this paper we analyze the relationship between faithfulness and the more recent condition of algorithmic Independence of Conditionals (IC) with respect to the Conditional Independencies (CIs) they allow. Both conditions have been extensively used for causal inference by refuting factorizations for which the condition does not hold. Violation of faithfulness happens when there are CIs that do not follow from the Markov condition. For those CIs, non-trivial constraints among some parameters of the Conditional Probability Distributions (CPDs) must hold. When such a constraint is defined over parameters of different CPDs, we prove that IC is also violated unless the parameters have a simple description. To understand which non-Markovian CIs are permitted we define a new condition closely related to IC: the Independence from Product Constraints (IPC). The condition reflects that CIs might be the result of specific parameterizations of individual CPDs but not from constraints on parameters of different CPDs. In that sense it is more restrictive than IC: parameters may have a simple description. On the other hand, IC also excludes other forms of algorithmic dependencies between CPDs. Finally, we prove that on top of the CIs permitted by the Markov condition (faithfulness), IPC allows non-minimality, deterministic relations and what we called proportional CPDs. These are the only cases in which a CI follows from a specific parameterization of a single CPD.",
        "keywords": [
            "faithfulness",
            "causality",
            "independence of conditionals",
            ""
        ],
        "author": [
            "Jan Lemeire"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-450/14-450.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Learning Theory for Distribution Regression",
        "abstract": "We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the  one-stage sampled minimax optimal rate (Caponnetto and De Vito, 2007; Steinwart et al., 2009). This result answers a -year-old open question, establishing the consistency of the classical set kernel (Haussler, 1999; GÃ¤rtner et al., 2002) in regression. We also cover consistency for more recent kernels on distributions, including those due to Christmann and Steinwart (2010).",
        "keywords": [],
        "author": [
            "Zoltán Szabó",
            "Bharath K. Sriperumbudur",
            "Barnabás Póczos",
            "Arthur Gretton"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-510/14-510.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights",
        "abstract": "We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.",
        "keywords": [
            ""
        ],
        "author": [
            "Weijie Su",
            "Stephen Boyd",
            "Emmanuel J. Candès"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-084/15-084.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Importance Weighting Without Importance Weights: An Efficient Algorithm for Combinatorial Semi-Bandits",
        "abstract": "We propose a sample-efficient alternative for importance weighting for situations where one only has sample access to the probability distribution that generates the observations. Our new method, called Geometric Resampling (GR), is described and analyzed in the context of online combinatorial optimization under semi-bandit feedback, where a learner sequentially selects its actions from a combinatorial decision set so as to minimize its cumulative loss. In particular, we show that the well-known Follow-the-Perturbed-Leader (FPL) prediction method coupled with Geometric Resampling yields the first computationally efficient reduction from offline to online optimization in this setting. We provide a thorough theoretical analysis for the resulting algorithm, showing that its performance is on par with previous, inefficient solutions. Our main contribution is showing that, despite the relatively large variance induced by the GR procedure, our performance guarantees hold with high probability rather than only in expectation. As a side result, we also improve the best known regret bounds for FPL in online combinatorial optimization with full feedback, closing the perceived performance gap between FPL and exponential weights in this setting. (A preliminary version of this paper was published as Neu and BartÃ³k (2013). Parts of this work were completed while Gergely Neu was with the SequeL team at INRIA Lille -- Nord Europe, France and GÃ¡bor BartÃ³k was with the Department of Computer Science at ETH ZÃ¼rich.)",
        "keywords": [
            "online learning",
            "combinatorial optimization",
            "bandit problems",
            "semi-bandit feedback",
            "follow the perturbed leader",
            ""
        ],
        "author": [
            "Gergely Neu",
            "Gábor Bartók"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-091/15-091.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "New Perspectives on k-Support and Cluster Norms",
        "abstract": "We study a regularizer which is defined as a parameterized infimum of quadratics, and which we call the box-norm. We show that the -support norm, a regularizer proposed by Argyriou et al. (2012) for sparse vector prediction problems, belongs to this family, and the box-norm can be generated as a perturbation of the former. We derive an improved algorithm to compute the proximity operator of the squared box-norm, and we provide a method to compute the norm. We extend the norms to matrices, introducing the spectral -support norm and spectral box-norm. We note that the spectral box-norm is essentially equivalent to the cluster norm, a multitask learning regularizer introduced by Jacob et al. (2009a), and which in turn can be interpreted as a perturbation of the spectral -support norm. Centering the norm is important for multitask learning and we also provide a method to use centered versions of the norms as regularizers. Numerical experiments indicate that the spectral -support and box-norms and their centered variants provide state of the art performance in matrix completion and multitask learning problems respectively.",
        "keywords": [
            "Convex optimization",
            "matrix completion",
            "multitask learning",
            "spectral regu-     larization",
            ""
        ],
        "author": [
            "Andrew M. McDonald",
            "Massimiliano Pontil",
            "Dimitris Stamos"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-151/15-151.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Minimum Density Hyperplanes",
        "abstract": "Associating distinct groups of objects (clusters) with contiguous regions of high probability density (high-density clusters), is central to many statistical and machine learning approaches to the classification of unlabelled data. We propose a novel hyperplane classifier for clustering and semi-supervised classification which is motivated by this objective. The proposed minimum density hyperplane minimises the integral of the empirical probability density function along it, thereby avoiding intersection with high density clusters. We show that the minimum density and the maximum margin hyperplanes are asymptotically equivalent, thus linking this approach to maximum margin clustering and semi-supervised support vector classifiers. We propose a projection pursuit formulation of the associated optimisation problem which allows us to find minimum density hyperplanes efficiently in practice, and evaluate its performance on a range of benchmark data sets. The proposed approach is found to be very competitive with state of the art methods for clustering and semi-supervised classification.",
        "keywords": [
            "low-density separation",
            "high-density clusters",
            "clustering",
            "semi-supervised     classification",
            ""
        ],
        "author": [
            "Nicos G. Pavlidis",
            "David P. Hofmeyr",
            "Sotiris K. Tasoulis"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-307/15-307.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Theoretical Analysis of the Optimal Free Responses of Graph-Based SFA for the Design of Training Graphs",
        "abstract": "",
        "keywords": [
            "slow feature analysis",
            "nonlinear regression",
            "image analysis",
            "pattern recogni-     tion",
            ""
        ],
        "author": [
            "Alberto N. Escalante-B.",
            "Laurenz Wiskott"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-311/15-311.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Universal Approximation Results for the Temporal Restricted Boltzmann Machine and the Recurrent Temporal Restricted Boltzmann Machine",
        "abstract": "The Restricted Boltzmann Machine (RBM) has proved to be a powerful tool in machine learning, both on its own and as the building block for Deep Belief Networks (multi-layer generative graphical models). The RBM and Deep Belief Network have been shown to be universal approximators for probability distributions on binary vectors. In this paper we prove several similar universal approximation results for two variations of the Restricted Boltzmann Machine with time dependence, the Temporal Restricted Boltzmann Machine (TRBM) and the Recurrent Temporal Restricted Boltzmann Machine (RTRBM). We show that the TRBM is a universal approximator for Markov chains and generalize the theorem to sequences with longer time dependence. We then prove that the RTRBM is a universal approximator for stochastic processes with finite time dependence. We conclude with a discussion on efficiency and how the constructions developed could explain some previous experimental results.",
        "keywords": [
            "TRBM",
            "RTRBM",
            "machine learning",
            ""
        ],
        "author": [
            "Simon Odense",
            "Roderick Edwards"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-478/15-478.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Exploration of the (Non-)Asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics",
        "abstract": "Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data sets is computationally infeasible. The recently proposed stochastic gradient Langevin dynamics (SGLD) method circumvents this problem in three ways: it generates proposed moves using only a subset of the data, it skips the Metropolis- Hastings accept-reject step, and it uses sequences of decreasing step sizes. In Teh et al. (2014), we provided the mathematical foundations for the decreasing step size SGLD, including consistency and a central limit theorem. However, in practice the SGLD is run for a relatively small number of iterations, and its step size is not decreased to zero. The present article investigates the behaviour of the SGLD with fixed step size. In particular we characterise the asymptotic bias explicitly, along with its dependence on the step size and the variance of the stochastic gradient. On that basis a modified SGLD which removes the asymptotic bias due to the variance of the stochastic gradients up to first order in the step size is derived. Moreover, we are able to obtain bounds on the finite-time bias, variance and mean squared error (MSE). The theory is illustrated with a Gaussian toy model for which the bias and the MSE for the estimation of moments can be obtained explicitly. For this toy model we study the gain of the SGLD over the standard Euler method in the limit of large data sets.",
        "keywords": [
            "Markov Chain Monte Carlo",
            "Langevin dynamics",
            "big data",
            ""
        ],
        "author": [
            "Sebastian J. Vollmer",
            "Konstantinos C. Zygalakis",
            "Yee Whye Teh"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-494/15-494.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A General Framework for Constrained Bayesian Optimization using Information-based Search",
        "abstract": "We present an information-theoretic framework for solving global black-box optimization problems that also have black-box constraints. Of particular interest to us is to efficiently solve problems with decoupled constraints, in which subsets of the objective and constraint functions may be evaluated independently. For example, when the objective is evaluated on a CPU and the constraints are evaluated independently on a GPU. These problems require an acquisition function that can be separated into the contributions of the individual function evaluations. We develop one such acquisition function and call it Predictive Entropy Search with Constraints (PESC). PESC is an approximation to the expected information gain criterion and it compares favorably to alternative approaches based on improvement in several synthetic and real- world problems. In addition to this, we consider problems with a mix of functions that are fast and slow to evaluate. These problems require balancing the amount of time spent in the meta- computation of PESC and in the actual evaluation of the target objective. We take a bounded rationality approach and develop a partial update for PESC which trades off accuracy against speed. We then propose a method for adaptively switching between the partial and full updates for PESC. This allows us to interpolate between versions of PESC that are efficient in terms of function evaluations and those that are efficient in terms of wall-clock time. Overall, we demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization.",
        "keywords": [
            "Bayesian optimization",
            "constraints",
            ""
        ],
        "author": [
            "Jos\\'{e} Miguel Hern\\'{a}ndez-Lobato",
            "Michael A. Gelbart",
            "Ryan P. Adams",
            "Matthew W. Hoffman",
            "Zoubin Ghahramani"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-616/15-616.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Optimal Estimation and Completion of Matrices with Biclustering Structures",
        "abstract": "Biclustering structures in data matrices were first formalized in a seminal paper by John Hartigan (Hartigan, 1972) where one seeks to cluster cases and variables simultaneously. Such structures are also prevalent in block modeling of networks. In this paper, we develop a theory for the estimation and completion of matrices with biclustering structures, where the data is a partially observed and noise contaminated matrix with a certain underlying biclustering structure. In particular, we show that a constrained least squares estimator achieves minimax rate-optimal performance in several of the most important scenarios. To this end, we derive unified high probability upper bounds for all sub-Gaussian data and also provide matching minimax lower bounds in both Gaussian and binary cases. Due to the close connection of graphon to stochastic block models, an immediate consequence of our general results is a minimax rate- optimal estimator for sparse graphons.",
        "keywords": [
            "Biclustering",
            "graphon",
            "matrix completion",
            "missing data",
            "stochastic block     models",
            ""
        ],
        "author": [
            "Chao Gao",
            "Yu Lu",
            "Zongming Ma",
            "Harrison H. Zhou"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-617/15-617.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "The Teaching Dimension of Linear Learners",
        "abstract": "Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization. This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression. We also exhibit optimal training sets that match these teaching dimensions. Our approach generalizes to other linear learners.",
        "keywords": [
            "Optimization based learner",
            "Karush-Kuhn-Tucker conditions",
            ""
        ],
        "author": [
            "Ji Liu",
            "Xiaojin Zhu"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-630/15-630.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Augmentable Gamma Belief Networks",
        "abstract": "To infer multilayer deep representations of high-dimensional discrete and nonnegative real vectors, we propose an augmentable gamma belief network (GBN) that factorizes each of its hidden layers into the product of a sparse connection weight matrix and the nonnegative real hidden units of the next layer. The GBN's hidden layers are jointly trained with an upward-downward Gibbs sampler that solves each layer with the same subroutine. The gamma-negative binomial process combined with a layer-wise training strategy allows inferring the width of each layer given a fixed budget on the width of the first layer. Example results illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the GBN can add more layers to improve its performance in both unsupervisedly extracting features and predicting heldout data. For exploratory data analysis, we extract trees and subnetworks from the learned deep network to visualize how the very specific factors discovered at the first hidden layer and the increasingly more general factors discovered at deeper hidden layers are related to each other, and we generate synthetic data by propagating random variables through the deep network from the top hidden layer back to the bottom data layer.",
        "keywords": [
            "Bayesian nonparametrics",
            "deep learning",
            "multilayer representation",
            "Poisson     factor analysis",
            "topic modeling",
            ""
        ],
        "author": [
            "Mingyuan Zhou",
            "Yulai Cong",
            "Bo Chen"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-633/15-633.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Optimal Estimation of Derivatives in Nonparametric Regression",
        "abstract": "We propose a simple framework for estimating derivatives without fitting the regression function in nonparametric regression. Unlike most existing methods that use the symmetric difference quotients, our method is constructed as a linear combination of observations. It is hence very flexible and applicable to both interior and boundary points, including most existing methods as special cases of ours. Within this framework, we define the variance-minimizing estimators for any order derivative of the regression function with a fixed bias-reduction level. For the equidistant design, we derive the asymptotic variance and bias of these estimators. We also show that our new method will, for the first time, achieve the asymptotically optimal convergence rate for difference-based estimators. Finally, we provide an effective criterion for selection of tuning parameters and demonstrate the usefulness of the proposed method through extensive simulation studies of the first- and second-order derivative estimators.",
        "keywords": [
            "Linear combination",
            "Nonparametric derivative estimation",
            "Nonparametric     regression",
            "Optimal sequence",
            ""
        ],
        "author": [
            "Wenlin Dai",
            "Tiejun Tong",
            "Marc G. Genton"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-640/15-640.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing",
        "abstract": "Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural no-free-lunch requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive- compatible mechanisms (that may or may not satisfy no-free- lunch), our mechanism makes the smallest possible payment to spammers. We further extend our results to a more general setting in which workers are required to provide a quantized confidence for each question. Interestingly, this unique mechanism takes a multiplicative form. The simplicity of the mechanism is an added benefit. In preliminary experiments involving over 900 worker-task pairs, we observe a significant drop in the error rates under this unique mechanism for the same or lower monetary expenditure.",
        "keywords": [
            "high-quality labels",
            "supervised learning",
            "crowdsourcing",
            "mechanism design",
            ""
        ],
        "author": [
            "Nihar B. Shah",
            "Dengyong Zhou"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-642/15-642.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Joint Structural Estimation of Multiple Graphical Models",
        "abstract": "Gaussian graphical models capture dependence relationships between random variables through the pattern of nonzero elements in the corresponding inverse covariance matrices. To date, there has been a large body of literature on both computational methods and analytical results on the estimation of a single graphical model. However, in many application domains, one has to estimate several related graphical models, a problem that has also received attention in the literature. The available approaches usually assume that all graphical models are globally related. On the other hand, in many settings different relationships between subsets of the node sets exist between different graphical models. We develop methodology that jointly estimates multiple Gaussian graphical models, assuming that there exists prior information on how they are structurally related. For many applications, such information is available from external data sources. The proposed method consists of first applying neighborhood selection with a group lasso penalty to obtain edge sets of the graphs, and a maximum likelihood refit for estimating the nonzero entries in the inverse covariance matrices. We establish consistency of the proposed method for sparse high-dimensional Gaussian graphical models and examine its performance using simulation experiments. Applications to a climate data set and a breast cancer data set are also discussed.",
        "keywords": [
            "Gaussian graphical model",
            "structured sparsity",
            "group lasso penalty",
            "consistency",
            ""
        ],
        "author": [
            "Jing Ma",
            "George Michailidis"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-656/15-656.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Support Vector Hazards Machine: A Counting Process Framework for Learning Risk Scores for Censored Outcomes",
        "abstract": "Learning risk scores to predict dichotomous or continuous outcomes using machine learning approaches has been studied extensively. However, how to learn risk scores for time-to-event outcomes subject to right censoring has received little attention until recently. Existing approaches rely on inverse probability weighting or rank-based regression, which may be inefficient. In this paper, we develop a new support vector hazards machine (SVHM) approach to predict censored outcomes. Our method is based on predicting the counting process associated with the time-to-event outcomes among subjects at risk via a series of support vector machines. Introducing counting processes to represent time-to-event data leads to a connection between support vector machines in supervised learning and hazards regression in standard survival analysis. To account for different at risk populations at observed event times, a time-varying offset is used in estimating risk scores. The resulting optimization is a convex quadratic programming problem that can easily incorporate non-linearity using kernel trick. We demonstrate an interesting link from the profiled empirical risk function of SVHM to the Cox partial likelihood. We then formally show that SVHM is optimal in discriminating covariate-specific hazard function from population average hazard function, and establish the consistency and learning rate of the predicted risk using the estimated risk scores. Simulation studies show improved prediction accuracy of the event times using SVHM compared to existing machine learning methods and standard conventional approaches. Finally, we analyze two real world biomedical study data where we use clinical markers and neuroimaging biomarkers to predict age-at- onset of a disease, and demonstrate superiority of SVHM in distinguishing high risk versus low risk subjects.",
        "keywords": [
            "support vector machine",
            "survival analysis",
            "risk bound",
            "risk prediction",
            "neu-     roimaging biomarkers",
            "early disease detectionc 2016 Yuanjia Wang",
            "Tianle Chen",
            ""
        ],
        "author": [
            "Yuanjia Wang",
            "Tianle Chen",
            "Donglin Zeng"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-007/16-007.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Stable Graphical Models",
        "abstract": "Stable random variables are motivated by the central limit theorem for densities with (potentially) unbounded variance and can be thought of as natural generalizations of the Gaussian distribution to skewed and heavy-tailed phenomenon. In this paper, we introduce -stable graphical (-SG) models, a class of multivariate stable densities that can also be represented as Bayesian networks whose edges encode linear dependencies between random variables. One major hurdle to the extensive use of stable distributions is the lack of a closed- form analytical expression for their densities. This makes penalized maximum-likelihood based learning computationally demanding. We establish theoretically that the Bayesian information criterion (BIC) can asymptotically be reduced to the computationally more tractable minimum dispersion criterion (MDC) and develop StabLe, a structure learning algorithm based on MDC. We use simulated datasets for five benchmark network topologies to empirically demonstrate how  StabLe improves upon ordinary least squares (OLS) regression. We also apply StabLe to microarray gene expression data for lymphoblastoid cells from 727 individuals belonging to eight global population groups. We establish that StabLe improves test set performance relative to OLS via ten-fold cross-validation. Finally, we develop SGEX, a method for quantifying differential expression of genes between different population groups.",
        "keywords": [
            "Bayesian networks",
            "stable distributions",
            "linear regression",
            "structure learning",
            "gene expression",
            ""
        ],
        "author": [
            "Navodit Misra",
            "Ercan E. Kuruoglu"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-150/14-150.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Bounding the Search Space for Global Optimization of Neural Networks Learning Error: An Interval Analysis Approach",
        "abstract": "Training a multilayer perceptron (MLP) with algorithms employing global search strategies has been an important research direction in the field of neural networks. Despite a number of significant results, an important matter concerning the bounds of the search region---typically defined as a box---where a global optimization method has to search for a potential global minimizer seems to be unresolved. The approach presented in this paper builds on interval analysis and attempts to define guaranteed bounds in the search space prior to applying a global search algorithm for training an MLP. These bounds depend on the machine precision and the term guaranteed denotes that the region defined surely encloses weight sets that are global minimizers of the neural network's error function. Although the solution set to the bounding problem for an MLP is in general non-convex, the paper presents the theoretical results that help deriving a box which is a convex set. This box is an outer approximation of the algebraic solutions to the interval equations resulting from the function implemented by the network nodes. An experimental study using well known benchmarks is presented in accordance with the theoretical results.",
        "keywords": [
            "neural network training",
            "bound constrained global optimization",
            "interval     analysis",
            "interval linear equations",
            ""
        ],
        "author": [
            "Stavros P. Adam",
            "George D. Magoulas",
            "Dimitrios A. Karras",
            "Michael N. Vrahatis"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-350/14-350.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "mlr: Machine Learning in R",
        "abstract": "The mlr package provides a generic, object- oriented, and extensible framework for classification, regression, survival analysis and clustering for the R language. It provides a unified interface to more than 160 basic learners and includes meta-algorithms and model selection techniques to improve and extend the functionality of basic learners with, e.g., hyperparameter tuning, feature selection, and ensemble construction. Parallel high-performance computing is natively supported. The package targets practitioners who want to quickly apply machine learning algorithms, as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment.",
        "keywords": [
            "machine learning",
            "hyperparameter tuning",
            "model selection",
            "feature selection",
            "benchmarking",
            "R",
            "visualization",
            ""
        ],
        "author": [
            "Bernd Bischl",
            "Michel Lang",
            "Lars Kotthoff",
            "Julia Schiffner",
            "Jakob Richter",
            "Erich Studerus",
            "Giuseppe Casalicchio",
            "Zachary M. Jones"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-066/15-066.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Feature-Level Domain Adaptation",
        "abstract": "Domain adaptation is the supervised learning setting in which the training and test data are sampled from different distributions: training data is sampled from a source domain, whilst test data is sampled from a target domain. This paper proposes and studies an approach, called feature-level domain adaptation (FLDA), that models the dependence between the two domains by means of a feature-level transfer model that is trained to describe the transfer from source to target domain. Subsequently, we train a domain-adapted classifier by minimizing the expected loss under the resulting transfer model. For linear classifiers and a large family of loss functions and transfer models, this expected loss can be computed or approximated analytically, and minimized efficiently. Our empirical evaluation of FLDA focuses on problems comprising binary and count data in which the transfer can be naturally modeled via a dropout distribution, which allows the classifier to adapt to differences in the marginal probability of features in the source and the target domain. Our experiments on several real- world problems show that FLDA performs on par with state- of- the-art domain-adaptation techniques.",
        "keywords": [
            "Domain adaptation",
            "transfer learning",
            "covariate shift",
            ""
        ],
        "author": [
            "Wouter M. Kouw",
            "Laurens J.P. van der Maaten",
            "Jesse H. Krijthe",
            "Marco Loog"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-206/15-206.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Semiparametric Mean Field Variational Bayes: General Principles and Numerical Issues",
        "abstract": "We introduce the term semiparametric mean field variational Bayes to describe the relaxation of mean field variational Bayes in which some density functions in the product density restriction are pre-specified to be members of convenient parametric families. This notion has appeared in various guises in the mean field variational Bayes literature during its history and we endeavor to unify this important topic. We lay down a general framework and explain how previous relevant methodologies fall within this framework. A major contribution is elucidation of numerical issues that impact semiparametric mean field variational Bayes in practice.",
        "keywords": [
            "Bayesian Computing",
            "Factor Graph",
            "Fixed-form Variational Bayes",
            "Fixed-     point Iteration",
            "Non-conjugate Variational Message Passing",
            ""
        ],
        "author": [
            "David Rohde",
            "Matt P. W",
            ""
        ],
        "ref": "http://jmlr.org/papers/volume17/15-276/15-276.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Online PCA with Optimal Regret",
        "abstract": "",
        "keywords": [
            "online learning",
            "regret bounds",
            "expert setting",
            "k-sets",
            "PCA",
            "Gradient Descent",
            ""
        ],
        "author": [
            "Jiazhong Nie",
            "Wojciech Kotlowski",
            "Manfred K. Warmuth"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-320/15-320.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Efficient Computation of Gaussian Process Regression for Large Spatial Data Sets by Patching Local Gaussian Processes",
        "abstract": "This paper develops an efficient computational method for solving a Gaussian process (GP) regression for large spatial data sets using a collection of suitably defined local GP regressions. The conventional local GP approach first partitions a domain into multiple non-overlapping local regions, and then fits an independent GP regression for each local region using the training data belonging to the region. Two key issues with the local GP are (1) the prediction around the boundary of a local region is not as accurate as the prediction at interior of the local region, and (2) two local GP regressions for two neighboring local regions produce different predictions at the boundary of the two regions, creating undesirable discontinuity in the prediction. We address these issues by constraining the predictions of local GP regressions sharing a common boundary to satisfy the same boundary constraints, which in turn are estimated by the data. The boundary constrained local GP regressions are solved by a finite element method. Our approach shows competitive performance when compared with several state- of-the-art methods using two synthetic data sets and three real data sets.",
        "keywords": [
            "constrained Gaussian process regression",
            "kriging",
            "local regression",
            "boundary     value problem",
            "spatial prediction",
            ""
        ],
        "author": [
            "Chiwoo Park",
            "Jianhua Z. Huang"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-327/15-327.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "bandicoot: a Python Toolbox for Mobile Phone Metadata",
        "abstract": "bandicoot is an open-source Python toolbox to extract more than 1442 features from standard mobile phone metadata. bandicoot makes it easy for machine learning researchers and practitioners to load mobile phone data, to analyze and visualize them, and to extract robust features which can be used for various classification and clustering tasks. Emphasis is put on ease of use, consistency, and documentation. bandicoot has no dependencies and is distributed under MIT license.",
        "keywords": [
            "Python",
            "feature engineering",
            "mobile phone metadata",
            "CDR",
            ""
        ],
        "author": [
            "Yves-Alexandre de Montjoye",
            "Luc Rocher",
            "Alex Sandy Pentland"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-593/15-593.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Input Output Kernel Regression: Supervised and Semi-Supervised Structured Output Prediction with Operator-Valued Kernels",
        "abstract": "In this paper, we introduce a novel approach, called Input Output Kernel Regression (IOKR), for learning mappings between structured inputs and structured outputs. The approach belongs to the family of Output Kernel Regression methods devoted to regression in feature space endowed with some output kernel. In order to take into account structure in input data and benefit from kernels in the input space as well, we use the Reproducing Kernel Hilbert Space theory for vector-valued functions. We first recall the ridge solution for supervised learning and then study the regularized hinge loss-based solution used in Maximum Margin Regression. Both models are also developed in the context of semi-supervised setting. In addition we derive an extension of Generalized Cross Validation for model selection in the case of the least-square model. Finally we show the versatility of the IOKR framework on two different problems: link prediction seen as a structured output problem and multi-task regression seen as a multiple and interdependent output problem. Eventually, we present a set of detailed numerical results that shows the relevance of the method on these two tasks.",
        "keywords": [
            "structured output prediction",
            "output kernel regression",
            "vector-valued RKHS",
            "operator-valued kernel",
            ""
        ],
        "author": [
            "Céline Brouard",
            "Marie Szafranski",
            "Florence d'Alché-Buc"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-602/15-602.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Note on the Sample Complexity of the Er-SpUD Algorithm by Spielman, Wang and Wright for Exact Recovery of Sparsely Used Dictionaries",
        "abstract": "We consider the problem of recovering an invertible  matrix  and a sparse  random matrix  based on the observation of  (up to a scaling and permutation of columns of  and rows of ). Using only elementary tools from the theory of empirical processes we show that a version of the Er-SpUD algorithm by Spielman, Wang and Wright with high probability recovers  and  exactly, provided that , which is optimal up to the constant .",
        "keywords": [
            "sparse dictionaries",
            "Er-SpUD algorithm",
            ""
        ],
        "author": [
            "Radoslaw Adamczak"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-047/16-047.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "The Asymptotic Performance of Linear Echo State Neural Networks",
        "abstract": "In this article, a study of the mean-square error (MSE) performance of linear echo-state neural networks is performed, both for training and testing tasks. Considering the realistic setting of noise present at the network nodes, we derive deterministic equivalents for the aforementioned MSE in the limit where the number of input data  and network size  both grow large. Specializing then the network connectivity matrix to specific random settings, we further obtain simple formulas that provide new insights on the performance of such networks.",
        "keywords": [
            ""
        ],
        "author": [
            "Romain Couillet",
            "Gilles Wainrib",
            "Harry Sevi",
            "Hafiz Tiomoko Ali"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-076/16-076.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "On the Consistency of the Likelihood Maximization Vertex Nomination Scheme: Bridging the Gap Between Maximum Likelihood Estimation and Graph Matching",
        "abstract": "Given a graph in which a few vertices are deemed interesting a priori, the vertex nomination task is to order the remaining vertices into a nomination list such that there is a concentration of interesting vertices at the top of the list. Previous work has yielded several approaches to this problem, with theoretical results in the setting where the graph is drawn from a stochastic block model (SBM), including a vertex nomination analogue of the Bayes optimal classifier. In this paper, we prove that maximum likelihood (ML)-based vertex nomination is consistent, in the sense that the performance of the ML-based scheme asymptotically matches that of the Bayes optimal scheme. We prove theorems of this form both when model parameters are known and unknown. Additionally, we introduce and prove consistency of a related, more scalable restricted-focus ML vertex nomination scheme. Finally, we incorporate vertex and edge features into ML-based vertex nomination and briefly explore the empirical effectiveness of this approach.",
        "keywords": [
            "vertex nomination",
            "graph matching",
            "graph inference",
            "stochastic block model",
            ""
        ],
        "author": [
            "Vince Lyzinski",
            "Keith Levin",
            "Donniell E. Fishkind",
            "Carey E. Priebe"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-343/16-343.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Characteristic Kernels and Infinitely Divisible Distributions",
        "abstract": "We connect shift-invariant characteristic kernels to infinitely divisible distributions on . Characteristic kernels play an important role in machine learning applications with their kernel means to distinguish any two probability measures. The contribution of this paper is twofold. First, we show, using the Levy--Khintchine formula, that any shift- invariant kernel given by a bounded, continuous, and symmetric probability density function (pdf) of an infinitely divisible distribution on  is characteristic. We mention some closure properties of such characteristic kernels under addition, pointwise product, and convolution. Second, in developing various kernel mean algorithms, it is fundamental to compute the following values: (i) kernel mean values , , and (ii) kernel mean RKHS inner products , for probability measures . If , and kernel  are Gaussians, then the computation of (i) and (ii) results in Gaussian pdfs that are tractable. We generalize this Gaussian combination to more general cases in the class of infinitely divisible distributions. We then introduce a conjugate kernel and a convolution trick, so that the above (i) and (ii) have the same pdf form, expecting tractable computation at least in some cases. As specific instances, we explore -stable distributions and a rich class of generalized hyperbolic distributions, where the Laplace, Cauchy, and Student's  distributions are included.",
        "keywords": [
            "Characteristic Kernel",
            "Kernel Mean",
            "Infinitely Divisible Distribution",
            "Con-     jugate Kernel",
            ""
        ],
        "author": [
            "Yu Nishiyama",
            "Kenji Fukumizu"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-132/14-132.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Consistency of Cheeger and Ratio Graph Cuts",
        "abstract": "This paper establishes the consistency of a family of graph-cut- based algorithms for clustering of data clouds. We consider point clouds obtained as samples of a ground-truth measure. We investigate approaches to clustering based on minimizing objective functionals defined on proximity graphs of the given sample. Our focus is on functionals based on graph cuts like the Cheeger and ratio cuts. We show that minimizers of these cuts converge as the sample size increases to a minimizer of a corresponding continuum cut (which partitions the ground truth measure). Moreover, we obtain sharp conditions on how the connectivity radius can be scaled with respect to the number of sample points for the consistency to hold. We provide results for two-way and for multiway cuts. Furthermore we provide numerical experiments that illustrate the results and explore the optimality of scaling in dimension two.",
        "keywords": [
            "data clustering",
            "balanced cut",
            "consistency",
            ""
        ],
        "author": [
            "Nicolás García Trillos",
            "Dejan Slep\\v{c}ev",
            "James von Brecht",
            "Thomas Laurent",
            "Xavier Bresson"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-490/14-490.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Jointly Informative Feature Selection Made Tractable by Gaussian Modeling",
        "abstract": "We address the problem of selecting groups of jointly informative, continuous, features in the context of classification and propose several novel criteria for performing this selection. The proposed class of methods is based on combining a Gaussian modeling of the feature responses with derived bounds on and approximations to their mutual information with the class label. Furthermore, specific algorithmic implementations of these criteria are presented which reduce the computational complexity of the proposed feature selection algorithms by up to two-orders of magnitude. Consequently we show that feature selection based on the joint mutual information of features and class label is in fact tractable; this runs contrary to prior works that largely depend on marginal quantities. An empirical evaluation using several types of classifiers on multiple data sets show that this class of methods outperforms state-of-the-art baselines, both in terms of speed and classification accuracy.",
        "keywords": [
            "feature selection",
            "mutual information",
            "entropy",
            ""
        ],
        "author": [
            "Leonidas Lefakis",
            "François Fleuret"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-026/15-026.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle",
        "abstract": "While machine learning has proven to be a powerful data-driven solution to many real-life problems,  its use in sensitive domains has been limited due to privacy concerns.  A popular approach known as differential privacy offers provable privacy guarantees, but it is  often observed in practice that it could substantially hamper learning accuracy. In this paper we study the learnability (whether a problem can be learned by any algorithm)  under Vapnik's general learning setting with differential privacy constraint, and reveal some  intricate relationships between privacy, stability and learnability. In particular, we show that a problem is privately learnable if an only if there is a private algorithm that asymptotically minimizes the empirical risk (AERM).  In contrast, for non- private learning AERM alone is not sufficient for learnability. This result suggests that when searching for private learning algorithms, we can restrict the search to algorithms  that are AERM. In light of this, we propose a conceptual procedure that always finds a universally consistent algorithm  whenever the problem is learnable under privacy constraint.  We also propose a generic and practical algorithm and show that under very general conditions it privately learns a wide  class of learning problems.  Lastly, we extend some of the results to the more practical -differential privacy and establish the existence of a phase-transition on the class of problems that are approximately privately learnable with respect  to how small  needs to be.",
        "keywords": [
            "differential privacy",
            "learnability",
            "characterization",
            "stability",
            ""
        ],
        "author": [
            "Yu-Xiang Wang",
            "Jing Lei",
            "Stephen E. Fienberg"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-313/15-313.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "fastFM: A Library for Factorization Machines",
        "abstract": "Factorization Machines (FM) are currently only used in a narrow range of applications and are not yet part of the standard machine learning toolbox, despite their great success in collaborative filtering and click-through rate prediction. However, Factorization Machines are a general model to deal with sparse and high dimensional features. Our Factorization Machine implementation (fastFM) provides easy access to many solvers and supports regression, classification and ranking tasks. Such an implementation simplifies the use of FM for a wide range of applications. Therefore, our implementation has the potential to improve understanding of the FM model and drive new development.",
        "keywords": [
            "Python",
            "MCMC",
            "matrix factorization",
            ""
        ],
        "author": [
            "Immanuel Bayer"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-355/15-355.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "The Factorized Self-Controlled Case Series Method: An Approach for Estimating the Effects of Many Drugs on Many Outcomes",
        "abstract": "We provide a hierarchical Bayesian model for estimating the effects of transient drug exposures on a collection of health outcomes, where the effects of all drugs on all outcomes are estimated simultaneously. The method possesses properties that allow it to handle important challenges of dealing with large- scale longitudinal observational databases. In particular, this model is a generalization of the self-controlled case series (SCCS) method, meaning that certain patient specific baseline rates never need to be estimated. Further, this model is formulated with layers of latent factors, which substantially reduces the number of parameters and helps with interpretability by illuminating latent classes of drugs and outcomes. We believe our work is the first to consider multivariate SCCS (in the sense of multiple outcomes) and is the first to couple latent factor analysis with SCCS. We demonstrate the approach by estimating the effects of various time-sensitive insulin treatments for diabetes.",
        "keywords": [
            "Bayesian Analysis",
            "Drug Safety",
            "Self-Controlled Case Series",
            "Matrix Factor-     ization",
            ""
        ],
        "author": [
            "Ramin Moghaddass",
            "Cynthia Rudin",
            "David Madigan"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-405/15-405.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Electronic Health Record Analysis via Deep Poisson Factor Models",
        "abstract": "Electronic Health Record (EHR) phenotyping utilizes patient data captured through normal medical practice, to identify features that may represent computational medical phenotypes. These features may be used to identify at-risk patients and improve prediction of patient morbidity and mortality. We present a novel deep multi-modality architecture for EHR analysis (applicable to joint analysis of multiple forms of EHR data), based on Poisson Factor Analysis (PFA) modules. Each modality, composed of observed counts, is represented as a Poisson distribution, parameterized in terms of hidden binary units. Information from different modalities is shared via a deep hierarchy of common hidden units. Activation of these binary units occurs with probability characterized as Bernoulli- Poisson link functions, instead of more traditional logistic link functions. In addition, we demonstrate that PFA modules can be adapted to discriminative modalities. To compute model parameters, we derive efficient Markov Chain Monte Carlo (MCMC) inference that scales efficiently, with significant computational gains when compared to related models based on logistic link functions. To explore the utility of these models, we apply them to a subset of patients from the Duke-Durham patient cohort. We identified a cohort of over 16,000 patients with Type 2 Diabetes Mellitus (T2DM) based on diagnosis codes and laboratory tests out of our patient population of over 240,000. Examining the common hidden units uniting the PFA modules, we identify patient features that represent medical concepts. Experiments indicate that our learned features are better able to predict mortality and morbidity than clinical features identified previously in a large-scale clinical trial.",
        "keywords": [],
        "author": [
            "Ricardo Henao",
            "James T. Lu",
            "Joseph E. Lucas",
            "Jeffrey Ferranti",
            "Lawrence Carin"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-429/15-429.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Low-Rank Doubly Stochastic Matrix Decomposition for Cluster Analysis",
        "abstract": "Cluster analysis by nonnegative low-rank approximations has experienced a remarkable progress in the past decade. However, the majority of such approximation approaches are still restricted to nonnegative matrix factorization (NMF) and suffer from the following two drawbacks: 1) they are unable to produce balanced partitions for large-scale manifold data which are common in real-world clustering tasks; 2) most existing NMF-type clustering methods cannot automatically determine the number of clusters. We propose a new low-rank learning method to address these two problems, which is beyond matrix factorization. Our method approximately decomposes a sparse input similarity in a normalized way and its objective can be used to learn both cluster assignments and the number of clusters. For efficient optimization, we use a relaxed formulation based on Data- Cluster-Data random walk, which is also shown to be equivalent to low-rank factorization of the doubly-stochastically normalized cluster incidence matrix. The probabilistic cluster assignments can thus be learned with a multiplicative majorization-minimization algorithm. Experimental results show that the new method is more accurate both in terms of clustering large-scale manifold data sets and of selecting the number of clusters.",
        "keywords": [
            "cluster analysis",
            "probabilistic relaxation",
            "doubly stochastic matrix",
            "manifold",
            ""
        ],
        "author": [
            "Zhirong Yang",
            "Jukka Cor",
            "er",
            "Erkki Oja"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-549/15-549.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A New Algorithm and Theory for Penalized Regression-based Clustering",
        "abstract": "Clustering is unsupervised and exploratory in nature. Yet, it can be performed through penalized regression with grouping pursuit, as demonstrated in Pan et al. (2013). In this paper, we develop a more efficient algorithm for scalable computation and a new theory of clustering consistency for the method. This algorithm, called DC-ADMM, combines difference of convex (DC) programming with the alternating direction method of multipliers (ADMM). This algorithm is shown to be more computationally efficient than the quadratic penalty based algorithm of Pan et al. (2013) because of the former's closed-form updating formulas. Numerically, we compare the DC- ADMM algorithm with the quadratic penalty algorithm to demonstrate its utility and scalability. Theoretically, we establish a finite-sample mis- clustering error bound for penalized regression based clustering with the  constrained regularization in a general setting. On this ground, we provide conditions for clustering consistency of the penalized clustering method. As an end product, we put R package prclust implementing PRclust with various loss and grouping penalty functions available on GitHub and CRAN.",
        "keywords": [
            ""
        ],
        "author": [
            "Chong Wu",
            "Sunghoon Kwon",
            "Xiaotong Shen",
            "Wei Pan"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-553/15-553.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Classification of Imbalanced Data with a Geometric Digraph Family",
        "abstract": "We use a geometric digraph family called class cover catch digraphs (CCCDs) to tackle the class imbalance problem in statistical classification. CCCDs provide graph theoretic solutions to the class cover problem and have been employed in classification. We assess the classification performance of CCCD classifiers by extensive Monte Carlo simulations, comparing them with other classifiers commonly used in the literature. In particular, we show that CCCD classifiers perform relatively well when one class is more frequent than the other in a two- class setting, an example of the class imbalance problem. We also point out the relationship between class imbalance and class overlapping problems, and their influence on the performance of CCCD classifiers and other classification methods as well as some state-of-the-art algorithms which are robust to class imbalance by construction. Experiments on both simulated and real data sets indicate that CCCD classifiers are robust to the class imbalance problem. CCCDs substantially undersample from the majority class while preserving the information on the discarded points during the undersampling process. Many state- of-the-art methods, however, keep this information by means of ensemble classifiers, but CCCDs yield only a single classifier with the same property, making it both appealing and fast.",
        "keywords": [
            "Class Cover Catch Digraphs",
            "Class Cover Problem",
            "Class Imbalance Problem",
            "Class Overlapping Problem",
            "Graph Domination",
            "Prototype Selection",
            ""
        ],
        "author": [
            "Artür Manukyan",
            "Elvan Ceyhan"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-604/15-604.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Variational Approach to Path Estimation and Parameter Inference of Hidden Diffusion Processes",
        "abstract": "We consider a hidden Markov model, where the signal process, given by a diffusion, is only indirectly observed through some noisy measurements. The article develops a variational method for approximating the hidden states of the signal process given the full set of observations. This, in particular, leads to systematic approximations of the smoothing densities of the signal process. The paper then demonstrates how an efficient inference scheme, based on this variational approach to the approximation of the hidden states, can be designed to estimate the unknown parameters of stochastic differential equations. Two examples at the end illustrate the efficacy and the accuracy of the presented method.",
        "keywords": [
            "Variational inference",
            "stochastic differential equations",
            "diffusion processes",
            "hidden Markov model",
            ""
        ],
        "author": [
            "Tobias Sutter",
            "Arnab Ganguly",
            "Heinz Koeppl"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-075/16-075.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "One-class classification of point patterns of extremes",
        "abstract": "Novelty detection or one-class classification starts from a model describing some type of `normal behaviour' and aims to classify deviations from this model as being either novelties or anomalies. In this paper the problem of novelty detection for point patterns  is treated where examples of anomalies are very sparse, or even absent. The latter complicates the tuning of hyperparameters in models commonly used for novelty detection, such as one-class support vector machines and hidden Markov models. To this end, the use of extreme value statistics is introduced to estimate explicitly a model for the abnormal class by means of extrapolation from a statistical model  for the normal class. We show how multiple types of information obtained from any available extreme instances of  can be combined to reduce the high false-alarm rate that is typically encountered when classes are strongly imbalanced, as often occurs in the one-class setting (whereby `abnormal' data are often scarce). The approach is illustrated using simulated data and then a real-life application is used as an exemplar, whereby accelerometry data from epileptic seizures are analysed - these are known to be extreme and rare with respect to normal accelerometer data.",
        "keywords": [
            ""
        ],
        "author": [
            "Stijn Luca",
            "David A. Clifton",
            "Bart Vanrumste"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-112/16-112.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "On the Influence of Momentum Acceleration on Online Learning",
        "abstract": "The article examines in some detail the convergence rate and mean-square-error performance of momentum stochastic gradient methods in the constant step-size and slow adaptation regime. The results establish that momentum methods are equivalent to the standard stochastic gradient method with a re-scaled (larger) step-size value. The size of the re-scaling is determined by the value of the momentum parameter. The equivalence result is established for all time instants and not only in steady-state. The analysis is carried out for general strongly convex and smooth risk functions, and is not limited to quadratic risks. One notable conclusion is that the well-known benefits of momentum constructions for deterministic optimization problems do not necessarily carry over to the adaptive online setting when small constant step-sizes are used to enable continuous adaptation and learning in the presence of persistent gradient noise. From simulations, the equivalence between momentum and standard stochastic gradient methods is also observed for non-differentiable and non-convex problems.",
        "keywords": [
            "Online Learning",
            "Stochastic Gradient",
            "Momentum Acceleration",
            "Heavy-ball     Method",
            ""
        ],
        "author": [
            "Kun Yuan",
            "Bicheng Ying",
            "Ali H. Sayed"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-157/16-157.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Data-driven Rank Breaking for Efficient Rank Aggregation",
        "abstract": "Rank aggregation systems collect ordinal preferences from individuals to produce a global ranking that represents the social preference. Rank-breaking is a common practice to reduce the computational complexity of learning the global ranking. The individual preferences are broken into pairwise comparisons and applied to efficient algorithms tailored for independent paired comparisons. However, due to the ignored dependencies in the data, naive rank-breaking approaches can result in inconsistent estimates. The key idea to produce accurate and consistent estimates is to treat the pairwise comparisons unequally, depending on the topology of the collected data. In this paper, we provide the optimal rank-breaking estimator, which not only achieves consistency but also achieves the best error bound. This allows us to characterize the fundamental tradeoff between accuracy and complexity. Further, the analysis identifies how the accuracy depends on the spectral gap of a corresponding comparison graph.",
        "keywords": [
            "Rank aggregation",
            "Plackett-Luce model",
            ""
        ],
        "author": [
            "Ashish Khetan",
            "Sewoong Oh"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-209/16-209.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Optimal Learning Rates for Localized SVMs",
        "abstract": "One of the limiting factors of using support vector machines (SVMs) in large scale applications are their super-linear computational requirements in terms of the number of training samples. To address this issue, several approaches that train SVMs on many small chunks separately have been proposed in the literature. With the exception of random chunks, which is also known as divide-and-conquer kernel ridge regression, however, these approaches have only been empirically investigated. In this work we investigate a spatially oriented method to generate the chunks. For the resulting localized SVM that uses Gaussian kernels and the least squares loss we derive an oracle inequality, which in turn is used to deduce learning rates that are essentially minimax optimal under some standard smoothness assumptions on the regression function. In addition, we derive local learning rates that are based on the local smoothness of the regression function. We further introduce a data-dependent parameter selection method for our local SVM approach and show that this method achieves the same almost optimal learning rates. Finally, we present a few larger scale experiments for our localized SVM showing that it achieves essentially the same test error as a global SVM for a fraction of the computational requirements. In addition, it turns out that the computational requirements for the local SVMs are similar to those of a vanilla random chunk approach, while the achieved test errors are significantly better.",
        "keywords": [
            "least squares regression",
            "support vector machines",
            ""
        ],
        "author": [
            "Mona Meister",
            "Ingo Steinwart"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-023/14-023.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Bipartite Ranking: a Risk-Theoretic Perspective",
        "abstract": "We present a systematic study of the bipartite ranking problem, with the aim of explicating its connections to the class- probability estimation problem. Our study focuses on the properties of the statistical risk for bipartite ranking with general losses, which is closely related to a generalised notion of the area under the ROC curve: we establish alternate representations of this risk, relate the Bayes-optimal risk to a class of probability divergences, and characterise the set of Bayes-optimal scorers for the risk. We further study properties of a generalised class of bipartite risks, based on the -norm push of Rudin (2009). Our analysis is based on the rich framework of proper losses, which are the central tool in the study of class-probability estimation. We show how this analytic tool makes transparent the generalisations of several existing results, such as the equivalence of the minimisers for four seemingly disparate risks from bipartite ranking and class- probability estimation. A novel practical implication of our analysis is the design of new families of losses for scenarios where accuracy at the head of ranked list is paramount, with comparable empirical performance to the -norm push.",
        "keywords": [
            "bipartite ranking",
            "class-probability estimation",
            "proper losses",
            "Bayes-optimality",
            ""
        ],
        "author": [
            "Aditya Krishna Menon",
            "Robert C. Williamson"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-265/14-265.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Bayesian group factor analysis with structured sparsity",
        "abstract": "Latent factor models are the canonical statistical tool for exploratory analyses of low-dimensional linear structure for a matrix of  features across  samples. We develop a structured Bayesian group factor analysis model that extends the factor model to multiple coupled observation matrices; in the case of two observations, this reduces to a Bayesian model of canonical correlation analysis. Here, we carefully define a structured Bayesian prior that encourages both element-wise and column-wise shrinkage and leads to desirable behavior on high- dimensional data. In particular, our model puts a structured prior on the joint factor loading matrix, regularizing at three levels, which enables element-wise sparsity and unsupervised recovery of latent factors corresponding to structured variance across arbitrary subsets of the observations. In addition, our structured prior allows for both dense and sparse latent factors so that covariation among either all features or only a subset of features can be recovered. We use fast parameter-expanded expectation-maximization for parameter estimation in this model. We validate our method on simulated data with substantial structure. We show results of our method applied to three high- dimensional data sets, comparing results against a number of state-of-the-art approaches. These results illustrate useful properties of our model, including i) recovering sparse signal in the presence of dense effects; ii) the ability to scale naturally to large numbers of observations; iii) flexible observation- and factor-specific regularization to recover factors with a wide variety of sparsity levels and percentage of variance explained; and iv) tractable inference that scales to modern genomic and text data sizes.",
        "keywords": [],
        "author": [
            "Shiwen Zhao",
            "Chuan Gao",
            "Sayan Mukherjee",
            "Barbara E Engelhardt"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-472/14-472.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Machine Learning in an Auction Environment",
        "abstract": "We consider a model of repeated online auctions in which an ad with an uncertain click-through rate faces a random distribution of competing bids in each auction and there is discounting of payoffs. We formulate the optimal solution to this explore/exploit problem as a dynamic programming problem and show that efficiency is maximized by making a bid for each advertiser equal to the advertiser's expected value for the advertising opportunity plus a term proportional to the variance in this value divided by the number of impressions the advertiser has received thus far. We then use this result to illustrate that the value of incorporating active exploration in an auction environment is exceedingly small.",
        "keywords": [
            "Auctions",
            ""
        ],
        "author": [
            "Patrick Hummel",
            "R. Preston McAfee"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-109/15-109.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Wavelet decompositions of Random Forests - smoothness analysis, sparse approximation and applications",
        "abstract": "In this paper we introduce, in the setting of machine learning, a generalization of wavelet analysis which is a popular approach to low dimensional structured signal analysis. The wavelet decomposition of a Random Forest provides a sparse approximation of any regression or classification high dimensional function at various levels of detail, with a concrete ordering of the Random Forest nodes: from `significant' elements to nodes capturing only `insignificant' noise. Motivated by function space theory, we use the wavelet decomposition to compute numerically a `weak- type' smoothness index that captures the complexity of the underlying function. As we show through extensive experimentation, this sparse representation facilitates a variety of applications such as improved regression for difficult datasets, a novel approach to feature importance, resilience to noisy or irrelevant features, compression of ensembles, etc.",
        "keywords": [
            "Random Forest",
            "Wavelets",
            "Besov spaces",
            "adaptive approximation",
            ""
        ],
        "author": [
            "Oren Elisha",
            "Shai Dekel"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-203/15-203.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Mutual Information Based Matching for Causal Inference with Observational Data",
        "abstract": "This paper presents an information theory-driven matching methodology for making causal inference from observational data. The paper adopts a âpotential outcomes frameworkâ view on evaluating the strength of cause-effect relationships: the population-wide average effects of binary treatments are estimated by comparing two groups of units -- the treated and untreated (control). To reduce the bias in such treatment effect estimation, one has to compose a control group in such a way that across the compared groups of units, treatment is independent of the units' covariates. This requirement gives rise to a subset selection / matching problem. This paper presents the models and algorithms that solve the matching problem by minimizing the mutual information (MI) between the covariates and the treatment variable. Such a formulation becomes tractable thanks to the derived optimality conditions that tackle the non-linearity of the sample-based MI function. Computational experiments with mixed integer-programming formulations and four matching algorithms demonstrate the utility of MI based matching for causal inference studies. The algorithmic developments culminate in a matching heuristic that allows for balancing the compared groups in polynomial (close to linear) time, thus allowing for treatment effect estimation with large data sets.",
        "keywords": [
            "Observational Causal Inference",
            "Mutual Information",
            "Matching",
            "Subset Se-     lection",
            ""
        ],
        "author": [
            "Lei Sun",
            "Alexander G. Nikolaev"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-420/15-420.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Online Trans-dimensional von Mises-Fisher Mixture Models for User Profiles",
        "abstract": "The proliferation of online communities has attracted much attention to modelling user behaviour in terms of social interaction, language adoption and contribution activity. Nevertheless, when applied to large-scale and cross-platform behavioural data, existing approaches generally suffer from expressiveness, scalability and generality issues. This paper proposes trans-dimensional von Mises-Fisher (TvMF) mixture models for  normalised behavioural data, which encapsulate: (1) a Bayesian framework for vMF mixtures that enables prior knowledge and information sharing among clusters, (2) an extended version of reversible jump MCMC algorithm that allows adaptive changes in the number of clusters for vMF mixtures when the model parameters are updated, and (3) an online TvMF mixture model that accommodates the dynamics of clusters for time-varying user behavioural data. We develop efficient collapsed Gibbs sampling techniques for posterior inference, which facilitates parallelism for parameter updates. Empirical results on simulated and real-world data show that the proposed TvMF mixture models can discover more interpretable and intuitive clusters than other widely-used models, such as k-means, non-negative matrix factorization (NMF), Dirichlet process Gaussian mixture models (DP-GMM), and dynamic topic models (DTM). We further evaluate the performance of proposed models in real-world applications, such as the churn prediction task, that shows the usefulness of the features generated.",
        "keywords": [
            "Mixture Models",
            "von Mises-Fisher",
            "Bayesian Nonparametric",
            "Temporal Evolution",
            ""
        ],
        "author": [
            "Xiangju Qin",
            "Pádraig Cunningham",
            "Michael Salter-Townshend"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-454/15-454.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Multivariate Spearman's $\\rho$ for Aggregating Ranks Using Copulas",
        "abstract": "We study the problem of rank aggregation: given a set of ranked lists, we want to form a consensus ranking. Furthermore, we consider the case of extreme lists: i.e., only the rank of the best or worst elements are known. We impute missing ranks and generalise Spearman's  to extreme ranks. Our main contribution is the derivation of a non-parametric estimator for rank aggregation based on multivariate extensions of Spearman's , which measures correlation between a set of ranked lists. Multivariate Spearman's  is defined using copulas, and we show that the geometric mean of normalised ranks maximises multivariate correlation. Motivated by this, we propose a weighted geometric mean approach for learning to rank which has a closed form least squares solution. When only the best (top-k) or worst (bottom-k) elements of a ranked list are known, we impute the missing ranks by the average value, allowing us to apply Spearman's . We discuss an optimistic and pessimistic imputation of missing values, which respectively maximise and minimise correlation, and show its effect on aggregating university rankings. Finally, we demonstrate good performance on the rank aggregation benchmarks MQ2007 and MQ2008.",
        "keywords": [],
        "author": [
            "Justin Bedő",
            "Cheng Soon Ong"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-625/15-625.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Nonparametric Network Models for Link Prediction",
        "abstract": "",
        "keywords": [
            "Dirichlet process",
            "networks",
            "Bayesian nonparametrics",
            "Gibbs sampling",
            ""
        ],
        "author": [
            "Sinead A. Williamson"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-032/16-032.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Guarding against Spurious Discoveries in High Dimensions",
        "abstract": "Many data mining and statistical machine learning algorithms have been developed to select a subset of covariates to associate with a response variable. Spurious discoveries can easily arise in high-dimensional data analysis due to enormous possibilities of such selections. How can we know statistically our discoveries better than those by chance? In this paper, we define a measure of goodness of spurious fit, which shows how good a response variable can be fitted by an optimally selected subset of covariates under the null model, and propose a simple and effective LAMM algorithm to compute it. It coincides with the maximum spurious correlation for linear models and can be regarded as a generalized maximum spurious correlation. We derive the asymptotic distribution of such goodness of spurious fit for generalized linear models and  regression. Such an asymptotic distribution depends on the sample size, ambient dimension, the number of variables used in the fit, and the covariance information. It can be consistently estimated by multiplier bootstrapping and used as a benchmark to guard against spurious discoveries. It can also be applied to model selection, which considers only candidate models with goodness of fits better than those by spurious fits. The theory and method are convincingly illustrated by simulated examples and an application to the binary outcomes from German Neuroblastoma Trials.",
        "keywords": [
            "Bootstrap",
            "Gaussian approximation",
            "generalized linear models",
            "L1 regression",
            "model selection",
            "sparsity",
            "spurious correlation",
            ""
        ],
        "author": [
            "Jianqing Fan",
            "Wen-Xin Zhou"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-068/16-068.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Bayesian Graphical Models for Multivariate Functional Data",
        "abstract": "Graphical models express conditional independence relationships among variables. Although methods for vector-valued data are well established, functional data graphical models remain underdeveloped. By functional data, we refer to data that are realizations of random functions varying over a continuum (e.g., images, signals). We introduce a notion of conditional independence between random functions, and construct a framework for Bayesian inference of undirected, decomposable graphs in the multivariate functional data context. This framework is based on extending Markov distributions and hyper Markov laws from random variables to random processes, providing a principled alternative to naive application of multivariate methods to discretized functional data. Markov properties facilitate the composition of likelihoods and priors according to the decomposition of a graph. Our focus is on Gaussian process graphical models using orthogonal basis expansions. We propose a hyper-inverse-Wishart-process prior for the covariance kernels of the infinite coefficient sequences of the basis expansion, and establish its existence and uniqueness. We also prove the strong hyper Markov property and the conjugacy of this prior under a finite rank condition of the prior kernel parameter. Stochastic search Markov chain Monte Carlo algorithms are developed for posterior inference, assessed through simulations, and applied to a study of brain activity and alcoholism.",
        "keywords": [
            "graphical model",
            "functional data analysis",
            "gaussian process",
            "model uncer-     tainty",
            ""
        ],
        "author": [
            "Hongxiao Zhu",
            "Nate Strawn",
            "David B. Dunson"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-164/16-164.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Neural Autoregressive Distribution Estimation",
        "abstract": "We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE.",
        "keywords": [
            "deep learning",
            "neural networks",
            "density modeling",
            ""
        ],
        "author": [
            "Benigno Uria",
            "Marc-Alexandre Côté",
            "Karol Gregor",
            "Iain Murray",
            "Hugo Larochelle"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-272/16-272.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "ERRATA: On the Estimation of the Gradient Lines of a Density and the Consistency of the Mean-Shift Algorithm",
        "abstract": "ERRATA to the paper On the  Estimation of the Gradient Lines of a Density and the Consistency of  the Mean-Shift Algorithm.",
        "keywords": [],
        "author": [
            "Ery Arias-Castro",
            "David Mason",
            "Bruno Pelletier"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-527/16-527.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Modelling Interactions in High-dimensional Data with Backtracking",
        "abstract": "",
        "keywords": [
            "high-dimensional data",
            "interactions",
            "Lasso",
            ""
        ],
        "author": [
            "Rajen D. Shah"
        ],
        "ref": "http://jmlr.org/papers/volume17/13-515/13-515.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Choice of V for V-Fold Cross-Validation in Least-Squares Density Estimation",
        "abstract": "This paper studies -fold cross-validation for model selection in least-squares density estimation. The goal is to provide theoretical grounds for choosing  in order to minimize the least-squares loss of the selected estimator. We first prove a non-asymptotic oracle inequality for -fold cross-validation and its bias-corrected version (-fold penalization). In particular, this result implies that -fold penalization is asymptotically optimal in the nonparametric case. Then, we compute the variance of -fold cross-validation and related criteria, as well as the variance of key quantities for model selection performance. We show that these variances depend on  like , at least in some particular cases, suggesting that the performance increases much from  to  or , and then is almost constant. Overall, this can explain the common advice to take ---at least in our setting and when the computational power is limited---, as supported by some simulation experiments. An oracle inequality and exact formulas for the variance are also proved for Monte- Carlo cross-validation, also known as repeated cross-validation, where the parameter  is replaced by the number  of random splits of the data.",
        "keywords": [
            "V -fold cross-validation",
            "Monte-Carlo cross-validation",
            "leave-one-out",
            "leave-p-     out",
            "resampling penalties",
            "density estimation",
            "model selection",
            ""
        ],
        "author": [
            "Sylvain Arlot",
            "Matthieu Lerasle"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-296/14-296.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Towards More Efficient SPSD Matrix Approximation and CUR Matrix Decomposition",
        "abstract": "Symmetric positive semi-definite (SPSD) matrix approximation methods have been extensively used to speed up large-scale eigenvalue computation and kernel learning methods. The standard sketch based method, which we call the prototype model, produces relatively accurate approximations, but is inefficient on large square matrices. The NystrÃ¶m method is highly efficient, but can only achieve low accuracy. In this paper we propose a novel model that we call the fast SPSD matrix approximation model. The fast model is nearly as efficient as the NystrÃ¶m method and as accurate as the prototype model. We show that the fast model can potentially solve eigenvalue problems and kernel learning problems in linear time with respect to the matrix size  to achieve  relative-error, whereas both the prototype model and the NystrÃ¶m method cost at least quadratic time to attain comparable error bound. Empirical comparisons among the prototype model, the NystrÃ¶m method, and our fast model demonstrate the superiority of the fast model. We also contribute new understandings of the NystrÃ¶m method. The NystrÃ¶m method is a special instance of our fast model and is approximation to the prototype model. Our technique can be straightforwardly applied to make the CUR matrix decomposition more efficiently computed without much affecting the accuracy.",
        "keywords": [
            "Kernel approximation",
            "matrix factorization",
            ""
        ],
        "author": [
            "Shusen Wang",
            "Zhihua Zhang",
            "Tong Zhang"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-190/15-190.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Multi-Objective Markov Decision Processes for Data-Driven Decision Support",
        "abstract": "We present new methodology based on Multi-Objective Markov Decision Processes for developing sequential decision support systems from data. Our approach uses sequential decision-making data to provide support that is useful to many different decision-makers, each with different, potentially time-varying preference. To accomplish this, we develop an extension of fitted- iteration for multiple objectives that computes policies for all scalarization functions, i.e. preference functions, simultaneously from continuous-state, finite-horizon data. We identify and address several conceptual and computational challenges along the way, and we introduce a new solution concept that is appropriate when different actions have similar expected outcomes. Finally, we demonstrate an application of our method using data from the Clinical Antipsychotic Trials of Intervention Effectiveness and show that our approach offers decision-makers increased choice by a larger class of optimal policies.",
        "keywords": [
            "multi-objective optimization",
            "reinforcement learning",
            "Markov decision pro-     cesses",
            "clinical decision support",
            ""
        ],
        "author": [
            "Daniel J. Lizotte",
            "Eric B. Laber"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-252/15-252.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Measuring Dependence Powerfully and Equitably",
        "abstract": "Given a high-dimensional data set, we often wish to find the strongest relationships within it. A common strategy is to evaluate a measure of dependence on every variable pair and retain the highest-scoring pairs for follow-up. This strategy works well if the statistic used (a) has good power to detect non-trivial relationships, and (b) is equitable, meaning that for some measure of noise it assigns similar scores to equally noisy relationships regardless of relationship type (e.g., linear, exponential, periodic). In this paper, we define and theoretically characterize two new statistics that together yield an efficient approach for obtaining both power and equitability. To do this, we first introduce a new population measure of dependence and show three equivalent ways that it can be viewed, including as a canonical smoothing of mutual information. We then introduce an efficiently computable consistent estimator of our population measure of dependence, and we empirically establish its equitability on a large class of noisy functional relationships. This new statistic has better bias/variance properties and better runtime complexity than a previous heuristic approach. Next, we derive a second, related statistic whose computation is a trivial side-product of our algorithm and whose goal is powerful independence testing rather than equitability. We prove that this statistic yields a consistent independence test and show in simulations that the test has good power against independence. Taken together, our results suggest that these two statistics are a valuable pair of tools for exploratory data analysis.",
        "keywords": [],
        "author": [
            "Yakir A. Reshef",
            "David N. Reshef",
            "Hilary K. Finucane",
            "Pardis C. Sabeti",
            "Michael Mitzenmacher"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-308/15-308.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Neyman-Pearson Classification under High-Dimensional Settings",
        "abstract": "Most existing binary classification methods target on the optimization of the overall classification risk and may fail to serve some real-world applications such as cancer diagnosis, where users are more concerned with the risk of misclassifying one specific class than the other. Neyman-Pearson (NP) paradigm was introduced in this context as a novel statistical framework for handling asymmetric type I/II error priorities. It seeks classifiers with a minimal type II error and a constrained type I error under a user specified level. This article is the first attempt to construct classifiers with guaranteed theoretical performance under the NP paradigm in high-dimensional settings. Based on the fundamental Neyman-Pearson Lemma, we used a plug-in approach to construct NP-type classifiers for Naive Bayes models. The proposed classifiers satisfy the NP oracle inequalities, which are natural NP paradigm counterparts of the oracle inequalities in classical binary classification. Besides their desirable theoretical properties, we also demonstrated their numerical advantages in prioritized error control via both simulation and real data studies.",
        "keywords": [
            "classification",
            "high-dimension",
            "Naive Bayes",
            ""
        ],
        "author": [
            "Anqi Zhao",
            "Yang Feng",
            "Lie Wang",
            "Xin Tong"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-418/15-418.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Statistical Perspective on Randomized Sketching for Ordinary Least-Squares",
        "abstract": "We consider statistical as well as algorithmic aspects of solving large-scale least-squares (LS) problems using randomized sketching algorithms. For a LS problem with input data , sketching algorithms use a sketching matrix, , where . Then, rather than solving the LS problem using the full data , sketching algorithms solve the LS problem using only the sketched data . Prior work has typically adopted an algorithmic perspective, in that it has made no statistical assumptions on the input  and , and instead it has been assumed that the data  are fixed and worst-case (WC). Prior results show that, when using sketching matrices such as random projections and leverage-score sampling algorithms, with , the WC error is the same as solving the original problem, up to a small constant. From a statistical perspective, we typically consider the mean-squared error performance of randomized sketching algorithms, when data  are generated according to a statistical linear model , where  is a noise process. In this paper, we provide a rigorous comparison of both perspectives leading to insights on how they differ. To do this, we first develop a framework for assessing, in a unified manner, algorithmic and statistical aspects of randomized sketching methods. We then consider the statistical prediction efficiency (PE) and the statistical residual efficiency (RE) of the sketched LS estimator; and we use our framework to provide upper bounds for several types of random projection and random sampling sketching algorithms. Among other results, we show that the RE can be upper bounded when  while the PE typically requires the sample size  to be substantially larger. Lower bounds developed in subsequent results show that our upper bounds on PE can not be improved. (A preliminary version of this paper appeared as Raskutti and Mahoney (2014, 2015).)",
        "keywords": [
            "algorithmic leveraging",
            "randomized linear algebra",
            "sketching",
            "random pro-     jection",
            "statistical leverage",
            ""
        ],
        "author": [
            "Garvesh Raskutti",
            "Michael W. Mahoney"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-440/15-440.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Learning Planar Ising Models",
        "abstract": "Inference and learning of graphical models are both well-studied problems in statistics and machine learning that have found many applications in science and engineering. However, exact inference is intractable in general graphical models, which suggests the problem of seeking the best approximation to a collection of random variables within some tractable family of graphical models. In this paper, we focus on the class of planar Ising models, for which exact inference is tractable using techniques of statistical physics. Based on these techniques and recent methods for planarity testing and planar embedding, we propose a greedy algorithm for learning the best planar Ising model to approximate an arbitrary collection of binary random variables (possibly from sample data). Given the set of all pairwise correlations among variables, we select a planar graph and optimal planar Ising model defined on this graph to best approximate that set of correlations. We demonstrate our method in simulations and for two applications: modeling senate voting records and identifying geo-chemical depth trends from Mars rover data.",
        "keywords": [
            "Ising models",
            ""
        ],
        "author": [
            "Jason K. Johnson",
            "Diane Oyen",
            "Michael Chertkov",
            "Praneeth Netrapalli"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-579/15-579.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Newton-Stein Method: An Optimization Method for GLMs via Stein's Lemma",
        "abstract": "We consider the problem of efficiently computing the maximum likelihood estimator in Generalized Linear Models (GLMs) when the number of observations is much larger than the number of coefficients (). In this regime, optimization algorithms can immensely benefit from approximate second order information. We propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a Stein-type lemma, which allows further improvements through sub-sampling and eigenvalue thresholding. Our algorithm enjoys fast convergence rates, resembling that of second order methods, with modest per-iteration cost. We provide its convergence analysis for the general case where the rows of the design matrix are samples from a sub-Gaussian distribution. We show that the convergence has two phases, a quadratic phase followed by a linear phase. Finally, we empirically demonstrate that our algorithm achieves the highest performance compared to various optimization algorithms on several data sets.",
        "keywords": [
            "Optimization",
            "Generalized Linear Models",
            ""
        ],
        "author": [
            "Murat A. Erdogdu"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-062/16-062.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Bayesian Decision Process for Cost-Efficient Dynamic Ranking via Crowdsourcing",
        "abstract": "Rank aggregation based on pairwise comparisons over a set of items has a wide range of applications. Although considerable research has been devoted to the development of rank aggregation algorithms, one basic question is how to efficiently collect a large amount of high-quality pairwise comparisons for the ranking purpose. Because of the advent of many crowdsourcing services, a crowd of workers are often hired to conduct pairwise comparisons with a small monetary reward for each pair they compare. Since different workers have different levels of reliability and different pairs have different levels of ambiguity, it is desirable to wisely allocate the limited budget for comparisons among the pairs of items and workers so that the global ranking can be accurately inferred from the comparison results. To this end, we model the active sampling problem in crowdsourced ranking as a Bayesian Markov decision process, which dynamically selects item pairs and workers to improve the ranking accuracy under a budget constraint. We further develop a computationally efficient sampling policy based on knowledge gradient as well as a moment matching technique for posterior approximation. Experimental evaluations on both synthetic and real data show that the proposed policy achieves high ranking accuracy with a lower labeling cost.",
        "keywords": [
            "crowdsourced ranking",
            "Bayesian",
            "Markov decision process",
            "dynamic program-     ming",
            "knowledge gradient",
            ""
        ],
        "author": [
            "Xi Chen",
            "Kevin Jiao",
            "Qihang Lin"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-066/16-066.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Multi-scale Classification using Localized Spatial Depth",
        "abstract": "In this article, we develop and investigate a new classifier based on features extracted using spatial depth. Our construction is based on fitting a generalized additive model to posterior probabilities of different competing classes. To cope with possible multi-modal as well as non-elliptic nature of the population distribution, we also develop a localized version of spatial depth and use that with varying degrees of localization to build the classifier. Final classification is done by aggregating several posterior probability estimates, each of which is obtained using this localized spatial depth with a fixed scale of localization. The proposed classifier can be conveniently used even when the dimension of the data is larger than the sample size, and its good discriminatory power for such data has been established using theoretical as well as numerical results.",
        "keywords": [
            "Bayes classifier",
            "elliptic distributions",
            "generalized additive models",
            "HDLSS     asymptotics",
            "uniform strong consistency",
            ""
        ],
        "author": [
            "Subhajit Dutta",
            "Soham Sarkar",
            "Anil K. Ghosh"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-113/16-113.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "On Bayes Risk Lower Bounds",
        "abstract": "This paper provides a general technique for lower bounding the Bayes risk of statistical estimation, applicable to arbitrary loss functions and arbitrary prior distributions. A lower bound on the Bayes risk not only serves as a lower bound on the minimax risk, but also characterizes the fundamental limit of any estimator given the prior knowledge. Our bounds are based on the notion of -informativity (CsiszÃ¡r, 1972), which is a function of the underlying class of probability measures and the prior. Application of our bounds requires upper bounds on the -informativity, thus we derive new upper bounds on -informativity which often lead to tight Bayes risk lower bounds. Our technique leads to generalizations of a variety of classical minimax bounds (e.g., generalized Fano's inequality). Our Bayes risk lower bounds can be directly applied to several concrete estimation problems, including Gaussian location models, generalized linear models, and principal component analysis for spiked covariance models. To further demonstrate the applications of our Bayes risk lower bounds to machine learning problems, we present two new theoretical results: (1) a precise characterization of the minimax risk of learning spherical Gaussian mixture models under the smoothed analysis framework, and (2) lower bounds for the Bayes risk under a natural prior for both the prediction and estimation errors for high-dimensional sparse linear regression under an improper learning setting.",
        "keywords": [
            "Bayes risk",
            "Minimax risk",
            "f -divergence",
            "f -informativity",
            ""
        ],
        "author": [
            "Xi Chen",
            "Adityan",
            "Guntuboyina",
            "Yuchen Zhang"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-185/16-185.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Weak Convergence Properties of Constrained Emphatic Temporal-difference Learning with Constant and Slowly Diminishing Stepsize",
        "abstract": "We consider the emphatic temporal-difference (TD) algorithm, ETD(), for learning the value functions of stationary policies in a discounted, finite state and action Markov decision process. The ETD() algorithm was recently proposed by Sutton, Mahmood, and White (2016) to solve a long-standing divergence problem of the standard TD algorithm when it is applied to off- policy training, where data from an exploratory policy are used to evaluate other policies of interest. The almost sure convergence of ETD() has been proved in our recent work under general off-policy training conditions, but for a narrow range of diminishing stepsize. In this paper we present convergence results for constrained versions of ETD() with constant stepsize and with diminishing stepsize from a broad range. Our results characterize the asymptotic behavior of the trajectory of iterates produced by those algorithms, and are derived by combining key properties of ETD() with powerful convergence theorems from the weak convergence methods in stochastic approximation theory. For the case of constant stepsize, in addition to analyzing the behavior of the algorithms in the limit as the stepsize parameter approaches zero, we also analyze their behavior for a fixed stepsize and bound the deviations of their averaged iterates from the desired solution. These results are obtained by exploiting the weak Feller property of the Markov chains associated with the algorithms, and by using ergodic theorems for weak Feller Markov chains, in conjunction with the convergence results we get from the weak convergence methods. Besides ETD(), our analysis also applies to the off-policy TD() algorithm, when the divergence issue is avoided by setting  sufficiently large. It yields, for that case, new results on the asymptotic convergence properties of constrained off-policy TD() with constant or slowly diminishing stepsize.",
        "keywords": [
            "Markov decision processes",
            "approximate policy evaluation",
            "reinforcement     learning",
            "temporal-difference methods",
            "importance sampling",
            "stochastic approximation",
            ""
        ],
        "author": [
            "Huizhen Yu"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-242/16-242.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "RLScore: Regularized Least-Squares Learners",
        "abstract": "RLScore is a Python open source module for kernel based machine learning. The library provides implementations of several regularized least-squares (RLS) type of learners. RLS methods for regression and classification, ranking, greedy feature selection, multi-task and zero-shot learning, and unsupervised classification are included. Matrix algebra based computational short-cuts are used to ensure efficiency of both training and cross-validation. A simple API and extensive tutorials allow for easy use of RLScore.",
        "keywords": [
            "cross-validation",
            "feature selection",
            "kernel methods",
            "Kronecker product kernel",
            "pair-input learning",
            "python",
            ""
        ],
        "author": [
            "Tapio Pahikkala",
            "Antti Airola"
        ],
        "ref": "http://jmlr.org/papers/volume17/16-470/16-470.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Stability and Generalization in Structured Prediction",
        "abstract": "Structured prediction models have been found to learn effectively from a few large examples--- sometimes even just one. Despite empirical evidence, canonical learning theory cannot guarantee  generalization in this setting because the error bounds decrease as a function of the number of examples.  We therefore propose new PAC-Bayesian generalization bounds for structured prediction that decrease  as a function of both the number of examples and the size of each example. Our analysis hinges on the  stability of joint inference and the smoothness of the data distribution. We apply our bounds to several  common learning scenarios, including max-margin and soft-max training of Markov random fields.  Under certain conditions, the resulting error bounds can be far more optimistic than previous results and  can even guarantee generalization from a single large example.",
        "keywords": [
            "structured prediction",
            "learning theory",
            "PAC-Bayes",
            ""
        ],
        "author": [
            "Ben London",
            "Bert Huang",
            "Lise Getoor"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-501/15-501.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Composite Multiclass Losses",
        "abstract": "We consider loss functions for multiclass prediction problems. We show when a multiclass loss can be expressed as a proper composite loss, which is the composition of a proper loss and a link function. We extend existing results for binary losses to multiclass losses. We subsume results on âclassification calibrationâ by relating it to properness. We determine the stationarity condition, Bregman representation, order- sensitivity, and quasi-convexity of multiclass proper losses. We then characterise the existence and uniqueness of the composite representation for multiclass losses. We show how the composite representation is related to other core properties of a loss: mixability, admissibility and (strong) convexity of multiclass losses which we characterise in terms of the Hessian of the Bayes risk. We show that the simple integral representation for binary proper losses can not be extended to multiclass losses but offer concrete guidance regarding how to design different loss functions. The conclusion drawn from these results is that the proper composite representation is a natural and convenient tool for the design of multiclass loss functions.",
        "keywords": [
            "Proper losses",
            "Multiclass losses",
            "Link Functions",
            "Convexity and quasi-convexity     of losses",
            "Margin losses",
            "Classification calibration",
            "Parametrisations and representations of loss     functions",
            "Admissibility",
            "Mixability",
            "Minimaxity",
            ""
        ],
        "author": [
            "Robert C. Williamson",
            "Elodie Vernet",
            "Mark D. Reid"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-294/14-294.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Learning Latent Variable Models by Pairwise Cluster Comparison: Part I - Theory and Overview",
        "abstract": "Identification of latent variables that govern a problem and the relationships among them, given measurements in the observed world, are important for causal discovery. This identification can be accomplished by analyzing the constraints imposed by the latents in the measurements. We introduce the concept of pairwise cluster comparison (PCC) to identify causal relationships from clusters of data points and provide a two- stage algorithm called learning PCC (LPCC) that learns a latent variable model (LVM) using PCC. First, LPCC learns exogenous latents and latent colliders, as well as their observed descendants, by using pairwise comparisons between data clusters in the measurement space that may explain latent causes. Since in this first stage LPCC cannot distinguish endogenous latent non-colliders from their exogenous ancestors, a second stage is needed to extract the former, with their observed children, from the latter. If the true graph has no serial connections, LPCC returns the true graph, and if the true graph has a serial connection, LPCC returns a pattern of the true graph. LPCC's most important advantage is that it is not limited to linear or latent-tree models and makes only mild assumptions about the distribution. The paper is divided in two parts: Part I (this paper) provides the necessary preliminaries, theoretical foundation to PCC, and an overview of LPCC; Part II formally introduces the LPCC algorithm and experimentally evaluates its merit in different synthetic and real domains. The code for the LPCC algorithm and data sets used in the experiments reported in Part II are available online.",
        "keywords": [
            "causal discovery",
            "clustering",
            "learning latent variable model",
            "multiple indica-     tor model",
            ""
        ],
        "author": [
            "Nuaman Asbeh",
            "Boaz Lerner"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-401/14-401.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "GenSVM: A Generalized Multiclass Support Vector Machine",
        "abstract": "Traditional extensions of the binary support vector machine (SVM) to multiclass problems are either heuristics or require solving a large dual optimization problem. Here, a generalized multiclass SVM is proposed called GenSVM. In this method classification boundaries for a -class problem are constructed in a -dimensional space using a simplex encoding. Additionally, several different weightings of the misclassification errors are incorporated in the loss function, such that it generalizes three existing multiclass SVMs through a single optimization problem. An iterative majorization algorithm is derived that solves the optimization problem without the need of a dual formulation. This algorithm has the advantage that it can use warm starts during cross validation and during a grid search, which significantly speeds up the training phase. Rigorous numerical experiments compare linear GenSVM with seven existing multiclass SVMs on both small and large data sets. These comparisons show that the proposed method is competitive with existing methods in both predictive accuracy and training time, and that it significantly outperforms several existing methods on these criteria.",
        "keywords": [
            "support vector machines",
            "SVM",
            "multiclass classification",
            "iterative majoriza-     tion",
            "MM algorithm",
            ""
        ],
        "author": [
            "Gerrit J.J. van den Burg",
            "Patrick J.F. Groenen"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-526/14-526.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Scalable Approximate Bayesian Inference for Outlier Detection under Informative Sampling",
        "abstract": "Government surveys of business establishments receive a large volume of submissions where a small subset contain errors. Analysts need a fast-computing algorithm to flag this subset due to a short time window between collection and reporting. We offer a computationally-scalable optimization method based on non-parametric mixtures of hierarchical Dirichlet processes that allows discovery of multiple industry-indexed local partitions linked to a set of global cluster centers. Outliers are nominated as those clusters containing few observations. We extend an existing approach with a new merge step that reduces sensitivity to hyperparameter settings. Survey data are typically acquired under an informative sampling design where the probability of inclusion depends on the surveyed response such that the distribution for the observed sample is different from the population. We extend the derivation of a penalized objective function to use a pseudo-posterior that incorporates sampling weights that undo the informative design. We provide a simulation study to demonstrate that our approach produces unbiased estimation for the outlying cluster under informative sampling. The method is applied for outlier nomination for the Current Employment Statistics survey conducted by the Bureau of Labor Statistics.",
        "keywords": [],
        "author": [
            "Terrance D. Savitsky"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-088/15-088.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Approximate Newton Methods for Policy Search in Markov Decision Processes",
        "abstract": "Approximate Newton methods are standard optimization tools which aim to maintain the benefits of Newton's method, such as a fast rate of convergence, while alleviating its drawbacks, such as computationally expensive calculation or estimation of the inverse Hessian. In this work we investigate approximate Newton methods for policy optimization in Markov decision processes (MDPs). We first analyse the structure of the Hessian of the total expected reward, which is a standard objective function for MDPs. We show that, like the gradient, the Hessian exhibits useful structure in the context of MDPs and we use this analysis to motivate two Gauss-Newton methods for MDPs. Like the Gauss- Newton method for non-linear least squares, these methods drop certain terms in the Hessian. The approximate Hessians possess desirable properties, such as negative definiteness, and we demonstrate several important performance guarantees including guaranteed ascent directions, invariance to affine transformation of the parameter space and convergence guarantees. We finally provide a unifying perspective of key policy search algorithms, demonstrating that our second Gauss- Newton algorithm is closely related to both the EM-algorithm and natural gradient ascent applied to MDPs, but performs significantly better in practice on a range of challenging domains.",
        "keywords": [
            "Markov decision processes",
            "reinforcement learning",
            "Newton method",
            ""
        ],
        "author": [
            "Thomas Furmston",
            "Guy Lever",
            "David Barber"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-414/15-414.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Gains and Losses are Fundamentally Different in Regret Minimization: The Sparse Case",
        "abstract": "We demonstrate that, in the classical non-stochastic regret minimization problem with  decisions, gains and losses to be respectively maximized or minimized are fundamentally different. Indeed, by considering the additional sparsity assumption (at each stage, at most  decisions incur a nonzero outcome), we derive optimal regret bounds of different orders. Specifically, with gains, we obtain an optimal regret guarantee after  stages of order , so the classical dependency in the dimension is replaced by the sparsity size. With losses, we provide matching upper and lower bounds of order , which is decreasing in . Eventually, we also study the bandit setting, and obtain an upper bound of order  when outcomes are losses. This bound is proven to be optimal up to the logarithmic factor .",
        "keywords": [
            "regret minimization",
            "bandit",
            ""
        ],
        "author": [
            "Joon Kwon",
            "Vianney Perchet"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-503/15-503.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Linear Convergence of Randomized Feasible Descent Methods Under the Weak Strong Convexity Assumption",
        "abstract": "In this paper we generalize the framework of the Feasible Descent Method (FDM) to a Randomized (R-FDM) and a Randomized Coordinate-wise Feasible Descent Method (RC-FDM) framework. We show that many machine learning algorithms, including the famous SDCA algorithm for optimizing the SVM dual problem, or the stochastic coordinate descent method for the LASSO problem, fits into the framework of RC-FDM. We prove linear convergence for both R-FDM and RC-FDM under the weak strong convexity assumption. Moreover, we show that the duality gap converges linearly for RC-FDM, which implies that the duality gap also converges linearly for SDCA applied to the SVM dual problem.",
        "keywords": [
            "feasible descent method",
            "stochastic methods",
            "iteration complexity",
            "conver-     gence theory",
            ""
        ],
        "author": [
            "Chenxin Ma",
            "Rachael Tappenden",
            "Martin  Takáč"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-541/15-541.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Practical Scheme and Fast Algorithm to Tune the Lasso With Optimality Guarantees",
        "abstract": "We introduce a novel scheme for choosing the regularization parameter in high-dimensional linear  regression with Lasso. This scheme, inspired by Lepskiâs method for bandwidth selection in non-parametric  regression, is equipped with both optimal finite-sample guarantees and a fast algorithm. In particular, for any design matrix such that the Lasso has low sup-norm error under an âoracle choiceâ of the  regularization parameter, we show that our method matches the oracle performance up to a small constant factor,  and show that it can be implemented by performing simple tests along a single Lasso path. By applying the  Lasso to simulated and real data, we find that our novel scheme can be faster and more accurate than  standard schemes such as Cross-Validation.",
        "keywords": [
            "Lasso",
            "regularization parameter",
            "tuning parameter",
            "high-dimensional regres-     sion",
            ""
        ],
        "author": [
            "Michael Chichignoud",
            "Johannes Lederer",
            "Martin J. Wainwright"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-605/15-605.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Learning Latent Variable Models by Pairwise Cluster Comparison: Part II - Algorithm and Evaluation",
        "abstract": "It is important for causal discovery to identify any latent variables that govern a problem and the relationships among them, given measurements in the observed world. In Part I of this paper, we were interested in learning a discrete latent variable model (LVM) and introduced the concept of pairwise cluster comparison (PCC) to identify causal relationships from clusters of data points and an overview of a two-stage algorithm for learning PCC (LPCC). First, LPCC learns exogenous latent variables and latent colliders, as well as their observed descendants, by using pairwise comparisons between data clusters in the measurement space that may explain latent causes. Second, LPCC identifies endogenous latent non- colliders with their observed children. In Part I, we showed that if the true graph has no serial connections, then LPCC returns the true graph, and if the true graph has a serial connection, then LPCC returns a pattern of the true graph. In this paper (Part II), we formally introduce the LPCC algorithm that implements the PCC concept. In addition, we thoroughly evaluate LPCC using simulated and real-world data sets in comparison to state-of-the-art algorithms. Besides using three real-world data sets, which have already been tested in learning an LVM, we also evaluate the algorithms using data sets that represent two original problems. The first problem is identifying young drivers' involvement in road accidents, and the second is identifying cellular subpopulations of the immune system from mass cytometry. The results of our evaluation show that LPCC improves in accuracy with the sample size, can learn large LVMs, and is accurate in learning compared to state-of- the-art algorithms. The code for the LPCC algorithm and data sets used in the experiments reported here are available online.",
        "keywords": [
            "learning latent variable models",
            "graphical models",
            "clustering",
            ""
        ],
        "author": [
            "Nuaman Asbeh",
            "Boaz Lerner"
        ],
        "ref": "http://jmlr.org/papers/volume17/14-402/14-402.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "A Characterization of Linkage-Based Hierarchical Clustering",
        "abstract": "The class of linkage-based algorithms is perhaps the most popular class of hierarchical algorithms. We identify two properties of hierarchical algorithms, and prove that linkage- based algorithms are the only ones that satisfy both of these properties. Our characterization clearly delineates the difference between linkage-based algorithms and other hierarchical methods. We formulate an intuitive notion of locality of a hierarchical algorithm that distinguishes between linkage-based and global hierarchical algorithms like bisecting -means, and prove that popular divisive hierarchical algorithms produce clusterings that cannot be produced by any linkage-based algorithm.",
        "keywords": [],
        "author": [
            "Margareta Ackerman",
            "Shai Ben-David"
        ],
        "ref": "http://jmlr.org/papers/volume17/11-198/11-198.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Integrative Analysis using Coupled Latent Variable Models for Individualizing Prognoses",
        "abstract": "Complex chronic diseases (e.g., autism, lupus, and Parkinson's) are remarkably heterogeneous across individuals. This heterogeneity makes treatment difficult for caregivers because they cannot accurately predict the way in which the disease will progress in order to guide treatment decisions. Therefore, tools that help to predict the trajectory of these complex chronic diseases can help to improve the quality of health care. To build such tools, we can leverage clinical markers that are collected at baseline when a patient first presents and longitudinally over time during follow-up visits. Because complex chronic diseases are typically systemic, the longitudinal markers often track disease progression in multiple organ systems. In this paper, our goal is to predict a function of time that models the future trajectory of a single target clinical marker tracking a disease process of interest. We want to make these predictions using the histories of many related clinical markers as input. Our proposed solution tackles several key challenges. First, we can easily handle irregularly and sparsely sampled markers, which are standard in clinical data. Second, the number of parameters and the computational complexity of learning our model grows linearly in the number of marker types included in the model. This makes our approach applicable to diseases where many different markers are recorded over time. Finally, our model accounts for latent factors influencing disease expression, whereas standard regression models rely on observed features alone to explain variability. Moreover, our approach can be applied dynamically in continous- time and updates its predictions as soon as any new data is available. We apply our approach to the problem of predicting lung disease trajectories in scleroderma, a complex autoimmune disease. We show that our model improves over state-of-the-art baselines in predictive accuracy and we provide a qualitative analysis of our model's output. Finally, the variability of disease presentation in scleroderma makes clinical trial recruitment challenging. We show that a prognostic tool that integrates multiple types of routinely collected longitudinal data can be used to identify individuals at greatest risk of rapid progression and to target trial recruitment.",
        "keywords": [
            "gaussian processes",
            "conditional random fields",
            "prediction of functional targets",
            "latent variable models",
            "disease trajectories",
            ""
        ],
        "author": [
            "Peter Schulam",
            "Suchi Saria"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-436/15-436.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "An Error Bound for L1-norm Support Vector Machine Coefficients in Ultra-high Dimension",
        "abstract": "Comparing with the standard -norm support vector machine (SVM), the -norm SVM enjoys the nice property of simultaneously preforming classification and feature selection. In this paper, we investigate the statistical performance of -norm SVM in ultra-high dimension, where the number of features  grows at an exponential rate of the sample size . Different from existing theory for SVM which has been mainly focused on the generalization error rates and empirical risk, we study the asymptotic behavior of the coefficients of -norm SVM. Our analysis reveals that the -norm SVM coefficients achieve near oracle rate, that is, with high probability, the  error bound of the estimated -norm SVM coefficients is of order , where  is the number of features with nonzero coefficients. Furthermore, we show that if the -norm SVM is used as an initial value for a recently proposed algorithm for solving non- convex penalized SVM (Zhang et al., 2016b), then in two iterative steps it is guaranteed to produce an estimator that possesses the oracle property in ultra-high dimension, which in particular implies that with probability approaching one the zero coefficients are estimated as exactly zero. Simulation studies demonstrate the fine performance of -norm SVM as a sparse classifier and its effectiveness to be utilized to solve non-convex penalized SVM problems in high dimension.",
        "keywords": [
            "feature selection",
            ""
        ],
        "author": [
            "Bo Peng",
            "Lan Wang",
            "Yichao Wu"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-654/15-654.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Blending Learning and Inference in Conditional Random Fields",
        "abstract": "Conditional random fields maximize the log-likelihood of training labels given the training data, e.g., objects given images. In many cases the training labels are structures that consist of a set of variables and the computational complexity for estimating their likelihood is exponential in the number of the variables. Learning algorithms relax this computational burden using approximate inference that is nested as a sub- procedure. In this paper we describe the objective function for nested learning and inference in conditional random fields. The devised objective maximizes the log-beliefs --- probability distributions over subsets of training variables that agree on their marginal probabilities. This objective is concave and consists of two types of variables that are related to the learning and inference tasks respectively. Importantly, we afterwards show how to blend the learning and inference procedure and effectively get to the identical optimum much faster. The proposed algorithm currently achieves the state-of- the-art in various computer vision applications.",
        "keywords": [],
        "author": [
            "Tamir Hazan",
            "Alexander G. Schwing",
            "Raquel Urtasun"
        ],
        "ref": "http://jmlr.org/papers/volume17/13-260/13-260.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "Distributed Submodular Maximization",
        "abstract": "Many large-scale machine learning problems--clustering, non- parametric learning, kernel machines, etc.--require selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Classical approaches to submodular optimization require centralized access to the full dataset, which is impractical for truly large-scale problems. In this paper, we consider the problem of submodular function maximization in a distributed fashion. We develop a simple, two- stage protocol GREEDI, that is easily implemented using MapReduce style computations. We theoretically analyze our approach, and show that under certain natural conditions, performance close to the centralized approach can be achieved. We begin with monotone submodular maximization subject to a cardinality constraint, and then extend this approach to obtain approximation guarantees for (not necessarily monotone) submodular maximization subject to more general constraints including matroid or knapsack constraints. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including sparse Gaussian process inference and exemplar based clustering on tens of millions of examples using Hadoop.",
        "keywords": [
            "distributed computing",
            "submodular functions",
            "approximation algorithms",
            "greedy algorithms",
            ""
        ],
        "author": [
            "Baharan Mirzasoleiman",
            "Amin Karbasi",
            "Rik Sarkar",
            "Andreas Krause"
        ],
        "ref": "http://jmlr.org/papers/volume17/mirzasoleiman16a/mirzasoleiman16a.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    },
    {
        "title": "On the properties of variational approximations of Gibbs posteriors",
        "abstract": "The PAC-Bayesian approach is a powerful set of techniques to derive non-asymptotic risk bounds for random estimators. The corresponding optimal distribution of estimators, usually called the Gibbs posterior, is unfortunately often intractable. One may sample from it using Markov chain Monte Carlo, but this is usually too slow for big datasets. We consider instead variational approximations of the Gibbs posterior, which are fast to compute. We undertake a general study of the properties of such approximations. Our main finding is that such a variational approximation has often the same rate of convergence as the original PAC-Bayesian procedure it approximates. In addition, we show that, when the risk function is convex, a variational approximation can be obtained in polynomial time using a convex solver. We give finite sample oracle inequalities for the corresponding estimator. We specialize our results to several learning tasks (classification, ranking, matrix completion), discuss how to implement a variational approximation in each case, and illustrate the good properties of said approximation on real datasets.",
        "keywords": [],
        "author": [
            "Pierre Alquier",
            "James Ridgway",
            "Nicolas Chopin"
        ],
        "ref": "http://jmlr.org/papers/volume17/15-290/15-290.pdf",
        "datasource": "Journal of Machine Learning Research",
        "datasource_url": "https://jmlr.csail.mit.edu"
    }
]
}