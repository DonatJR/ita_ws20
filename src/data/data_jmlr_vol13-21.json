{
    "papers": [
        {
            "title": "Distance Metric Learning with Eigenvalue Optimization",
            "abstract": "The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric.  Within this context, we introduce a novel metric learning approach called DML-eig  which is shown to be equivalent to  a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996).  Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues  to the design of efficient metric learning algorithms.   Indeed,  first-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their convergence characteristics are rigorously established.  Various experiments on benchmark data sets show the competitive performance  of our new approaches. In addition, we report an encouraging result on a difficult and challenging face verification data set called Labeled Faces in the Wild (LFW).",
            "keywords": [
                "metric learning",
                "convex optimization",
                "semi-definite programming",
                "first-order methods",
                "eigenvalue optimization",
                "matrix factorization"
            ],
            "author": [
                "Yiming Ying",
                "Peng Li"
            ],
            "ref": "http://jmlr.org/papers/volume13/ying12a/ying12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection",
            "abstract": "We present a unifying framework for information theoretic feature selection, bringing almost two decades of research on heuristic filter criteria under a single theoretical interpretation.  This is in response to the question: \"what are the implicit statistical assumptions of feature selection criteria based on mutual information?\".  To answer this, we adopt a different strategy than is usual in the feature selection literature−instead of trying to define a criterion, we derive one, directly from a clearly specified objective function: the conditional likelihood of the training labels.  While many hand-designed heuristic criteria try to optimize a definition of feature 'relevancy' and 'redundancy', our approach leads to a probabilistic framework which naturally incorporates these concepts.  As a result we can unify the numerous criteria published over the last two decades, and show them to be low-order approximations to the exact (but intractable) optimisation problem.  The primary contribution is to show that common heuristics for information based feature selection (including Markov Blanket algorithms as a special case) are approximate iterative maximisers of the conditional likelihood. A large empirical study provides strong evidence to favour certain classes of criteria, in particular those that balance the relative size of the relevancy/redundancy terms.  Overall we conclude that the JMI criterion (Yang and Moody, 1999; Meyer et al., 2008) provides the best tradeoff in terms of accuracy, stability, and flexibility with small data samples.",
            "keywords": [
                "feature selection",
                "mutual information"
            ],
            "author": [
                "Gavin Brown",
                "Adam Pocock",
                "Ming-Jie Zhao",
                "Mikel Luján"
            ],
            "ref": "http://jmlr.org/papers/volume13/brown12a/brown12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Plug-in Approach to Active Learning",
            "abstract": "We present a new active learning algorithm based on nonparametric estimators of the regression function.  Our investigation provides probabilistic bounds for the rates of convergence of the generalization error achievable by proposed method over a broad class of underlying distributions.  We also prove minimax lower bounds which show that the obtained rates are almost tight.",
            "keywords": [
                "active learning",
                "selective sampling",
                "model selection",
                "classification"
            ],
            "author": [
                "Stanislav Minsker"
            ],
            "ref": "http://jmlr.org/papers/volume13/minsker12a/minsker12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity",
            "abstract": "Given a set V of  n elements we wish to linearly order them given pairwise preference labels which may be non-transitive (due to irrationality or arbitrary noise).   The goal is to linearly order the elements while disagreeing with as few pairwise preference labels as possible.  Our performance is measured by two parameters:  The number of disagreements (loss) and the query complexity (number of pairwise preference labels).  Our algorithm adaptively queries  at most O(ε-6n log5 n) preference labels for a regret of ε times the optimal loss.  As a function of n, this is asymptotically better than standard (non-adaptive) learning bounds achievable for the same problem.   Our main result takes us a step closer toward settling an open problem posed by learning-to-rank (from pairwise information) theoreticians and practitioners: What is a provably correct way to sample preference labels?  To further  show the power and practicality of our solution, we analyze a typical test case in which a large margin linear relaxation is used for efficiently solving the simpler learning problems in our decomposition.",
            "keywords": [
                "statistical learning theory",
                "active learning",
                "ranking",
                "pairwise ranking"
            ],
            "author": [
                "Nir Ailon"
            ],
            "ref": "http://jmlr.org/papers/volume13/ailon12a/ailon12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Distributed Online Prediction Using Mini-Batches",
            "abstract": "Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms.  We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment.  We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem.",
            "keywords": [
                "distributed computing",
                "online learning",
                "stochastic optimization",
                "regret bounds"
            ],
            "author": [
                "Ofer Dekel",
                "Ran Gilad-Bachrach",
                "Ohad Shamir",
                "Lin Xiao"
            ],
            "ref": "http://jmlr.org/papers/volume13/dekel12a/dekel12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Clustering of Biological Sequences",
            "abstract": "Given a point set S and an unknown metric d on S, we study the problem of efficiently partitioning S into k clusters while querying few distances between the points.  In our model we assume that we have access to one versus all queries that given a point s ∈ S return the distances between s and all other points.  We show that given a natural assumption about the structure of the instance, we can efficiently find an accurate clustering using only O(k) distance queries.   Our algorithm uses an active selection strategy to choose a small set of points that we call landmarks, and considers only the distances between landmarks and other points to produce a clustering.  We use our procedure to cluster proteins by sequence similarity.  This setting nicely fits our model because we can use a fast sequence database search program to query a sequence against an entire data set.  We conduct an empirical study that shows that even though we query a small fraction of the distances between the points, we produce clusterings that are close to a desired clustering given by manual classification.",
            "keywords": [
                "clustering",
                "active clustering",
                "k-median",
                "approximation algorithms",
                "approximation sta-     bility",
                "clustering accuracy"
            ],
            "author": [
                "Konstantin Voevodski",
                "Maria-Florina Balcan",
                "Heiko Röglin",
                "Shang-Hua Teng",
                "Yu Xia"
            ],
            "ref": "http://jmlr.org/papers/volume13/voevodski12a/voevodski12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Learning via Perfect Selective Classification",
            "abstract": "We discover a strong relation between two known learning models: stream-based active learning and perfect selective classification (an extreme case of 'classification with a reject option').  For these models, restricted to the realizable case, we show a reduction of active learning to selective classification that preserves fast rates.  Applying this reduction to recent results for selective classification, we derive exponential target-independent label complexity speedup for actively learning general (non-homogeneous) linear classifiers when the data distribution is an arbitrary high dimensional mixture of Gaussians. Finally, we study the relation between the proposed technique and existing label complexity measures, including teaching dimension and disagreement coefficient.",
            "keywords": [
                "classification with a reject option",
                "perfect classification",
                "selective classification",
                "ac-     tive learning",
                "selective sampling",
                "disagreement coefficient",
                "teaching dimension"
            ],
            "author": [
                "Ran El-Yaniv",
                "Yair Wiener"
            ],
            "ref": "http://jmlr.org/papers/volume13/el-yaniv12a/el-yaniv12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Search for Hyper-Parameter Optimization",
            "abstract": "Grid search and manual search are the most widely used strategies for hyper-parameter optimization.  This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid.  Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks.  Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time.  Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space.  Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven.  A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets.  This phenomenon makes grid search a poor choice for configuring algorithms for new data sets.  Our analysis casts some light on why recent \"High Throughput\" methods achieve surprising success−they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much.  We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.",
            "keywords": [
                "global optimization",
                "model selection",
                "neural networks",
                "deep learning"
            ],
            "author": [
                "James Bergstra",
                "Yoshua Bengio"
            ],
            "ref": "http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bounding the Probability of Error for High Precision Optical Character Recognition",
            "abstract": "We consider a model for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low recall. If some variables can be identified with near certainty, they can be conditioned upon, allowing further inference to be done efficiently.  Specifically, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. This \"clean set\" is subsequently used as document-specific training data.  While OCR systems produce confidence measures for the identity of each letter or word, thresholding these values still produces a significant number of errors.\n\nWe introduce a novel technique for identifying a set of correct words with very high precision. Rather than estimating posterior probabilities, we bound the probability that any given word is incorrect using an approximate worst case analysis.  We give empirical results on a data set of difficult historical newspaper scans, demonstrating that our method for identifying correct words makes only two errors in 56 documents.  Using document-specific character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1% from an initial OCR system's translation.",
            "keywords": [
                "optical character recognition",
                "probability bounding",
                "document-specific modeling"
            ],
            "author": [
                "Gary B. Huang",
                "Andrew Kae",
                "Carl Doersch",
                "Erik Learned-Miller"
            ],
            "ref": "http://jmlr.org/papers/volume13/huang12a/huang12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming",
            "abstract": "Sparse additive models are families of d-variate functions with the additive decomposition f* = ∑j ∈ S f*j, where S is an unknown subset of cardinality s << d. In this paper, we consider the case where each univariate component function f*j lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f* based on kernels combined with l1-type convex regularization.  Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L2(P) and L2(Pn) norms over the class  Fd,s,H of sparse additive models with each univariate function f*j in the unit ball of a univariate RKHS with bounded kernel function.  We complement our upper bounds by deriving minimax lower bounds on the L2(P) error, thereby showing the optimality of our method.  Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes.  We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much faster estimation rates are possible for any sparsity s = Ω(√n), showing that global boundedness is a significant restriction in the high-dimensional setting.",
            "keywords": [
                "sparsity",
                "kernel",
                "non-parametric",
                "convex"
            ],
            "author": [
                "Garvesh Raskutti",
                "Martin J. Wainwright",
                "Bin Yu"
            ],
            "ref": "http://jmlr.org/papers/volume13/raskutti12a/raskutti12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Learning in the Embedded Manifold of Low-rank Matrices",
            "abstract": "When learning models that are represented in matrix forms, enforcing a low-rank constraint can dramatically improve the memory and run time complexity, while providing a natural regularization of the model. However, naive approaches to minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low-rank matrix). We build on recent advances in optimization over manifolds, and describe an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold. While the ideal retraction is costly to compute, and so is the projection operator that approximates it, we describe another retraction that can be computed efficiently. It has run time and memory complexity of O ( (n+m)k ) for a rank-k matrix of dimension m X n, when using an online procedure with rank-one gradients. We use this algorithm, LORETA, to learn a matrix-form similarity measure over pairs of documents represented as high dimensional vectors. LORETA improves the mean average precision over a passive-aggressive approach in a factorized model, and also improves over a full model trained on pre-selected features using the same memory requirements. We further adapt LORETA to learn positive semi-definite low-rank matrices, providing an online algorithm for low-rank metric learning. LORETA also shows consistent improvement over standard weakly supervised methods in a large (1600 classes and 1 million images, using ImageNet) multi-label image classification task.",
            "keywords": [
                "low rank",
                "Riemannian manifolds",
                "metric learning",
                "retractions",
                "multitask learning"
            ],
            "author": [
                "Uri Shalit",
                "Daphna Weinshall",
                "Gal Chechik"
            ],
            "ref": "http://jmlr.org/papers/volume13/shalit12a/shalit12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Assignment Clustering for Boolean Data",
            "abstract": "We propose a probabilistic model for clustering Boolean data where an object can be simultaneously assigned to multiple clusters. By explicitly modeling the underlying generative process that combines the  individual source emissions, highly structured data are expressed with substantially fewer clusters compared to single-assignment clustering. As a consequence, such a model provides robust parameter estimators even when the number of samples is low. We extend the model with different noise processes and demonstrate that maximum-likelihood estimation with multiple assignments consistently infers source parameters more accurately than single-assignment clustering. Our model is primarily motivated by the task of role mining for role-based access control, where users of a system are assigned one or more roles. In experiments with real-world access-control data, our model exhibits better generalization performance than state-of-the-art approaches.",
            "keywords": [
                "clustering",
                "multi-assignments",
                "overlapping clusters",
                "Boolean data",
                "role mining"
            ],
            "author": [
                "Mario Frank",
                "Andreas P. Streich",
                "David Basin",
                "Joachim M. Buhmann"
            ],
            "ref": "http://jmlr.org/papers/volume13/frank12a/frank12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks",
            "abstract": "With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a data set labeled by multiple annotators in a short amount of time.  Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Since we do not have control over the quality of the annotators, very often the annotations can be dominated by spammers, defined as annotators who assign labels randomly without actually looking at the instance.  Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the final consensus labels.  In this paper we propose an empirical Bayesian algorithm called SpEM that iteratively eliminates the spammers and estimates the consensus labels based only on the good annotators.  The algorithm is motivated by defining a spammer score that can be used to rank the annotators.  Experiments on simulated and real data show that the proposed approach is better than (or as good as) the earlier approaches in terms of the accuracy and uses a significantly smaller number of annotators.",
            "keywords": [
                "crowdsourcing",
                "multiple annotators",
                "ranking annotators"
            ],
            "author": [
                "Vikas C. Raykar",
                "Shipeng Yu"
            ],
            "ref": "http://jmlr.org/papers/volume13/raykar12a/raykar12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Metric and Kernel Learning Using a Linear Transformation",
            "abstract": "Metric and kernel learning arise in several machine learning applications.  However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points.  In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem.  In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework---that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework.  Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations.  We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction.",
            "keywords": [
                "metric learning",
                "kernel learning",
                "linear transformation",
                "matrix divergences"
            ],
            "author": [
                "Prateek Jain",
                "Brian Kulis",
                "Jason V. Davis",
                "Inderjit S. Dhillon"
            ],
            "ref": "http://jmlr.org/papers/volume13/jain12a/jain12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MULTIBOOST: A Multi-purpose Boosting Package",
            "abstract": "The MULTIBOOST package provides a fast C++ implementation of multi-class/multi-label/multi-task boosting algorithms. It is based on ADABOOST.MH but it also implements popular cascade classifiers and FILTERBOOST. The package contains common multi-class base learners (stumps, trees, products, Haar filters). Further base learners and strong learners following the boosting paradigm can be easily implemented in a flexible framework.",
            "keywords": [
                "boosting"
            ],
            "author": [
                "Djalel Benbouzid",
                "Róbert Busa-Fekete",
                "Norman Casagrande",
                "François-David Collin",
                "Balázs Kégl"
            ],
            "ref": "http://jmlr.org/papers/volume13/benbouzid12a/benbouzid12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel",
            "abstract": "Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classification analyses in a systematic yet flexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. ML-Flex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net.",
            "keywords": [
                "toolbox",
                "classification",
                "parallel",
                "ensemble"
            ],
            "author": [
                "Stephen R. Piccolo",
                "Lewis J. Frey"
            ],
            "ref": "http://jmlr.org/papers/volume13/piccolo12a/piccolo12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Primal-Dual Convergence Analysis of Boosting",
            "abstract": "Boosting combines weak learners into a predictor with low empirical risk.  Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated.  This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing:  • Weak learnability aids the whole loss family: for any ε > 0, O(ln(1/ε)) iterations suffice to produce a predictor with empirical risk ε-close to the infimum;  • The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O(ln(1/ε));  • Arbitrary instances may be decomposed into the above two, granting rate O(1/ε), with a matching lower bound provided for the logistic loss.",
            "keywords": [
                "boosting",
                "convex analysis",
                "weak learnability",
                "coordinate descent"
            ],
            "author": [
                "Matus Telgarsky"
            ],
            "ref": "http://jmlr.org/papers/volume13/telgarsky12a/telgarsky12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-Sparse Multiple Kernel Fisher Discriminant Analysis",
            "abstract": "Sparsity-inducing multiple kernel Fisher discriminant analysis (MK-FDA) has been studied in the literature. Building on recent advances in non-sparse multiple kernel learning (MKL), we propose a non-sparse version of MK-FDA, which imposes a general lp norm regularisation on the kernel weights. We formulate the associated optimisation problem as a semi-infinite program (SIP), and adapt an iterative wrapper algorithm to solve it. We then discuss, in light of latest advances in MKL optimisation techniques, several reformulations and optimisation strategies that can potentially lead to significant improvements in the efficiency and scalability of MK-FDA. We carry out extensive experiments on six datasets from various application areas, and compare closely the performance of lp MK-FDA, fixed norm MK-FDA, and several variants of SVM-based MKL (MK-SVM). Our results demonstrate that lp MK-FDA improves upon sparse MK-FDA in many practical situations. The results also show that on image categorisation problems, lp MK-FDA tends to outperform its SVM counterpart. Finally, we also discuss the connection between (MK-)FDA and (MK-)SVM, under the unified framework of regularised kernel machines.",
            "keywords": [
                "multiple kernel learning",
                "kernel fisher discriminant analysis",
                "regularised least squares"
            ],
            "author": [
                "Fei Yan",
                "Josef Kittler",
                "Krystian Mikolajczyk",
                "Atif Tahir"
            ],
            "ref": "http://jmlr.org/papers/volume13/yan12a/yan12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Algorithms for the Classification Restricted Boltzmann Machine",
            "abstract": "Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely semi-supervised and multitask learning.",
            "keywords": [
                "restricted Boltzmann machine",
                "classification",
                "discriminative learning"
            ],
            "author": [
                "Hugo Larochelle",
                "Michael Mandel",
                "Razvan Pascanu",
                "Yoshua Bengio"
            ],
            "ref": "http://jmlr.org/papers/volume13/larochelle12a/larochelle12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structured Sparsity and Generalization",
            "abstract": "We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an infinite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels.",
            "keywords": [
                "empirical processes",
                "Rademacher average"
            ],
            "author": [
                "Andreas Maurer",
                "Massimiliano Pontil"
            ],
            "ref": "http://jmlr.org/papers/volume13/maurer12a/maurer12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Case Study on Meta-Generalising: A Gaussian Processes Approach",
            "abstract": "We propose a novel model for meta-generalisation, that is, performing prediction on novel tasks based on information from multiple different but related tasks. The model is based on two coupled Gaussian processes with structured covariance function; one model performs predictions by learning a constrained covariance function encapsulating the relations between the various training tasks, while the second model determines the similarity of new tasks to previously seen tasks. We demonstrate empirically on several real and synthetic data sets both the strengths of the approach and its limitations due to the distributional assumptions underpinning it.",
            "keywords": [
                "transfer learning",
                "meta-generalising",
                "multi-task learning",
                "Gaussian processes"
            ],
            "author": [
                "Grigorios Skolidis",
                "Guido Sanguinetti"
            ],
            "ref": "http://jmlr.org/papers/volume13/skolidis12a/skolidis12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Kernel Two-Sample Test",
            "abstract": "We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions.  Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).  We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic.  The MMD can be computed in quadratic time, although efficient linear time approximations are available.  Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS.  We apply our two-sample tests  to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly.  Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.",
            "keywords": [],
            "author": [
                "Arthur Gretton",
                "Karsten M. Borgwardt",
                "Malte J. Rasch",
                "Bernhard Schölkopf",
                "Alexander Smola"
            ],
            "ref": "http://jmlr.org/papers/volume13/gretton12a/gretton12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression",
            "abstract": "This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper.",
            "keywords": [
                "Gaussian process regression",
                "domain decomposition method",
                "partial independent con-     ditional",
                "bagging for Gaussian process"
            ],
            "author": [
                "Chiwoo Park",
                "Jianhua Z. Huang",
                "Yu Ding"
            ],
            "ref": "http://jmlr.org/papers/volume13/park12a/park12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso",
            "abstract": "We consider the sparse inverse covariance regularization problem or graphical lasso with regularization  parameter λ.  Suppose the sample covariance graph formed by thresholding the entries of the sample covariance matrix at λ is decomposed into connected components.  We show that the vertex-partition induced by the connected components of the thresholded sample covariance graph (at λ) is exactly equal to that induced by the connected components of the estimated concentration graph, obtained by solving the graphical lasso problem for the same λ.  This characterizes a very interesting property of a path of graphical lasso solutions.  Furthermore, this simple rule, when used as a wrapper around existing algorithms for the graphical lasso, leads to enormous performance gains. For a range of values of λ, our proposal splits a large graphical lasso problem into smaller tractable problems, making it possible to solve an otherwise infeasible large-scale problem. We illustrate the graceful scalability of our proposal via synthetic and real-life microarray examples.",
            "keywords": [
                "sparse inverse covariance selection",
                "sparsity",
                "graphical lasso",
                "Gaussian graphical mod-      els",
                "graph connected components",
                "concentration graph"
            ],
            "author": [
                "Rahul Mazumder",
                "Trevor Hastie"
            ],
            "ref": "http://jmlr.org/papers/volume13/mazumder12a/mazumder12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Algorithms for Learning Kernels Based on Centered Alignment",
            "abstract": "This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difficult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classification and regression.  Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efficient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization.  Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classification and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classification and regression.",
            "keywords": [
                "kernel methods",
                "learning kernels"
            ],
            "author": [
                "Corinna Cortes",
                "Mehryar Mohri",
                "Afshin Rostamizadeh"
            ],
            "ref": "http://jmlr.org/papers/volume13/cortes12a/cortes12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Bounds and Observable Constraints for Non-deterministic Models",
            "abstract": "Conditional independence relations involving latent variables do not necessarily imply observable independences. They may imply inequality constraints on observable parameters and causal bounds, which can be used for falsification and identification. The literature on computing such constraints often involve a deterministic underlying data generating process in a counterfactual framework. If an analyst is ignorant of the nature of the underlying mechanisms then they may wish to use a model which allows the underlying mechanisms to be probabilistic. A method of computation for a weaker model without any determinism is given here and demonstrated for the instrumental variable model, though applicable to other models. The approach is based on the analysis of mappings with convex polytopes in a decision theoretic framework and can be implemented in readily available polyhedral computation software. Well known constraints and bounds are replicated in a probabilistic model and novel ones are computed for instrumental variable models without non-deterministic versions of the randomization, exclusion restriction and monotonicity assumptions respectively.",
            "keywords": [
                "instrumental variables",
                "instrumental inequality",
                "causal bounds",
                "convex polytope",
                "latent     variables"
            ],
            "author": [
                "Roland R. Ramsahai"
            ],
            "ref": "http://jmlr.org/papers/volume13/ramsahai12a/ramsahai12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "NIMFA : A Python Library for Nonnegative Matrix Factorization",
            "abstract": "NIMFA is an open-source Python library that provides a unified interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA's component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks.",
            "keywords": [
                "nonnegative matrix factorization",
                "initialization methods",
                "quality measures",
                "scripting"
            ],
            "author": [
                "Marinka Žitnik",
                "Blaž Zupan"
            ],
            "ref": "http://jmlr.org/papers/volume13/zitnik12a/zitnik12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stability of Density-Based Clustering",
            "abstract": "High density clusters can be characterized by the connected components of a level set L(λ) = {x: p(x)>λ} of the underlying probability density function p generating the data, at some appropriate level λ ≥ 0. The complete hierarchical clustering can be characterized by a cluster tree T= ∪λL(λ).  In this paper, we study the behavior of a density level set estimate  L̂(λ) and cluster tree estimate T̂ based on a kernel density estimator with kernel bandwidth h. We define two notions of instability to measure the variability of L̂(λ) and T̂ as a function of h, and investigate the theoretical properties of these instability measures.",
            "keywords": [
                "clustering",
                "density estimation",
                "level sets",
                "stability"
            ],
            "author": [
                "Alessandro Rinaldo",
                "Aarti Singh",
                "Rebecca Nugent",
                "Larry Wasserman"
            ],
            "ref": "http://jmlr.org/papers/volume13/rinaldo12a/rinaldo12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features",
            "abstract": "This paper proposes several novel methods, based on machine learning, to detect malware in executable files without any need for preprocessing, such as unpacking or disassembling. The basic method (Mal-ID) is a new static (form-based) analysis methodology that uses common segment analysis in order to detect malware files. By using common segment analysis, Mal-ID is able to discard malware parts that originate from benign code. In addition, Mal-ID uses a new kind of feature, termed meta-feature, to better capture the properties of the analyzed segments. Rather than using the entire file, as is usually the case with machine learning based techniques, the new approach detects malware on the segment level. This study also introduces two Mal-ID extensions that improve the Mal-ID basic method in various aspects. We rigorously evaluated Mal-ID and its two extensions with more than ten performance measures, and compared them to the highly rated boosted decision tree method under identical settings. The evaluation demonstrated that Mal-ID and the two Mal-ID extensions outperformed the boosted decision tree method in almost all respects. In addition, the results indicated that by extracting meaningful features, it is sufficient to employ one simple detection rule for classifying executable files.",
            "keywords": [
                "computer security",
                "malware detection",
                "common segment analysis"
            ],
            "author": [
                "Gil Tahan",
                "Lior Rokach",
                "Yuval Shahar"
            ],
            "ref": "http://jmlr.org/papers/volume13/tahan12a/tahan12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sampling Methods for the Nystr&#246;m Method",
            "abstract": "The Nyström method is an efficient technique to generate low-rank matrix approximations and is used in several large-scale learning applications.  A key aspect of this method is the procedure according to which columns are sampled from the original matrix.  In this work, we explore the efficacy of a variety of fixed and adaptive sampling schemes.  We also propose a family of ensemble-based sampling algorithms for the Nyström method. We report results of extensive experiments that provide a detailed comparison of various fixed and adaptive sampling techniques, and demonstrate the performance improvement associated with the ensemble Nyström method when used in conjunction with either fixed or adaptive sampling schemes.  Corroborating these empirical findings, we present a theoretical analysis of the Nyström method, providing novel error bounds guaranteeing a better convergence rate of the ensemble Nyström method in comparison to the standard Nyström method.",
            "keywords": [
                "low-rank approximation"
            ],
            "author": [
                "Sanjiv Kumar",
                "Mehryar Mohri",
                "Ameet Talwalkar"
            ],
            "ref": "http://jmlr.org/papers/volume13/kumar12a/kumar12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Positive Semidefinite Metric Learning Using Boosting-like Algorithms",
            "abstract": "The success of many machine learning and pattern recognition methods relies heavily upon the identification of an appropriate distance metric on the input data.  It is often beneficial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance.  In this work, we propose a boosting-based technique, termed BOOSTMETRIC, for learning a quadratic Mahalanobis distance metric.  Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semidefinite.  Semidefinite programming is often used to enforce this constraint, but does not scale well and is not easy to implement.  BOOSTMETRIC is instead based on the observation that any positive semidefinite matrix can be decomposed into a linear combination of trace-one rank-one matrices.  BOOSTMETRIC thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process.  The resulting methods are easy to implement, efficient, and can accommodate various types of constraints.  We extend traditional boosting algorithms in that its weak learner is a positive semidefinite matrix with trace and rank being one rather than a classifier or regressor.  Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classification accuracy and running time.",
            "keywords": [
                "Mahalanobis distance",
                "semidefinite programming",
                "column generation",
                "boosting",
                "La-     grange duality"
            ],
            "author": [
                "Chunhua Shen",
                "Junae Kim",
                "Lei Wang",
                "Anton van den Hengel"
            ],
            "ref": "http://jmlr.org/papers/volume13/shen12a/shen12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistent Model Selection Criteria on High Dimensions",
            "abstract": "Asymptotic properties of model selection criteria for high-dimensional regression models are studied where the dimension of covariates is much larger than the sample size. Several sufficient conditions for model selection consistency are provided.  Non-Gaussian error distributions are considered and it is shown that the maximal number of covariates for model selection consistency depends on the tail behavior of the error distribution. Also, sufficient conditions for model selection consistency are given when the variance of the noise is neither known nor estimated consistently.  Results of simulation studies as well as real data analysis are given to illustrate that finite sample performances of consistent model selection criteria can be quite different.",
            "keywords": [
                "model selection consistency",
                "general information criteria",
                "high dimension"
            ],
            "author": [
                "Yongdai Kim",
                "Sunghoon Kwon",
                "Hosik Choi"
            ],
            "ref": "http://jmlr.org/papers/volume13/kim12a/kim12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The huge Package for High-dimensional Undirected Graph Estimation in R",
            "abstract": "We describe an R package named  huge which provides easy-to-use functions for estimating high dimensional undirected graphs from data.  This package implements recent results in the literature, including Friedman et al. (2007), Liu et al. (2009, 2012) and Liu et al. (2010).   Compared with the existing graph estimation package glasso, the huge package provides extra features: (1) instead of using Fortan, it is written in C, which makes the code more portable and easier to modify; (2) besides fitting  Gaussian graphical models, it also provides functions for fitting high dimensional semiparametric Gaussian copula models; (3) more functions like data-dependent model selection, data generation and graph visualization; (4) a minor convergence problem of the graphical lasso algorithm is corrected; (5) the package allows the user to apply both lossless and lossy screening rules to scale up large-scale problems, making a tradeoff between computational and statistical efficiency.",
            "keywords": [
                "high-dimensional undirected graph estimation",
                "glasso",
                "huge",
                "semiparametric graph     estimation",
                "data-dependent model selection",
                "lossless screening"
            ],
            "author": [
                "Tuo Zhao",
                "Han Liu",
                "Kathryn Roeder",
                "John Lafferty",
                "Larry Wasserman"
            ],
            "ref": "http://jmlr.org/papers/volume13/zhao12a/zhao12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analysis of a Random Forests Model",
            "abstract": "Random forests are a scheme proposed by Leo Breiman in the 2000's for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and  practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm.  In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman (2004), which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present.",
            "keywords": [
                "random forests",
                "randomization",
                "sparsity",
                "dimension reduction",
                "consistency"
            ],
            "author": [
                "Gérard Biau"
            ],
            "ref": "http://jmlr.org/papers/volume13/biau12a/biau12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies",
            "abstract": "We present methods able to predict the presence and strength of conditional and unconditional dependencies (correlations) between two variables Y and Z never jointly measured on the same samples, based on multiple data sets measuring a set of common variables. The algorithms are specializations of prior work on learning causal structures from overlapping variable sets.  This problem has also been addressed in the field of statistical matching. The proposed methods are applied to a wide range of domains and are shown to accurately predict the presence of thousands of dependencies. Compared against prototypical statistical matching algorithms and within the scope of our experiments, the proposed algorithms make predictions that are better correlated with the sample estimates of the unknown parameters on test data ; this is particularly the case when the number of commonly measured variables is low.   The enabling idea behind the methods is to induce one or all causal models that are simultaneously consistent with (fit) all available data sets and prior knowledge and reason with them. This allows constraints stemming from causal assumptions (e.g., Causal Markov Condition, Faithfulness) to propagate. Several methods have been developed based on this idea, for which we propose the unifying name Integrative Causal Analysis (INCA). A contrived example is presented demonstrating the theoretical potential to develop more general methods for co-analyzing heterogeneous data sets. The computational experiments with the novel methods provide evidence that causally-inspired assumptions such as Faithfulness often hold to a good degree of approximation in many real systems and could be exploited for statistical inference. Code, scripts, and data are available at www.mensxmachina.org.",
            "keywords": [
                "integrative causal analysis",
                "causal discovery",
                "Bayesian networks",
                "maximal ancestral     graphs",
                "structural equation models",
                "causality",
                "statistical matching"
            ],
            "author": [
                "Ioannis Tsamardinos",
                "Sofia Triantafillou",
                "Vincenzo Lagani"
            ],
            "ref": "http://jmlr.org/papers/volume13/tsamardinos12a/tsamardinos12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hope and Fear for Discriminative Training of Statistical Translation Models",
            "abstract": "In machine translation, discriminative models have almost entirely supplanted the classical noisy-channel model, but are standardly trained using a method that is reliable only in low-dimensional spaces. Two strands of research have tried to adapt more scalable discriminative training methods to machine translation: the first uses log-linear probability models and either maximum likelihood or minimum risk, and the other uses linear models and large-margin methods. Here, we provide an overview of the latter. We compare several learning algorithms and describe in detail some novel extensions suited to properties of the translation task: no single correct output, a large space of structured outputs, and slow inference. We present experimental results on a large-scale Arabic-English translation task, demonstrating large gains in translation accuracy.",
            "keywords": [
                "machine translation",
                "structured prediction",
                "large-margin methods",
                "online learning"
            ],
            "author": [
                "David Chiang"
            ],
            "ref": "http://jmlr.org/papers/volume13/chiang12a/chiang12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Multi-Stage Framework for Dantzig Selector and LASSO",
            "abstract": "We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X∈ ℝn✕ m (m >> n) and a noisy observation vector y∈ ℝn satisfying y=Xβ*+ε where ε is the noise vector following a Gaussian distribution N(0,σ2I), how to recover the signal (or parameter vector) β* when the signal is sparse?   The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively refines the target signal β*. We show that if X obeys a certain condition, then with a large probability the difference between the solution β̂ estimated by the proposed method and the true solution β* measured in terms of the lp norm (p≥ 1) is bounded as  ||β̂-β*||p≤ (C(s-N)1/p√log m+Δ)σ,  where C is a constant, s is the number of nonzero entries in β*, the risk of the oracle estimator Δ is independent of m and is much smaller than the first term, and N is the number of entries of β* larger than a certain value in the order of O(σ√log m). The proposed method improves the estimation bound of the standard Dantzig selector approximately from Cs1/p√log mσ to C(s-N)1/p√log mσ where the value N depends on the number of large entries in β*. When N=s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector.  Finally, we extend this multi-stage procedure to the LASSO case.",
            "keywords": [
                "multi-stage",
                "Dantzig selector",
                "LASSO"
            ],
            "author": [
                "Ji Liu",
                "Peter Wonka",
                "Jieping Ye"
            ],
            "ref": "http://jmlr.org/papers/volume13/liu12a/liu12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Geometric Approach to Sample Compression",
            "abstract": "The Sample Compression Conjecture of Littlestone & Warmuth has remained unsolved for a quarter century. While maximum classes (concept classes meeting Sauer's Lemma with equality) can be compressed, the compression of general concept classes reduces to compressing maximal classes (classes that cannot be expanded without increasing VC dimension). Two promising ways forward are: embedding maximal classes into maximum classes with at most a polynomial increase to VC dimension, and compression via operating on geometric representations. This paper presents positive results on the latter approach and a first negative result on the former, through a systematic investigation of finite maximum classes.  Simple arrangements of hyperplanes in hyperbolic space are shown to represent maximum classes, generalizing the corresponding Euclidean result.  We show that sweeping a generic hyperplane across such arrangements forms an unlabeled compression scheme of size VC dimension and corresponds to a special case of peeling the one-inclusion graph, resolving a recent conjecture of Kuzmin & Warmuth. A bijection between finite maximum classes and certain arrangements of piecewise-linear (PL) hyperplanes in either a ball or Euclidean space is established.  Finally we show that d-maximum classes corresponding to PL-hyperplane arrangements in ℝd have cubical complexes homeomorphic to a d-ball, or equivalently complexes that are manifolds with boundary.  A main result is that PL arrangements can be swept by a moving hyperplane to unlabeled d-compress any finite maximum class, forming a peeling scheme as conjectured by Kuzmin & Warmuth. A corollary is that some d-maximal classes cannot be embedded into any maximum class of VC-dimension d+k, for any constant k. The construction of the PL sweeping involves Pachner moves on the one-inclusion graph, corresponding to moves of a hyperplane across the intersection of d other hyperplanes. This extends the well known Pachner moves for triangulations to cubical complexes.",
            "keywords": [
                "sample compression",
                "hyperplane arrangements",
                "hyperbolic and piecewise-linear ge-     ometry"
            ],
            "author": [
                "Benjamin I.P. Rubinstein",
                "J. Hyam Rubinstein"
            ],
            "ref": "http://jmlr.org/papers/volume13/rubinstein12a/rubinstein12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Manifold Estimation",
            "abstract": "We find the minimax rate of convergence in Hausdorff distance for estimating a manifold M of dimension d embedded in ℝD given a noisy sample from the manifold.  Under certain conditions, we show that the optimal rate of convergence is n-2/(2+d).  Thus, the minimax rate depends only on the dimension of the manifold, not on the dimension of the space in which M is embedded.",
            "keywords": [
                "manifold learning"
            ],
            "author": [
                "Christopher Genovese",
                "Marco Perone-Pacifico",
                "Isabella Verdinelli",
                "Larry Wasserman"
            ],
            "ref": "http://jmlr.org/papers/volume13/genovese12a/genovese12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Query Strategies for Evading Convex-Inducing Classifiers",
            "abstract": "Classifiers are often used to detect miscreant activities. We study how an adversary can systematically query a classifier to elicit information that allows the attacker to evade detection while incurring a near-minimal cost of modifying their intended malfeasance. We generalize the theory of Lowd and Meek (2005) to the family of convex-inducing classifiers that partition their feature space into two sets, one of which is convex. We present query algorithms for this family that construct undetected instances of approximately minimal cost using only polynomially-many queries in the dimension of the space and in the level of approximation. Our results demonstrate that near-optimal evasion can be accomplished for this family without reverse engineering the classifier's decision boundary. We also consider general lp costs and show that near-optimal evasion on the family of convex-inducing classifiers is generally efficient for both positive and negative convexity for all levels of approximation if p=1.",
            "keywords": [
                "query algorithms",
                "evasion",
                "reverse engineering"
            ],
            "author": [
                "Blaine Nelson",
                "Benjamin I. P. Rubinstein",
                "Ling Huang",
                "Anthony D. Joseph",
                "Steven J. Lee",
                "Satish Rao",
                "J. D. Tygar"
            ],
            "ref": "http://jmlr.org/papers/volume13/nelson12a/nelson12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Transfer in Reinforcement Learning via Shared Features",
            "abstract": "We present a framework for  transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.",
            "keywords": [
                "reinforcement learning",
                "transfer",
                "shaping"
            ],
            "author": [
                "George Konidaris",
                "Ilya Scheidwasser",
                "Andrew Barto"
            ],
            "ref": "http://jmlr.org/papers/volume13/konidaris12a/konidaris12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Ranking and Generalization Bounds",
            "abstract": "The problem of ranking is to predict or to guess the ordering between objects on the basis of their observed features. In this paper we consider ranking estimators that minimize the empirical convex risk. We prove generalization bounds for the excess risk of such estimators with rates that are faster than 1/√n. We apply our results to commonly used ranking algorithms, for instance boosting or support vector machines. Moreover, we study the performance of considered estimators on real data sets.",
            "keywords": [
                "convex risk minimization",
                "excess risk",
                "support vector machine",
                "empirical process"
            ],
            "author": [
                "Wojciech Rejchel"
            ],
            "ref": "http://jmlr.org/papers/volume13/rejchel12a/rejchel12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Feature Selection via Dependence Maximization",
            "abstract": "We introduce a framework for feature selection based on dependence maximization between the selected features and the labels of an estimation problem, using the Hilbert-Schmidt Independence Criterion. The key idea is that good features should be highly dependent on the labels.  Our approach leads to a greedy procedure for feature selection.  We show that a number of existing feature selectors are special cases of this framework. Experiments on both artificial and real-world data show that our feature selector works well in practice.",
            "keywords": [
                "kernel methods",
                "feature selection",
                "independence measure",
                "Hilbert-Schmidt indepen-     dence criterion"
            ],
            "author": [
                "Le Song",
                "Alex Smola",
                "Arthur Gretton",
                "Justin Bedo",
                "Karsten Borgwardt"
            ],
            "ref": "http://jmlr.org/papers/volume13/song12a/song12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structured Sparsity via Alternating Direction Methods",
            "abstract": "We consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm that incorporates prior knowledge of the group structure of the features.  Such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term.  In this paper, we focus on two commonly adopted sparsity-inducing regularization terms, the overlapping Group Lasso penalty l1/l2-norm and the l1/l∞-norm.  We propose a unified framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efficiently solved.  As one of the core building-blocks of this framework, we develop new algorithms using a partial-linearization/splitting technique and prove that the accelerated versions of these algorithms require O(1/√ε) iterations to obtain an ε-optimal solution.  We compare the performance of these algorithms against that of the alternating direction augmented Lagrangian and FISTA methods on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms.",
            "keywords": [
                "structured sparsity",
                "overlapping Group Lasso",
                "alternating direction methods",
                "variable     splitting"
            ],
            "author": [
                "Zhiwei Qin",
                "Donald Goldfarb"
            ],
            "ref": "http://jmlr.org/papers/volume13/qin12a/qin12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Activized Learning: Transforming Passive to Active with Improved Label Complexity",
            "abstract": "We study the theoretical advantages of active learning over passive learning.  Specifically, we prove that, in noise-free classifier learning for VC classes, any passive learning algorithm can be transformed into an active learning algorithm with asymptotically strictly superior label complexity for all nontrivial target functions and distributions.  We further provide a general characterization of the magnitudes of these improvements in terms of a novel generalization of the disagreement coefficient.  We also extend these results to active learning in the presence of label noise, and find that even under broad classes of noise distributions, we can typically guarantee strict improvements over the known results for passive learning.",
            "keywords": [
                "active learning",
                "selective sampling",
                "sequential design",
                "statistical learning theory",
                "PAC     learning"
            ],
            "author": [
                "Steve Hanneke"
            ],
            "ref": "http://jmlr.org/papers/volume13/hanneke12a/hanneke12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives",
            "abstract": "In cognitive science and neuroscience, there have been two leading models describing how humans perceive and classify facial expressions of emotion---the continuous and the categorical model. The continuous model defines each facial expression of emotion as a feature vector in a face space. This model explains, for example, how expressions of emotion can be seen at different intensities. In contrast, the categorical model consists of C classifiers, each tuned to a specific emotion category. This model explains, among other findings, why the images in a morphing sequence between a happy and a surprise face are perceived as either happy or surprise but not something in between. While the continuous model has a more difficult time justifying this latter finding, the categorical model is not as good when it comes to explaining how expressions are recognized at different intensities or modes. Most importantly, both models have problems explaining how one can recognize combinations of emotion categories such as happily surprised versus angrily surprised versus surprise. To resolve these issues, in the past several years, we have worked on a revised model that justifies the results reported in the cognitive science and neuroscience literature. This model consists of C distinct continuous spaces. Multiple (compound) emotion categories can be recognized by linearly combining these C face spaces. The dimensions of these spaces are shown to be mostly configural. According to this model, the major task for the classification of facial expressions of emotion is precise, detailed detection of facial landmarks rather than recognition. We provide an overview of the literature justifying the model, show how the resulting model can be employed to build algorithms for the recognition of facial expression of emotion, and propose research directions in machine learning and computer vision researchers to keep pushing the state of the art in these areas.  We also discuss how the model can aid in studies of human perception, social interactions and disorders.",
            "keywords": [
                "vision",
                "face perception",
                "emotions",
                "computational modeling",
                "categorical perception"
            ],
            "author": [
                "Aleix Martinez",
                "Shichuan Du"
            ],
            "ref": "http://jmlr.org/papers/volume13/martinez12a/martinez12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models",
            "abstract": "We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random fields (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter fitting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE).  We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random field via the graphical lasso.",
            "keywords": [],
            "author": [
                "Neil D. Lawrence"
            ],
            "ref": "http://jmlr.org/papers/volume13/lawrence12a/lawrence12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Mixability is Bayes Risk Curvature Relative to Log Loss",
            "abstract": "Mixability of a loss characterizes fast rates in the online learning setting of prediction with expert advice. The determination of the mixability constant for binary losses is straightforward but opaque. In the binary case we make this transparent and simpler by characterising mixability in terms of the second derivative of the Bayes risk of proper losses.  We then extend this result to multiclass proper losses where there are few existing results.  We show that mixability is governed by the maximum eigenvalue of the Hessian of the Bayes risk, relative to the Hessian of the Bayes risk for log loss. We conclude by comparing our result to other work that bounds prediction performance in terms of the geometry of the Bayes risk. Although all calculations are for proper losses, we also show how to carry the results across to improper losses.",
            "keywords": [
                "mixability",
                "multiclass",
                "prediction with expert advice",
                "proper loss"
            ],
            "author": [
                "Tim van Erven",
                "Mark D. Reid",
                "Robert C. Williamson"
            ],
            "ref": "http://jmlr.org/papers/volume13/vanerven12a/vanerven12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise",
            "abstract": "We consider the matrix completion problem under a form of row/column weighted entrywise sampling, including the case of uniform entrywise sampling as a special case.  We analyze the associated random observation operator, and prove that with high probability, it satisfies a form of restricted strong convexity with respect to weighted Frobenius norm.  Using this property, we obtain as corollaries a number of error bounds on matrix completion in the weighted Frobenius norm under noisy sampling and for both exact and near low-rank matrices.  Our results are based on measures of the \"spikiness\" and \"low-rankness\" of matrices that are less restrictive than the incoherence conditions imposed in previous work.  Our technique involves an M-estimator that includes controls on both the rank and spikiness of the solution, and we establish non-asymptotic error bounds in weighted Frobenius norm for recovering matrices lying with lq-\"balls\" of bounded spikiness.  Using information-theoretic methods, we show that no algorithm can achieve better estimates (up to a logarithmic factor) over these same sets, showing that our conditions on matrices and associated rates are essentially optimal.",
            "keywords": [
                "matrix completion",
                "collaborative filtering"
            ],
            "author": [
                "Sahand Negahban",
                "Martin J. Wainwright"
            ],
            "ref": "http://jmlr.org/papers/volume13/negahban12a/negahban12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "glm-ie: Generalised Linear Models Inference & Estimation Toolbox",
            "abstract": "The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean field.  Scalable and efficient inference in fully-connected undirected graphical models or Markov random fields with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling.  We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX files to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classification as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit.",
            "keywords": [
                "sparse linear models",
                "generalised linear models",
                "Bayesian inference",
                "approximate     inference",
                "probabilistic regression and classification",
                "penalised least squares estimation"
            ],
            "author": [
                "Hannes Nickisch"
            ],
            "ref": "http://jmlr.org/papers/volume13/nickisch12a/nickisch12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning",
            "abstract": "Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a low-dimensional manifold of parameter space along which the regularizer is smooth. (When an l1 regularizer is used to induce sparsity in the solution, for example, this manifold is defined by the set of nonzero components of the parameter vector.)  This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a \"local phase\" that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identification property and to illustrate the effectiveness of this approach.",
            "keywords": [
                "regularization",
                "dual averaging",
                "partly smooth manifold"
            ],
            "author": [
                "Sangkyun Lee",
                "Stephen J. Wright"
            ],
            "ref": "http://jmlr.org/papers/volume13/lee12a/lee12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Multinomial Logit Gaussian Process",
            "abstract": "Gaussian process prior with an appropriate likelihood function is a flexible non-parametric model for a variety of learning tasks.  One important and standard task is multi-class classification, which is the categorization of an item into one of several fixed classes.  A usual likelihood function for this is the multinomial logistic likelihood function.  However, exact inference with this model has proved to be difficult because high-dimensional integrations are required.  In this paper, we propose a variational approximation to this model, and we describe the optimization of the variational parameters.  Experiments have shown our approximation to be tight.  In addition, we provide data-independent bounds on the marginal likelihood of the model, one of which is shown to be much tighter than the existing variational mean-field bound in the experiments.  We also derive a proper lower bound on the predictive likelihood that involves the Kullback-Leibler divergence between the approximating and the true posterior.  We combine our approach with a recently proposed sparse approximation to give a variational sparse approximation to the Gaussian process multi-class  model.  We also derive criteria which can be used to select the inducing set, and we show the effectiveness of these criteria over random selection in an experiment.",
            "keywords": [
                "Gaussian process",
                "probabilistic classification",
                "multinomial logistic",
                "variational ap-     proximation"
            ],
            "author": [
                "Kian Ming A. Chai"
            ],
            "ref": "http://jmlr.org/papers/volume13/chai12a/chai12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Entropy Search for Information-Efficient Global Optimization",
            "abstract": "Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation.",
            "keywords": [
                "optimization",
                "probability",
                "information",
                "Gaussian processes"
            ],
            "author": [
                "Philipp Hennig",
                "Christian J. Schuler"
            ],
            "ref": "http://jmlr.org/papers/volume13/hennig12a/hennig12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications",
            "abstract": "The l1-penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets.  Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted l1-penalized estimators for convex loss functions of a general form, including the generalized linear models. We study  the estimation, prediction, selection and sparsity  properties of the weighted l1-penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case.  A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively.  We provide prediction and estimation oracle inequalities for single- and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results.",
            "keywords": [
                "variable selection",
                "penalized estimation",
                "oracle inequality",
                "generalized linear models",
                "selection consistency"
            ],
            "author": [
                "Jian Huang",
                "Cun-Hui Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume13/huang12b/huang12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularization Techniques for Learning with Matrices",
            "abstract": "There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm.  This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques.  In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate.   Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms.  We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning.",
            "keywords": [
                "regularization",
                "strong convexity",
                "regret bounds",
                "generalization bounds",
                "multi-task     learning",
                "multi-class learning"
            ],
            "author": [
                "Sham M. Kakade",
                "Shai Shalev-Shwartz",
                "Ambuj Tewari"
            ],
            "ref": "http://jmlr.org/papers/volume13/kakade12a/kakade12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Confidence-Weighted Linear Classification for Text Categorization",
            "abstract": "Confidence-weighted online learning is a generalization of margin-based learning of linear classifiers in which the margin constraint is replaced by a probabilistic constraint based on a distribution over classifier weights that is updated online as examples are observed. The distribution captures a notion of confidence on classifier weights, and in some cases it can also be interpreted as replacing a single learning rate by adaptive per-weight rates. Confidence-weighted learning was motivated by the statistical properties of natural-language classification tasks, where most of the informative features are relatively rare.  We investigate several versions of confidence-weighted learning that use a Gaussian distribution over weight vectors, updated at each observed example to achieve high probability of correct classification for the example. Empirical evaluation on a range of text-categorization tasks show that our algorithms improve over other state-of-the-art online and batch methods, learn faster in the online setting, and lead to better classifier combination for a type of distributed training commonly used in cloud computing.",
            "keywords": [
                "online learning",
                "confidence prediction"
            ],
            "author": [
                "Koby Crammer",
                "Mark Dredze",
                "Fernando Pereira"
            ],
            "ref": "http://jmlr.org/papers/volume13/crammer12a/crammer12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Integrating a Partial Model into Model Free Reinforcement Learning",
            "abstract": "In reinforcement learning an agent uses online feedback from the environment in order to adaptively select an effective policy.  Model free approaches address this task by directly mapping environmental states to actions, while model based methods attempt to construct a model of the environment, followed by a selection of optimal actions based on that model. Given the complementary advantages of both approaches, we suggest a novel procedure which augments a model free algorithm with a partial model. The resulting hybrid algorithm switches between a model based and a model free mode, depending on the current state and the agent's knowledge. Our method relies on a novel definition for a partially known model, and an estimator that incorporates such knowledge in order to reduce uncertainty in stochastic approximation iterations. We prove that such an approach leads to improved policy evaluation whenever environmental knowledge is available, without compromising performance when such knowledge is absent. Numerical simulations demonstrate the effectiveness of the approach on policy gradient and Q-learning algorithms, and its usefulness in solving a call admission control problem.",
            "keywords": [
                "reinforcement learning",
                "temporal difference",
                "stochastic approximation",
                "markov deci-     sion processes"
            ],
            "author": [
                "Aviv Tamar",
                "Dotan Di Castro",
                "Ron Meir"
            ],
            "ref": "http://jmlr.org/papers/volume13/tamar12a/tamar12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences",
            "abstract": "Jstacs is an object-oriented Java library for analysing and classifying sequence data, which emerged from the need for a standardized implementation of statistical models, learning principles, classifiers, and performance measures. In Jstacs, these components can be used, combined, and extended easily, which allows for a direct comparison of different approaches and fosters the development of new components.  Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. Jstacs is freely available at http://www.jstacs.de under the GNU GPL license including an API documentation, a cookbook, and code examples.",
            "keywords": [
                "machine learning",
                "statistical models",
                "Java",
                "bioinformatics"
            ],
            "author": [
                "Jan Grau",
                "Jens Keilwagen",
                "André Gohr",
                "Berit Haldemann",
                "Stefan Posch",
                "Ivo Grosse"
            ],
            "ref": "http://jmlr.org/papers/volume13/grau12a/grau12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality",
            "abstract": "The varying-coefficient model is flexible and powerful for modeling the dynamic changes of regression coefficients.  It is important to identify significant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size.  We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L0 penalty, and we investigate the global optimality properties of the varying-coefficient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefficient modeling could be infinite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefficient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data.  Our simulations and numerical examples indicate that the difference convex algorithm is efficient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches.",
            "keywords": [
                "coordinate decent algorithm",
                "difference convex programming",
                "L0 - regularization",
                "large-p small-n",
                "model selection",
                "nonparametric function",
                "oracle property"
            ],
            "author": [
                "Lan Xue",
                "Annie Qu"
            ],
            "ref": "http://jmlr.org/papers/volume13/xue12a/xue12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Improved GLMNET for L1-regularized Logistic Regression",
            "abstract": "Recently, Yuan et al. (2010) conducted a comprehensive comparison on software for L1-regularized classification.  They concluded that a carefully designed coordinate descent implementation CDN is the fastest among state-of-the-art solvers.  In this paper, we point out that CDN is less competitive on loss functions that are expensive to compute.  In particular, CDN for logistic regression is much slower than CDN for SVM because the logistic loss involves expensive exp/log operations.   In optimization, Newton methods are known to have fewer iterations although each iteration costs more.  Because solving the Newton sub-problem is independent of the loss calculation, this type of methods may surpass CDN under some circumstances.  In L1-regularized classification, GLMNET by Friedman et al. is already a Newton-type method, but experiments in Yuan et al. (2010) indicated that the existing GLMNET implementation may face difficulties for some large-scale problems.  In this paper, we propose an improved GLMNET to address some theoretical and implementation issues.  In particular, as a Newton-type method, GLMNET achieves fast local convergence, but may fail to quickly obtain a useful solution.  By a careful design to adjust the effort for each iteration, our method is efficient for both loosely or strictly solving the optimization problem.  Experiments demonstrate that our improved GLMNET is more efficient than CDN for L1-regularized logistic regression.",
            "keywords": [
                "L1 regularization",
                "linear classification",
                "optimization methods",
                "logistic regression"
            ],
            "author": [
                "Guo-Xun Yuan",
                "Chia-Hua Ho",
                "Chih-Jen Lin"
            ],
            "ref": "http://jmlr.org/papers/volume13/yuan12a/yuan12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "EP-GIG Priors and Applications in Bayesian Sparse Learning",
            "abstract": "In this paper we propose a novel framework for the construction of  sparsity-inducing priors. In particular, we define such priors as a mixture of exponential power distributions with a generalized inverse Gaussian density (EP-GIG).  EP-GIG is a  variant of generalized hyperbolic distributions, and the special cases include Gaussian scale mixtures and Laplace scale mixtures.  Furthermore, Laplace scale mixtures can subserve a Bayesian framework for sparse learning with nonconvex penalization.  The densities of EP-GIG can be explicitly expressed.  Moreover, the corresponding posterior distribution also follows a generalized inverse Gaussian distribution. We exploit these properties to develop EM algorithms for sparse empirical Bayesian learning.  We also show that these algorithms bear an interesting resemblance to iteratively reweighted l2 or l1 methods. Finally, we present two extensions for grouped variable selection and logistic regression.",
            "keywords": [
                "sparsity priors",
                "scale mixtures of exponential power distributions",
                "generalized inverse     Gaussian distributions",
                "expectation-maximization algorithms"
            ],
            "author": [
                "Zhihua Zhang",
                "Shusen Wang",
                "Dehua Liu",
                "Michael I. Jordan"
            ],
            "ref": "http://jmlr.org/papers/volume13/zhang12b/zhang12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pattern for Python",
            "abstract": "Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classifiers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern.",
            "keywords": [
                "Python",
                "data mining",
                "natural language processing",
                "machine learning"
            ],
            "author": [
                "Tom De Smedt",
                "Walter Daelemans"
            ],
            "ref": "http://jmlr.org/papers/volume13/desmedt12a/desmedt12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimistic Bayesian Sampling in Contextual-Bandit Problems",
            "abstract": "In sequential decision problems in an unknown environment, the decision maker often faces a dilemma over whether to explore to discover more about the environment, or to exploit current knowledge. We address the exploration-exploitation dilemma in a general setting encompassing both standard and contextualised bandit problems. The contextual bandit problem has recently resurfaced in attempts to maximise click-through rates in web based applications, a task with significant commercial interest.   In this article we consider an approach of Thompson (1933) which makes use of samples from the posterior distributions for the instantaneous value of each action. We extend the approach by introducing a new algorithm, Optimistic Bayesian Sampling (OBS), in which the probability of playing an action increases with the uncertainty in the estimate of the action value. This results in better directed exploratory behaviour.   We prove that, under unrestrictive assumptions, both approaches result in optimal behaviour with respect to the average reward criterion of Yang and Zhu (2002).  We implement OBS and measure its performance in simulated Bernoulli bandit and linear regression domains, and also when tested with the task of personalised news article recommendation on a Yahoo! Front Page Today Module data set. We find that OBS performs competitively when compared to recently proposed benchmark algorithms and outperforms Thompson's method throughout.",
            "keywords": [
                "multi-armed bandits",
                "contextual bandits",
                "exploration-exploitation",
                "sequential alloca-     tion"
            ],
            "author": [
                "Benedict C. May",
                "Nathan Korda",
                "Anthony Lee",
                "David S. Leslie"
            ],
            "ref": "http://jmlr.org/papers/volume13/may12a/may12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Comparison of the Lasso and  Marginal Regression",
            "abstract": "The lasso is an important method for sparse, high-dimensional regression problems, with efficient algorithms available, a long history of practical success, and a large body of theoretical results supporting and explaining its performance.  But even with the best available algorithms, finding the lasso solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points.   Marginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than the lasso solutions.  The question that remains is how the statistical performance of the method compares to that of the lasso in these cases.   In this paper, we study the relative statistical performance of the lasso and marginal regression for sparse, high-dimensional regression problems.  We consider the problem of learning which coefficients are non-zero.  Our main results are as follows: (i) we compare the conditions under which the lasso and marginal regression guarantee exact recovery in the fixed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the fixed design, noise free, random coefficients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefficients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric.   In light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study.",
            "keywords": [
                "high-dimensional regression",
                "lasso",
                "phase diagram"
            ],
            "author": [
                "Christopher R. Genovese",
                "Jiashun Jin",
                "Larry Wasserman",
                "Zhigang Yao"
            ],
            "ref": "http://jmlr.org/papers/volume13/genovese12b/genovese12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Necessity of Irrelevant Variables",
            "abstract": "This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers.  The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing.  The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant.  We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.",
            "keywords": [
                "feature selection",
                "generalization"
            ],
            "author": [
                "David P. Helmbold",
                "Philip M. Long"
            ],
            "ref": "http://jmlr.org/papers/volume13/helmbold12a/helmbold12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DEAP: Evolutionary Algorithms Made Easy",
            "abstract": "DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks.  Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license.",
            "keywords": [
                "distributed evolutionary algorithms"
            ],
            "author": [
                "Félix-Antoine Fortin",
                "François-Michel De Rainville",
                "Marc-André Gardner",
                "Marc Parizeau",
                "Christian Gagné"
            ],
            "ref": "http://jmlr.org/papers/volume13/fortin12a/fortin12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Introduction to Artificial Prediction Markets for Classification",
            "abstract": "Prediction markets are used in real life to predict outcomes of interest such as presidential elections. This paper presents a mathematical theory of artificial prediction markets for supervised learning of conditional probability estimators. The artificial prediction market is a novel method for fusing the prediction information of features or trained classifiers, where the fusion result is the contract price on the possible outcomes. The market can be trained online by updating the participants' budgets using training examples. Inspired by the real prediction markets, the equations that govern the market are derived from simple and reasonable assumptions. Efficient numerical algorithms are presented for solving these equations. The obtained artificial prediction market is shown to be a maximum likelihood estimator. It generalizes linear aggregation, existent in boosting and random forest, as well as logistic regression and some kernel methods. Furthermore, the market mechanism allows the aggregation of specialized classifiers that participate only on specific instances. Experimental comparisons show that the artificial prediction markets often outperform random forest and implicit online learning on synthetic data and real UCI data sets. Moreover, an extensive evaluation for pelvic and abdominal lymph node detection in CT data shows that the prediction market improves adaboost's detection rate from 79.6% to 81.2% at 3 false positives/volume.",
            "keywords": [
                "online learning",
                "ensemble methods",
                "supervised learning",
                "random forest"
            ],
            "author": [
                "Adrian Barbu",
                "Nathan Lay"
            ],
            "ref": "http://jmlr.org/papers/volume13/barbu12a/barbu12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Topic Modeling Toolbox Using Belief Propagation",
            "abstract": "Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology.  This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms.  TMBP toolbox is implemented by MEX C++/Matlab/Octave for either Windows 7 or Linux.  Compared with existing topic modeling packages, the novelty of this toolbox lies in the BP algorithms for learning LDA-based topic models.  The current version includes BP algorithms for latent Dirichlet allocation (LDA), author-topic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA).  This toolbox is an ongoing project and more BP-based algorithms for various topic models will be added in the near future.  Interested users may also extend BP algorithms for learning more complicated topic models.  The source codes are freely available under the GNU General Public Licence, Version 1.0 at https://mloss.org/software/view/399/.",
            "keywords": [
                "topic models",
                "belief propagation",
                "variational Bayes"
            ],
            "author": [
                "Jia Zeng"
            ],
            "ref": "http://jmlr.org/papers/volume13/zeng12a/zeng12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MedLDA: Maximum Margin Supervised Topic Models",
            "abstract": "A supervised topic model can use side information such as ratings or labels associated with documents or images to discover more predictive low dimensional topical representations of the data. However, existing supervised topic models predominantly employ likelihood-driven objective functions for learning and inference, leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper, we propose the maximum entropy discrimination latent Dirichlet allocation (MedLDA) model, which integrates the mechanism behind the max-margin prediction models (e.g., SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g., LDA) under a unified constrained optimization framework, and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classification or regression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. Efficient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance; and 3) be more efficient than existing supervised topic models, especially for classification.",
            "keywords": [
                "supervised topic models",
                "max-margin learning",
                "maximum entropy discrimination",
                "latent Dirichlet allocation"
            ],
            "author": [
                "Jun Zhu",
                "Amr Ahmed",
                "Eric P. Xing"
            ],
            "ref": "http://jmlr.org/papers/volume13/zhu12a/zhu12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pairwise Support Vector Machines and their Application to Large Scale Problems",
            "abstract": "Pairwise classification is the task to predict whether the examples a,b of a pair (a,b) belong to the same class or to different classes. In particular, interclass generalization problems can be treated in this way.  In pairwise classification, the order of the two input examples should not affect the classification result. To achieve this, particular kernels as well as the use of symmetric training sets in the framework of support vector machines were suggested. The paper discusses both approaches in a general way and establishes a strong connection between them. In addition, an efficient implementation is discussed which allows the training of several millions of pairs. The value of these contributions is confirmed by excellent results on the labeled faces in the wild benchmark.",
            "keywords": [
                "pairwise support vector machines",
                "interclass generalization",
                "pairwise kernels"
            ],
            "author": [
                "Carl Brunner",
                "Andreas Fischer",
                "Klaus Luig",
                "Thorsten Thies"
            ],
            "ref": "http://jmlr.org/papers/volume13/brunner12a/brunner12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion",
            "abstract": "We consider the problem of high-dimensional Gaussian graphical model selection. We  identify a set of graphs for which an efficient estimation algorithm exists, and this algorithm is  based on thresholding of  empirical conditional covariances. Under a set of transparent conditions, we establish structural consistency (or sparsistency) for the proposed algorithm, when the number of samples n=Ω(Jmin-2 log p), where p is the number of variables and Jmin is the minimum (absolute) edge potential of the graphical model. The sufficient conditions for sparsistency are based on the notion of walk-summability of the model and the presence of sparse local vertex separators in the underlying graph. We also derive novel non-asymptotic  necessary conditions on the number of samples required  for sparsistency.",
            "keywords": [
                "Gaussian graphical model selection",
                "high-dimensional learning",
                "local-separation prop-     erty",
                "walk-summability"
            ],
            "author": [
                "Animashree Anandkumar",
                "Vincent Y.F. Tan",
                "Furong Huang",
                "Alan S. Willsky"
            ],
            "ref": "http://jmlr.org/papers/volume13/anandkumar12a/anandkumar12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally",
            "abstract": "The second eigenvalue of the Laplacian matrix and its associated eigenvector are fundamental features of an undirected graph, and as such they have found widespread use in scientific computing, machine learning, and data analysis.  In many applications, however, graphs that arise have several local regions of interest, and the second eigenvector will typically fail to provide information fine-tuned to each local region.  In this paper, we introduce a locally-biased analogue of the second eigenvector, and we demonstrate its usefulness at highlighting local properties of data graphs in a semi-supervised manner.  To do so, we first view the second eigenvector as the solution to a constrained optimization problem, and we incorporate the local information as an additional constraint; we then characterize the optimal solution to this new problem and show that it can be interpreted as a generalization of a Personalized PageRank vector; and finally, as a consequence, we show that the solution can be computed in nearly-linear time.  In addition, we show that this locally-biased vector can be used to compute an approximation to the best partition near an input seed set in a manner analogous to the way in which the second eigenvector of the Laplacian can be used to obtain an approximation to the best partition in the entire input graph.  Such a primitive is useful for identifying and refining clusters locally, as it allows us to focus on a local region of interest in a semi-supervised manner.  Finally, we provide a detailed empirical evaluation of our method by showing how it can applied to finding locally-biased sparse cuts around an input vertex seed set in social and information networks.",
            "keywords": [
                "spectral graph partitioning",
                "local spectral algorithms",
                "Laplacian matrix",
                "semi-supervised     learning"
            ],
            "author": [
                "Michael W. Mahoney",
                "Lorenzo Orecchia",
                "Nisheeth K. Vishnoi"
            ],
            "ref": "http://jmlr.org/papers/volume13/mahoney12a/mahoney12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Target Regression with Rule Ensembles",
            "abstract": "Methods for learning decision rules are being successfully applied to many problem domains, in particular when understanding and interpretation of the learned model is necessary. In many real life problems, we would like to predict multiple related (nominal or numeric) target attributes simultaneously. While several methods for learning rules that predict multiple targets at once exist, they are all based on the covering algorithm, which does not work well for regression problems. A better solution for regression is the rule ensemble approach that transcribes an ensemble of decision trees into a large collection of rules. An optimization procedure is then used to select the best (and much smaller) subset of these rules and to determine their respective weights.   We introduce the FIRE algorithm for solving multi-target regression problems, which employs the rule ensembles approach. We improve the accuracy of the algorithm by adding simple linear functions to the ensemble. We also extensively evaluate the algorithm with and without linear functions. The results show that the accuracy of multi-target regression rule ensembles is high. They are more accurate than, for instance, multi-target regression trees, but not quite as accurate as multi-target random forests. The rule ensembles are significantly more concise than random forests, and it is also possible to create compact rule sets that are smaller than a single regression tree but still comparable in accuracy.",
            "keywords": [
                "multi-target prediction",
                "rule learning",
                "rule ensembles"
            ],
            "author": [
                "Timo Aho",
                "Bernard Ženko",
                "Sašo Džeroski",
                "Tapio Elomaa"
            ],
            "ref": "http://jmlr.org/papers/volume13/aho12a/aho12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs",
            "abstract": "The investigation of directed acyclic graphs (DAGs) encoding the same Markov property, that is the same conditional independence relations of multivariate observational distributions, has a long tradition; many algorithms exist for model selection and structure learning in Markov equivalence classes.  In this paper, we extend the notion of Markov equivalence of DAGs to the case of interventional distributions arising from multiple intervention experiments.  We show that under reasonable assumptions on the intervention experiments, interventional Markov equivalence defines a finer partitioning of DAGs than observational Markov equivalence and hence improves the identifiability of causal models.  We give a graph theoretic criterion for two DAGs being Markov equivalent under interventions and show that each interventional Markov equivalence class can, analogously to the observational case, be uniquely represented by a chain graph called interventional essential graph (also known as CPDAG in the observational case).  These are key insights for deriving a generalization of the Greedy Equivalence Search algorithm aimed at structure learning from interventional data.  This new algorithm is evaluated in a simulation study.",
            "keywords": [
                "causal inference",
                "interventions",
                "graphical model",
                "Markov equivalence"
            ],
            "author": [
                "Alain Hauser",
                "Peter Bühlmann"
            ],
            "ref": "http://jmlr.org/papers/volume13/hauser12a/hauser12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Convergence Rate of <i>l</i><sub>p</sub>-Norm Multiple Kernel Learning",
            "abstract": "We derive an upper bound on the local Rademacher complexity of lp-norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p=1 only while our analysis covers all cases 1≤p≤∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated.  We also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely fast convergence rates of the order O(n-α/1+α), where α is the minimum eigenvalue decay rate of the individual kernels.",
            "keywords": [
                "multiple kernel learning",
                "learning kernels",
                "generalization bounds"
            ],
            "author": [
                "Marius Kloft",
                "Gilles Blanchard"
            ],
            "ref": "http://jmlr.org/papers/volume13/kloft12a/kloft12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints",
            "abstract": "In this paper we propose efficient algorithms for solving constrained online convex optimization problems. Our motivation stems from the observation that most  algorithms proposed for online convex optimization require a projection onto the convex set K from which the decisions are made. While  the projection is straightforward for simple shapes (e.g., Euclidean ball), for arbitrary complex sets it is  the main computational challenge and may be inefficient in practice. In this paper, we consider an alternative online convex optimization problem. Instead of requiring that decisions belong to K  for all rounds, we only require that the constraints, which define the set K, be satisfied in the long run.  By turning the problem into an online convex-concave optimization problem, we propose an efficient algorithm which achieves O(√T)  regret bound and O(T3/4) bound on the violation of constraints. Then, we modify the algorithm in order to guarantee that the constraints are satisfied in the long run. This gain is achieved at the price of getting O(T3/4)  regret bound. Our second algorithm is based on the mirror prox method (Nemirovski, 2005) to solve variational inequalities which achieves O(T2/3) bound for both regret and the violation of constraints  when the domain K can be described by a finite number of linear constraints. Finally, we extend the results to the setting where we only have partial access to the convex set K and propose a multipoint bandit feedback algorithm with the same bounds in expectation as our first algorithm.",
            "keywords": [
                "online convex optimization",
                "convex-concave optimization",
                "bandit feedback"
            ],
            "author": [
                "Mehrdad Mahdavi",
                "Rong Jin",
                "Tianbao Yang"
            ],
            "ref": "http://jmlr.org/papers/volume13/mahdavi12a/mahdavi12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Kernel Density Estimation",
            "abstract": "We propose a method for nonparametric density estimation that exhibits robustness to contamination of the training sample. This method achieves robustness by combining a traditional kernel density estimator (KDE) with ideas from classical M-estimation. We interpret the KDE based on a positive semi-definite kernel as a sample mean in the associated reproducing kernel Hilbert space. Since the sample mean is sensitive to outliers, we estimate it robustly via M-estimation, yielding a robust kernel density estimator (RKDE).   An RKDE can be computed efficiently via a kernelized iteratively re-weighted least squares (IRWLS) algorithm. Necessary and sufficient conditions are given for kernelized IRWLS to converge to the global minimizer of the M-estimator objective function. The robustness of the RKDE is demonstrated with a representer theorem, the influence function, and experimental results for density estimation and anomaly detection.",
            "keywords": [
                "outlier",
                "reproducing kernel Hilbert space",
                "kernel trick",
                "influence function"
            ],
            "author": [
                "JooSeuk Kim",
                "Clayton D. Scott"
            ],
            "ref": "http://jmlr.org/papers/volume13/kim12b/kim12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric Guidance of Autoencoder Representations using Label Information",
            "abstract": "While unsupervised learning has long been useful for density modeling, exploratory data analysis and visualization, it has become increasingly important for discovering features that will later be used for discriminative tasks.  Discriminative algorithms often work best with highly-informative features; remarkably, such features can often be learned without the labels.  One particularly effective way to perform such unsupervised learning has been to use autoencoder neural networks, which find latent representations that are constrained but nevertheless informative for reconstruction.  However, pure unsupervised learning with autoencoders can find representations that may or may not be useful for the ultimate discriminative task.  It is a continuing challenge to guide the training of an autoencoder so that it finds features which will be useful for predicting labels.  Similarly, we often have a priori information regarding what statistical variation will be irrelevant to the ultimate discriminative task, and we would like to be able to use this for guidance as well.  Although a typical strategy would be to include a parametric discriminative model as part of the autoencoder training, here we propose a nonparametric approach that uses a Gaussian process to guide the representation.  By using a nonparametric model, we can ensure that a useful discriminative function exists for a given set of features, without explicitly instantiating it.  We demonstrate the superiority of this guidance mechanism on four data sets, including a real-world application to rehabilitation research.  We also show how our proposed approach can learn to explicitly ignore statistically significant covariate information that is label-irrelevant, by evaluating on the small NORB image recognition problem in which pose and lighting labels are available.",
            "keywords": [
                "autoencoder",
                "gaussian process",
                "gaussian process latent variable model",
                "representation     learning"
            ],
            "author": [
                "Jasper Snoek",
                "Ryan P. Adams",
                "Hugo Larochelle"
            ],
            "ref": "http://jmlr.org/papers/volume13/snoek12a/snoek12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs",
            "abstract": "We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is first transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences.",
            "keywords": [
                "pattern extraction",
                "sign language recognition",
                "signeme extraction",
                "sign modeling"
            ],
            "author": [
                "Sunita Nayak",
                "Kester Duncan",
                "Sudeep Sarkar",
                "Barbara Loeding"
            ],
            "ref": "http://jmlr.org/papers/volume13/nayak12a/nayak12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Static Prediction Games for Adversarial Learning Problems",
            "abstract": "The standard assumption of identically distributed training and test data is violated when the test data are generated in response to the presence of a predictive model. This becomes apparent, for example, in the context of email spam filtering. Here, email service providers employ spam filters, and spam senders engineer campaign templates to achieve a high rate of successful deliveries despite the filters. We model the interaction between the learner and the data generator as a static game in which the cost functions of the learner and the data generator are not necessarily antagonistic. We identify conditions under which this prediction game has a unique Nash equilibrium and derive algorithms that find the equilibrial prediction model. We derive two instances, the Nash logistic regression and the Nash support vector machine, and empirically explore their properties in a case study on email spam filtering.",
            "keywords": [
                "static prediction games",
                "adversarial classification"
            ],
            "author": [
                "Michael Brückner",
                "Christian Kanzow",
                "Tobias Scheffer"
            ],
            "ref": "http://jmlr.org/papers/volume13/brueckner12a/brueckner12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Selective Sampling and Active Learning from Single and Multiple Teachers",
            "abstract": "We present a new online learning algorithm in the selective sampling framework, where labels must be actively queried before they are revealed. We prove bounds on the regret of our algorithm and on the number of labels it queries when faced with an adaptive adversarial strategy of generating the instances. Our bounds both generalize and strictly improve over previous bounds in similar settings. Additionally, our selective sampling algorithm can be converted into an efficient statistical active learning algorithm. We extend our algorithm and analysis to the multiple-teacher setting, where the algorithm can choose which subset of teachers to query for each label. Finally, we demonstrate the effectiveness of our techniques on a real-world Internet search problem.",
            "keywords": [
                "online learning",
                "regret",
                "label-efficient"
            ],
            "author": [
                "Ofer Dekel",
                "Claudio Gentile",
                "Karthik Sridharan"
            ],
            "ref": "http://jmlr.org/papers/volume13/dekel12b/dekel12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PREA: Personalized Recommendation Algorithms Toolkit",
            "abstract": "Recommendation systems are important business applications with significant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms.",
            "keywords": [
                "recommender systems",
                "collaborative filtering"
            ],
            "author": [
                "Joonseok Lee",
                "Mingxuan Sun",
                "Guy Lebanon"
            ],
            "ref": "http://jmlr.org/papers/volume13/lee12b/lee12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Coherence Functions with Applications in Large-Margin Classification Methods",
            "abstract": "Support vector machines (SVMs) naturally embody sparseness due to their use of hinge loss functions. However, SVMs can not directly estimate conditional class probabilities. In this paper we propose and study a family of coherence functions, which are convex and differentiable, as surrogates of the hinge function. The coherence function is derived by using the maximum-entropy principle and is characterized by a temperature parameter. It bridges the hinge function and the logit function in logistic regression.  The limit of the coherence function at zero temperature corresponds to the hinge function, and the limit of the minimizer of its expected error is the minimizer of  the expected error of the hinge loss.  We refer to the use of the coherence function in large-margin classification as \"C-learning,\" and we present efficient coordinate descent algorithms for the training of regularized C-learning models.",
            "keywords": [
                "large-margin classifiers",
                "hinge functions",
                "logistic functions",
                "coherence functions"
            ],
            "author": [
                "Zhihua Zhang",
                "Dehua Liu",
                "Guang Dai",
                "Michael I. Jordan"
            ],
            "ref": "http://jmlr.org/papers/volume13/zhang12c/zhang12c.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear Regression With Random Projections",
            "abstract": "We investigate a method for regression that makes use of a randomly generated subspace GP⊂F (of finite dimension P) of a given large (possibly infinite) dimensional function space F, for example, L2([0,1]d;ℜ).  GP is defined as the span of P random features  that are linear combinations of a basis functions of F weighted by random Gaussian i.i.d. coefficients.  We show practical motivation for the use of this approach, detail the link that this random projections method share with RKHS and Gaussian objects theory and prove, both in deterministic and random design, approximation error bounds when searching for the best regression function in GP rather than in F, and derive excess risk bounds for a specific regression algorithm (least squares regression in GP). This paper stresses the motivation to study such methods, thus the analysis developed is kept simple for explanations purpose and leaves room for future developments.",
            "keywords": [
                "regression",
                "random matrices"
            ],
            "author": [
                "Odalric-Ambrym Maillard",
                "Rémi Munos"
            ],
            "ref": "http://jmlr.org/papers/volume13/maillard12a/maillard12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-task Regression using Minimal Penalties",
            "abstract": "In this paper we study the kernel  multiple ridge regression framework, which we refer to as multi-task regression, using penalization techniques. The theoretical analysis of this problem shows that the key element appearing for an optimal calibration is the covariance matrix of the noise between the different tasks. We present a new algorithm to estimate this covariance matrix, based on the concept of minimal penalty, which was previously used in the single-task regression framework to estimate the variance of the noise. We show, in a non-asymptotic setting and under mild assumptions on the target function, that this estimator converges towards the covariance matrix. Then plugging this estimator into the corresponding ideal penalty leads to an oracle inequality. We illustrate the behavior of our algorithm on synthetic examples.",
            "keywords": [
                "multi-task",
                "oracle inequality"
            ],
            "author": [
                "Matthieu Solnon",
                "Sylvain Arlot",
                "Francis Bach"
            ],
            "ref": "http://jmlr.org/papers/volume13/solnon12a/solnon12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss",
            "abstract": "Many performance metrics have been introduced in the literature for the evaluation of classification performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into refinement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassification costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: fixed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the refinement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibration in choosing the threshold choice method.",
            "keywords": [
                "classification performance metrics",
                "cost-sensitive evaluation",
                "operating condition",
                "Brier score"
            ],
            "author": [
                "José Hernández-Orallo",
                "Peter Flach",
                "Cèsar Ferri"
            ],
            "ref": "http://jmlr.org/papers/volume13/hernandez-orallo12a/hernandez-orallo12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local and Global Scaling Reduce Hubs in Space",
            "abstract": "'Hubness' has recently been identified as a general problem of high dimensional data spaces, manifesting itself in the emergence of objects, so-called hubs, which tend to be among the k nearest neighbors of a large number of data items. As a consequence many nearest neighbor relations in the distance space are asymmetric, that is, object y is amongst the nearest neighbors of x but not vice versa. The work presented here discusses two classes of methods that try to symmetrize nearest neighbor relations and investigates to what extent they can mitigate the negative effects of hubs. We evaluate local distance scaling and propose a global variant which has the advantage of being easy to approximate for large data sets and of having a probabilistic interpretation. Both local and global approaches are shown to be effective especially for high-dimensional data sets, which are affected by high hubness. Both methods lead to a strong decrease of hubness in these data sets, while at the same time improving properties like classification accuracy. We evaluate the methods on a large number of public machine learning data sets and synthetic data. Finally we present a real-world application where we are able to achieve significantly higher retrieval quality.",
            "keywords": [
                "local and global scaling",
                "shared near neighbors",
                "hubness",
                "classification",
                "curse of     dimensionality"
            ],
            "author": [
                "Dominik Schnitzer",
                "Arthur Flexer",
                "Markus Schedl",
                "Gerhard Widmer"
            ],
            "ref": "http://jmlr.org/papers/volume13/schnitzer12a/schnitzer12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Submodular Minimization",
            "abstract": "We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efficient and are Hannan-consistent in both the full information and partial feedback settings.",
            "keywords": [
                "submodular optimization",
                "online learning"
            ],
            "author": [
                "Elad Hazan",
                "Satyen Kale"
            ],
            "ref": "http://jmlr.org/papers/volume13/hazan12a/hazan12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices",
            "abstract": "In this paper we study the problem of designing SVM classifiers when the kernel matrix, K, is affected by uncertainty. Specifically K is modeled as a positive affine combination of given positive semi definite kernels, with the coefficients ranging in a norm-bounded uncertainty set. We treat the problem using the Robust Optimization methodology. This reduces the uncertain SVM problem into a deterministic conic quadratic problem which can be solved in principle by a polynomial time Interior Point (IP) algorithm. However, for large-scale classification problems,  IP methods become intractable and one has to resort to first-order gradient type methods. The strategy we use here is to reformulate the robust counterpart of the uncertain SVM problem as a saddle point problem and employ a special gradient scheme which works directly on the convex-concave saddle function. The algorithm is a simplified version of a general scheme due to Juditski and Nemirovski (2011). It achieves an O(1/T2) reduction of the initial error after T iterations. A comprehensive empirical study on both synthetic data and real-world protein structure data sets show that the proposed formulations achieve the desired robustness, and the saddle point based algorithm outperforms the IP method significantly.",
            "keywords": [
                "robust optimization",
                "uncertain classification"
            ],
            "author": [
                "Aharon Ben-Tal",
                "Sahely Bhadra",
                "Chiranjib Bhattacharyya",
                "Arkadi Nemirovski"
            ],
            "ref": "http://jmlr.org/papers/volume13/ben-tal12a/ben-tal12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Facilitating Score and Causal Inference Trees for Large Observational Studies",
            "abstract": "Assessing treatment effects in observational studies is a multifaceted problem that not only involves heterogeneous mechanisms of how the treatment or cause is exposed to subjects, known as propensity, but also differential causal effects across sub-populations. We introduce a concept termed the facilitating score to account for both the confounding and interacting impacts of covariates on the treatment effect. Several approaches for estimating the facilitating score are discussed. In particular, we put forward a machine learning method, called causal inference tree (CIT), to provide a piecewise constant approximation of the facilitating score. With interpretable rules, CIT splits data  in such a way that both the propensity and the treatment effect become more homogeneous within each resultant partition. Causal inference at different levels can be made on the basis of CIT.  Together with an aggregated grouping procedure, CIT stratifies data into strata where causal effects can be conveniently assessed within each. Besides, a feasible way of predicting individual causal effects (ICE) is made available by aggregating ensemble CIT models. Both the stratified results and the estimated ICE provide an assessment of heterogeneity of causal effects and can be integrated for estimating the average causal effect (ACE). Mean square consistency of CIT is also established. We evaluate the performance of proposed methods with simulations and illustrate their use with the NSW data in Dehejia and Wahba (1999) where the objective is to assess the impact of a labor training program, the National Supported Work (NSW) demonstration, on post-intervention earnings.",
            "keywords": [],
            "author": [
                "Xiaogang Su",
                "Joseph Kang",
                "Juanjuan Fan",
                "Richard A. Levine",
                "Xin Yan"
            ],
            "ref": "http://jmlr.org/papers/volume13/su12a/su12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Oger: Modular Learning Architectures For Large-Scale Sequential Processing",
            "abstract": "Oger (OrGanic Environment for Reservoir computing) is a Python toolbox for building, training and evaluating modular learning architectures on large data sets. It builds on MDP for its modularity, and adds processing of sequential data sets, gradient descent training, several cross-validation schemes and parallel parameter optimization methods. Additionally, several learning algorithms are implemented, such as different reservoir implementations (both sigmoid and spiking), ridge regression, conditional restricted Boltzmann machine (CRBM) and others, including GPU accelerated versions. Oger is released under the GNU LGPL, and is available from http://organic.elis.ugent.be/oger.",
            "keywords": [
                "Python",
                "modular architectures"
            ],
            "author": [
                "David Verstraeten",
                "Benjamin Schrauwen",
                "Sander Dieleman",
                "Philemon Brakel",
                "Pieter Buteneers",
                "Dejan Pecevski"
            ],
            "ref": "http://jmlr.org/papers/volume13/verstraeten12a/verstraeten12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Instance Learning with Any Hypothesis Class",
            "abstract": "In the supervised learning setting termed Multiple-Instance Learning (MIL), the examples are bags of instances, and the bag label is a function of the labels of its instances. Typically, this function is the Boolean OR. The learner observes a sample of bags and the bag labels, but not the instance labels that determine the bag labels. The learner is then required to emit a classification rule for bags based on the sample. MIL has numerous applications, and many heuristic algorithms have been used successfully on this problem, each adapted to specific settings or applications.  In this work we provide a unified theoretical analysis for MIL, which holds for any underlying hypothesis class, regardless of a specific application or problem domain. We show that the sample complexity of MIL is only poly-logarithmically dependent on the size of the bag, for any underlying hypothesis class.  In addition, we introduce a new PAC-learning algorithm for MIL, which uses a regular supervised learning algorithm as an oracle. We prove that efficient PAC-learning for MIL can be generated from any efficient non-MIL supervised learning algorithm that handles one-sided error. The computational complexity of the resulting algorithm is only polynomially dependent on the bag size.",
            "keywords": [
                "multiple-instance learning",
                "learning theory",
                "sample complexity",
                "PAC learning"
            ],
            "author": [
                "Sivan Sabato",
                "Naftali Tishby"
            ],
            "ref": "http://jmlr.org/papers/volume13/sabato12a/sabato12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Finite-Sample Analysis of Least-Squares Policy Iteration",
            "abstract": "In this paper, we report a performance bound for the widely used least-squares policy iteration (LSPI) algorithm. We first consider the problem of policy evaluation in reinforcement learning, that is, learning the value function of a fixed policy, using the least-squares temporal-difference (LSTD) learning method, and report finite-sample analysis for this algorithm. To do so, we first derive a bound on the performance of the LSTD solution evaluated at the states generated by the Markov chain and used by the algorithm to learn an estimate of the value function. This result is general in the sense that no assumption is made on the existence of a stationary distribution for the Markov chain. We then derive generalization bounds in the case when the Markov chain possesses a stationary distribution and is β-mixing. Finally, we analyze how the error at each policy evaluation step is propagated through the iterations of a policy iteration method, and derive a performance bound for the LSPI algorithm.",
            "keywords": [
                "Markov decision processes",
                "reinforcement learning",
                "least-squares temporal-difference",
                "least-squares policy iteration",
                "generalization bounds"
            ],
            "author": [
                "Alessandro Lazaric",
                "Mohammad Ghavamzadeh",
                "Rémi Munos"
            ],
            "ref": "http://jmlr.org/papers/volume13/lazaric12a/lazaric12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition",
            "abstract": "We consider the problem of parsing human poses and recognizing their actions in static images with part-based models. Most previous work in part-based models only considers rigid parts (e.g., torso, head, half limbs) guided by human anatomy. We argue that this representation of parts is not necessarily appropriate. In this paper, we introduce hierarchical poselets---a new representation for modeling the pose configuration of human bodies. Hierarchical poselets can be rigid parts, but they can also be parts that cover large portions of human bodies (e.g., torso + left arm). In the extreme case, they can be the whole bodies. The hierarchical poselets are organized in a hierarchical way via a structured model. Human parsing can be achieved by inferring the optimal labeling of this hierarchical model. The pose information captured by this hierarchical model can also be used as a intermediate representation for other high-level tasks. We demonstrate it in action recognition from static images.",
            "keywords": [
                "human parsing",
                "action recognition",
                "part-based models",
                "hierarchical poselets"
            ],
            "author": [
                "Yang Wang",
                "Duan Tran",
                "Zicheng Liao",
                "David Forsyth"
            ],
            "ref": "http://jmlr.org/papers/volume13/wang12a/wang12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training",
            "abstract": "Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Specifically, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efficiency both in time and space during training and prediction.",
            "keywords": [
                "SVM",
                "large-scale learning",
                "online learning",
                "stochastic gradient descent"
            ],
            "author": [
                "Zhuang Wang",
                "Koby Crammer",
                "Slobodan Vucetic"
            ],
            "ref": "http://jmlr.org/papers/volume13/wang12b/wang12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets",
            "abstract": "Classification algorithms are frequently used on data with a natural hierarchical structure. For instance, classifiers are often trained and tested on trial-wise measurements, separately for each subject within a group. One important question is how classification outcomes observed in individual subjects can be generalized to the population from which the group was sampled. To address this question, this paper introduces novel statistical models that are guided by three desiderata. First, all models explicitly respect the hierarchical nature of the data, that is, they are mixed-effects models that simultaneously account for within-subjects (fixed-effects) and across-subjects (random-effects) variance components. Second, maximum-likelihood estimation is replaced by full Bayesian inference in order to enable natural regularization of the estimation problem and to afford conclusions in terms of posterior probability statements. Third, inference on classification accuracy is complemented by inference on the balanced accuracy, which avoids inflated accuracy estimates for imbalanced data sets. We introduce hierarchical models that satisfy these criteria and demonstrate their advantages over conventional methods using MCMC implementations for model inversion and model selection on both synthetic and empirical data. We envisage that our approach will improve the sensitivity and validity of statistical inference in future hierarchical classification studies.",
            "keywords": [],
            "author": [
                "Kay H. Brodersen",
                "Christoph Mathys",
                "Justin R. Chumbley",
                "Jean Daunizeau",
                "Cheng Soon Ong",
                "Joachim M. Buhmann",
                "Klaas E. Stephan"
            ],
            "ref": "http://jmlr.org/papers/volume13/brodersen12a/brodersen12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantum Set Intersection and its Application to Associative Memory",
            "abstract": "We describe a quantum algorithm for computing the intersection of two sets and its application to associative memory. The algorithm is based on a modification of Grover's quantum search algorithm (Grover, 1996). We present algorithms for pattern retrieval, pattern completion, and pattern correction. We show that the quantum associative memory can store an exponential number of memories and retrieve them in sub-exponential time. We prove that this model has advantages over known classical associative memories as well as previously proposed quantum models.",
            "keywords": [
                "associative memory",
                "pattern completion",
                "pattern correction",
                "quantum computation"
            ],
            "author": [
                "Tamer Salman",
                "Yoram Baram"
            ],
            "ref": "http://jmlr.org/papers/volume13/salman12a/salman12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Policy Programming",
            "abstract": "In this paper, we propose a novel policy iteration method, called dynamic policy programming (DPP), to estimate the optimal policy in the infinite-horizon Markov decision processes.  DPP is an incremental algorithm that forces a gradual change in policy update.  This allows us to prove finite-iteration and asymptotic  l∞-norm performance-loss bounds in the presence of approximation/estimation error which depend on the average accumulated error as opposed to the standard bounds which are expressed in terms of  the supremum of the errors.  The dependency on the average error is important in problems with limited number of samples per iteration, for which the average of the errors can be significantly smaller in size than the supremum of the errors. Based on these theoretical results, we prove that a sampling-based variant of DPP (DPP-RL) asymptotically converges to the optimal policy. Finally, we illustrate numerically the applicability of these results on some benchmark problems and compare the performance of the approximate variants of DPP with some existing reinforcement learning (RL) methods.",
            "keywords": [
                "approximate dynamic programming",
                "reinforcement learning",
                "Markov decision pro-     cesses",
                "Monte-Carlo methods"
            ],
            "author": [
                "Mohammad Gheshlaghi Azar",
                "Vicenç Gómez",
                "Hilbert J. Kappen"
            ],
            "ref": "http://jmlr.org/papers/volume13/azar12a/azar12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sally: A Tool for Embedding Strings in Vector Spaces",
            "abstract": "Strings and sequences are ubiquitous in many areas of data analysis. However, only few learning methods can be directly applied to this form of data.  We present Sally, a tool for embedding strings in vector spaces that allows for applying a wide range of learning methods to string data.  Sally implements a generalized form of the bag-of-words model, where strings are mapped to a vector space that is spanned by a set of string features, such as words or n-grams of words. The implementation of Sally builds on efficient string algorithms and enables processing millions of strings and features. The tool supports several data formats and is capable of interfacing with common learning environments, such as Weka, Shogun, Matlab, or Pylab. Sally has been successfully applied for learning with natural language text, DNA sequences and monitored program behavior.",
            "keywords": [
                "string embedding",
                "bag-of-words models"
            ],
            "author": [
                "Konrad Rieck",
                "Christian Wressnegger",
                "Alexander Bikadorov"
            ],
            "ref": "http://jmlr.org/papers/volume13/rieck12a/rieck12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear Fitted-Q Iteration with Multiple Reward Functions",
            "abstract": "We present a general and detailed development of an algorithm for finite-horizon fitted-Q iteration with an arbitrary number of reward signals and linear value function approximation using an arbitrary number of state features. This includes a detailed treatment of the 3-reward function case using triangulation primitives from computational geometry and a method for identifying globally dominated actions.  We also present an example of how our methods can be used to construct a real-world decision aid by considering symptom reduction, weight gain, and quality of life in sequential treatments for schizophrenia. Finally, we discuss future directions in which to take this work that will further enable our methods to make a positive impact on the field of evidence-based clinical decision support.",
            "keywords": [
                "reinforcement learning",
                "dynamic programming",
                "decision making",
                "linear regression"
            ],
            "author": [
                "Daniel J. Lizotte",
                "Michael Bowling",
                "Susan A. Murphy"
            ],
            "ref": "http://jmlr.org/papers/volume13/lizotte12a/lizotte12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Human Gesture Recognition on Product Manifolds",
            "abstract": "Action videos are multidimensional data and can be naturally represented as data tensors.  While tensor computing is widely used in computer vision, the geometry of tensor space is often ignored. The aim of this paper is to demonstrate the importance of the intrinsic geometry of tensor space which yields a very discriminating structure for action recognition. We characterize data tensors as points on a product manifold and model it statistically using least squares regression. To this aim, we factorize a data tensor relating to each order of the tensor using Higher Order Singular Value Decomposition (HOSVD) and then impose each factorized element on a Grassmann manifold. Furthermore, we account for underlying geometry on manifolds and formulate least squares regression as a composite function. This gives a natural extension from Euclidean space to manifolds. Consequently, classification is performed using geodesic distance on a product manifold where each factor manifold is Grassmannian.  Our method exploits appearance and motion without explicitly modeling the shapes and dynamics. We assess the proposed method using three gesture databases, namely the Cambridge hand-gesture, the UMD Keck body-gesture, and the CHALEARN gesture challenge data sets. Experimental results reveal that not only does the proposed method perform well on the standard benchmark data sets, but also it generalizes well on the one-shot-learning gesture challenge. Furthermore, it is based on a simple statistical model and the intrinsic geometry of tensor space.",
            "keywords": [
                "gesture recognition",
                "action recognition",
                "Grassmann manifolds",
                "product manifolds",
                "one-shot-learning"
            ],
            "author": [
                "Yui Man Lui"
            ],
            "ref": "http://jmlr.org/papers/volume13/lui12a/lui12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large-scale Linear Support Vector Regression",
            "abstract": "Support vector regression (SVR) and support vector classification (SVC) are popular learning techniques, but their use with kernels is often time consuming.  Recently, linear SVC without kernels has been shown to give competitive accuracy for some applications, but enjoys much faster training/testing.  However, few studies have focused on linear SVR.  In this paper, we extend state-of-the-art training methods for linear SVC to linear SVR.  We show that the extension is straightforward for some methods, but is not trivial for some others.  Our experiments demonstrate that for some problems, the proposed linear-SVR training methods can very efficiently produce models that are as good as kernel SVR.",
            "keywords": [
                "support vector regression",
                "Newton methods"
            ],
            "author": [
                "Chia-Hua Ho",
                "Chih-Jen Lin"
            ],
            "ref": "http://jmlr.org/papers/volume13/ho12a/ho12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing",
            "abstract": "Nonnegative matrix factorization (NMF) has become a very popular technique in machine learning because it automatically extracts meaningful features through a sparse and part-based representation. However, NMF has the drawback of being highly ill-posed, that is, there typically exist many different but equivalent factorizations.  In this paper, we introduce a completely new way to obtaining more well-posed NMF problems whose solutions are sparser. Our technique is based on the preprocessing of the nonnegative input data matrix, and relies on the theory of M-matrices and the geometric interpretation of NMF.  This approach provably leads to optimal and sparse solutions under the separability assumption of Donoho and Stodden (2003), and, for rank-three matrices, makes the number of exact factorizations finite. We illustrate the effectiveness of our technique on several image data sets.",
            "keywords": [
                "nonnegative matrix factorization",
                "data preprocessing",
                "uniqueness",
                "sparsity"
            ],
            "author": [
                "Nicolas Gillis"
            ],
            "ref": "http://jmlr.org/papers/volume13/gillis12a/gillis12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Linear Cyclic Causal Models with Latent Variables",
            "abstract": "Identifying cause-effect relationships between variables of interest is a central problem in science. Given a set of experiments we describe a procedure that identifies linear models that may contain cycles and latent variables. We provide a detailed description of the model family, full proofs of the necessary and sufficient conditions for identifiability, a search algorithm that is complete, and a discussion of what can be done when the identifiability conditions are not satisfied. The algorithm is comprehensively tested in simulations, comparing it to competing algorithms in the literature. Furthermore, we adapt the procedure to the problem of cellular network inference, applying it to the biologically realistic data of the DREAM challenges. The paper provides a full theoretical foundation for the causal discovery procedure first presented by Eberhardt et al. (2010) and Hyttinen et al. (2010).",
            "keywords": [
                "causality",
                "graphical models",
                "randomized experiments",
                "structural equation models",
                "latent variables",
                "latent confounders"
            ],
            "author": [
                "Antti Hyttinen",
                "Frederick Eberhardt",
                "Patrik O. Hoyer"
            ],
            "ref": "http://jmlr.org/papers/volume13/hyttinen12a/hyttinen12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Approximation of Matrix Coherence and Statistical Leverage",
            "abstract": "The statistical leverage scores of a matrix A are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score.  These quantities are of interest in recently-popular problems such as matrix completion and Nyström-based low-rank matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms.  Our main result is a randomized algorithm that takes as input an arbitrary n × d matrix A, with n >> d, and that returns as output relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of n and d) in O(n d log n)  time, as opposed to the O(nd2) time required by the naïve algorithm that involves computing an orthogonal basis for the range of A. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with n ≈ d, and the extension to streaming environments.",
            "keywords": [
                "matrix coherence",
                "statistical leverage"
            ],
            "author": [
                "Petros Drineas",
                "Malik Magdon-Ismail",
                "Michael W. Mahoney",
                "David P. Woodruff"
            ],
            "ref": "http://jmlr.org/papers/volume13/drineas12a/drineas12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PAC-Bayes Bounds with Data Dependent Priors",
            "abstract": "This paper presents the prior PAC-Bayes bound and explores its capabilities as a tool to provide tight predictions of SVMs' generalization. The computation of the bound involves estimating a prior of the distribution of classifiers from the available data, and then manipulating this prior in the usual PAC-Bayes generalization bound. We explore two alternatives: to learn the prior from a separate data set, or to consider an expectation prior that does not need this separate data set. The prior PAC-Bayes bound motivates two SVM-like classification algorithms, prior SVM and η-prior SVM, whose regularization term  pushes towards the minimization of the prior PAC-Bayes bound. The experimental work illustrates that the new bounds can be significantly tighter than the original PAC-Bayes bound when applied to SVMs, and among them the combination of the prior PAC-Bayes bound and the prior SVM algorithm gives the tightest bound.",
            "keywords": [
                "PAC-Bayes bound",
                "support vector machine",
                "generalization capability prediction"
            ],
            "author": [
                "Emilio Parrado-Hernández",
                "Amiran Ambroladze",
                "John Shawe-Taylor",
                "Shiliang Sun"
            ],
            "ref": "http://jmlr.org/papers/volume13/parrado12a/parrado12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DARWIN: A Framework for Machine Learning and Computer Vision Research and Development",
            "abstract": "We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data flows.",
            "keywords": [
                "machine learning",
                "graphical models",
                "computer vision"
            ],
            "author": [
                "Stephen Gould"
            ],
            "ref": "http://jmlr.org/papers/volume13/gould12a/gould12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Bundle Methods for Convex and Non-Convex Risks",
            "abstract": "Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efficient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efficient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random fields, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difficult and requires a stronger and more disputable assumption. Yet we provide experimental results on artificial test problems, and on five standard and difficult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms.",
            "keywords": [
                "optimization",
                "non-convex",
                "non-smooth",
                "cutting plane",
                "bundle method"
            ],
            "author": [
                "Trinh Minh Tri Do",
                "Thierry Artières"
            ],
            "ref": "http://jmlr.org/papers/volume13/do12a/do12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Symbolic Representations of Hybrid Dynamical Systems",
            "abstract": "A hybrid dynamical system is a mathematical model suitable for describing an extensive spectrum of multi-modal, time-series behaviors, ranging from bouncing balls to air traffic controllers. This paper describes multi-modal symbolic regression (MMSR): a learning algorithm to construct non-linear symbolic representations of discrete dynamical systems with continuous mappings from unlabeled, time-series data. MMSR consists of two subalgorithms---clustered symbolic regression, a method to simultaneously identify distinct behaviors while formulating their mathematical expressions, and transition modeling, an algorithm to infer symbolic inequalities that describe binary classification boundaries. These subalgorithms are combined to infer hybrid dynamical systems as a collection of apt, mathematical expressions. MMSR is evaluated on a collection of four synthetic data sets and outperforms other multi-modal machine learning approaches in both accuracy and interpretability, even in the presence of noise. Furthermore, the versatility of MMSR is demonstrated by identifying and inferring classical expressions of transistor modes from recorded measurements.",
            "keywords": [
                "hybrid dynamical systems",
                "evolutionary computation",
                "symbolic piecewise functions"
            ],
            "author": [
                "Daniel L. Ly",
                "Hod Lipson"
            ],
            "ref": "http://jmlr.org/papers/volume13/ly12a/ly12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SVDFeature: A Toolkit for Feature-based Collaborative Filtering",
            "abstract": "In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative filtering. SVDFeature is designed to efficiently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information.  The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efficient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive  years.",
            "keywords": [
                "large-scale collaborative filtering",
                "context-aware recommendation"
            ],
            "author": [
                "Tianqi Chen",
                "Weinan Zhang",
                "Qiuxia Lu",
                "Kailong Chen",
                "Zhao Zheng",
                "Yong Yu"
            ],
            "ref": "http://jmlr.org/papers/volume13/chen12a/chen12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Smoothing Multivariate Performance Measures",
            "abstract": "Optimizing multivariate performance measure is an important task in Machine Learning.  Joachims (2005) introduced a Support Vector Method whose underlying optimization problem is commonly solved by cutting plane methods (CPMs) such as SVM-Perf and BMRM.  It can be shown that CPMs converge to an ε accurate solution in O(1/λ ε) iterations, where λ is the trade-off parameter between the regularizer and the loss function.  Motivated by the impressive convergence rate of CPM on a number of practical problems, it was conjectured that these rates can be further improved.  We disprove this conjecture in this paper by constructing counter examples.  However, surprisingly, we further discover that these problems are not inherently hard, and we develop a novel smoothing strategy, which in conjunction with Nesterov's accelerated gradient method, can find an ε accurate solution in O* (min {1/ε, 1/√λε}) iterations.  Computationally, our smoothing technique is also particularly advantageous for optimizing multivariate performance scores such as precision/recall break-even point and ROCArea; the cost per iteration remains the same as that of CPMs.  Empirical evaluation on some of the largest publicly available data sets shows that our method converges significantly faster than CPMs without sacrificing generalization ability.",
            "keywords": [
                "non-smooth optimization",
                "max-margin methods",
                "multivariate performance measures",
                "Support Vector Machines"
            ],
            "author": [
                "Xinhua Zhang",
                "Ankan Saha",
                "S.V.N. Vishwanathan"
            ],
            "ref": "http://jmlr.org/papers/volume13/zhang12d/zhang12d.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Security Analysis of Online Centroid Anomaly Detection",
            "abstract": "Security issues are crucial in a number of machine learning applications, especially in scenarios dealing with human activity rather than natural phenomena (e.g., information ranking, spam detection, malware detection, etc.). In such cases, learning algorithms may have to cope with manipulated data aimed at hampering decision making. Although some previous work addressed the issue of handling malicious data in the context of supervised learning, very little is known about the behavior of anomaly detection methods in such scenarios. In this contribution, we analyze the performance of a particular method---online centroid anomaly detection---in the presence of adversarial noise.  Our analysis addresses the following security-related issues: formalization of learning and attack processes, derivation of an optimal attack, and analysis of attack efficiency and limitations. We derive bounds on the effectiveness of a poisoning attack against centroid anomaly detection under different conditions: attacker's full or limited control over the traffic and bounded false positive rate. Our bounds show that whereas a poisoning attack can be effectively staged in the unconstrained case, it can be made arbitrarily difficult (a strict upper bound on the attacker's gain) if external constraints are properly used. Our experimental evaluation, carried out on real traces of HTTP and exploit traffic, confirms the tightness of our theoretical bounds and the practicality of our protection mechanisms.",
            "keywords": [
                "anomaly detection",
                "adversarial",
                "security analysis",
                "support vector data description",
                "computer security"
            ],
            "author": [
                "Marius Kloft",
                "Pavel Laskov"
            ],
            "ref": "http://jmlr.org/papers/volume13/kloft12b/kloft12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exploration in Relational Domains for Model-based Reinforcement Learning",
            "abstract": "A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E3 and R-MAX algorithms. Efficient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efficiency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efficient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques.",
            "keywords": [
                "reinforcement learning",
                "statistical relational learning",
                "exploration",
                "relational transition     models"
            ],
            "author": [
                "Tobias Lang",
                "Marc Toussaint",
                "Kristian Kersting"
            ],
            "ref": "http://jmlr.org/papers/volume13/lang12a/lang12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses",
            "abstract": "Manifold regularization (Belkin et al., 2006) is a geometrically motivated framework for machine learning within which several semi- supervised algorithms have been constructed. Here we try to provide some theoretical understanding of this approach. Our main result is to expose the natural structure of a class of problems on which manifold regularization methods are helpful. We show that for such problems, no supervised learner can learn effectively. On the other hand, a manifold based learner (that knows the manifold or âlearnsâ it from unlabeled examples) can learn with relatively few labeled examples. Our analysis follows a minimax style with an emphasis on finite sample results (in terms of : the number of labeled examples). These results allow us to properly interpret manifold regularization and related spectral and geometric algorithms in terms of their potential use in semi-supervised learning.",
            "keywords": [
                "semi-supervised learning",
                "manifold regularization",
                "graph Laplacian"
            ],
            "author": [
                "Partha Niyogi"
            ],
            "ref": "http://jmlr.org/papers/volume14/niyogi13a/niyogi13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Spanning Trees and the Prediction of Weighted Graphs",
            "abstract": "We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world data sets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice.",
            "keywords": [
                "online learning",
                "learning on graphs",
                "graph prediction"
            ],
            "author": [
                "Nicolò Cesa-Bianchi",
                "Claudio Gentile",
                "Fabio Vitale",
                "Giovanni Zappella"
            ],
            "ref": "http://jmlr.org/papers/volume14/cesa-bianchi13a/cesa-bianchi13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularization-Free Principal Curve Estimation",
            "abstract": "Principal curves and manifolds provide a framework to formulate manifold learning within a statistical context. Principal curves define the notion of a curve passing through the middle of a distribution. While the intuition is clear, the formal definition leads to some technical and practical difficulties. In particular, principal curves are saddle points of the mean- squared projection distance, which poses severe challenges for estimation and model selection. This paper demonstrates that the difficulties in model selection associated with the saddle point property of principal curves are intrinsically tied to the minimization of the mean-squared projection distance. We introduce a new objective function, facilitated through a modification of the principal curve estimation approach, for which all critical points are principal curves and minima. Thus, the new formulation removes the fundamental issue for model selection in principal curve estimation. A gradient-descent- based estimator demonstrates the effectiveness of the new formulation for controlling model complexity on numerical experiments with synthetic and real data.",
            "keywords": [
                "principal curve",
                "manifold estimation",
                "unsupervised learning",
                "model complexity"
            ],
            "author": [
                "Samuel Gerber",
                "Ross Whitaker"
            ],
            "ref": "http://jmlr.org/papers/volume14/gerber13a/gerber13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Variational Inference",
            "abstract": "We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.",
            "keywords": [
                "Bayesian inference",
                "variational inference",
                "stochastic optimization",
                "topic models"
            ],
            "author": [
                "Matthew D. Hoffman",
                "David M. Blei",
                "Chong Wang",
                "John Paisley"
            ],
            "ref": "http://jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Finding Optimal Bayesian Networks Using Precedence Constraints",
            "abstract": "We consider the problem of finding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space , to within a factor polynomial in the number of nodes . In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: first, the user may trade space against time; second, the proposed algorithms easily and efficiently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order  on the nodes, an optimal DAG compatible with  can be found in time and space roughly proportional to the number of ideals of , which can be significantly less than . Considering sufficiently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders.",
            "keywords": [
                "exact algorithm",
                "parallelization",
                "partial order",
                "space-time tradeoff"
            ],
            "author": [
                "Pekka Parviainen",
                "Mikko Koivisto"
            ],
            "ref": "http://jmlr.org/papers/volume14/parviainen13a/parviainen13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "JKernelMachines: A Simple Framework for Kernel Machines",
            "abstract": "JKernelMachines is a Java library for learning with kernels. It is primarily designed to deal with custom kernels that are not easily found in standard libraries, such as kernels on structured data. These types of kernels are often used in computer vision or bioinformatics applications. We provide several kernels leading to state of the art classification performances in computer vision, as well as various kernels on sets. The main focus of the library is to be easily extended with new kernels. Standard SVM optimization algorithms are available, but also more sophisticated learning-based kernel combination methods such as Multiple Kernel Learning (MKL), and a recently published algorithm to learn powered products of similarities (Product Kernel Learning).",
            "keywords": [
                "classification",
                "support vector machines",
                "kernel"
            ],
            "author": [
                "David Picard",
                "Nicolas Thome",
                "Matthieu Cord"
            ],
            "ref": "http://jmlr.org/papers/volume14/picard13a/picard13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems",
            "abstract": "There are two main approaches to binary classification problems: the loss function approach and the uncertainty set approach. The loss function approach is widely used in real-world data analysis. Statistical decision theory has been used to elucidate its properties such as statistical consistency. Conditional probabilities can also be estimated by using the minimum solution of the loss function. In the uncertainty set approach, an uncertainty set is defined for each binary label from training samples. The best separating hyperplane between the two uncertainty sets is used as the decision function. Although the uncertainty set approach provides an intuitive understanding of learning algorithms, its statistical properties have not been sufficiently studied. In this paper, we show that the uncertainty set is deeply connected with the convex conjugate of a loss function. On the basis of the conjugate relation, we propose a way of revising the uncertainty set approach so that it will have good statistical properties such as statistical consistency. We also introduce statistical models corresponding to uncertainty sets in order to estimate conditional probabilities. Finally, we present numerical experiments, verifying that the learning with revised uncertainty sets improves the prediction accuracy.",
            "keywords": [
                "loss function",
                "uncertainty set",
                "convex conjugate"
            ],
            "author": [
                "Takafumi Kanamori",
                "Akiko Takeda",
                "Taiji Suzuki"
            ],
            "ref": "http://jmlr.org/papers/volume14/kanamori13a/kanamori13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Learnability of Shuffle Ideals",
            "abstract": "PAC learning of unrestricted regular languages is long known to be a difficult problem. The class of shuffle ideals is a very restricted subclass of regular languages, where the shuffle ideal generated by a string  is the collection of all strings containing  as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shuffle ideals appears quite difficult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP  NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efficient algorithm for properly learning shuffle ideals in the statistical query (and therefore also PAC) model under the uniform distribution.",
            "keywords": [
                "PAC learning",
                "statistical queries",
                "regular languages",
                "deterministic finite automata",
                "shuffle ideals"
            ],
            "author": [
                "Dana Angluin",
                "James Aspnes",
                "Sarah Eisenstat",
                "Aryeh Kontorovich"
            ],
            "ref": "http://jmlr.org/papers/volume14/angluin13a/angluin13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Generalized Subset Scan for Anomalous Pattern Detection",
            "abstract": "We propose Fast Generalized Subset Scan (FGSS), a new method for detecting anomalous patterns in general categorical data sets. We frame the pattern detection problem as a search over subsets of data records and attributes, maximizing a nonparametric scan statistic over all such subsets. We prove that the nonparametric scan statistics possess a novel property that allows for efficient optimization over the exponentially many subsets of the data without an exhaustive search, enabling FGSS to scale to massive and high-dimensional data sets. We evaluate the performance of FGSS in three real-world application domains (customs monitoring, disease surveillance, and network intrusion detection), and demonstrate that FGSS can successfully detect and characterize relevant patterns in each domain. As compared to three other recently proposed detection algorithms, FGSS substantially decreased run time and improved detection power for massive multivariate data sets.",
            "keywords": [
                "pattern detection",
                "anomaly detection",
                "knowledge discovery",
                "Bayesian networks"
            ],
            "author": [
                "Edward McFowl",
                "III",
                "Skyler Speakman",
                "Daniel B. Neill"
            ],
            "ref": "http://jmlr.org/papers/volume14/mcfowland13a/mcfowland13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion",
            "abstract": "Constraint-based learning of Bayesian networks (BN) from limited data can lead to multiple testing problems when recovering dense areas of the skeleton and to conflicting results in the orientation of edges. In this paper, we present a new constraint-based algorithm, light mutual min (LMM) for improved accuracy of BN learning from small sample data. LMM improves the assessment of candidate edges by using a ranking criterion that considers conditional independence on neighboring variables at both sides of an edge simultaneously. The algorithm also employs an adaptive relaxation of constraints that, selectively, allows some nodes not to condition on some neighbors. This relaxation aims at reducing the incorrect rejection of true edges connecting high degree nodes due to multiple testing. LMM additionally incorporates a new criterion for ranking v-structures that is used to recover the completed partially directed acyclic graph (CPDAG) and to resolve conflicting v-structures, a common problem in small sample constraint-based learning. Using simulated data, each of these components of LMM is shown to significantly improve network inference compared to commonly applied methods when learning from limited data, including more accurate recovery of skeletons and CPDAGs compared to the PC, MaxMin, and MaxMin hill climbing algorithms. A proof of asymptotic correctness is also provided for LMM for recovering the correct skeleton and CPDAG.",
            "keywords": [
                "Bayesian networks",
                "skeleton",
                "constraint-based learning"
            ],
            "author": [
                "Rami Mahdi",
                "Jason Mezey"
            ],
            "ref": "http://jmlr.org/papers/volume14/mahdi13a/mahdi13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dimension Independent Similarity Computation",
            "abstract": "We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high-dimensional sparse vectors. All of our results are provably independent of dimension, meaning that apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension; thus the dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard similarity measures. For Jaccard similarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems with large scale experiments using data from the social networking site Twitter. At time of writing, our algorithms are live in production at twitter.com.",
            "keywords": [
                "cosine",
                "Jaccard",
                "overlap",
                "dice",
                "similarity",
                "MapReduce"
            ],
            "author": [
                "Reza Bosagh Zadeh",
                "Ashish Goel"
            ],
            "ref": "http://jmlr.org/papers/volume14/bosagh-zadeh13a/bosagh-zadeh13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos",
            "abstract": "We propose the novel approach of dynamic affine-invariant shape- appearance model (Aff-SAM) and employ it for handshape classification and sign recognition in sign language (SL) videos. Aff-SAM offers a compact and descriptive representation of hand configurations as well as regularized model-fitting, assisting hand tracking and extracting handshape features. We construct SA images representing the hand's shape and appearance without landmark points. We model the variation of the images by linear combinations of eigenimages followed by affine transformations, accounting for 3D hand pose changes and improving model's compactness. We also incorporate static and dynamic handshape priors, offering robustness in occlusions, which occur often in signing. The approach includes an affine signer adaptation component at the visual level, without requiring training from scratch a new singer-specific model. We rather employ a short development data set to adapt the models for a new signer. Experiments on the Boston- University-400 continuous SL corpus demonstrate improvements on handshape classification when compared to other feature extraction approaches. Supplementary evaluations of sign recognition experiments, are conducted on a multi-signer, 100-sign data set, from the Greek sign language lemmas corpus. These explore the fusion with movement cues as well as signer adaptation of Aff- SAM to multiple signers providing promising results.",
            "keywords": [
                "affine-invariant shape-appearance model",
                "landmarks-free shape representation",
                "static     and dynamic priors",
                "feature extraction"
            ],
            "author": [
                "Anastasios Roussos",
                "Stavros Theodorakis",
                "Vassilis Pitsikalis",
                "Petros Maragos"
            ],
            "ref": "http://jmlr.org/papers/volume14/roussos13a/roussos13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric Sparsity and Regularization",
            "abstract": "In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose a new notion of nonparametric sparsity and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art methods.",
            "keywords": [
                "sparsity",
                "nonparametric",
                "variable selection",
                "regularization",
                "proximal methods"
            ],
            "author": [
                "Lorenzo Rosasco",
                "Silvia Villa",
                "Sofia Mosci",
                "Matteo Santoro",
                "Aless",
                "ro Verri"
            ],
            "ref": "http://jmlr.org/papers/volume14/rosasco13a/rosasco13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Similarity-based Clustering by Left-Stochastic Matrix Factorization",
            "abstract": "For similarity-based clustering, we propose modeling the entries of a given similarity matrix as the inner products of the unknown cluster probabilities. To estimate the cluster probabilities from the given similarity matrix, we introduce a left-stochastic non-negative matrix factorization problem. A rotation-based algorithm is proposed for the matrix factorization. Conditions for unique matrix factorizations and clusterings are given, and an error bound is provided. The algorithm is particularly efficient for the case of two clusters, which motivates a hierarchical variant for cases where the number of desired clusters is large. Experiments show that the proposed left-stochastic decomposition clustering model produces relatively high within-cluster similarity on most data sets and can match given class labels, and that the efficient hierarchical variant performs surprisingly well.",
            "keywords": [
                "clustering",
                "non-negative matrix factorization",
                "rotation",
                "indefinite kernel",
                "similarity"
            ],
            "author": [
                "Raman Arora",
                "Maya R. Gupta",
                "Amol Kapila",
                "Maryam Fazel"
            ],
            "ref": "http://jmlr.org/papers/volume14/arora13a/arora13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Convergence of Maximum Variance Unfolding",
            "abstract": "Maximum Variance Unfolding is one of the main methods for (nonlinear) dimensionality reduction. We study its large sample limit, providing specific rates of convergence under standard assumptions. We find that it is consistent when the underlying submanifold is isometric to a convex subset, and we provide some simple examples where it fails to be consistent.",
            "keywords": [
                "maximum variance unfolding",
                "isometric embedding",
                "U-processes",
                "empirical pro-     cesses"
            ],
            "author": [
                "Ery Arias-Castro",
                "Bruno Pelletier"
            ],
            "ref": "http://jmlr.org/papers/volume14/arias-castro13a/arias-castro13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit",
            "abstract": "The performance of orthogonal matching pursuit (OMP) for variable selection is analyzed for random designs. When contrasted with the deterministic case, since the performance is here measured after averaging over the distribution of the design matrix, one can have far less stringent sparsity constraints on the coefficient vector. We demonstrate that for exact sparse vectors, the performance of the OMP is similar to known results on the Lasso algorithm (Wainwright, 2009). Moreover, variable selection under a more relaxed sparsity assumption on the coefficient vector, whereby one has only control on the  norm of the smaller coefficients, is also analyzed. As consequence of these results, we also show that the coefficient estimate satisfies strong oracle type inequalities.",
            "keywords": [
                "high dimensional regression",
                "greedy algorithms",
                "Lasso"
            ],
            "author": [
                "Antony Joseph"
            ],
            "ref": "http://jmlr.org/papers/volume14/joseph13a/joseph13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs",
            "abstract": "We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to significant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is significantly more accurate than previous approximations and should become exact in the limit of large random graphs.",
            "keywords": [
                "Gaussian process",
                "generalisation error",
                "learning curve",
                "cavity method",
                "belief propaga-     tion",
                "graph"
            ],
            "author": [
                "Matthew J. Urry",
                "Peter Sollich"
            ],
            "ref": "http://jmlr.org/papers/volume14/urry13a/urry13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributions of Angles in Random Packing on Spheres",
            "abstract": "This paper studies the asymptotic behaviors of the pairwise angles among  randomly and uniformly distributed unit vectors in  as the number of points , while the dimension  is either fixed or growing with . For both settings, we derive the limiting empirical distribution of the random angles and the limiting distributions of the extreme angles. The results reveal interesting differences in the two settings and provide a precise characterization of the folklore that âall high-dimensional random vectors are almost always nearly orthogonal to each other\". Applications to statistics and machine learning and connections with some open problems in physics and mathematics are also discussed.",
            "keywords": [
                "random angle",
                "uniform distribution on sphere",
                "empirical law",
                "maximum of random     variables",
                "minimum of random variables",
                "extreme-value distribution"
            ],
            "author": [
                "Tony Cai",
                "Jianqing Fan",
                "Tiefeng Jiang"
            ],
            "ref": "http://jmlr.org/papers/volume14/cai13a/cai13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty",
            "abstract": "Clustering analysis is widely used in many fields. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classification and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classification and regression, such as model selection criteria to select the number of clusters, a difficult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method's promising performance.",
            "keywords": [
                "generalized degrees of freedom",
                "grouping",
                "K-means clustering",
                "Lasso",
                "penalized re-     gression"
            ],
            "author": [
                "Wei Pan",
                "Xiaotong Shen",
                "Binghui Liu"
            ],
            "ref": "http://jmlr.org/papers/volume14/pan13a/pan13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation",
            "abstract": "We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efficiently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about specific groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy.",
            "keywords": [
                "group feature selection",
                "generalized spike-and-slab priors",
                "expectation propagation",
                "sparse linear model",
                "approximate inference",
                "sequential experimental design"
            ],
            "author": [
                "Daniel Hernández-Lobato",
                "José Miguel Hernández-Lobato",
                "Pierre Dupont"
            ],
            "ref": "http://jmlr.org/papers/volume14/hernandez-lobato13a/hernandez-lobato13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Alleviating Naive Bayes Attribute Independence Assumption by Attribute Weighting",
            "abstract": "Despite the simplicity of the Naive Bayes classifier, it has continued to perform well against more sophisticated newcomers and has remained, therefore, of great interest to the machine learning community. Of numerous approaches to refining the naive Bayes classifier, attribute weighting has received less attention than it warrants. Most approaches, perhaps influenced by attribute weighting in other machine learning algorithms, use weighting to place more emphasis on highly predictive attributes than those that are less predictive. In this paper, we argue that for naive Bayes attribute weighting should instead be used to alleviate the conditional independence assumption. Based on this premise, we propose a weighted naive Bayes algorithm, called WANBIA, that selects weights to minimize either the negative conditional log likelihood or the mean squared error objective functions. We perform extensive evaluations and find that WANBIA is a competitive alternative to state of the art classifiers like Random Forest, Logistic Regression and A1DE.",
            "keywords": [
                "classification",
                "naive Bayes",
                "attribute independence assumption"
            ],
            "author": [
                "Nayyar A. Zaidi",
                "Jesús Cerquides",
                "Mark J. Carman",
                "Geoffrey I. Webb"
            ],
            "ref": "http://jmlr.org/papers/volume14/zaidi13a/zaidi13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximating the Permanent with Fractional Belief Propagation",
            "abstract": "We discuss schemes for exact and approximate computations of permanents, and compare them with each other. Specifically, we analyze the belief propagation (BP) approach and its fractional belief propagation (FBP) generalization for computing the permanent of a non-negative matrix. Known bounds and Conjectures are verified in experiments, and some new theoretical relations, bounds and Conjectures are proposed. The fractional free energy (FFE) function is parameterized by a scalar parameter , where  corresponds to the BP limit and  corresponds to the exclusion principle (but ignoring perfect matching constraints) mean-field (MF) limit. FFE shows monotonicity and continuity with respect to . For every non-negative matrix, we define its special value  to be the  for which the minimum of the -parameterized FFE function is equal to the permanent of the matrix, where the lower and upper bounds of the -interval corresponds to respective bounds for the permanent. Our experimental analysis suggests that the distribution of  varies for different ensembles but  always lies within the  interval. Moreover, for all ensembles considered, the behavior of  is highly distinctive, offering an empirical practical guidance for estimating permanents of non-negative matrices via the FFE approach.",
            "keywords": [
                "permanent",
                "graphical models",
                "belief propagation",
                "exact and approximate algorithms"
            ],
            "author": [
                "Michael Chertkov",
                "Adam B. Yedidia"
            ],
            "ref": "http://jmlr.org/papers/volume14/chertkov13a/chertkov13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Construction of Approximation Spaces for Reinforcement Learning",
            "abstract": "Linear reinforcement learning (RL) algorithms like  least-squares temporal difference learning (LSTD) require  basis functions that span approximation spaces of potential value functions. This article investigates methods to construct these bases from samples. We hypothesize that an ideal approximation spaces should encode diffusion distances and that slow feature analysis (SFA) constructs such spaces. To validate our hypothesis we provide theoretical statements about the LSTD value approximation error and induced metric of approximation spaces constructed by SFA and the state-of-the-art methods Krylov bases and proto-value functions (PVF). In particular, we prove that SFA minimizes the average (over all tasks in the same environment) bound on the above approximation error. Compared to other methods, SFA is very sensitive to sampling and can sometimes fail to encode the whole state space. We derive a novel importance sampling modification to compensate for this effect. Finally, the LSTD and least squares policy iteration (LSPI) performance of approximation spaces constructed by Krylov bases, PVF, SFA and PCA is compared in benchmark tasks and a visual robot navigation experiment (both in a realistic simulation and with a robot). The results support our hypothesis and suggest that (i) SFA provides subspace-invariant features for MDPs with  self-adjoint transition operators, which allows strong guarantees on the approximation error, (ii) the modified SFA algorithm is best suited for LSPI in both discrete and continuous state spaces and (iii) approximation spaces encoding diffusion distances facilitate LSPI performance.",
            "keywords": [
                "reinforcement learning",
                "diffusion distance",
                "proto value functions",
                "slow feature analy-     sis",
                "least-squares policy iteration"
            ],
            "author": [
                "Wendelin Böhmer",
                "Steffen Grünewälder",
                "Yun Shen",
                "Marek Musial",
                "Klaus Obermayer"
            ],
            "ref": "http://jmlr.org/papers/volume14/boehmer13a/boehmer13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distribution-Dependent Sample Complexity of Large Margin Learning",
            "abstract": "We obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with  regularization: We introduce the margin-adapted dimension, which is a simple function of the second order statistics of the data distribution, and show distribution- specific upper  and lower bounds on the sample complexity, both governed by the margin-adapted dimension of the data distribution. The upper bounds are universal, and the lower bounds hold for the rich family of sub- Gaussian distributions with independent features. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. To prove the lower bound, we develop several new tools of independent interest. These include new connections between shattering and hardness of learning, new properties of shattering with linear classifiers, and a new lower bound on the smallest eigenvalue of a random Gram matrix generated by sub- Gaussian variables. Our results can be used to quantitatively compare large margin learning to other learning rules, and to improve the effectiveness of methods that use sample complexity bounds, such as active learning.",
            "keywords": [
                "supervised learning",
                "sample complexity",
                "linear classifiers"
            ],
            "author": [
                "Sivan Sabato",
                "Nathan Srebro",
                "Naftali Tishby"
            ],
            "ref": "http://jmlr.org/papers/volume14/sabato13a/sabato13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convex and Scalable Weakly Labeled SVMs",
            "abstract": "In this paper, we study the problem of learning from weakly labeled data, where labels of the training examples are incomplete. This includes, for example, (i) semi- supervised learning where labels are partially known; (ii) multi-instance learning where labels are implicitly known; and (iii) clustering where labels are completely unknown. Unlike supervised learning, learning with weak labels involves a difficult Mixed-Integer Programming (MIP) problem. Therefore, it can suffer from poor scalability and may also get stuck in local minimum. In this paper, we focus on SVMs and propose the WELLSVM via a novel label generation strategy. This leads to a convex relaxation of the original MIP, which is at least as tight as existing convex Semi-Definite Programming (SDP) relaxations. Moreover, the WELLSVM can be solved via a sequence of SVM subproblems that are much more scalable than previous convex SDP relaxations. Experiments on three weakly labeled learning tasks, namely, (i) semi-supervised learning; (ii) multi-instance learning for locating regions of interest in content-based information retrieval; and (iii) clustering, clearly demonstrate improved performance, and WELLSVM is also readily applicable on large data sets.",
            "keywords": [
                "weakly labeled data",
                "semi-supervised learning",
                "multi-instance learning",
                "clustering",
                "cutting plane"
            ],
            "author": [
                "Yu-Feng Li",
                "Ivor W. Tsang",
                "James T. Kwok",
                "Zhi-Hua Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume14/li13a/li13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Language-Motivated Approaches to Action Recognition",
            "abstract": "We present language-motivated approaches to detecting, localizing and classifying activities and gestures in videos. In order to obtain statistical insight into the underlying patterns of motions in activities, we develop a dynamic, hierarchical Bayesian model which connects low-level visual features in videos with poses, motion patterns and classes of activities. This process is somewhat analogous to the method of detecting topics or categories from documents based on the word content of the documents, except that our documents are dynamic. The proposed generative model harnesses both the temporal ordering power of dynamic Bayesian networks such as hidden Markov models (HMMs) and the automatic clustering power of hierarchical Bayesian models such as the latent Dirichlet allocation (LDA) model. We also introduce a probabilistic framework for detecting and localizing pre-specified activities (or gestures) in a video sequence, analogous to the use of filler models for keyword detection in speech processing. We demonstrate the robustness of our classification model and our spotting framework by recognizing activities in unconstrained real-life video sequences and by spotting gestures via a one-shot-learning approach.",
            "keywords": [
                "dynamic hierarchical Bayesian networks",
                "topic models",
                "activity recognition",
                "gesture     spotting"
            ],
            "author": [
                "Manavender R. Malgireddy",
                "Ifeoma Nwogu",
                "Venu Govindaraju"
            ],
            "ref": "http://jmlr.org/papers/volume14/malgireddy13a/malgireddy13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Segregating Event Streams and Noise with a Markov Renewal Process Model",
            "abstract": "We describe an inference task in which a set of timestamped event observations must be clustered into an unknown number of temporal sequences with independent and varying rates of observations. Various existing approaches to multi-object tracking assume a fixed number of sources and/or a fixed observation rate; we develop an approach to inferring structure in timestamped data produced by a mixture of an unknown and varying number of similar Markov renewal processes, plus independent clutter noise. The inference simultaneously distinguishes signal from noise as well as clustering signal observations into separate source streams. We illustrate the technique via synthetic experiments as well as an experiment to track a mixture of singing birds. Source code is available.",
            "keywords": [
                "multi-target tracking",
                "clustering",
                "point processes",
                "flow network"
            ],
            "author": [
                "Dan Stowell",
                "Mark D. Plumbley"
            ],
            "ref": "http://jmlr.org/papers/volume14/stowell13a/stowell13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gaussian Kullback-Leibler Approximate Inference",
            "abstract": "We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufficient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student- or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design.",
            "keywords": [
                "generalised linear models",
                "latent linear models",
                "variational approximate inference",
                "large scale inference",
                "sparse learning",
                "experimental design",
                "active learning"
            ],
            "author": [
                "Edward Challis",
                "David Barber"
            ],
            "ref": "http://jmlr.org/papers/volume14/challis13a/challis13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Message-Passing Algorithms for Quadratic Minimization",
            "abstract": "Gaussian belief propagation (GaBP) is an iterative algorithm for computing the mean (and variances) of a multivariate Gaussian distribution, or equivalently, the minimum of a multivariate positive definite quadratic function. Sufficient conditions, such as walk-summability, that guarantee the convergence and correctness of GaBP are known, but GaBP may fail to converge to the correct solution given an arbitrary positive definite covariance matrix. As was observed by Malioutov et al. (2006), the GaBP algorithm fails to converge if the computation trees produced by the algorithm are not positive definite. In this work, we will show that the failure modes of the GaBP algorithm can be understood via graph covers, and we prove that a parameterized generalization of the min-sum algorithm can be used to ensure that the computation trees remain positive definite whenever the input matrix is positive definite. We demonstrate that the resulting algorithm is closely related to other iterative schemes for quadratic minimization such as the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically, that there always exists a choice of parameters such that the above generalization of the GaBP algorithm converges.",
            "keywords": [
                "belief propagation",
                "Gaussian graphical models"
            ],
            "author": [
                "Nicholas Ruozzi",
                "Sekhar Tatikonda"
            ],
            "ref": "http://jmlr.org/papers/volume14/ruozzi13a/ruozzi13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Rate of Convergence of AdaBoost",
            "abstract": "The AdaBoost algorithm was designed to combine many âweakâ hypotheses that perform slightly better than random guessing into a âstrongâ hypothesis that has very low error. We study the rate at which AdaBoost iteratively converges to the minimum of the âexponential lossâ. Unlike previous work, our proofs do not require a weak-learning assumption, nor do they require that minimizers of the exponential loss are finite. Our first result shows that the exponential loss of AdaBoost's computed parameter vector will be at most  more than that of any parameter vector of -norm bounded by  in a number of rounds that is at most a polynomial in  and . We also provide lower bounds showing that a polynomial dependence is necessary. Our second result is that within  iterations, AdaBoost achieves a value of the exponential loss that is at most  more than the best possible value, where  depends on the data set. We show that this dependence of the rate on  is optimal up to constant factors, that is, at least  rounds are necessary to achieve within  of the optimal exponential loss.",
            "keywords": [
                "AdaBoost",
                "optimization",
                "coordinate descent"
            ],
            "author": [
                "Indraneel Mukherjee",
                "Cynthia Rudin",
                "Robert E. Schapire"
            ],
            "ref": "http://jmlr.org/papers/volume14/mukherjee13b/mukherjee13b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Orange: Data Mining Toolbox in Python",
            "abstract": "Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the flexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C implementations of computationally-intensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining.",
            "keywords": [
                "Python",
                "data mining",
                "machine learning",
                "toolbox"
            ],
            "author": [
                "Janez Demšar",
                "Tomaž Curk",
                "Aleš Erjavec",
                "Črt Gorup",
                "Tomaž Hočevar",
                "Mitar Milutinovič",
                "Martin Možina",
                "Matija Polajnar",
                "Marko Toplak",
                "Anže Starič",
                "Miha Štajdohar",
                "Lan Umek",
                "Lan Žagar",
                "Jure Žbontar",
                "Marinka Žitnik",
                "Blaž Zupan"
            ],
            "ref": "http://jmlr.org/papers/volume14/demsar13a/demsar13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tapkee: An Efficient Dimension Reduction Library",
            "abstract": "We present Tapkee, a C++ template library that provides efficient implementations of more than  widely used dimensionality reduction techniques ranging from Locally Linear Embedding (Roweis and Saul, 2000) and Isomap (de Silva and Tenenbaum, 2002) to the recently introduced Barnes- Hut-SNE (van der Maaten, 2013). Our library was designed with a focus on performance and flexibility. For performance, we combine efficient multi-core algorithms, modern data structures and state-of-the-art low-level libraries. To achieve flexibility, we designed a clean interface for applying methods to user data and provide a callback API that facilitates integration with the library. The library is freely available as open-source software and is distributed under the permissive BSD 3-clause license. We encourage the integration of Tapkee into other open-source toolboxes and libraries. For example, Tapkee has been integrated into the codebase of the Shogun toolbox (Sonnenburg et al., 2010), giving us access to a rich set of kernels, distance measures and bindings to common programming languages including Python, Octave, Matlab, R, Java, C#, Ruby, Perl and Lua. Source code, examples and documentation are available at http://tapkee.lisitsyn.me.",
            "keywords": [
                "dimensionality reduction",
                "machine learning"
            ],
            "author": [
                "Sergey Lisitsyn",
                "Christian Widmer",
                "Fernando J. Iglesias Garcia"
            ],
            "ref": "http://jmlr.org/papers/volume14/lisitsyn13a/lisitsyn13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Mutual Nearest Neighbors Estimate in Regression",
            "abstract": "Motivated by promising experimental results, this paper investigates the theoretical properties of a recently proposed nonparametric estimator, called the Mutual Nearest Neighbors rule, which estimates the regression function  as follows: first identify the  nearest neighbors of  in the sample , then keep only those for which  is itself one of the  nearest neighbors, and finally take the average over the corresponding response variables. We prove that this estimator is consistent and that its rate of convergence is optimal. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, we also present adaptation results by data- splitting.",
            "keywords": [
                "nonparametric estimation",
                "nearest neighbor methods"
            ],
            "author": [
                "Arnaud Guyader",
                "Nick Hengartner"
            ],
            "ref": "http://jmlr.org/papers/volume14/guyader13a/guyader13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distance Preserving Embeddings for General n-Dimensional Manifolds",
            "abstract": "Low dimensional embeddings of manifold data have gained popularity in the last decade. However, a systematic finite sample analysis of manifold embedding algorithms largely eludes researchers. Here we present two algorithms that embed a general -dimensional manifold into  (where  only depends on some key manifold properties such as its intrinsic dimension, volume and curvature) that guarantee to approximately preserve all interpoint geodesic distances.",
            "keywords": [
                "manifold learning",
                "isometric embeddings",
                "non-linear dimensionality reduction"
            ],
            "author": [
                "Nakul Verma"
            ],
            "ref": "http://jmlr.org/papers/volume14/verma13a/verma13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows",
            "abstract": "We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called âpath codingâ penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efficiently solve by leveraging network flow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs.",
            "keywords": [
                "convex and non-convex optimization",
                "network flow optimization"
            ],
            "author": [
                "Julien Mairal",
                "Bin Yu"
            ],
            "ref": "http://jmlr.org/papers/volume14/mairal13a/mairal13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Greedy Feature Selection for Subspace Clustering",
            "abstract": "Unions of subspaces provide a powerful generalization of single subspace models for collections of high-dimensional data; however, learning multiple subspaces from data is challenging due to the fact that segmentation---the identification of points that live in the same subspace---and subspace estimation must be performed simultaneously. Recently, sparse recovery methods were shown to provide a provable and robust strategy for exact feature selection (EFS)---recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with -minimization, in this paper, we develop sufficient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide significant advantages over NN methods and that the gap between the two approaches is particularly pronounced when the sampling of subspaces in the data set is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble.",
            "keywords": [
                "subspace clustering",
                "unions of subspaces",
                "hybrid linear models",
                "sparse approximation",
                "structured sparsity",
                "nearest neighbors"
            ],
            "author": [
                "Eva L. Dyer",
                "Aswin C. Sankaranarayanan",
                "Richard G. Baraniuk"
            ],
            "ref": "http://jmlr.org/papers/volume14/dyer13a/dyer13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Bilinear Model for Matching Queries and Documents",
            "abstract": "The task of matching data from two heterogeneous domains naturally arises in various areas such as web search, collaborative filtering, and drug design. In web search, existing work has designed relevance models to match queries and documents by exploiting either user clicks or content of queries and documents. To the best of our knowledge, however, there has been little work on principled approaches to leveraging both clicks and content to learn a matching model for search. In this paper, we propose a framework for learning to match heterogeneous objects. The framework learns two linear mappings for two objects respectively, and matches them via the dot product of their images after mapping. Moreover, when different regularizations are enforced, the framework renders a rich family of matching models. With orthonormal constraints on mapping functions, the framework subsumes Partial Least Squares (PLS) as a special case. Alternatively, with a + regularization, we obtain a new model called Regularized Mapping to Latent Structures (RMLS). RMLS enjoys many advantages over PLS, including lower time complexity and easy parallelization. To further understand the matching framework, we conduct generalization analysis and apply the result to both PLS and RMLS. We apply the framework to web search and implement both PLS and RMLS using a click-through bipartite with metadata representing features of queries and documents. We test the efficacy and scalability of RMLS and PLS on large scale web search problems. The results show that both PLS and RMLS can significantly outperform baseline methods, while RMLS substantially speeds up the learning process.",
            "keywords": [
                "web search",
                "partial least squares",
                "regularized mapping to latent structures"
            ],
            "author": [
                "Wei Wu",
                "Zhengdong  Lu",
                "Hang Li"
            ],
            "ref": "http://jmlr.org/papers/volume14/wu13a/wu13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "One-shot Learning Gesture Recognition from RGB-D Data Using Bag of Features",
            "abstract": "For one-shot learning gesture recognition, two important challenges are: how to extract distinctive features and how to learn a discriminative model from only one training sample per gesture class. For feature extraction, a new spatio-temporal feature representation called 3D enhanced motion scale-invariant feature transform (3D EMoSIFT) is proposed, which fuses RGB-D data. Compared with other features, the new feature set is invariant to scale and rotation, and has more compact and richer visual representations. For learning a discriminative model, all features extracted from training samples are clustered with the k-means algorithm to learn a visual codebook. Then, unlike the traditional bag of feature (BoF) models using vector quantization (VQ) to map each feature into a certain visual codeword, a sparse coding method named simulation orthogonal matching pursuit (SOMP) is applied and thus each feature can be represented by some linear combination of a small number of codewords. Compared with VQ, SOMP leads to a much lower reconstruction error and achieves better performance. The proposed approach has been evaluated on ChaLearn gesture database and the result has been ranked amongst the top best performing techniques on ChaLearn gesture challenge (round 2).",
            "keywords": [
                "gesture recognition"
            ],
            "author": [
                "Jun Wan",
                "Qiuqi Ruan",
                "Wei Li",
                "Shuang Deng"
            ],
            "ref": "http://jmlr.org/papers/volume14/wan13a/wan13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Active Learning of Halfspaces: An Aggressive Approach",
            "abstract": "We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efficient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches. Our efficient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in significantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings.",
            "keywords": [
                "active learning",
                "linear classifiers",
                "margin"
            ],
            "author": [
                "Alon Gonen",
                "Sivan Sabato",
                "Shai Shalev-Shwartz"
            ],
            "ref": "http://jmlr.org/papers/volume14/gonen13a/gonen13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Keep It Simple And Sparse: Real-Time Action Recognition",
            "abstract": "Sparsity has been showed to be one of the most important properties for visual recognition purposes. In this paper we show that sparse representation plays a fundamental role in achieving one-shot learning and real-time recognition of actions. We start off from RGBD images, combine motion and appearance cues and extract state-of-the-art features in a computationally efficient way. The proposed method relies on descriptors based on 3D Histograms of Scene Flow (3DHOFs) and Global Histograms of Oriented Gradient (GHOGs); adaptive sparse coding is applied to capture high-level patterns from data. We then propose a simultaneous on-line video segmentation and recognition of actions using linear SVMs. The main contribution of the paper is an effective real-time system for one-shot action modeling and recognition; the paper highlights the effectiveness of sparse coding techniques to represent 3D actions. We obtain very good results on three different data sets: a benchmark data set for one-shot action learning (the ChaLearn Gesture Data Set), an in-house data set acquired by a Kinect sensor including complex actions and gestures differing by small details, and a data set created for human-robot interaction purposes. Finally we demonstrate that our system is effective also in a human-robot interaction setting and propose a memory game, âAll Gestures You Canâ, to be played against a humanoid robot.",
            "keywords": [
                "real-time action recognition",
                "sparse representation",
                "one-shot action learning"
            ],
            "author": [
                "Sean Ryan Fanello",
                "Ilaria Gori",
                "Giorgio Metta",
                "Francesca Odone"
            ],
            "ref": "http://jmlr.org/papers/volume14/fanello13a/fanello13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Volume Clustering: A New Discriminative Clustering Approach",
            "abstract": "The large volume principle proposed by Vladimir Vapnik, which advocates that hypotheses lying in an equivalence class with a larger volume are more preferable, is a useful alternative to the large margin principle. In this paper, we introduce a new discriminative clustering model based on the large volume principle called maximum volume clustering (MVC), and then propose two approximation schemes to solve this MVC model: A soft-label MVC method using sequential quadratic programming and a hard-label MVC method using  semi-definite programming, respectively. The proposed MVC is theoretically advantageous for three reasons. The optimization involved in hard-label MVC is convex, and under mild conditions, the optimization involved in soft-label MVC is akin to a convex one in terms of the resulting clusters. Secondly, the soft-label MVC method possesses a clustering error bound. Thirdly, MVC includes the optimization problems of a spectral clustering, two relaxed -means clustering and an information-maximization clustering as special limit cases when its regularization parameter goes to infinity. Experiments on several artificial and benchmark data sets demonstrate that the proposed MVC compares favorably with state-of-the-art clustering methods.",
            "keywords": [],
            "author": [
                "Gang Niu",
                "Bo Dai",
                "Lin Shang",
                "Masashi Sugiyama"
            ],
            "ref": "http://jmlr.org/papers/volume14/niu13a/niu13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse/Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory",
            "abstract": "",
            "keywords": [],
            "author": [
                "Aleks",
                "r Y. Aravkin",
                "James V. Burke",
                "Gianluigi Pillonetto"
            ],
            "ref": "http://jmlr.org/papers/volume14/aravkin13a/aravkin13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling",
            "abstract": "The CUR matrix decomposition and the Nystrom approximation are two important low-rank matrix approximation techniques. The Nystrom method approximates a symmetric positive semidefinite matrix in terms of a small number of its columns, while CUR approximates an arbitrary data matrix by a small number of its columns and rows. Thus, CUR decomposition can be regarded as an extension of the Nystrom approximation. In this paper we establish a more general error bound for the adaptive column/row sampling algorithm, based on which we propose more accurate CUR and Nystrom algorithms with expected relative-error bounds. The proposed CUR and Nystrom algorithms also have low time complexity and can avoid maintaining the whole data matrix in RAM. In addition, we give theoretical analysis for the lower error bounds of the standard Nystrom method and the ensemble Nystrom method. The main theoretical results established in this paper are novel, and our analysis makes no special assumption on the data matrices.",
            "keywords": [
                "large-scale matrix computation",
                "CUR matrix decomposition"
            ],
            "author": [
                "Shusen Wang",
                "Zhihua Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume14/wang13c/wang13c.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees",
            "abstract": "The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP fixed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefficients via Monte Carlo techniques and damped updates. We prove that the SOSMP iterates converge to a -neighborhood of the unique BP fixed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefficients as a function of the desired approximation accuracy  and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical flow estimation.",
            "keywords": [
                "graphical models",
                "sum-product for continuous state spaces",
                "low-complexity belief     propagation",
                "stochastic approximation",
                "Monte Carlo methods"
            ],
            "author": [
                "Nima Noorshams",
                "Martin J. Wainwright"
            ],
            "ref": "http://jmlr.org/papers/volume14/noorshams13a/noorshams13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems",
            "abstract": "A metric between time-series distributions is proposed that can be evaluated using binary classification methods, which were originally developed to work on i.i.d. data. It is shown how this metric can be used for solving statistical problems that are seemingly unrelated to classification and concern highly dependent time series. Specifically, the problems of time-series clustering, homogeneity testing and the three-sample problem are addressed. Universal consistency of the resulting algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data.",
            "keywords": [
                "time series",
                "reductions",
                "stationary ergodic",
                "clustering"
            ],
            "author": [
                "Daniil Ryabko",
                "Jérémie Mary"
            ],
            "ref": "http://jmlr.org/papers/volume14/ryabko13a/ryabko13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models",
            "abstract": "Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian field, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model's partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP's local matching of moments. Through the expansion, we see that EP is correct to first order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree- structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution.",
            "keywords": [
                "expectation consistent inference",
                "expectation propagation",
                "perturbation correction",
                "Wick expansions",
                "Ising model"
            ],
            "author": [
                "Manfred Opper",
                "Ulrich Paquet",
                "Ole Winther"
            ],
            "ref": "http://jmlr.org/papers/volume14/opper13a/opper13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Near-Optimal Algorithm for Differentially-Private Principal Components",
            "abstract": "The principal components analysis (PCA) algorithm is a standard tool for identifying good low-dimensional approximations to high-dimensional data. Many data sets of interest contain private or sensitive information about individuals. Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs. Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs. In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output. We show that the sample complexity of the proposed method differs from the existing procedure in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling. We furthermore illustrate our results, showing that on real data there is a large performance gap between the existing method and our method.",
            "keywords": [
                "differential privacy",
                "principal components analysis"
            ],
            "author": [
                "Kamalika Chaudhuri",
                "An",
                "D. Sarwate"
            ],
            "ref": "http://jmlr.org/papers/volume14/chaudhuri13a/chaudhuri13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Stage Multi-Task Feature Learning",
            "abstract": "Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an -type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel non-convex regularizer. To solve the non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm; we also provide intuitive interpretations, detailed convergence and reproducibility analysis for the proposed algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms.",
            "keywords": [
                "multi-task learning",
                "multi-stage",
                "non-convex"
            ],
            "author": [
                "Pinghua Gong",
                "Jieping Ye",
                "Changshui Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume14/gong13a/gong13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Plug-in Approach to Neyman-Pearson Classification",
            "abstract": "The Neyman-Pearson (NP) paradigm in binary classification treats type I and type II errors with different priorities. It seeks classifiers that minimize type II error, subject to a type I error constraint under a user specified level . In this paper, plug-in classifiers are developed under the NP paradigm. Based on the fundamental Neyman-Pearson Lemma, we propose two related plug-in classifiers which amount to thresholding respectively the class conditional density ratio and the regression function. These two classifiers handle different sampling schemes. This work focuses on theoretical properties of the proposed classifiers; in particular, we derive oracle inequalities that can be viewed as finite sample versions of risk bounds. NP classification can be used to address anomaly detection problems, where asymmetry in errors is an intrinsic property. As opposed to a common practice in anomaly detection that consists of thresholding normal class density, our approach does not assume a specific form for anomaly distributions. Such consideration is particularly necessary when the anomaly class density is far from uniformly distributed.",
            "keywords": [
                "plug-in approach",
                "Neyman-Pearson paradigm",
                "nonparametric statistics",
                "oracle in-     equality"
            ],
            "author": [
                "Xin Tong"
            ],
            "ref": "http://jmlr.org/papers/volume14/tong13a/tong13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Experiment Selection for Causal Discovery",
            "abstract": "Randomized controlled experiments are often described as the most reliable tool available to scientists for discovering causal relationships among quantities of interest. However, it is often unclear how many and which different experiments are needed to identify the full (possibly cyclic) causal structure among some given (possibly causally insufficient) set of variables. Recent results in the causal discovery literature have explored various identifiability criteria that depend on the assumptions one is able to make about the underlying causal process, but these criteria are not directly constructive for selecting the optimal set of experiments. Fortunately, many of the needed constructions already exist in the combinatorics literature, albeit under terminology which is unfamiliar to most of the causal discovery community. In this paper we translate the theoretical results and apply them to the concrete problem of experiment selection. For a variety of settings we give explicit constructions of the optimal set of experiments and adapt some of the general combinatorics results to answer questions relating to the problem of experiment selection.",
            "keywords": [
                "causality",
                "randomized experiments",
                "experiment selection",
                "separating systems",
                "com-     pletely separating systems"
            ],
            "author": [
                "Antti Hyttinen",
                "Frederick Eberhardt",
                "Patrik O. Hoyer"
            ],
            "ref": "http://jmlr.org/papers/volume14/hyttinen13a/hyttinen13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stationary-Sparse Causality Network Learning",
            "abstract": "Recently, researchers have proposed penalized maximum likelihood to identify network topology underlying a dynamical system modeled by multivariate time series. The time series of interest are assumed to be stationary, but this restriction is never taken into consideration by existing estimation methods. Moreover, practical problems of interest may have ultra-high dimensionality and obvious node collinearity. In addition, none of the available algorithms provides a probabilistic measure of the uncertainty for the obtained network topology which is informative in reliable network identification. The main purpose of this paper is to tackle these challenging issues. We propose the  learning framework, which stands for stationary- sparse network learning. We propose a novel algorithm referred to as the Berhu iterative sparsity pursuit with stationarity (BISPS), where the Berhu regularization can improve the Lasso in detection and estimation. The algorithm is extremely easy to implement, efficient in computation and has a theoretical guarantee to converge to a global optimum. We also incorporate a screening technique into BISPS to tackle ultra- high dimensional problems and enhance computational efficiency. Furthermore, a stationary bootstrap technique is applied to provide connection occurring frequency for reliable topology learning. Experiments show that our method can achieve stationary and sparse causality network learning and is scalable for high-dimensional problems.",
            "keywords": [
                "stationarity",
                "sparsity",
                "Berhu",
                "screening"
            ],
            "author": [
                "Yuejia He",
                "Yiyuan She",
                "Dapeng Wu"
            ],
            "ref": "http://jmlr.org/papers/volume14/he13a/he13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Algorithms and Hardness Results for Parallel Large Margin Learning",
            "abstract": "We consider the problem of learning an unknown large-margin halfspace in the context of parallel computation, giving both positive and negative results. As our main positive result, we give a parallel algorithm for learning a large-margin halfspace, based on an algorithm of Nesterov's that performs gradient descent with a momentum term. We show that this algorithm can learn an unknown -margin halfspace over  dimensions using  processors and running in time . In contrast, naive parallel algorithms that learn a -margin halfspace in time that depends polylogarithmically on  have an inverse quadratic running time dependence on the margin parameter . Our negative result deals with boosting, which is a standard approach to learning large-margin halfspaces. We prove that in the original PAC framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized. More precisely, we show that, if the algorithm is allowed to call the weak learner multiple times in parallel within a single boosting stage, this ability does not reduce the overall number of successive stages of boosting needed for learning by even a single stage. Our proof is information-theoretic and does not rely on unproven assumptions.",
            "keywords": [
                "PAC learning",
                "parallel learning algorithms",
                "halfspace learning"
            ],
            "author": [
                "Philip M. Long",
                "Rocco A. Servedio"
            ],
            "ref": "http://jmlr.org/papers/volume14/long13a/long13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large-scale SVD and Manifold Learning",
            "abstract": "This paper examines the efficacy of sampling-based low-rank approximation techniques when applied to large dense kernel matrices. We analyze two common approximate singular value decomposition techniques, namely the Nystrom and Column sampling methods. We present a theoretical comparison between these two methods, provide novel insights regarding their suitability for various tasks and present experimental results that support our theory. Our results illustrate the relative strengths of each method. We next examine the performance of these two techniques on the large-scale task of extracting low-dimensional manifold structure given millions of high-dimensional face images. We address the computational challenges of non-linear dimensionality reduction via Isomap and Laplacian Eigenmaps, using a graph containing about  million nodes and  million edges. We present extensive experiments on learning low- dimensional embeddings for two large face data sets: CMU-PIE ( thousand faces) and a web data set ( million faces). Our comparisons show that the Nystrom approximation is superior to the Column sampling method for this task. Furthermore, approximate Isomap tends to perform better than Laplacian Eigenmaps on both clustering and classification with the labeled CMU-PIE data set.",
            "keywords": [
                "low-rank approximation",
                "manifold learning"
            ],
            "author": [
                "Ameet Talwalkar",
                "Sanjiv Kumar",
                "Mehryar Mohri",
                "Henry Rowley"
            ],
            "ref": "http://jmlr.org/papers/volume14/talwalkar13a/talwalkar13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "QuantMiner for Mining Quantitative Association Rules",
            "abstract": "In this paper, we propose QuantMiner, a mining quantitative association rules system. This system is based on a genetic algorithm that dynamically discovers âgoodâ intervals in association rules by optimizing both the support and the confidence. The experiments on real and artificial databases have shown the usefulness of QuantMiner as an interactive, exploratory data mining tool.",
            "keywords": [
                "association rules",
                "numerical and categorical attributes",
                "unsupervised discretization",
                "genetic algorithm"
            ],
            "author": [
                "Ansaf Salleb-Aouissi",
                "Christel Vrain",
                "Cyril Nortet",
                "Xiangrong Kong",
                "Vivek Rathod",
                "Daniel Cassard"
            ],
            "ref": "http://jmlr.org/papers/volume14/salleb-aouissi13a/salleb-aouissi13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Divvy: Fast and Intuitive Exploratory Data Analysis",
            "abstract": "Divvy is an application for applying unsupervised machine learning techniques (clustering and dimensionality reduction) to the data analysis process. Divvy provides a novel UI that allows researchers to tighten the action-perception loop of changing algorithm parameters and seeing a visualization of the result. Machine learning researchers can use Divvy to publish easy to use reference implementations of their algorithms, which helps the machine learning field have a greater impact on research practices elsewhere.",
            "keywords": [
                "clustering",
                "dimensionality reduction",
                "open source software",
                "human computer interac-     tion"
            ],
            "author": [
                "Joshua M. Lewis",
                "Virginia R. de Sa",
                "Laurens van der Maaten"
            ],
            "ref": "http://jmlr.org/papers/volume14/lewis13a/lewis13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Algorithms for Marginal MAP",
            "abstract": "The marginal maximum a posteriori probability (MAP) estimation problem, which calculates the mode of the marginal posterior distribution of a subset of variables with the remaining variables marginalized, is an important inference problem in many models, such as those with hidden variables or uncertain parameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has attracted less attention in the literature compared to the joint MAP (maximization) and marginalization problems. We derive a general dual representation for marginal MAP that naturally integrates the marginalization and maximization operations into a joint variational optimization problem, making it possible to easily extend most or all variational-based algorithms to marginal MAP. In particular, we derive a set of âmixed-product\" message passing algorithms for marginal MAP, whose form is a hybrid of max-product, sum-product and a novel âargmax-product\" message updates. We also derive a class of convergent algorithms based on proximal point methods, including one that transforms the marginal MAP problem into a sequence of standard marginalization problems. Theoretically, we provide guarantees under which our algorithms give globally or locally optimal solutions, and provide novel upper bounds on the optimal objectives. Empirically, we demonstrate that our algorithms significantly outperform the existing approaches, including a state-of-the-art algorithm based on local search methods.",
            "keywords": [
                "graphical models",
                "message passing",
                "belief propagation",
                "variational methods",
                "maxi-     mum a posteriori",
                "marginal-MAP"
            ],
            "author": [
                "Qiang Liu",
                "Alexander Ihler"
            ],
            "ref": "http://jmlr.org/papers/volume14/liu13b/liu13b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GURLS: A Least Squares Library for Supervised Learning",
            "abstract": "We present GURLS, a least squares, modular, easy-to-extend software library for efficient supervised learning. GURLS is targeted to machine learning practitioners, as well as non- specialists. It offers a number state-of-the-art training strategies for medium and large-scale learning, and routines for efficient model selection. The library is particularly well suited for multi-output problems (multi-category/multi-label). GURLS is currently available in two independent implementations: Matlab and C++. It takes advantage of the favorable properties of regularized least squares algorithm to exploit advanced tools in linear algebra. Routines to handle computations with very large matrices by means of memory-mapped storage and distributed task execution are available. The package is distributed under the BSD license and is available for download at https://github.com/LCSL/GURLS.",
            "keywords": [
                "regularized least squares",
                "big data"
            ],
            "author": [
                "Andrea Tacchetti",
                "Pavan K. Mallapragada",
                "Matteo Santoro",
                "Lorenzo Rosasco"
            ],
            "ref": "http://jmlr.org/papers/volume14/tacchetti13a/tacchetti13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising",
            "abstract": "This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine.",
            "keywords": [
                "causation",
                "counterfactual reasoning"
            ],
            "author": [
                "Léon Bottou",
                "Jonas Peters",
                "Joaquin Quiñonero-Candela",
                "Denis X. Charles",
                "D. Max Chickering",
                "Elon Portugaly",
                "Dipankar Ray",
                "Patrice Simard",
                "Ed Snelson"
            ],
            "ref": "http://jmlr.org/papers/volume14/bottou13a/bottou13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multivariate Convex Regression with Adaptive Partitioning",
            "abstract": "We propose a new, nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function. Convexity constraints are common in economics, statistics, operations research, financial engineering and optimization, but there is currently no multivariate method that is stable and computationally feasible for more than a few thousand observations. We introduce convex adaptive partitioning (CAP), which creates a globally convex regression model from locally linear estimates fit on adaptively selected covariate partitions. CAP is a computationally efficient, consistent method for convex regression. We demonstrate empirical performance by comparing the performance of CAP to other shape-constrained and unconstrained regression methods for predicting weekly wages and value function approximation for pricing American basket options.",
            "keywords": [
                "adaptive partitioning",
                "convex regression",
                "nonparametric regression",
                "shape constraint"
            ],
            "author": [
                "Lauren A. Hannah",
                "David B. Dunson"
            ],
            "ref": "http://jmlr.org/papers/volume14/hannah13a/hannah13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast MCMC Sampling for Markov Jump Processes and Extensions",
            "abstract": "Markov jump processes (or continuous-time Markov chains) are a simple and important class of continuous-time dynamical systems. In this paper, we tackle the problem of simulating from the posterior distribution over paths in these models, given partial and noisy observations. Our approach is an auxiliary variable Gibbs sampler, and is based on the idea of uniformization. This sets up a Markov chain over paths by alternately sampling a finite set of virtual jump times given the current path, and then sampling a new path given the set of extant and virtual jump times. The first step involves simulating a piecewise-constant inhomogeneous Poisson process, while for the second, we use a standard hidden Markov model forward filtering-backward sampling algorithm. Our method is exact and does not involve approximations like time- discretization. We demonstrate how our sampler extends naturally to MJP-based models like Markov-modulated Poisson processes and continuous-time Bayesian networks, and show significant computational benefits over state-of-the-art MCMC samplers for these models.",
            "keywords": [
                "Markov jump process",
                "MCMC",
                "Gibbs sampler",
                "uniformization",
                "Markov-modulated     Poisson process"
            ],
            "author": [
                "Vinayak Rao",
                "Yee Whye Teh"
            ],
            "ref": "http://jmlr.org/papers/volume14/rao13a/rao13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Communication-Efficient Algorithms for Statistical Optimization",
            "abstract": "We analyze two communication-efficient algorithms for distributed optimization in statistical settings involving large-scale data sets. The first algorithm is a standard averaging method that distributes the  data samples evenly to  machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error (MSE) that decays as . Whenever , this guarantee matches the best possible rate achievable by a centralized algorithm having access to all  samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as , and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean- squared error decaying as , easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efficiently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with  samples and  covariates.",
            "keywords": [
                "distributed learning",
                "stochastic optimization",
                "averaging"
            ],
            "author": [
                "Yuchen Zhang",
                "John C. Duchi",
                "Martin J. Wainwright"
            ],
            "ref": "http://jmlr.org/papers/volume14/zhang13b/zhang13b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PC Algorithm for Nonparanormal Graphical Models",
            "abstract": "The PC algorithm uses conditional independence tests for model selection in graphical modeling with acyclic directed graphs. In Gaussian models, tests of conditional independence are typically based on Pearson correlations, and high-dimensional consistency results have been obtained for the PC algorithm in this setting. Analyzing the error propagation from marginal to partial correlations, we prove that high-dimensional consistency carries over to a broader class of Gaussian copula or nonparanormal models when using rank-based measures of correlation. For graph sequences with bounded degree, our consistency result is as strong as prior Gaussian results. In simulations, the `Rank PC' algorithm works as well as the `Pearson PC' algorithm for normal data and considerably better for non-normal data, all the while incurring a negligible increase of computation time. While our interest is in the PC algorithm, the presented analysis of error propagation could be applied to other algorithms that test the vanishing of low-order partial correlations.",
            "keywords": [
                "Gaussian copula",
                "graphical model",
                "model selection",
                "multivariate normal distribution"
            ],
            "author": [
                "Naftali Harris",
                "Mathias Drton"
            ],
            "ref": "http://jmlr.org/papers/volume14/harris13a/harris13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Matrix Inversion with Scaled Lasso",
            "abstract": "",
            "keywords": [
                "precision matrix",
                "concentration matrix",
                "inverse matrix",
                "graphical model",
                "scaled Lasso",
                "linear regression"
            ],
            "author": [
                "Tingni Sun",
                "Cun-Hui Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume14/sun13a/sun13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistent Selection of Tuning Parameters via Variable Selection Stability",
            "abstract": "Penalized regression models are popularly used in high- dimensional data analysis to conduct variable selection and model fitting simultaneously. Whereas success has been widely reported in literature, their performances largely depend on the tuning parameters that balance the trade-off between model fitting and model sparsity. Existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability, such as cross validation, AIC and BIC. This article introduces a general tuning parameter selection criterion based on variable selection stability. The key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection. The asymptotic selection consistency is established for both fixed and diverging dimensions. Its effectiveness is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data.",
            "keywords": [
                "kappa coefficient",
                "penalized regression",
                "selection consistency",
                "stability"
            ],
            "author": [
                "Wei Sun",
                "Junhui Wang",
                "Yixin Fang"
            ],
            "ref": "http://jmlr.org/papers/volume14/sun13b/sun13b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Comment on \"Robustness and Regularization of Support Vector Machines\" by H. Xu et al. (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009)",
            "abstract": "This paper comments on the published work dealing with robustness and regularization of support vector machines (Journal of Machine Learning Research, Vol. 10, pp. 1485-1510, 2009) by H. Xu et al. They proposed a theorem to show that it is possible to relate robustness in the feature space and robustness in the sample space directly. In this paper, we propose a counter example that rejects their theorem.",
            "keywords": [
                "kernel",
                "robustness"
            ],
            "author": [
                "Yahya Forghani",
                "Hadi Sadoghi"
            ],
            "ref": "http://jmlr.org/papers/volume14/forghani13a/forghani13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lovasz theta function, SVMs and Finding Dense Subgraphs",
            "abstract": "In this paper we establish that the LovÃ¡sz  function on a graph can be restated as a kernel learning problem. We introduce the notion of SVM- graphs, on which LovÃ¡sz   function can be approximated well by a Support vector machine (SVM). We show that ErdÃ¶s-RÃ©nyi random  graphs are SVM- graphs for . Even if we embed a large clique of size  in a  graph the resultant graph still remains a LovÃ¡sz  graph. This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. Associated with the  function is the notion of orthogonal labellings. We introduce common orthogonal labellings which extends the idea of orthogonal labellings to multiple graphs. This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude.",
            "keywords": [
                "orthogonal labellings of graphs",
                "planted cliques",
                "random graphs"
            ],
            "author": [
                "Vinay Jethava",
                "Anders Martinsson",
                "Chiranjib Bhattacharyya",
                "Devdatt Dubhashi"
            ],
            "ref": "http://jmlr.org/papers/volume14/jethava13a/jethava13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars",
            "abstract": "Standard models of language learning are concerned with weak learning: the learner, receiving as input only information about the strings in the language, must learn to generalise and to generate the correct, potentially infinite, set of strings generated by some target grammar. Here we define the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees. We formalise this using a modification of Gold's identification in the limit model, requiring convergence to a grammar that is isomorphic to the target grammar. We take as our starting point a simple learning algorithm for substitutable context-free languages, based on principles of distributional learning, and modify it so that it will converge to a canonical grammar for each language. We prove a corresponding strong learning result for a subclass of context-free grammars.",
            "keywords": [
                "context-free grammars",
                "grammatical inference",
                "identification in the limit"
            ],
            "author": [
                "Alexander Clark"
            ],
            "ref": "http://jmlr.org/papers/volume14/clark13a/clark13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classifying With Confidence From Incomplete Information",
            "abstract": "We consider the problem of classifying a test sample given incomplete information. This problem arises naturally when data about a test sample is collected over time, or when costs must be incurred to compute the classification features. For example, in a distributed sensor network only a fraction of the sensors may have reported measurements at a certain time, and additional time, power, and bandwidth is needed to collect the complete data to classify. A practical goal is to assign a class label as soon as enough data is available to make a good decision. We formalize this goal through the notion of reliability---the probability that a label assigned given incomplete data would be the same as the label assigned given the complete data, and we propose a method to classify incomplete data only if some reliability threshold is met. Our approach models the complete data as a random variable whose distribution is dependent on the current incomplete data and the (complete) training data. The method differs from standard imputation strategies in that our focus is on determining the reliability of the classification decision, rather than just the class label. We show that the method provides useful reliability estimates of the correctness of the imputed class labels on a set of experiments on time- series data sets, where the goal is to classify the time-series as early as possible while still guaranteeing that the reliability threshold is met.",
            "keywords": [
                "classification",
                "sensor networks",
                "signals"
            ],
            "author": [
                "Nathan Parrish",
                "Hyrum S. Anderson",
                "Maya R. Gupta",
                "Dun Yu Hsiao"
            ],
            "ref": "http://jmlr.org/papers/volume14/parrish13a/parrish13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classifier Selection using the Predicate Depth",
            "abstract": "Typically, one approaches a supervised machine learning problem by writing down an objective function and finding a hypothesis that minimizes it. This is equivalent to finding the Maximum A Posteriori (MAP) hypothesis for a Boltzmann distribution. However, MAP is not a robust statistic. We present an alternative approach by defining a median of the distribution, which we show is both more robust, and has good generalization guarantees. We present algorithms to approximate this median. One contribution of this work is an efficient method for approximating the Tukey median. The Tukey median, which is often used for data visualization and outlier detection, is a special case of the family of medians we define: however, computing it exactly is exponentially slow in the dimension. Our algorithm approximates such medians in polynomial time while making weaker assumptions than those required by previous work.",
            "keywords": [
                "classification",
                "estimation",
                "median"
            ],
            "author": [
                "Ran Gilad-Bachrach",
                "Christopher J.C. Burges"
            ],
            "ref": "http://jmlr.org/papers/volume14/gilad-bachrach13a/gilad-bachrach13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion",
            "abstract": "We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max- norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed.",
            "keywords": [
                "1-bit matrix completion",
                "low-rank matrix",
                "max-norm",
                "trace-norm",
                "constrained opti-     mization",
                "maximum likelihood estimate"
            ],
            "author": [
                "Tony Cai",
                "Wen-Xin Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume14/cai13b/cai13b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming",
            "abstract": "We present NrSample, a framework for program synthesis in inductive logic programming. NrSample uses propositional logic constraints to exclude undesirable candidates from the search. This is achieved by representing constraints as propositional formulae and solving the associated constraint satisfaction problem. We present a variety of such constraints: pruning, input-output, functional (arithmetic), and variable splitting. NrSample is also capable of detecting search space exhaustion, leading to further speedups in clause induction and optimality. We benchmark NrSample against enumeration search (Aleph's default) and Progol's  search in the context of program synthesis. The results show that, on large program synthesis problems, NrSample induces between 1 and 1358 times faster than enumeration (236 times faster on average), always with similar or better accuracy. Compared to Progol , NrSample is 18 times faster on average with similar or better accuracy except for two problems: one in which Progol  substantially sacrificed accuracy to induce faster, and one in which Progol  was a clear winner. Functional constraints provide a speedup of up to 53 times (21 times on average) with similar or better accuracy. We also benchmark using a few concept learning (non-program synthesis) problems. The results indicate that without strong constraints, the overhead of solving constraints is not compensated for.",
            "keywords": [
                "inductive logic programming",
                "program synthesis",
                "theory induction",
                "constraint satis-     faction"
            ],
            "author": [
                "John Ahlgren",
                "Shiu Yin Yuen"
            ],
            "ref": "http://jmlr.org/papers/volume14/ahlgren13a/ahlgren13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Joint Harmonic Functions and Their Supervised Connections",
            "abstract": "The cluster assumption had a significant impact on the reasoning behind semi-supervised classification methods in graph-based learning. The literature includes numerous applications where harmonic functions provided estimates that conformed to data satisfying this well-known assumption, but the relationship between this assumption and harmonic functions is not as well- understood theoretically. We investigate these matters from the perspective of supervised kernel classification and provide concrete answers to two fundamental questions. (i) Under what conditions do semi-supervised harmonic approaches satisfy this assumption? (ii) If such an assumption is satisfied then why precisely would an observation sacrifice its own supervised estimate in favor of the cluster? First, a harmonic function is guaranteed to assign labels to data in harmony with the cluster assumption if a specific condition on the boundary of the harmonic function is satisfied. Second, it is shown that any harmonic function estimate within the interior is a probability weighted average of supervised estimates, where the weight is focused on supervised kernel estimates near labeled cases. We demonstrate that the uniqueness criterion for harmonic estimators is sensitive when the graph is sparse or the size of the boundary is relatively small. This sets the stage for a third contribution, a new regularized joint harmonic function for semi-supervised learning based on a joint optimization criterion. Mathematical properties of this estimator, such as its uniqueness even when the graph is sparse or the size of the boundary is relatively small, are proven. A main selling point is its ability to operate in circumstances where the cluster assumption may not be fully satisfied on real data by compromising between the purely harmonic and purely supervised estimators. The competitive stature of the new regularized joint harmonic approach is established.",
            "keywords": [
                "harmonic function",
                "joint training",
                "cluster assumption"
            ],
            "author": [
                "Mark Vere Culp",
                "Kenneth Joseph Ryan"
            ],
            "ref": "http://jmlr.org/papers/volume14/culp13a/culp13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels",
            "abstract": "A kernel method for realizing Bayes' rule is proposed, based on representations of probabilities in reproducing kernel Hilbert spaces. Probabilities are uniquely characterized by the mean of the canonical map to the RKHS. The prior and conditional probabilities are expressed in terms of RKHS functions of an empirical sample: no explicit parametric model is needed for these quantities. The posterior is likewise an RKHS mean of a weighted sample. The estimator for the expectation of a function of the posterior is derived, and rates of consistency are shown. Some representative applications of the kernel Bayes' rule are presented, including Bayesian computation without likelihood and filtering with a nonparametric state-space model.",
            "keywords": [
                "kernel method"
            ],
            "author": [
                "Kenji Fukumizu",
                "Le Song",
                "Arthur Gretton"
            ],
            "ref": "http://jmlr.org/papers/volume14/fukumizu13a/fukumizu13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimally Fuzzy Temporal Memory",
            "abstract": "Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register---a moving window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an independent general purpose learner cannot a priori know the characteristic prediction- relevant timescale of the signal. Moreover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by resource availability. Here we construct a fuzzy memory system that optimally sacrifices the temporal accuracy of information in a scale-free fashion in order to represent prediction- relevant information from exponentially long timescales. Using several illustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better off committing to such a fuzzy memory system.",
            "keywords": [
                "temporal information compression"
            ],
            "author": [
                "Karthik H. Shankar",
                "Marc W. Howard"
            ],
            "ref": "http://jmlr.org/papers/volume14/shankar13a/shankar13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "BudgetedSVM: A Toolbox for Scalable SVM Approximations",
            "abstract": "We present BudgetedSVM, an open-source C++ toolbox comprising highly-optimized implementations of recently proposed algorithms for scalable training of Support Vector Machine (SVM) approximators: Adaptive Multi-hyperplane Machines, Low-rank Linearization SVM, and Budgeted Stochastic Gradient Descent. BudgetedSVM trains models with accuracy comparable to LibSVM in time comparable to LibLinear, solving non-linear problems with millions of high-dimensional examples within minutes on a regular computer. We provide command-line and Matlab interfaces to BudgetedSVM, an efficient API for handling large-scale, high- dimensional data sets, as well as detailed documentation to help developers use and further extend the toolbox.",
            "keywords": [
                "non-linear classification",
                "large-scale learning",
                "SVM"
            ],
            "author": [
                "Nemanja Djuric",
                "Liang Lan",
                "Slobodan Vucetic",
                "Zhuang Wang"
            ],
            "ref": "http://jmlr.org/papers/volume14/djuric13a/djuric13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bridging Viterbi and Posterior Decoding: A Generalized Risk Approach to Hidden Path Inference Based on Hidden Markov Models",
            "abstract": "Motivated by the unceasing interest in hidden Markov models (HMMs), this paper re-examines hidden path inference in these models, using primarily a risk-based framework. While the most common maximum a posteriori (MAP), or Viterbi, path estimator and the minimum error, or Posterior Decoder (PD) have long been around, other path estimators, or decoders, have been either only hinted at or applied more recently and in dedicated applications generally unfamiliar to the statistical learning community. Over a decade ago, however, a family of algorithmically defined decoders aiming to hybridize the two standard ones was proposed elsewhere. The present paper gives a careful analysis of this hybridization approach, identifies several problems and issues with it and other previously proposed approaches, and proposes practical resolutions of those. Furthermore, simple modifications of the classical criteria for hidden path recognition are shown to lead to a new class of decoders. Dynamic programming algorithms to compute these decoders in the usual forward-backward manner are presented. A particularly interesting subclass of such estimators can be also viewed as hybrids of the MAP and PD estimators. Similar to previously proposed MAP-PD hybrids, the new class is parameterized by a small number of tunable parameters. Unlike their algorithmic predecessors, the new risk- based decoders are more clearly interpretable, and, most importantly, work \"out-of-the box\" in practice, which is demonstrated on some real bioinformatics tasks and data. Some further generalizations and applications are discussed in the conclusion.",
            "keywords": [
                "admissible path",
                "decoder",
                "HMM",
                "hybrid",
                "interpolation",
                "MAP sequence",
                "min-     imum error",
                "optimal accuracy",
                "power transform",
                "risk",
                "segmental classification",
                "symbol-by-     symbol",
                "posterior decoding"
            ],
            "author": [
                "Jüri Lember",
                "Alexey A. Koloydenko"
            ],
            "ref": "http://jmlr.org/papers/volume15/lember14a/lember14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast SVM Training Using Approximate Extreme Points",
            "abstract": "Applications of non-linear kernel support vector machines (SVMs) to large data sets is seriously hampered by its excessive training time. We propose a modification, called the approximate extreme points support vector machine (AESVM), that is aimed at overcoming this burden. Our approach relies on conducting the SVM optimization over a carefully selected subset, called the representative set, of the training data set. We present analytical results that indicate the similarity of AESVM and SVM solutions. A linear time algorithm based on convex hulls and extreme points is used to compute the representative set in kernel space. Extensive computational experiments on nine data sets compared AESVM to LIBSVM (Chang and Lin, 2011), CVM (Tsang et al., 2005), BVM (Tsang et al., 2007), LASVM (Bordes et al., 2005), SVMperf (Joachims and Yu, 2009), and the random features method (Rahimi and Recht, 2007). Our AESVM implementation was found to train much faster than the other methods, while its classification accuracy was similar to that of LIBSVM in all cases. In particular, for a seizure detection data set, AESVM training was almost 500 times faster than LIBSVM and LASVM and 20 times faster than CVM and BVM. Additionally, AESVM also gave competitively fast classification times.",
            "keywords": [
                "support vector machines",
                "convex hulls",
                "large scale classification",
                "non-linear     kernels"
            ],
            "author": [
                "Manu Nandan",
                "Pramod P. Khargonekar",
                "Sachin S. Talathi"
            ],
            "ref": "http://jmlr.org/papers/volume15/nandan14a/nandan14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Detecting Click Fraud in Online Advertising: A Data Mining Approach",
            "abstract": "Click fraud--the deliberate clicking on advertisements with no real interest on the product or service offered--is one of the most daunting problems in online advertising. Building an effective fraud detection method is thus pivotal for online advertising businesses. We organized a Fraud Detection in Mobile Advertising (FDMA) 2012 Competition, opening the opportunity for participants to work on real-world fraud data from BuzzCity Pte. Ltd., a global mobile advertising company based in Singapore. In particular, the task is to identify fraudulent publishers who generate illegitimate clicks, and distinguish them from normal publishers. The competition was held from September 1 to September 30, 2012, attracting 127 teams from more than 15 countries. The mobile advertising data are unique and complex, involving heterogeneous information, noisy patterns with missing values, and highly imbalanced class distribution. The competition results provide a comprehensive study on the usability of data mining-based fraud detection approaches in practical setting. Our principal findings are that features derived from fine-grained time-series analysis are crucial for accurate fraud detection, and that ensemble methods offer promising solutions to highly-imbalanced nonlinear classification tasks with mixed variable types and noisy/missing patterns. The competition data remain available for further studies at palanteer.sis.smu.edu.sg/fdma2012.",
            "keywords": [],
            "author": [
                "Richard Oentaryo",
                "Ee-Peng Lim",
                "Michael Finegold",
                "David Lo",
                "Feida Zhu",
                "Clifton Phua",
                "Eng-Yeow Cheu",
                "Ghim-Eng Yap",
                "Kelvin Sim",
                "Minh Nhut Nguyen",
                "Kasun Perera",
                "Bijay Neupane",
                "Mustafa Faisal",
                "Zeyar Aung",
                "Wei Lee Woon",
                "Wei Chen",
                "Dhaval Patel",
                "Daniel Berrar"
            ],
            "ref": "http://jmlr.org/papers/volume15/oentaryo14a/oentaryo14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "EnsembleSVM: A Library for Ensemble Learning Using Support Vector Machines",
            "abstract": "EnsembleSVM is a free software package containing efficient routines to perform ensemble learning with support vector machine (SVM) base models. It currently offers ensemble methods based on binary SVM models. Our implementation avoids duplicate storage and evaluation of support vectors which are shared between constituent models. Experimental results show that using ensemble approaches can drastically reduce training complexity while maintaining high predictive accuracy. The EnsembleSVM software package is freely available online at esat.kuleuven.be/stadius/ensemblesvm.",
            "keywords": [
                "classification",
                "ensemble learning",
                "support vector machine"
            ],
            "author": [
                "Marc Claesen",
                "Frank De Smet",
                "Johan A.K. Suykens",
                "Bart De Moor"
            ],
            "ref": "http://jmlr.org/papers/volume15/claesen14a/claesen14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Junction Tree Framework for Undirected Graphical Model Selection",
            "abstract": "An undirected graphical model is a joint probability distribution defined on an undirected graph , where the vertices in the graph index a collection of random variables and the edges encode conditional independence relationships among random variables. The undirected graphical model selection (UGMS) problem is to estimate the graph  given observations drawn from the undirected graphical model. This paper proposes a framework for decomposing the UGMS problem into multiple subproblems over clusters and subsets of the separators in a junction tree. The junction tree is constructed using a graph that contains a superset of the edges in . We highlight three main properties of using junction trees for UGMS. First, different regularization parameters or different UGMS algorithms can be used to learn different parts of the graph. This is possible since the subproblems we identify can be solved independently of each other. Second, under certain conditions, a junction tree based UGMS algorithm can produce consistent results with fewer observations than the usual requirements of existing algorithms. Third, both our theoretical and experimental results show that the junction tree framework does a significantly better job at finding the weakest edges in a graph than existing methods. This property is a consequence of both the first and second properties. Finally, we note that our framework is independent of the choice of the UGMS algorithm and can be used as a wrapper around standard UGMS algorithms for more accurate graph estimation.",
            "keywords": [
                "Graphical models",
                "Markov random fields",
                "junction trees",
                "model selection",
                "graphical model selection",
                "high-dimensional statistics"
            ],
            "author": [
                "Divyanshu Vats",
                "Robert D. Nowak"
            ],
            "ref": "http://jmlr.org/papers/volume15/vats14a/vats14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Axioms for Graph Clustering Quality Functions",
            "abstract": "",
            "keywords": [
                "graph clustering",
                "modularity"
            ],
            "author": [
                "Twan van Laarhoven",
                "Elena Marchiori"
            ],
            "ref": "http://jmlr.org/papers/volume15/vanlaarhoven14a/vanlaarhoven14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convex vs Non-Convex Estimators for Regression and Sparse Estimation: the Mean Squared Error Properties of ARD and GLasso",
            "abstract": "We study a simple linear regression problem for grouped variables; we are interested in methods which jointly perform estimation and variable selection, that is, that automatically set to zero groups of variables in the regression vector. The Group Lasso (GLasso), a well known approach used to tackle this problem which is also a special case of Multiple Kernel Learning (MKL), boils down to solving convex optimization problems. On the other hand, a Bayesian approach commonly known as Sparse Bayesian Learning (SBL), a version of which is the well known Automatic Relevance Determination (ARD), lead to non- convex problems. In this paper we discuss the relation between ARD (and a penalized version which we call PARD) and Glasso, and study their asymptotic properties in terms of the Mean Squared Error in estimating the unknown parameter. The theoretical arguments developed here are independent of the correctness of the prior models and clarify the advantages of PARD over GLasso.",
            "keywords": [
                "Lasso",
                "Group Lasso",
                "Multiple Kernel Learning",
                "Bayesian regularization"
            ],
            "author": [
                "Aleksandr Aravkin",
                "James V. Burke",
                "Alessandro Chiuso",
                "Gianluigi Pillonetto"
            ],
            "ref": "http://jmlr.org/papers/volume15/aravkin14a/aravkin14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Using Trajectory Data to Improve Bayesian Optimization for Reinforcement Learning",
            "abstract": "Recently, Bayesian Optimization (BO) has been used to successfully optimize parametric policies in several challenging Reinforcement Learning (RL) applications. BO is attractive for this problem because it exploits Bayesian prior information about the expected return and exploits this knowledge to select new policies to execute. Effectively, the BO framework for policy search addresses the exploration-exploitation tradeoff. In this work, we show how to more effectively apply BO to RL by exploiting the sequential trajectory information generated by RL agents. Our contributions can be broken into two distinct, but mutually beneficial, parts. The first is a new Gaussian process (GP) kernel for measuring the similarity between policies using trajectory data generated from policy executions. This kernel can be used in order to improve posterior estimates of the expected return thereby improving the quality of exploration. The second contribution, is a new GP mean function which uses learned transition and reward functions to approximate the surface of the objective. We show that the model-based approach we develop can recover from model inaccuracies when good transition and reward models cannot be learned. We give empirical results in a standard set of RL benchmarks showing that both our model-based and model-free approaches can speed up learning compared to competing methods. Further, we show that our contributions can be combined to yield synergistic improvement in some domains.",
            "keywords": [
                "reinforcement learning",
                "Bayesian",
                "optimization",
                "policy search",
                "Markov deci-     sion process"
            ],
            "author": [
                "Aaron Wilson",
                "Alan Fern",
                "Prasad Tadepalli"
            ],
            "ref": "http://jmlr.org/papers/volume15/wilson14a/wilson14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Information Theoretical Estimators Toolbox",
            "abstract": "We present ITE (information theoretical estimators) a free and open source, multi-platform, Matlab/Octave toolbox that is capable of estimating many different variants of entropy, mutual information, divergence, association measures, cross quantities, and kernels on distributions. Thanks to its highly modular design, ITE supports additionally (i) the combinations of the estimation techniques, (ii) the easy construction and embedding of novel information theoretical estimators, and (iii) their immediate application in information theoretical optimization problems. ITE also includes a prototype application in a central problem class of signal processing, independent subspace analysis and its extensions.",
            "keywords": [
                "entropy-",
                "mutual information-",
                "association-",
                "divergence-",
                "distribution kernel     estimation",
                "independent subspace analysis and its extensions",
                "modularity"
            ],
            "author": [
                "Zoltán Szabó"
            ],
            "ref": "http://jmlr.org/papers/volume15/szabo14a/szabo14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Off-policy Learning With Eligibility Traces: A Survey",
            "abstract": "In the framework of Markov Decision Processes, we consider linear off-policy learning, that is the problem of learning a linear approximation of the value function of some fixed policy from one trajectory possibly generated by some other policy. We briefly review on-policy learning algorithms of the literature (gradient-based and least-squares- based),  adopting a unified algorithmic view. Then, we highlight a systematic approach for adapting them to off-policy learning with eligibility traces. This leads to some known algorithms---off-policy LSTD(), LSPE(), TD(), TDC/GQ()---and suggests new extensions ---off-policy FPKF(), BRM(), gBRM(), GTD2(). We describe a comprehensive algorithmic derivation of all algorithms in a recursive and memory-efficent form, discuss their known convergence properties and illustrate their relative empirical behavior on Garnet problems. Our experiments suggest that the most standard algorithms on and off-policy LSTD()/LSPE()---and TD() if the feature space dimension is too large for a least-squares approach---perform the best.",
            "keywords": [
                "reinforcement learning",
                "value function estimation",
                "off-policy learning"
            ],
            "author": [
                "Matthieu Geist",
                "Bruno Scherrer"
            ],
            "ref": "http://jmlr.org/papers/volume15/geist14a/geist14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Early Stopping and Non-parametric Regression: An Optimal Data-dependent Stopping Rule",
            "abstract": "Early stopping is a form of regularization based on choosing when to stop running an iterative algorithm. Focusing on non- parametric regression in a reproducing kernel Hilbert space, we analyze the early stopping strategy for a form of gradient- descent applied to the least-squares loss function. We propose a data-dependent stopping rule that does not involve hold-out or cross-validation data, and we prove upper bounds on the squared error of the resulting function estimate, measured in either the  and  norm. These upper bounds lead to minimax-optimal rates for various kernel classes, including Sobolev smoothness classes and other forms of reproducing kernel Hilbert spaces. We show through simulation that our stopping rule compares favorably to two other stopping rules, one based on hold-out data and the other based on Stein's unbiased risk estimate. We also establish a tight connection between our early stopping strategy and the solution path of a kernel ridge regression estimator.",
            "keywords": [
                "early stopping",
                "non-parametric regression",
                "kernel ridge regression",
                "stopping     rule",
                "reproducing kernel hilbert space",
                "rademacher complexity"
            ],
            "author": [
                "Garvesh Raskutti",
                "Martin J. Wainwright",
                "Bin Yu"
            ],
            "ref": "http://jmlr.org/papers/volume15/raskutti14a/raskutti14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Node-Based Learning of Multiple Gaussian Graphical Models",
            "abstract": "We consider the problem of estimating high-dimensional Gaussian graphical models corresponding to a single set of variables under several distinct conditions. This problem is motivated by the task of recovering transcriptional regulatory networks on the basis of gene expression data containing heterogeneous samples, such as different disease states, multiple species, or different developmental stages. We assume that most aspects of the conditional dependence networks are shared, but that there are some structured differences between them. Rather than assuming that similarities and differences between networks are driven by individual edges, we take a node-based approach, which in many cases provides a more intuitive interpretation of the network differences. We consider estimation under two distinct assumptions: (1) differences between the  networks are due to individual nodes that are perturbed across conditions, or (2) similarities among the  networks are due to the presence of common hub nodes that are shared across all  networks. Using a row-column overlap norm penalty function, we formulate two convex optimization problems that correspond to these two assumptions. We solve these problems using an alternating direction method of multipliers algorithm, and we derive a set of necessary and sufficient conditions that allows us to decompose the problem into independent subproblems so that our algorithm can be scaled to high-dimensional settings. Our proposal is illustrated on synthetic data, a webpage data set, and a brain cancer gene expression data set.",
            "keywords": [
                "graphical model",
                "structured sparsity",
                "alternating direction method of multi-     pliers",
                "gene regulatory network",
                "lasso",
                "multivariate normalc 2014 Karthik Mohan",
                "Palma London",
                "Maryam Fazel"
            ],
            "author": [
                "Karthik Mohan",
                "Palma London",
                "Maryam Fazel",
                "Daniela Witten",
                "Su-In Lee"
            ],
            "ref": "http://jmlr.org/papers/volume15/mohan14a/mohan14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The FASTCLIME Package for Linear Programming and Large-Scale Precision Matrix Estimation in R",
            "abstract": "We develop an R package FASTCLIME for solving a family of regularized linear programming (LP) problems. Our package efficiently implements the parametric simplex algorithm, which provides a scalable and sophisticated tool for solving large- scale linear programs. As an illustrative example, one use of our LP solver is to implement an important sparse precision matrix estimation method called CLIME (Constrained  Minimization Estimator). Compared with existing packages for this problem such as CLIME and FLARE, our package has three advantages: (1) it efficiently calculates the full piecewise- linear regularization path; (2) it provides an accurate dual certificate as stopping criterion; (3) it is completely coded in C and is highly portable. This package is designed to be useful to statisticians and machine learning researchers for solving a wide range of problems.",
            "keywords": [
                "high dimensional data",
                "sparse precision matrix",
                "linear programming",
                "para-     metric simplex method"
            ],
            "author": [
                "Haotian Pang",
                "Han Liu",
                "Robert V",
                "erbei"
            ],
            "ref": "http://jmlr.org/papers/volume15/pang14a/pang14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "LIBOL: A Library for Online Learning Algorithms",
            "abstract": "LIBOL is an open-source library for large-scale online learning, which consists of a large family of efficient and scalable state-of-the-art online learning algorithms for large- scale online classification tasks. We have offered easy-to-use command-line tools and examples for users and developers, and also have made comprehensive documents available for both beginners and advanced users. LIBOL is not only a machine learning toolbox, but also a comprehensive experimental platform for conducting online learning research.",
            "keywords": [
                "online learning",
                "massive-scale classification"
            ],
            "author": [
                "Steven C.H. Hoi",
                "Jialei Wang",
                "Peilin Zhao"
            ],
            "ref": "http://jmlr.org/papers/volume15/hoi14a/hoi14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improving Markov Network Structure Learning Using Decision Trees",
            "abstract": "Most existing algorithms for learning Markov network structure either are limited to learning interactions among few variables or are very slow, due to the large space of possible structures. In this paper, we propose three new methods for using decision trees to learn Markov network structures. The advantage of using decision trees is that they are very fast to learn and can represent complex interactions among many variables. The first method, DTSL, learns a decision tree to predict each variable and converts each tree into a set of conjunctive features that define the Markov network structure. The second, DT-BLM, builds on DTSL by using it to initialize a search-based Markov network learning algorithm recently proposed by Davis and Domingos (2010). The third, DT+L1, combines the features learned by DTSL with those learned by an L1-regularized logistic regression method (L1) proposed by Ravikumar et al. (2009). In an extensive empirical evaluation on 20 data sets, DTSL is comparable to L1 and significantly faster and more accurate than two other baselines. DT-BLM is slower than DTSL, but obtains slightly higher accuracy. DT+L1 combines the strengths of DTSL and L1 to perform significantly better than either of them with only a modest increase in training time.",
            "keywords": [
                "Markov networks",
                "structure learning",
                "decision trees"
            ],
            "author": [
                "Daniel Lowd",
                "Jesse Davis"
            ],
            "ref": "http://jmlr.org/papers/volume15/lowd14a/lowd14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ground Metric Learning",
            "abstract": "Optimal transport distances have been used for more than a decade in machine learning to compare histograms of features. They have one parameter: the ground metric, which can be any metric between the features themselves. As is the case for all parameterized distances, optimal transport distances can only prove useful in practice when this parameter is carefully chosen. To date, the only option available to practitioners to set the ground metric parameter was to rely on a priori knowledge of the features, which limited considerably the scope of application of optimal transport distances. We propose to lift this limitation and consider instead algorithms that can learn the ground metric using only a training set of labeled histograms. We call this approach ground metric learning. We formulate the problem of learning the ground metric as the minimization of the difference of two convex polyhedral functions over a convex set of metric matrices. We follow the presentation of our algorithms with promising experimental results which show that this approach is useful both for retrieval and binary/multiclass classification tasks.",
            "keywords": [
                "optimal transport distance"
            ],
            "author": [
                "Marco Cuturi",
                "David Avis"
            ],
            "ref": "http://jmlr.org/papers/volume15/cuturi14a/cuturi14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Link Prediction in Graphs with Autoregressive Features",
            "abstract": "In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices. On the adjacency matrix it takes into account both sparsity and low rank properties and on the VAR it encodes the sparsity. The analysis involves oracle inequalities that illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank. The estimate is computed efficiently using proximal methods, and evaluated through numerical experiments.",
            "keywords": [
                "graphs",
                "link prediction",
                "low-rank",
                "sparsity"
            ],
            "author": [
                "Emile Richard",
                "Stéphane Gaïffas",
                "Nicolas Vayatis"
            ],
            "ref": "http://jmlr.org/papers/volume15/richard14a/richard14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptivity of Averaged Stochastic Gradient Descent to Local Strong Convexity for Logistic Regression",
            "abstract": "In this paper, we consider supervised learning problems such as logistic regression and study the stochastic gradient method with averaging, in the usual stochastic approximation setting where observations are used only once. We show that after  iterations, with a constant step-size proportional to  where  is the number of observations and  is the maximum norm of the observations, the convergence rate is always of order , and improves to  where  is the lowest eigenvalue of the Hessian at the global optimum (when this eigenvalue is greater than ). Since  does not need to be known in advance, this shows that averaged stochastic gradient is adaptive to unknown local strong convexity of the objective function. Our proof relies on the generalized self-concordance properties of the logistic loss and thus extends to all generalized linear models with uniformly bounded features.",
            "keywords": [
                "stochastic approximation",
                "logistic regression"
            ],
            "author": [
                "Francis Bach"
            ],
            "ref": "http://jmlr.org/papers/volume15/bach14a/bach14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Intersection Trees",
            "abstract": "Finding interactions between variables in large and high- dimensional data sets is often a serious computational challenge. Most approaches build up interaction sets incrementally, adding variables in a greedy fashion. The drawback is that potentially informative high-order interactions may be overlooked. Here, we propose an alternative approach for classification problems with binary predictor variables, called Random Intersection Trees. It works by starting with a maximal interaction that includes all variables, and then gradually removing variables if they fail to appear in randomly chosen observations of a class of interest. We show that informative interactions are retained with high probability, and the computational complexity of our procedure is of order , where  is the number of predictor variables. The value of  can reach values as low as 1 for very sparse data; in many more general settings, it will still beat the exponent  obtained when using a brute force search constrained to order  interactions. In addition, by using some new ideas based on min-wise hash schemes, we are able to further reduce the computational cost. Interactions found by our algorithm can be used for predictive modelling in various forms, but they are also often of interest in their own right as useful characterisations of what distinguishes a certain class from others.",
            "keywords": [
                "high-dimensional classification",
                "interactions",
                "min-wise hashing"
            ],
            "author": [
                "Rajen Dinesh Shah",
                "Nicolai Meinshausen"
            ],
            "ref": "http://jmlr.org/papers/volume15/shah14a/shah14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Reinforcement Learning for Closed-Loop Propofol Anesthesia: A Study in Human Volunteers",
            "abstract": "Clinical research has demonstrated the efficacy of closed-loop control of anesthesia using the bispectral index of the electroencephalogram as the controlled variable. These controllers have evolved to yield patient-specific anesthesia, which is associated with improved patient outcomes. Despite progress, the problem of patient-specific anesthesia remains unsolved. A variety of factors confound good control, including variations in human physiology, imperfect measures of drug effect, and delayed, hysteretic response to drug delivery. Reinforcement learning (RL) appears to be uniquely equipped to overcome these challenges; however, the literature offers no precedent for RL in anesthesia. To begin exploring the role RL might play in improving anesthetic care, we investigated the method's application in the delivery of patient-specific, propofol-induced hypnosis in human volunteers. When compared to performance metrics reported in the anesthesia literature, RL demonstrated patient-specific control marked by improved accuracy and stability. Furthermore, these results suggest that RL may be considered a viable alternative for solving other difficult closed-loop control problems in medicine. More rigorous clinical study, beyond the confines of controlled human volunteer studies, is needed to substantiate these findings.",
            "keywords": [
                "reinforcement learning",
                "bispectral index",
                "propofol",
                "anesthesia",
                "hypnosis"
            ],
            "author": [
                "Brett L Moore",
                "Larry D Pyeatt",
                "Vivekan",
                "Kulkarni",
                "Periklis Panousis",
                "Kevin Padrez",
                "Anthony G Doufas"
            ],
            "ref": "http://jmlr.org/papers/volume15/moore14a/moore14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Clustering Hidden Markov Models with Variational HEM",
            "abstract": "The hidden Markov model (HMM) is a widely-used generative model that copes with sequential data, assuming that each observation is conditioned on the state of a hidden Markov chain. In this paper, we derive a novel algorithm to cluster HMMs based on the hierarchical EM (HEM) algorithm. The proposed algorithm i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a âcluster centerâ, that is, a novel HMM that is representative for the group, in a manner that is consistent with the underlying generative model of the HMM. To cope with intractable inference in the E-step, the HEM algorithm is formulated as a variational optimization problem, and efficiently solved for the HMM case by leveraging an appropriate variational approximation. The benefits of the proposed algorithm, which we call variational HEM (VHEM), are demonstrated on several tasks involving time-series data, such as hierarchical clustering of motion capture sequences, and automatic annotation and retrieval of music and of online hand- writing data, showing improvements over current methods. In particular, our variational HEM algorithm effectively leverages large amounts of data when learning annotation models by using an efficient hierarchical estimation procedure, which reduces learning times and memory requirements, while improving model robustness through better regularization.",
            "keywords": [
                "Hierarchical EM algorithm",
                "clustering",
                "hidden Markov model",
                "hidden Markov     mixture model",
                "variational approximation"
            ],
            "author": [
                "Emanuele Coviello",
                "Antoni B. Chan",
                "Gert R.G. Lanckriet"
            ],
            "ref": "http://jmlr.org/papers/volume15/coviello14a/coviello14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Novel M-Estimator for Robust PCA",
            "abstract": "We study the basic problem of robust subspace recovery. That is, we assume a data set that some of its points are sampled around a fixed subspace and the rest of them are spread in the whole ambient space, and we aim to recover the fixed underlying subspace. We first estimate ârobust inverse sample covarianceâ by solving a convex minimization procedure; we then recover the subspace by the bottom eigenvectors of this matrix (their number correspond to the number of eigenvalues close to 0). We guarantee exact subspace recovery under some conditions on the underlying data. Furthermore, we propose a fast iterative algorithm, which linearly converges to the matrix minimizing the convex problem. We also quantify the effect of noise and regularization and discuss many other practical and theoretical issues for improving the subspace recovery in various settings. When replacing the sum of terms in the convex energy function (that we minimize) with the sum of squares of terms, we obtain that the new minimizer is a scaled version of the inverse sample covariance (when exists). We thus interpret our minimizer and its subspace (spanned by its bottom eigenvectors) as robust versions of the empirical inverse covariance and the PCA subspace respectively. We compare our method with many other algorithms for robust PCA on synthetic and real data sets and demonstrate state-of-the-art speed and accuracy.",
            "keywords": [
                "principal components analysis",
                "robust statistics",
                "M-estimator",
                "iteratively     re-weighted least squares"
            ],
            "author": [
                "Teng Zhang",
                "Gilad Lerman"
            ],
            "ref": "http://jmlr.org/papers/volume15/zhang14a/zhang14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Policy Evaluation with Temporal Differences: A Survey and Comparison",
            "abstract": "",
            "keywords": [
                "temporal differences",
                "policy evaluation",
                "value function estimation"
            ],
            "author": [
                "Christoph Dann",
                "Gerhard Neumann",
                "Jan Peters"
            ],
            "ref": "http://jmlr.org/papers/volume15/dann14a/dann14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Learning Using Smooth Relative Regret Approximations with Applications",
            "abstract": "",
            "keywords": [
                "active learning",
                "learning to rank from pairwise preferences",
                "semi-supervised     clustering",
                "clustering with side information",
                "disagreement coefficient"
            ],
            "author": [
                "Nir Ailon",
                "Ron Begleiter",
                "Esther Ezra"
            ],
            "ref": "http://jmlr.org/papers/volume15/ailon14a/ailon14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Extension of Slow Feature Analysis for Nonlinear Blind Source Separation",
            "abstract": "We present and test an extension of slow feature analysis as a novel approach to nonlinear blind source separation. The algorithm relies on temporal correlations and iteratively reconstructs a set of statistically independent sources from arbitrary nonlinear instantaneous mixtures. Simulations show that it is able to invert a complicated nonlinear mixture of two audio signals with a high reliability. The algorithm is based on a mathematical analysis of slow feature analysis for the case of input data that are generated from statistically independent sources.",
            "keywords": [
                "slow feature analysis",
                "nonlinear blind source separation",
                "statistical indepen-     dence",
                "independent component analysis"
            ],
            "author": [
                "Henning Sprekeler",
                "Tiziano Zito",
                "Laurenz Wiskott"
            ],
            "ref": "http://jmlr.org/papers/volume15/sprekeler14a/sprekeler14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Natural Evolution Strategies",
            "abstract": "This paper presents Natural Evolution Strategies (NES), a recent family of black-box optimization algorithms that use the natural gradient to update a parameterized search distribution in the direction of higher expected fitness. We introduce a collection of techniques that address issues of convergence, robustness, sample complexity, computational complexity and sensitivity to hyperparameters. This paper explores a number of implementations of the NES family, such as general-purpose multi-variate normal distributions and separable distributions tailored towards search in high dimensional spaces. Experimental results show best published performance on various standard benchmarks, as well as competitive performance on others.",
            "keywords": [
                "natural gradient",
                "stochastic search",
                "evolution strategies",
                "black-box optimiza-     tion"
            ],
            "author": [
                "Daan Wierstra",
                "Tom Schaul",
                "Tobias Glasmachers",
                "Yi Sun",
                "Jan Peters",
                "J\\\"{u}rgen Schmidhuber"
            ],
            "ref": "http://jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Conditional Random Field with High-order Dependencies for Sequence Labeling and Segmentation",
            "abstract": "Dependencies among neighboring labels in a sequence are important sources of information for sequence labeling and segmentation. However, only first-order dependencies, which are dependencies between adjacent labels or segments, are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account. In this paper, we give efficient inference algorithms to handle high-order dependencies between labels or segments in conditional random fields, under the assumption that the number of distinct label patterns used in the features is small. This leads to efficient learning algorithms for these conditional random fields. We show experimentally that exploiting high-order dependencies can lead to substantial performance improvements for some problems, and we discuss conditions under which high-order features can be effective.",
            "keywords": [
                "conditional random field",
                "semi-Markov conditional random field",
                "high-order     feature",
                "sequence labeling",
                "segmentation"
            ],
            "author": [
                "Nguyen Viet Cuong",
                "Nan Ye",
                "Wee Sun Lee",
                "Hai Leong Chieu"
            ],
            "ref": "http://jmlr.org/papers/volume15/cuong14a/cuong14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ellipsoidal Rounding for Nonnegative Matrix Factorization Under Noisy Separability",
            "abstract": "We present a numerical algorithm for nonnegative matrix factorization (NMF) problems under noisy separability. An NMF problem under separability can be stated as one of finding all vertices of the convex hull of data points. The research interest of this paper is to find the vectors as close to the vertices as possible in a situation in which noise is added to the data points. Our algorithm is designed to capture the shape of the convex hull of data points by using its enclosing ellipsoid. We show that the algorithm has correctness and robustness properties from theoretical and practical perspectives; correctness here means that if the data points do not contain any noise, the algorithm can find the vertices of their convex hull; robustness means that if the data points contain noise, the algorithm can find the near-vertices. Finally, we apply the algorithm to document clustering, and report the experimental results.",
            "keywords": [
                "nonnegative matrix factorization",
                "separability",
                "robustness to noise",
                "enclosing     ellipsoid"
            ],
            "author": [
                "Tomohiko Mizutani"
            ],
            "ref": "http://jmlr.org/papers/volume15/mizutani14a/mizutani14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improving Prediction from Dirichlet Process Mixtures via Enrichment",
            "abstract": "Flexible covariate-dependent density estimation can be achieved by modelling the joint density of the response and covariates as a Dirichlet process mixture. An appealing aspect of this approach is that computations are relatively easy. In this paper, we examine the predictive performance of these models with an increasing number of covariates. Even for a moderate number of covariates, we find that the likelihood for  tends to dominate the posterior of the latent random partition, degrading the predictive performance of the model. To overcome this, we suggest using a different nonparametric prior, namely an enriched Dirichlet process. Our proposal maintains a simple allocation rule, so that computations remain relatively simple. Advantages are shown through both predictive equations and examples, including an application to diagnosis Alzheimer's disease.",
            "keywords": [
                "Bayesian nonparametrics",
                "density regression",
                "predictive distribution",
                "random     partition"
            ],
            "author": [
                "Sara Wade",
                "David B. Dunson",
                "Sonia Petrone",
                "Lorenzo Trippa"
            ],
            "ref": "http://jmlr.org/papers/volume15/wade14a/wade14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gibbs Max-margin Topic Models with Data Augmentation",
            "abstract": "Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max- margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restrictive assumptions and no need to solve SVM subproblems. Furthermore, each step of the âaugment-and-collapse\" Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results on several medium-sized and large-scale data sets demonstrate significant improvements on time efficiency. The classification performance is also improved over competitors on binary, multi- class and multi-label classification tasks.",
            "keywords": [
                "supervised topic models",
                "max-margin learning",
                "Gibbs classifiers",
                "regularized     Bayesian inference"
            ],
            "author": [
                "Jun Zhu",
                "Ning Chen",
                "Hugh Perkins",
                "Bo Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume15/zhu14a/zhu14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Reliable Effective Terascale Linear Learning System",
            "abstract": "We present a system and a set of techniques for learning linear predictors with convex losses on terascale data sets, with trillions of features, (The number of features here refers to the number of non-zero entries in the data matrix.) billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature. (All the empirical evaluation reported in this work was carried out between May-Oct 2011.) We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices.",
            "keywords": [
                "distributed machine learning",
                "Hadoop",
                "AllReduce",
                "repeated online averaging"
            ],
            "author": [
                "Alekh Agarwal",
                "Oliveier Chapelle",
                "Miroslav Dud\\'{i}k",
                "John Langford"
            ],
            "ref": "http://jmlr.org/papers/volume15/agarwal14a/agarwal14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "New Learning Methods for Supervised and Unsupervised Preference Aggregation",
            "abstract": "In this paper we present a general treatment of the preference aggregation problem, in which multiple preferences over objects must be combined into a single consensus ranking. We consider two instances of this problem: unsupervised aggregation where no information about a target ranking is available, and supervised aggregation where ground truth preferences are provided. For each problem class we develop novel learning methods that are applicable to a wide range of preference types. (The code for all models introduced in this paper is available at www.cs.toronto.edu/~mvolkovs.)  Specifically, for unsupervised aggregation we introduce the Multinomial Preference model (MPM) which uses a multinomial generative process to model the observed preferences. For the supervised problem we develop a supervised extension for MPM and then propose two fully supervised models. The first model employs SVD factorization to derive effective item features, transforming the aggregation problems into a learning-to-rank one. The second model aims to eliminate the costly SVD factorization and instantiates a probabilistic CRF framework, deriving unary and pairwise potentials directly from the observed preferences. Using a probabilistic framework allows us to directly optimize the expectation of any target metric, such as NDCG or ERR. All the proposed models operate on pairwise preferences and can thus be applied to a wide range of preference types. We empirically validate the models on rank aggregation and collaborative filtering data sets and demonstrate superior empirical accuracy.",
            "keywords": [
                "preference aggregation",
                "meta-search",
                "learning-to-rank"
            ],
            "author": [
                "Maksims N. Volkovs",
                "Richard S. Zemel"
            ],
            "ref": "http://jmlr.org/papers/volume15/volkovs14a/volkovs14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Prediction and Clustering in Signed Networks: A Local to Global Perspective",
            "abstract": "The study of social networks is a burgeoning research area. However, most existing work is on networks that simply encode whether relationships exist or not. In contrast, relationships in signed networks can be positive (âlike\", âtrust\") or negative (âdislike\", âdistrust\"). The theory of social balance shows that signed networks tend to conform to some local patterns that, in turn, induce certain global characteristics. In this paper, we exploit both local as well as global aspects of social balance theory for two fundamental problems in the analysis of signed networks: sign prediction and clustering. Local patterns of social balance have been used in the past for sign prediction. We define more general measures of social imbalance (MOIs) based on -cycles in the network and give a simple sign prediction rule. Interestingly, by examining measures of social imbalance, we show that the classic Katz measure, which is used widely in unsigned link prediction, also has a balance theoretic interpretation when applied to signed networks. Motivated by the global structure of balanced networks, we propose an effective low rank modeling approach for both sign prediction and clustering. We provide theoretical performance guarantees for our low-rank matrix completion approach via convex relaxations, scale it up to large problem sizes using a matrix factorization based algorithm, and provide extensive experimental validation including comparisons with local approaches. Our experimental results indicate that, by adopting a more global viewpoint of social balance, we get significant performance and computational gains in prediction and clustering tasks on signed networks. Our work therefore highlights the usefulness of the global aspect of balance theory for the analysis of signed networks.",
            "keywords": [
                "signed networks",
                "sign prediction",
                "balance theory",
                "low rank model",
                "matrix     completion"
            ],
            "author": [
                "Kai-Yang Chiang",
                "Cho-Jui Hsieh",
                "Nagarajan Natarajan",
                "Inderjit S. Dhillon",
                "Ambuj Tewari"
            ],
            "ref": "http://jmlr.org/papers/volume15/chiang14a/chiang14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Nonparametric Comorbidity Analysis of Psychiatric Disorders",
            "abstract": "The analysis of comorbidity is an open and complex research field in the branch of psychiatry, where clinical experience and several studies suggest that the relation among the psychiatric disorders may have etiological and treatment implications. In this paper, we are interested in applying latent feature modeling to find the latent structure behind the psychiatric disorders that can help to examine and explain the relationships among them. To this end, we use the large amount of information collected in the National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database and propose to model these data using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the discrete nature of the data, we first need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial- logit likelihood model. We also provide a variational inference algorithm for this model, which provides a complementary (and less expensive in terms of computational complexity) alternative to the Gibbs sampler allowing us to deal with a larger number of data. Finally, we use the model to analyze comorbidity among the psychiatric disorders diagnosed by experts from the NESARC database.",
            "keywords": [
                "Bayesian nonparametrics",
                "Indian buffet process",
                "categorical observations",
                "multinomial-logit function",
                "Laplace approximation"
            ],
            "author": [
                "Francisco J. R. Ruiz",
                "Isabel Valera",
                "Carlos Blanco",
                "Fern",
                "o Perez-Cruz"
            ],
            "ref": "http://jmlr.org/papers/volume15/ruiz14a/ruiz14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Near-Separable Nonnegative Matrix Factorization Using Linear Optimization",
            "abstract": "Nonnegative matrix factorization (NMF) has been shown recently to be tractable under the separability assumption, under which all the columns of the input data matrix belong to the convex cone generated by only a few of these columns. Bittorf, Recht, RÃ© and Tropp (`Factoring nonnegative matrices with linear programs', NIPS 2012) proposed a linear programming (LP) model, referred to as Hottopixx, which is robust under any small perturbation of the input matrix. However, Hottopixx has two important drawbacks: (i) the input matrix has to be normalized, and (ii) the factorization rank has to be known in advance. In this paper, we generalize Hottopixx in order to resolve these two drawbacks, that is, we propose a new LP model which does not require normalization and detects the factorization rank automatically. Moreover, the new LP model is more flexible, significantly more tolerant to noise, and can easily be adapted to handle outliers and other noise models. Finally, we show on several synthetic data sets that it outperforms Hottopixx while competing favorably with two state-of-the-art methods.",
            "keywords": [
                "nonnegative matrix factorization",
                "separability",
                "linear programming",
                "convex     optimization",
                "robustness to noise",
                "pure-pixel assumption"
            ],
            "author": [
                "Nicolas Gillis",
                "Robert Luce"
            ],
            "ref": "http://jmlr.org/papers/volume15/gillis14a/gillis14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Follow the Leader If You Can, Hedge If You Must",
            "abstract": "Follow-the-Leader (FTL) is an intuitive sequential prediction strategy that guarantees constant regret in the stochastic setting, but has poor performance for worst-case data. Other hedging strategies have better worst-case guarantees but may perform much worse than FTL if the data are not maximally adversarial. We introduce the FlipFlop algorithm, which is the first method that provably combines the best of both worlds. As a stepping stone for our analysis, we develop AdaHedge, which is a new way of dynamically tuning the learning rate in Hedge without using the doubling trick. AdaHedge refines a method by Cesa-Bianchi, Mansour, and Stoltz (2007), yielding improved worst-case  guarantees. By interleaving AdaHedge and FTL, FlipFlop achieves regret within a constant factor of the FTL regret, without sacrificing AdaHedge's worst-case guarantees. AdaHedge and FlipFlop do not need to know the range of the losses in advance; moreover, unlike earlier methods, both have the intuitive property that the issued weights are invariant under rescaling and translation of the losses. The losses are also allowed to be negative, in which case they may be interpreted as gains.",
            "keywords": [
                "Hedge",
                "learning rate",
                "mixability",
                "online learning"
            ],
            "author": [
                "Steven de Rooij",
                "Tim van Erven",
                "Peter D. Grünwald",
                "Wouter M. Koolen"
            ],
            "ref": "http://jmlr.org/papers/volume15/rooij14a/rooij14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structured Prediction via Output Space Search",
            "abstract": "We consider a framework for structured prediction based on search in the space of complete structured outputs. Given a structured input, an output is produced by running a time- bounded search procedure guided by a learned cost function, and then returning the least cost output uncovered during the search. This framework can be instantiated for a wide range of search spaces and search procedures, and easily incorporates arbitrary structured-prediction loss functions. In this paper, we make two main technical contributions. First, we describe a novel approach to automatically defining an effective search space over structured outputs, which is able to leverage the availability of powerful classification learning algorithms. In particular, we define the limited-discrepancy search space and relate the quality of that space to the quality of learned classifiers. We also define a sparse version of the search space to improve the efficiency of our overall approach. Second, we give a generic cost function learning approach that is applicable to a wide range of search procedures. The key idea is to learn a cost function that attempts to mimic the behavior of conducting searches guided by the true loss function. Our experiments on six benchmark domains show that a small amount of search in limited discrepancy search space is often sufficient for significantly improving on state-of-the-art structured- prediction  performance. We also demonstrate significant speed improvements for our approach using sparse search spaces with little or no loss in accuracy.",
            "keywords": [
                "structured prediction",
                "state space search",
                "imitation learning"
            ],
            "author": [
                "Janardhan Rao Doppa",
                "Alan Fern",
                "Prasad Tadepalli"
            ],
            "ref": "http://jmlr.org/papers/volume15/doppa14a/doppa14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Towards Ultrahigh Dimensional Feature Selection for Big Data",
            "abstract": "In this paper, we present a new adaptive feature scaling scheme for ultrahigh-dimensional feature selection on Big Data, and then reformulate it as a convex semi-infinite programming (SIP) problem. To address the SIP, we propose an efficient feature generating paradigm. Different from traditional gradient-based approaches that conduct optimization on all input features, the proposed paradigm iteratively activates a group of features, and solves a sequence of multiple kernel learning (MKL) subproblems. To further speed up the training, we propose to solve the MKL subproblems in their primal forms through a modified accelerated proximal gradient approach. Due to such optimization scheme, some efficient cache techniques are also developed. The feature generating paradigm is guaranteed to converge globally under mild conditions, and can achieve lower feature selection bias. Moreover, the proposed method can tackle two challenging tasks in feature selection: 1) group-based feature selection with complex structures, and 2) nonlinear feature selection with explicit feature mappings. Comprehensive experiments on a wide range of synthetic and real-world data sets of tens of million data points with  features demonstrate the competitive performance of the proposed method over state-of-the-art feature selection methods in terms of generalization performance and training efficiency.",
            "keywords": [
                "big data",
                "ultrahigh dimensionality",
                "feature selection",
                "nonlinear feature selec-     tion",
                "multiple kernel learning"
            ],
            "author": [
                "Mingkui Tan",
                "Ivor W. Tsang",
                "Li Wang"
            ],
            "ref": "http://jmlr.org/papers/volume15/tan14a/tan14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Sampling for Large Scale Boosting",
            "abstract": "Classical boosting algorithms, such as AdaBoost, build a strong classifier without concern for the computational cost. Some applications, in particular in computer vision, may involve millions of training examples and very large feature spaces. In such contexts, the training time of off-the-shelf boosting algorithms may become prohibitive. Several methods exist to accelerate training, typically either by sampling the features or the examples used to train the weak learners. Even if some of these methods provide a guaranteed speed improvement, they offer no insurance of being more efficient than any other, given the same amount of time. The contributions of this paper are twofold: (1) a strategy to better deal with the increasingly common case where features come from multiple sources (for example, color, shape, texture, etc., in the case of images) and therefore can be partitioned into meaningful subsets; (2) new algorithms which balance at every boosting iteration the number of weak learners and the number of training examples to look at in order to maximize the expected loss reduction. Experiments in image classification and object recognition on four standard computer vision data sets show that the adaptive methods we propose outperform basic sampling and state-of-the-art bandit methods.",
            "keywords": [
                "boosting",
                "large scale learning"
            ],
            "author": [
                "Charles Dubout",
                "Francois Fleuret"
            ],
            "ref": "http://jmlr.org/papers/volume15/dubout14a/dubout14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Manopt, a Matlab Toolbox for Optimization on Manifolds",
            "abstract": "",
            "keywords": [
                "Riemannian optimization",
                "nonlinear programming",
                "non convex",
                "orthogonality     constraints",
                "rank constraints",
                "optimization with symmetries"
            ],
            "author": [
                "Nicolas Boumal",
                "Bamdev Mishra",
                "P.-A. Absil",
                "Rodolphe Sepulchre"
            ],
            "ref": "http://jmlr.org/papers/volume15/boumal14a/boumal14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Locally Adaptive Factor Processes for Multivariate Time Series",
            "abstract": "In modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process. In particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. If such time- varying smoothness is not accounted for, one can obtain misleading inferences and predictions, with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. This can lead to mis-calibration of predictive intervals, which can be substantially too narrow or wide depending on the time. We propose a locally adaptive factor process for characterizing multivariate mean-covariance changes in continuous time, allowing locally varying smoothness in both the mean and covariance matrix. This process is constructed utilizing latent dictionary functions evolving in time through nested Gaussian processes and linearly related to the observed data with a sparse mapping. Using a differential equation representation, we bypass usual computational bottlenecks in obtaining MCMC and online algorithms for approximate Bayesian inference. The performance is assessed in simulations and illustrated in a financial application.",
            "keywords": [
                "Bayesian nonparametrics",
                "locally varying smoothness",
                "multivariate time se-     ries",
                "nested Gaussian process"
            ],
            "author": [
                "Daniele Durante",
                "Bruno Scarpa",
                "David B. Dunson"
            ],
            "ref": "http://jmlr.org/papers/volume15/durante14a/durante14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Iteration Complexity of Feasible Descent Methods for Convex Optimization",
            "abstract": "In many machine learning problems such as the dual form of SVM, the objective function to be minimized is convex but not strongly convex. This fact causes difficulties in obtaining the complexity of some commonly used optimization algorithms. In this paper, we proved the global linear convergence on a wide range of algorithms when they are applied to some non-strongly convex problems. In particular, we are the first to prove  time complexity of cyclic coordinate descent methods on dual problems of support vector classification and regression.",
            "keywords": [
                "convergence rate",
                "convex optimization",
                "iteration complexity"
            ],
            "author": [
                "Po-Wei Wang",
                "Chih-Jen Lin"
            ],
            "ref": "http://jmlr.org/papers/volume15/wang14a/wang14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-Dimensional Covariance Decomposition into Sparse Markov and Independence Models",
            "abstract": "Fitting high-dimensional data involves a delicate tradeoff between faithful representation and the use of sparse models. Too often, sparsity assumptions on the fitted model are too restrictive to provide a faithful representation of the observed data. In this paper, we present a novel framework incorporating sparsity in different domains. We decompose the observed covariance matrix into a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse independence model (with a sparse covariance matrix). Our framework incorporates sparse covariance and sparse precision estimation as special cases and thus introduces a richer class of high-dimensional models. %We posit the observed data as generated from a linear combination of a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse Gaussian independence model (with a sparse covariance matrix). We characterize sufficient conditions for identifiability of the two models, viz., Markov and independence models. We propose an efficient decomposition method based on a modification of the popular -penalized maximum- likelihood  estimator (-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both Markov and independence models, when the number of samples  scales as ,  where  is the number of variables and  is the maximum node degree in the Markov model. Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation.",
            "keywords": [
                "high-dimensional covariance estimation",
                "sparse graphical model selection",
                "sparse covariance models",
                "sparsistency"
            ],
            "author": [
                "Majid Janzamin",
                "Animashree Anandkumar"
            ],
            "ref": "http://jmlr.org/papers/volume15/janzamin14a/janzamin14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
            "abstract": "Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size  and a desired number of steps . In particular, if  is too small then the algorithm exhibits undesirable random walk behavior, while if  is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps . NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more efficiently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter  on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient âturnkeyâ samplers.",
            "keywords": [
                "Markov chain Monte Carlo",
                "Hamiltonian Monte Carlo",
                "Bayesian inference",
                "adaptive Monte Carlo"
            ],
            "author": [
                "Matthew D. Hoffman",
                "Andrew Gelman"
            ],
            "ref": "http://jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife",
            "abstract": "We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2013) that are based on the jackknife and the infinitesimal jackknife (IJ). In practice, bagged predictors are computed using a finite number  of bootstrap replicates, and working with a large  can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require  bootstrap replicates to converge, where  is the size of the training set. We propose improved versions that only require  replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our findings with multiple experiments and simulation studies.",
            "keywords": [
                "bagging",
                "jackknife methods",
                "Monte Carlo noise"
            ],
            "author": [
                "Stefan Wager",
                "Trevor Hastie",
                "Bradley Efron"
            ],
            "ref": "http://jmlr.org/papers/volume15/wager14a/wager14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Surrogate Regret Bounds for Bipartite Ranking via Strongly Proper Losses",
            "abstract": "The problem of bipartite ranking, where instances are labeled positive or negative and the goal is to learn a scoring function that minimizes the probability of mis-ranking a pair of positive and negative instances (or equivalently, that maximizes the area under the ROC curve), has been widely studied in recent years. A dominant theoretical and algorithmic framework for the problem has been to reduce bipartite ranking to pairwise classification; in particular, it is well known that the bipartite ranking regret can be formulated as a pairwise classification regret, which in turn can be upper bounded using usual regret bounds for classification problems. Recently, Kotlowski et al. (2011) showed regret bounds for bipartite ranking in terms of the regret associated with balanced versions of the standard (non- pairwise) logistic and exponential losses. In this paper, we show that such (non-pairwise) surrogate regret bounds for bipartite ranking can be obtained in terms of a broad class of proper (composite) losses that we term as strongly proper. Our proof technique is much simpler than that of Kotlowski et al. (2011), and relies on properties of proper (composite) losses as elucidated recently by Reid and Williamson (2010, 2011) and others. Our result yields explicit surrogate bounds (with no hidden balancing terms) in terms of a variety of strongly proper losses, including for example logistic, exponential, squared and squared hinge losses as special cases. An important consequence is that standard algorithms minimizing a (non-pairwise) strongly proper loss, such as logistic regression and boosting algorithms (assuming a universal function class and appropriate regularization), are in fact consistent for bipartite ranking; moreover, our results allow us to quantify the bipartite ranking regret in terms of the corresponding surrogate regret. We also obtain tighter surrogate bounds under certain low-noise conditions via a recent result of Clemencon and Robbiano (2011).",
            "keywords": [
                "bipartite ranking"
            ],
            "author": [
                "Shivani Agarwal"
            ],
            "ref": "http://jmlr.org/papers/volume15/agarwal14b/agarwal14b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Minimax Regression Estimation over Sparse $\\ell_q$-Hulls",
            "abstract": "Given a dictionary of  predictors, in a random design regression setting with  observations, we construct estimators that target the best performance among all the linear combinations of the predictors under a sparse -norm () constraint on the linear coefficients. Besides identifying the optimal rates of convergence, our universal aggregation strategies by model mixing achieve the optimal rates simultaneously over the full range of  for any  and without knowledge of the -norm of the best linear coefficients to represent the regression function. To allow model misspecification, our upper bound results are obtained in a framework of aggregation of estimates. A striking feature is that no specific relationship among the predictors is needed to achieve the upper rates of convergence (hence permitting basically arbitrary correlations between the predictors). Therefore, whatever the true regression function (assumed to be uniformly bounded), our estimators automatically exploit any sparse representation of the regression function (if any), to the best extent possible within the -constrained linear combinations for any . A sparse approximation result in the -hulls turns out to be crucial to adaptively achieve minimax rate optimal aggregation. It precisely characterizes the number of terms needed to achieve a prescribed accuracy of approximation to the best linear combination in an -hull for . It offers the insight that the minimax rate of -aggregation is basically determined by an effective model size, which is a sparsity index that depends on , , , and the -norm bound in an easily interpretable way based on a classical model selection theory that deals with a large number of models.",
            "keywords": [],
            "author": [
                "Zhan Wang",
                "Sandra Paterlini",
                "Fuchang Gao",
                "Yuhong Yang"
            ],
            "ref": "http://jmlr.org/papers/volume15/wang14b/wang14b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graph Estimation From Multi-Attribute Data",
            "abstract": "Undirected graphical models are important in a number of modern applications that involve exploring or exploiting dependency structures underlying the data. For example, they are often used to explore complex systems where connections between entities are not well understood, such as in functional brain networks or genetic networks. Existing methods for estimating structure of undirected graphical models focus on scenarios where each node represents a scalar random variable, such as a binary neural activation state or a continuous mRNA abundance measurement, even though in many real world problems, nodes can represent multivariate variables with much richer meanings, such as whole images, text documents, or multi-view feature vectors. In this paper, we propose a new principled framework for estimating the structure of undirected graphical models from such multivariate (or multi-attribute) nodal data. The structure of a graph is inferred through estimation of non-zero partial canonical correlation between nodes. Under a Gaussian model, this strategy is equivalent to estimating conditional independencies between random vectors represented by the nodes and it generalizes the classical problem of covariance selection (Dempster, 1972).  We relate the problem of estimating non-zero partial canonical correlations to maximizing a penalized Gaussian likelihood objective and develop a method that efficiently maximizes this objective. Extensive simulation studies demonstrate the effectiveness of the method under various conditions. We provide illustrative applications to uncovering gene regulatory networks from gene and protein profiles, and uncovering brain connectivity graph from positron emission tomography data. Finally, we provide sufficient conditions under which the true graphical structure can be recovered correctly.",
            "keywords": [
                "graphical model selection",
                "multi-attribute data",
                "network analysis"
            ],
            "author": [
                "Mladen Kolar",
                "Han Liu",
                "Eric P. Xing"
            ],
            "ref": "http://jmlr.org/papers/volume15/kolar14a/kolar14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hitting and Commute Times in Large Random Neighborhood Graphs",
            "abstract": "In machine learning, a popular tool to analyze the structure of graphs is the hitting time and the commute distance (resistance distance). For two vertices  and , the hitting time  is the expected time it takes a random walk to travel from  to . The commute distance is its symmetrized version . In our paper we study the behavior of hitting times and commute distances when the number  of vertices in the graph tends to infinity. We focus on random geometric graphs (-graphs, kNN graphs and Gaussian similarity graphs), but our results also extend to graphs with a given expected degree distribution or Erdos-Renyi graphs with planted partitions. We prove that in these graph families, the suitably rescaled hitting time  converges to  and the rescaled commute time to  where  and  denote the degrees of vertices  and . In these cases, hitting and commute times do not provide information about the structure of the graph, and their use is discouraged in many machine learning applications.",
            "keywords": [
                "commute distance",
                "resistance",
                "random graph",
                "k-nearest neighbor graph"
            ],
            "author": [
                "Ulrike von Luxburg",
                "Agnes Radl",
                "Matthias Hein"
            ],
            "ref": "http://jmlr.org/papers/volume15/vonluxburg14a/vonluxburg14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Inference with Posterior Regularization and Applications to Infinite Latent SVMs",
            "abstract": "Existing Bayesian models, especially nonparametric Bayesian methods, rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations. While priors affect posterior distributions through Bayes' rule, imposing posterior regularization is arguably more direct and in some cases more natural and general. In this paper, we present regularized Bayesian inference (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation. RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks. When the regularization is induced from a linear operator on the posterior distributions, such as the expectation operator, we present a general convex-analysis theorem to characterize the solution of RegBayes. Furthermore, we present two concrete examples of RegBayes, infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large- margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark data sets, which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics. Such results contribute to push forward the interface between these two important subfields, which have been largely treated as isolated in the community.",
            "keywords": [
                "Bayesian inference",
                "posterior regularization",
                "Bayesian nonparametrics",
                "large-margin learning",
                "classification"
            ],
            "author": [
                "Jun Zhu",
                "Ning Chen",
                "Eric P. Xing"
            ],
            "ref": "http://jmlr.org/papers/volume15/zhu14b/zhu14b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Expectation Propagation for Neural Networks with Sparsity-Promoting Priors",
            "abstract": "We propose a novel approach for nonlinear regression using a two-layer neural network (NN) model structure with sparsity- favoring hierarchical priors on the network weights. We present an expectation propagation (EP) approach for approximate integration over the posterior distribution of the weights, the hierarchical scale parameters of the priors, and the residual scale. Using a factorized posterior approximation we derive a computationally efficient algorithm, whose complexity scales similarly to an ensemble of independent sparse linear models. The approach enables flexible definition of weight priors with different sparseness properties such as independent Laplace priors with a common scale parameter or Gaussian automatic relevance determination (ARD) priors with different relevance parameters for all inputs. The approach can be extended beyond standard activation functions and NN model structures to form flexible nonlinear predictors from multiple sparse linear models. The effects of the hierarchical priors and the predictive performance of the algorithm are assessed using both simulated and real-world data. Comparisons are made to two alternative models with ARD priors: a Gaussian process with a NN covariance function and marginal maximum a posteriori estimates of the relevance parameters, and a NN with Markov chain Monte Carlo integration over all the unknown model parameters.",
            "keywords": [
                "expectation propagation",
                "neural network",
                "multilayer perceptron",
                "linear model",
                "sparse prior"
            ],
            "author": [
                "Pasi Jylänki",
                "Aapo Nummenmaa",
                "Aki Vehtari"
            ],
            "ref": "http://jmlr.org/papers/volume15/jylanki14a/jylanki14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pattern Alternating Maximization Algorithm for Missing Data in High-Dimensional Problems",
            "abstract": "We propose a novel and efficient algorithm for maximizing the observed log-likelihood of a multivariate normal data matrix with missing values. We show that our procedure, based on iteratively regressing the missing on the observed variables, generalizes the standard EM algorithm by alternating between different complete data spaces and performing the E-Step incrementally. In this non-standard setup we prove numerical convergence to a stationary point of the observed log- likelihood. For high-dimensional data, where the number of variables may greatly exceed sample size, we perform regularization using a Lasso-type penalty. This introduces sparsity in the regression coefficients used for imputation, permits fast computation and warrants competitive performance in terms of estimating the missing entries. We show on simulated and real data that the new method often improves upon other modern imputation techniques such as k-nearest neighbors imputation, nuclear norm minimization or a penalized likelihood approach with an -penalty on the concentration matrix.",
            "keywords": [
                "missing data",
                "observed likelihood"
            ],
            "author": [
                "Nicolas Städler",
                "Daniel J. Stekhoven",
                "Peter Bühlmann"
            ],
            "ref": "http://jmlr.org/papers/volume15/staedler14a/staedler14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
            "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different âthinnedâ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
            "keywords": [
                "neural networks",
                "regularization",
                "model combination"
            ],
            "author": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "ref": "http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Factor Analysis for Learning and Content Analytics",
            "abstract": "We develop a new model and algorithms for machine learning-based learning analytics, which estimate a learner's knowledge of the concepts underlying a domain, and {\\em{content analytics}}, which estimate the relationships among a collection of questions and those concepts. Our model represents the probability that a learner provides the correct response to a question in terms of three factors: their understanding of a set of underlying concepts, the concepts involved in each question, and each question's intrinsic difficulty. We estimate these factors given the graded responses to a collection of questions. The underlying estimation problem is ill-posed in general, especially when only a subset of the questions are answered. The key observation that enables a well-posed solution is the fact that typical educational domains of interest involve only a small number of key concepts. Leveraging this observation, we develop both a bi-convex maximum-likelihood-based solution and a Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem. We also incorporate user-defined tags on questions to facilitate the interpretability of the estimated factors. Experiments with synthetic and real-world data demonstrate the efficacy of our approach. Finally, we make a connection between SPARFA and noisy, binary-valued (1-bit) dictionary learning that is of independent interest.",
            "keywords": [
                "factor analysis",
                "sparse probit regression",
                "sparse logistic regression",
                "Bayesian     latent factor analysis"
            ],
            "author": [
                "Andrew S. Lan",
                "Andrew E. Waters",
                "Christoph Studer",
                "Richard G. Baraniuk"
            ],
            "ref": "http://jmlr.org/papers/volume15/lan14a/lan14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Discovery with Continuous Additive Noise Models",
            "abstract": "We consider the problem of learning causal directed acyclic graphs from an observational joint distribution. One can use these graphs to predict the outcome of interventional experiments, from which data are often not available. We show that if the observational distribution follows a structural equation model with an additive noise structure, the directed acyclic graph becomes identifiable from the distribution under mild conditions. This constitutes an interesting alternative to traditional methods that assume faithfulness and identify only the Markov equivalence class of the graph, thus leaving some edges undirected. We provide practical algorithms for finitely many samples, RESIT (regression with subsequent independence test) and two methods based on an independence score. We prove that RESIT is correct in the population setting and provide an empirical evaluation.",
            "keywords": [
                "causal inference",
                "structural equation models",
                "additive noise",
                "identifiability",
                "causal minimality"
            ],
            "author": [
                "Jonas Peters",
                "Joris M. Mooij",
                "Dominik Janzing",
                "Bernhard Schölkopf"
            ],
            "ref": "http://jmlr.org/papers/volume15/peters14a/peters14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Student-t Mixture as a Natural Image Patch Prior with Application to Image Compression",
            "abstract": "Recent results have shown that Gaussian mixture models (GMMs) are remarkably good at density modeling of natural image patches, especially given their simplicity. In terms of log likelihood on real-valued data they are comparable with the best performing techniques published, easily outperforming more advanced ones, such as deep belief networks. They can be applied to various image processing tasks, such as image denoising, deblurring and inpainting, where they improve on other generic prior methods, such as sparse coding and field of experts. Based on this we propose the use of another, even richer mixture model based image prior: the Student-t mixture model (STM). We demonstrate that it convincingly surpasses GMMs in terms of log likelihood, achieving performance competitive with the state of the art in image patch modeling. We apply both the GMM and STM to the task of lossy and lossless image compression, and propose efficient coding schemes that can easily be extended to other unsupervised machine learning models. Finally, we show that the suggested techniques outperform JPEG, with results comparable to or better than JPEG 2000.",
            "keywords": [
                "image compression",
                "mixture models",
                "GMM",
                "density modeling"
            ],
            "author": [
                "A{\\\"a}ron van den Oord",
                "Benjamin Schrauwen"
            ],
            "ref": "http://jmlr.org/papers/volume15/vandenoord14a/vandenoord14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parallel MCMC with Generalized Elliptical Slice Sampling",
            "abstract": "Probabilistic models are conceptually powerful tools for finding structure in data, but their practical effectiveness is often limited by our ability to perform inference in them. Exact inference is frequently intractable, so approximate inference is often performed using Markov chain Monte Carlo (MCMC). To achieve the best possible results from MCMC, we want to efficiently simulate many steps of a rapidly mixing Markov chain which leaves the target distribution invariant. Of particular interest in this regard is how to take advantage of multi-core computing to speed up MCMC-based inference, both to improve mixing and to distribute the computational load. In this paper, we present a parallelizable Markov chain Monte Carlo algorithm for efficiently sampling from continuous probability distributions that can take advantage of hundreds of cores. This method shares information between parallel Markov chains to build a scale-location mixture of Gaussians approximation to the density function of the target distribution. We combine this approximation with a recently developed method known as elliptical slice sampling to create a Markov chain with no step- size parameters that can mix rapidly without requiring gradient or curvature computations.",
            "keywords": [
                "Markov chain Monte Carlo",
                "parallelism",
                "slice sampling",
                "elliptical slice sam-     pling"
            ],
            "author": [
                "Robert Nishihara",
                "Iain Murray",
                "Ryan P. Adams"
            ],
            "ref": "http://jmlr.org/papers/volume15/nishihara14a/nishihara14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classifier Cascades and Trees for Minimizing Feature Evaluation Cost",
            "abstract": "Machine learning algorithms have successfully entered industry through many real-world applications (e.g., search engines and product recommendations). In these applications, the test-time CPU cost must be budgeted and accounted for. In this paper, we examine two main components of the test-time CPU cost, classifier evaluation cost and feature extraction cost, and show how to balance these costs with the classifier accuracy. Since the computation required for feature extraction dominates the test-time cost of a classifier in these settings, we develop two algorithms to efficiently balance the performance with the test-time cost. Our first contribution describes how to construct and optimize a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific sub-partition of the input space. Our second contribution is a natural reduction of the tree of classifiers into a cascade. The cascade is particularly useful for class-imbalanced data sets as the majority of instances can be early-exited out of the cascade when the algorithm is sufficiently confident in its prediction. Because both approaches only compute features for inputs that benefit from them the most, we find our trained classifiers lead to high accuracies at a small fraction of the computational cost.",
            "keywords": [
                "budgeted learning",
                "resource efficient machine learning",
                "feature cost sensitive     learning",
                "web-search ranking"
            ],
            "author": [
                "Zhixiang (Eddie) Xu",
                "Matt J. Kusner",
                "Kilian Q. Weinberger",
                "Minmin Chen",
                "Olivier Chapelle"
            ],
            "ref": "http://jmlr.org/papers/volume15/xu14a/xu14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Particle Gibbs with Ancestor Sampling",
            "abstract": "Particle Markov chain Monte Carlo (pmcmc) is a systematic way of combining the two main tools used for Monte Carlo statistical inference: sequential Monte Carlo (smc) and Markov chain Monte Carlo (mcmc). We present a new pmcmc algorithm that we refer to as particle Gibbs with ancestor sampling (pgas). pgas provides the data analyst with an off-the-shelf class of Markov kernels that can be used to simulate, for instance, the typically high-dimensional and highly autocorrelated state trajectory in a state-space model. The ancestor sampling procedure enables fast mixing of the pgas kernel even when using seemingly few particles in the underlying smc sampler. This is important as it can significantly reduce the computational burden that is typically associated with using smc. pgas is conceptually similar to the existing pg with backward simulation (pgbs) procedure. Instead of using separate forward and backward sweeps as in pgbs, however, we achieve the same effect in a single forward sweep. This makes pgas well suited for addressing inference problems not only in state-space models, but also in models with more complex dependencies, such as non-Markovian, Bayesian nonparametric, and general probabilistic graphical models.",
            "keywords": [
                "particle Markov chain Monte Carlo",
                "sequential Monte Carlo",
                "Bayesian infer-     ence",
                "non-Markovian models"
            ],
            "author": [
                "Fredrik Lindsten",
                "Michael I. Jordan",
                "Thomas B. Schön"
            ],
            "ref": "http://jmlr.org/papers/volume15/lindsten14a/lindsten14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Clustering Partially Observed Graphs via Convex Optimization",
            "abstract": "This paper considers the problem of clustering a partially observed unweighted graph---i.e., one where for some node pairs we know there is an edge between them, for some others we know there is no edge, and for the remaining we do not know whether or not there is an edge. We want to organize the nodes into disjoint clusters so that there is relatively dense (observed) connectivity within clusters, and sparse across clusters. We take a novel yet natural approach to this problem, by focusing on finding the clustering that minimizes the number of âdisagreementsâ---i.e., the sum of the number of (observed) missing edges within clusters, and (observed) present edges across clusters. Our algorithm uses convex optimization; its basis is a reduction of disagreement minimization to the problem of recovering an (unknown) low-rank matrix and an (unknown) sparse matrix from their partially observed sum. We evaluate the performance of our algorithm on the classical Planted Partition/Stochastic Block Model. Our main theorem provides sufficient conditions for the success of our algorithm as a function of the minimum cluster size, edge density and observation probability; in particular, the results characterize the tradeoff between the observation probability and the edge density gap. When there are a constant number of clusters of equal size, our results are optimal up to logarithmic factors.",
            "keywords": [
                "graph clustering",
                "convex optimization"
            ],
            "author": [
                "Yudong Chen",
                "Ali Jalali",
                "Sujay Sanghavi",
                "Huan Xu"
            ],
            "ref": "http://jmlr.org/papers/volume15/chen14a/chen14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Tensor Approach to Learning Mixed Membership Community Models",
            "abstract": "Community detection is the task of detecting hidden communities from observed interactions. Guaranteed community detection has so far been mostly limited to models with non-overlapping communities such as the stochastic block model. In this paper, we remove this restriction, and provide guaranteed community detection for a family of probabilistic network models with overlapping communities, termed as the mixed membership Dirichlet model, first introduced by Airoldi et al. (2008). This model allows for nodes to have fractional memberships in multiple communities and assumes that the community memberships are drawn from a Dirichlet distribution. Moreover, it contains the stochastic block model as a special case. We propose a unified approach to learning these models via a tensor spectral decomposition method. Our estimator is based on low-order moment tensor of the observed network, consisting of -star counts. Our learning method is fast and is based on simple linear algebraic operations, e.g., singular value decomposition and tensor power iterations. We provide guaranteed recovery of community memberships and model parameters and present a careful finite sample analysis of our learning method. As an important special case, our results match the best known scaling requirements for the (homogeneous) stochastic block model.",
            "keywords": [
                "community detection",
                "spectral methods",
                "tensor methods",
                "moment-based esti-     mation",
                "mixed membership modelsc 2014 Anima Anandkumar",
                "Rong Ge",
                "Daniel Hsu"
            ],
            "author": [
                "Animashree  An",
                "kumar",
                "Rong Ge",
                "Daniel Hsu",
                "Sham M. Kakade"
            ],
            "ref": "http://jmlr.org/papers/volume15/anandkumar14a/anandkumar14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cover Tree Bayesian Reinforcement Learning",
            "abstract": "This paper proposes an online tree-based Bayesian approach for reinforcement learning. For inference, we employ a generalised context tree model. This defines a distribution on multivariate Gaussian piecewise-linear models, which can be updated in closed form. The tree structure itself is constructed using the cover tree method, which remains efficient in high dimensional spaces. We combine the model with Thompson sampling and approximate dynamic programming to obtain effective exploration policies in unknown environments. The flexibility and computational simplicity of the model render it suitable for many reinforcement learning problems in continuous state spaces. We demonstrate this in an experimental comparison with a Gaussian process model, a linear model and simple least squares policy iteration.",
            "keywords": [
                "Bayesian inference",
                "non-parametric statistics"
            ],
            "author": [
                "Nikolaos Tziortziotis",
                "Christos Dimitrakakis",
                "Konstantinos Blekas"
            ],
            "ref": "http://jmlr.org/papers/volume15/tziortziotis14a/tziortziotis14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient State-Space Inference of Periodic Latent Force Models",
            "abstract": "Latent force models (LFM) are principled approaches to incorporating solutions to differential equations within non- parametric inference methods. Unfortunately, the development and application of LFMs can be inhibited by their computational cost, especially when closed-form solutions for the LFM are unavailable, as is the case in many real world problems where these latent forces exhibit periodic behaviour. Given this, we develop a new sparse representation of LFMs which considerably improves their computational efficiency, as well as broadening their applicability, in a principled way, to domains with periodic or near periodic latent forces. Our approach uses a linear basis model to approximate one generative model for each periodic force. We assume that the latent forces are generated from Gaussian process priors and develop a linear basis model which fully expresses these priors. We apply our approach to model the thermal dynamics of domestic buildings and show that it is effective at predicting day-ahead temperatures within the homes. We also apply our approach within queueing theory in which quasi-periodic arrival rates are modelled as latent forces. In both cases, we demonstrate that our approach can be implemented efficiently using state-space methods which encode the linear dynamic systems via LFMs. Further, we show that state estimates obtained using periodic latent force models can reduce the root mean squared error to 17% of that from non-periodic models and 27% of the nearest rival approach which is the resonator model (Sarkka et al., 2012; Hartikainen et al. 2012).",
            "keywords": [],
            "author": [
                "Steven Reece",
                "Siddhartha Ghosh",
                "Alex Rogers",
                "Stephen Roberts",
                "Nicholas R. Jennings"
            ],
            "ref": "http://jmlr.org/papers/volume15/reece14a/reece14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral Learning of Latent-Variable PCFGs: Algorithms and Sample Complexity",
            "abstract": "We introduce a spectral learning algorithm for latent-variable PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006). Under a separability (singular value) condition, we prove that the method provides statistically consistent parameter estimates. Our result rests on three theorems: the first gives a tensor form of the inside- outside algorithm for PCFGs; the second shows that the required tensors can be estimated directly from training examples where hidden-variable values are missing; the third gives a PAC-style convergence bound for the estimation method.",
            "keywords": [
                "latent-variable PCFGs"
            ],
            "author": [
                "Shay B. Cohen",
                "Karl Stratos",
                "Michael Collins",
                "Dean P. Foster",
                "Lyle Ungar"
            ],
            "ref": "http://jmlr.org/papers/volume15/cohen14a/cohen14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Multilabel Classification and Ranking with Bandit Feedback",
            "abstract": "We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd- order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show  regret bounds, which improve in several ways on the existing results. We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on diverse real- world multilabel data sets, often obtaining comparable performance.",
            "keywords": [
                "contextual bandits",
                "structured prediction",
                "ranking",
                "online learning",
                "regret     bounds"
            ],
            "author": [
                "Claudio Gentile",
                "Francesco Orabona"
            ],
            "ref": "http://jmlr.org/papers/volume15/gentile14a/gentile14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Beyond the Regret Minimization Barrier: Optimal Algorithms for Stochastic Strongly-Convex Optimization",
            "abstract": "",
            "keywords": [
                "stochastic gradient descent",
                "convex optimization",
                "regret minimization"
            ],
            "author": [
                "Elad Hazan",
                "Satyen Kale"
            ],
            "ref": "http://jmlr.org/papers/volume15/hazan14a/hazan14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "One-Shot-Learning Gesture Recognition using HOG-HOF Features",
            "abstract": "The purpose of this paper is to describe one-shot-learning gesture recognition systems developed on the ChaLearn Gesture Dataset (ChaLearn). We use RGB and depth images and combine appearance (Histograms of Oriented Gradients) and motion descriptors (Histogram of Optical Flow) for parallel temporal segmentation and recognition. The Quadratic-Chi distance family is used to measure differences between histograms to capture cross-bin relationships. We also propose a new algorithm for trimming videos---to remove all the unimportant frames from videos. We present two methods that use a combination of HOG-HOF descriptors together with variants of a Dynamic Time Warping technique. Both methods outperform other published methods and help narrow the gap between human performance and algorithms on this task. The code is publicly available in the MLOSS repository.",
            "keywords": [
                "ChaLearn",
                "histogram of oriented gradients",
                "histogram of optical flow"
            ],
            "author": [
                "Jakub Konecny",
                "Michal Hagara"
            ],
            "ref": "http://jmlr.org/papers/volume15/konecny14a/konecny14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Contextual Bandits with Similarity Information",
            "abstract": "",
            "keywords": [
                "multi-armed bandits",
                "contextual bandits",
                "regret",
                "Lipschitz-continuity"
            ],
            "author": [
                "Aleks",
                "rs Slivkins"
            ],
            "ref": "http://jmlr.org/papers/volume15/slivkins14a/slivkins14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Boosting Algorithms for Detector Cascade Learning",
            "abstract": "The problem of learning classifier cascades is considered. A new cascade boosting algorithm, fast cascade boosting (FCBoost), is proposed. FCBoost is shown to have a number of interesting properties, namely that it 1) minimizes a Lagrangian risk that jointly accounts for classification accuracy and speed, 2) generalizes adaboost, 3) can be made cost-sensitive to support the design of high detection rate cascades, and 4) is compatible with many predictor structures suitable for sequential decision making. It is shown that a rich family of such structures can be derived recursively from cascade predictors of two stages, denoted cascade generators. Generators are then proposed for two new cascade families,  last-stage and multiplicative cascades, that generalize the two most popular cascade architectures in the literature. The concept of neutral predictors is finally introduced, enabling FCBoost to automatically determine the cascade configuration, i.e., number of stages and number of weak learners per stage, for the learned cascades. Experiments on face and pedestrian detection show that the resulting cascades outperform current state-of-the-art methods in both detection accuracy and speed.",
            "keywords": [
                "complexity-constrained learning",
                "detector cascades",
                "sequential decision-making",
                "boosting",
                "ensemble methods",
                "cost-sensitive learning"
            ],
            "author": [
                "Mohammad Saberian",
                "Nuno Vasconcelos"
            ],
            "ref": "http://jmlr.org/papers/volume15/saberian14a/saberian14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Estimation of Causal Direction in Acyclic Structural Equation Models with Individual-specific Confounder Variables and Non-Gaussian Distributions",
            "abstract": "Several existing methods have been shown to consistently estimate causal direction assuming linear or some form of nonlinear relationship and no latent confounders. However, the estimation results could be distorted if either assumption is violated. We develop an approach to determining the possible causal direction between two observed variables when latent confounding variables are present. We first propose a new linear non-Gaussian acyclic structural equation model with individual- specific effects that are sometimes the source of confounding. Thus, modeling individual-specific effects as latent variables allows latent confounding to be considered. We then propose an empirical Bayesian approach for estimating possible causal direction using the new model. We demonstrate the effectiveness of our method using artificial and real-world data.",
            "keywords": [
                "structural equation models",
                "Bayesian networks",
                "estimation of causal direc-     tion",
                "latent confounding variables"
            ],
            "author": [
                "Shohei Shimizu",
                "Kenneth Bollen"
            ],
            "ref": "http://jmlr.org/papers/volume15/shimizu14a/shimizu14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Truncated EM Approach for Spike-and-Slab Sparse Coding",
            "abstract": "We study inference and learning based on a sparse coding model with `spike-and-slab' prior. As in standard sparse coding, the model used assumes independent latent sources that linearly combine to generate data points. However, instead of using a standard sparse prior such as a Laplace distribution, we study the application of a more flexible `spike-and-slab' distribution which models the absence or presence of a source's contribution independently of its strength if it contributes. We investigate two approaches to optimize the parameters of spike-and-slab sparse coding: a novel truncated EM approach and, for comparison, an approach based on standard factored variational distributions. The truncated approach can be regarded as a variational approach with truncated posteriors as variational distributions. In applications to source separation we find that both approaches improve the state-of-the-art in a number of standard benchmarks, which argues for the use of `spike-and- slab' priors for the corresponding data domains. Furthermore, we find that the truncated EM approach improves on the standard factored approach in source separation tasks---which hints to biases introduced by assuming posterior independence in the factored variational approach. Likewise, on a standard benchmark for image denoising, we find that the truncated EM approach improves on the factored variational approach. While the performance of the factored approach saturates with increasing numbers of hidden dimensions, the performance of the truncated approach improves the state-of-the-art for higher noise levels.",
            "keywords": [
                "sparse coding",
                "spike-and-slab distributions",
                "approximate EM",
                "variational     Bayes",
                "unsupervised learning",
                "source separation"
            ],
            "author": [
                "Abdul-Saboor Sheikh",
                "Jacquelyn A. Shelton",
                "Jörg Lücke"
            ],
            "ref": "http://jmlr.org/papers/volume15/sheikh14a/sheikh14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Occlusive Components Analysis",
            "abstract": "We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, specified in the model parameters, combine to form the image. We show that the object parameters can be learned from an unlabeled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. Tractable approximations can be derived, however, by applying a truncated variational approach to Expectation Maximization (EM). In numerical experiments it is shown that these approximations recover the underlying set of object parameters including data noise and sparsity. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating components. The studied approach demonstrates that the multiple-causes generative approach can be generalized to extract occluding components, which links research on occlusion to the field of sparse coding approaches.",
            "keywords": [
                "generative models",
                "occlusion",
                "unsupervised learning",
                "sparse coding",
                "expecta-tion truncationc 2014 Marc Henniges"
            ],
            "author": [
                "Marc Henniges",
                "Richard E. Turner",
                "Maneesh Sahani",
                "Julian Eggert",
                "Jörg Lücke"
            ],
            "ref": "http://jmlr.org/papers/volume15/henniges14a/henniges14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimality of Graphlet Screening in High Dimensional Variable Selection",
            "abstract": "Consider a linear model , where  has  rows and  columns and . We assume both  and  are large, including the case of . The unknown signal vector  is assumed to be sparse in the sense that only a small fraction of its components is nonzero. The goal is to identify such nonzero coordinates (i.e., variable selection). We are primarily interested in the regime where signals are both rare and weak so that successful variable selection is challenging but is still possible. We assume the Gram matrix  is sparse in the sense that each row has relatively few large entries (diagonals of  are normalized to ). The sparsity of  naturally induces the sparsity of the so-called Graph of Strong Dependence (GOSD). The key insight is that there is an interesting interplay between the signal sparsity and graph sparsity: in a broad context, the signals decompose into many small-size components of GOSD that are disconnected to each other. We propose Graphlet Screening for variable selection. This is a two-step Screen and Clean procedure, where in the first step, we screen subgraphs of GOSD with sequential -tests, and in the second step, we clean with penalized MLE. The main methodological innovation is to use GOSD to guide both the screening and cleaning processes. For any variable selection procedure , we measure its performance by the Hamming distance between the sign vectors of  and , and assess the optimality by the minimax Hamming distance. Compared with more stringent criteria such as exact support recovery or oracle property, which demand strong signals, the Hamming distance criterion is more appropriate for weak signals since it naturally allows a small fraction of errors. We show that in a broad class of situations, Graphlet Screening achieves the optimal rate of convergence in terms of the Hamming distance. Unlike Graphlet Screening, well- known procedures such as the -penalization methods do not utilize local graphic structure for variable selection, so they generally do not achieve the optimal rate of convergence, even in very simple settings and even if the tuning parameters are ideally set. The the presented algorithm is implemented as R-CRAN package ScreenClean and in matlab (available at stat.cmu.edu).",
            "keywords": [],
            "author": [
                "Jiashun Jin",
                "Cun-Hui Zhang",
                "Qi Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume15/jin14a/jin14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tensor Decompositions for Learning Latent Variable Models",
            "abstract": "This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models---including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation---which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.",
            "keywords": [],
            "author": [
                "Animashree Anandkumar",
                "Rong Ge",
                "Daniel Hsu",
                "Sham M. Kakade",
                "Matus Telgarsky"
            ],
            "ref": "http://jmlr.org/papers/volume15/anandkumar14b/anandkumar14b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Entropy Estimation for Countable Discrete Distributions",
            "abstract": "We consider the problem of estimating Shannon's entropy  from discrete data, in cases where the number of possible symbols is unknown or even countably infinite. The Pitman-Yor process, a generalization of Dirichlet process, provides a tractable prior distribution over the space of countably infinite discrete distributions, and has found major applications in Bayesian non- parametric statistics and machine learning. Here we show that it provides a natural family of priors for Bayesian entropy estimation, due to the fact that moments of the induced posterior distribution over  can be computed analytically. We derive formulas for the posterior mean (Bayes' least squares estimate) and variance under Dirichlet and Pitman-Yor process priors. Moreover, we show that a fixed Dirichlet or Pitman-Yor process prior implies a narrow prior distribution over , meaning the prior strongly determines the entropy estimate in the under-sampled regime. We derive a family of continuous measures for mixing Pitman-Yor processes to produce an approximately flat prior over . We show that the resulting \"Pitman-Yor Mixture\" (PYM) entropy estimator is consistent for a large class of distributions. Finally, we explore the theoretical properties of the resulting estimator, and show that it performs well both in simulation and in application to real data.",
            "keywords": [
                "entropy",
                "information theory",
                "Bayesian estimation",
                "Bayesian nonparametrics",
                "Dirichlet     process"
            ],
            "author": [
                "Evan Archer",
                "Il Memming Park",
                "Jonathan W. Pillow"
            ],
            "ref": "http://jmlr.org/papers/volume15/archer14a/archer14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Confidence Intervals and Hypothesis Testing for High-Dimensional Regression",
            "abstract": "",
            "keywords": [
                "hypothesis testing",
                "confidence intervals",
                "LASSO",
                "high-dimensional models"
            ],
            "author": [
                "Adel Javanmard",
                "Andrea Montanari"
            ],
            "ref": "http://jmlr.org/papers/volume15/javanmard14a/javanmard14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "QUIC: Quadratic Approximation for Sparse Inverse Covariance Estimation",
            "abstract": "The -regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program. In contrast to recent state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton's method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We show that our method is superlinearly convergent, and present experimental results using synthetic and real-world application data that demonstrate the considerable improvements in performance of our method when compared to previous methods.",
            "keywords": [
                "covariance",
                "graphical model",
                "regularization",
                "optimization"
            ],
            "author": [
                "Cho-Jui Hsieh",
                "Mátyás A. Sustik",
                "Inderjit S. Dhillon",
                "Pradeep Ravikumar"
            ],
            "ref": "http://jmlr.org/papers/volume15/hsieh14a/hsieh14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multimodal Learning with Deep Boltzmann Machines",
            "abstract": "Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bi-modal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.",
            "keywords": [
                "Boltzmann machines",
                "unsupervised learning",
                "multimodal learning",
                "neural     networks"
            ],
            "author": [
                "Nitish Srivastava",
                "Ruslan Salakhutdinov"
            ],
            "ref": "http://jmlr.org/papers/volume15/srivastava14b/srivastava14b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Data Collection For Informative Rankings Expose Well-Connected Graphs",
            "abstract": "Given a graph where vertices represent alternatives and arcs represent pairwise comparison data, the statistical ranking problem is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with the pairwise comparisons. Our goal in this paper is to develop a method for collecting data for which the least squares estimator for the ranking problem has maximal Fisher information. Our approach, based on experimental design, is to view data collection as a bi-level optimization problem where the inner problem is the ranking problem and the outer problem is to identify data which maximizes the informativeness of the ranking. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding multigraphs with large algebraic connectivity. This reduction of the data collection problem to graph-theoretic questions is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating data set and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking. As another application, we study the 2011-12 NCAA football schedule and propose schedules with the same number of games which are significantly more informative. Using spectral clustering methods to identify highly-connected communities within the division, we argue that the NCAA could improve its notoriously poor rankings by simply scheduling more out-of- conference games.",
            "keywords": [
                "ranking",
                "active learning",
                "scheduling",
                "optimal experimental design",
                "graph     synthesis",
                "algebraic connectivityc 2014 Braxton Osting",
                "Christoph Brune"
            ],
            "author": [
                "Braxton Osting",
                "Christoph Brune",
                "Stanley J.  Osher"
            ],
            "ref": "http://jmlr.org/papers/volume15/osting14a/osting14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-Dimensional Learning of Linear Causal Networks via Inverse Covariance Estimation",
            "abstract": "We establish a new framework for statistical estimation of directed acyclic graphs (DAGs) when data are generated from a linear, possibly non-Gaussian structural equation model. Our framework consists of two parts: (1) inferring the moralized graph from the support of the inverse covariance matrix; and (2) selecting the best-scoring graph amongst DAGs that are consistent with the moralized graph. We show that when the error variances are known or estimated to close enough precision, the true DAG is the unique minimizer of the score computed using the reweighted squared -loss. Our population-level results have implications for the identifiability of linear SEMs when the error covariances are specified up to a constant multiple. On the statistical side, we establish rigorous conditions for high-dimensional consistency of our two-part algorithm, defined in terms of a \"gap\" between the true DAG and the next best candidate. Finally, we demonstrate that dynamic programming may be used to select the optimal DAG in linear time when the treewidth of the moralized graph is bounded.",
            "keywords": [
                "causal inference",
                "dynamic programming",
                "identifiability",
                "inverse covariance     matrix estimation"
            ],
            "author": [
                "Po-Ling Loh",
                "Peter Bühlmann"
            ],
            "ref": "http://jmlr.org/papers/volume15/loh14a/loh14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?",
            "abstract": "We evaluate 179 classifiers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classifiers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearest-neighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classifiers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large- scale problems) and other own real problems, in order to achieve significant conclusions about the classifier behavior, not dependent on the data set collection. The classifiers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1% of the maximum accuracy overcoming 90% in the 84.3% of the data sets. However, the difference is not statistically significant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3% of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classifiers (3 out of 5 bests classifiers are RF), followed by SVM (4 classifiers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).",
            "keywords": [
                "classification",
                "UCI data base",
                "random forest",
                "support vector machine",
                "neural     networks",
                "decision trees",
                "ensembles",
                "rule-based classifiers",
                "discriminant analysis",
                "Bayesian     classifiers",
                "generalized linear models",
                "partial least squares and principal component re-     gression",
                "multiple adaptive regression splines",
                "nearest-neighbors"
            ],
            "author": [
                "Manuel Fernández-Delgado",
                "Eva Cernadas",
                "Senén Barro",
                "Dinani Amorim"
            ],
            "ref": "http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ooDACE Toolbox: A Flexible Object-Oriented Kriging Implementation",
            "abstract": "When analyzing data from computationally expensive simulation codes, surrogate modeling methods are firmly established as facilitators for design space exploration, sensitivity analysis, visualization and optimization. Kriging is a popular surrogate modeling technique used for the Design and Analysis of Computer Experiments (DACE). Hence, the past decade Kriging has been the subject of extensive research and many extensions have been proposed, e.g., co-Kriging, stochastic Kriging, blind Kriging, etc. However, few Kriging implementations are publicly available and tailored towards scientists and engineers. Furthermore, no Kriging toolbox exists that unifies several Kriging flavors. This paper addresses this need by presenting an efficient object-oriented Kriging implementation and several Kriging extensions, providing a flexible and easily extendable framework to test and implement new Kriging flavors while reusing as much code as possible.",
            "keywords": [
                "Kriging",
                "Gaussian process",
                "co-Kriging",
                "blind Kriging",
                "surrogate modeling",
                "metamodeling"
            ],
            "author": [
                "Ivo Couckuyt",
                "Tom Dhaene",
                "Piet Demeester"
            ],
            "ref": "http://jmlr.org/papers/volume15/couckuyt14a/couckuyt14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Online Gesture Recognition with Crowdsourced Annotations",
            "abstract": "Crowdsourcing is a promising way to reduce the effort of collecting annotations for training gesture recognition systems. Crowdsourced annotations suffer from \"noise\" such as mislabeling, or inaccurate identification of start and end time of gesture instances. In this paper we present SegmentedLCSS and WarpingLCSS, two template-matching methods offering robustness when trained with noisy crowdsourced annotations to spot gestures from wearable motion sensors. The methods quantize signals into strings of characters and then apply variations of the longest common subsequence algorithm (LCSS) to spot gestures. We compare the noise robustness of our methods against baselines which use dynamic time warping (DTW) and support vector machines (SVM). The experiments are performed on data sets with various gesture classes (10-17 classes) recorded from accelerometers on arms, with both real and synthetic crowdsourced annotations. WarpingLCSS has similar or better performance than baselines in absence of noisy annotations. In presence of 60% mislabeled instances, WarpingLCSS outperformed SVM by 22% F1-score and outperformed DTW-based methods by 36% F1-score on average. SegmentedLCSS yields similar performance as WarpingLCSS, however it performs one order of magnitude slower. Additionally, we show to use our methods to filter out the noise in the crowdsourced annotation before training a traditional classifier. The filtering increases the performance of SVM by 20% F1-score and of DTW-based methods by 8% F1-score on average in the noisy real crowdsourced annotations.",
            "keywords": [
                "gesture spotting",
                "crowdsourced annotation",
                "longest common subsequence",
                "template matching methods"
            ],
            "author": [
                "Long-Van Nguyen-Dinh",
                "Alberto Calatroni",
                "Gerhard Tr\\\"{o}ster"
            ],
            "ref": "http://jmlr.org/papers/volume15/nguyendinh14a/nguyendinh14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Accelerating t-SNE using Tree-Based Algorithms",
            "abstract": "The paper investigates the acceleration of t-SNE--an embedding technique that is commonly used for the visualization of high- dimensional data in scatter plots--using two tree-based algorithms. In particular, the paper develops variants of the Barnes-Hut algorithm and of the dual-tree algorithm that approximate the gradient used for learning t-SNE embeddings in . Our experiments show that the resulting algorithms substantially accelerate t-SNE, and that they make it possible to learn embeddings of data sets with millions of objects. Somewhat counterintuitively, the Barnes-Hut variant of t-SNE appears to outperform the dual-tree variant.",
            "keywords": [
                "embedding",
                "multidimensional scaling",
                "t-SNE",
                "space-partitioning trees",
                "Barnes-Hut algorithm"
            ],
            "author": [
                "Laurens van der Maaten"
            ],
            "ref": "http://jmlr.org/papers/volume15/vandermaaten14a/vandermaaten14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Set-Valued Approachability and Online Learning with Partial Monitoring",
            "abstract": "Approachability has become a standard tool in analyzing learning algorithms in the adversarial online learning setup. We develop a variant of approachability for games where there is ambiguity in the obtained reward: it belongs to a set rather than being a single vector. Using this variant we tackle the problem of approachability in games with partial monitoring and develop a simple and generally efficient strategy (i.e., with constant per-step complexity) for this setup. As an important example, we instantiate our general strategy to the case when external regret or internal regret is to be minimized under partial monitoring.",
            "keywords": [
                "online learning",
                "approachability",
                "regret"
            ],
            "author": [
                "Shie Mannor",
                "Vianney Perchet",
                "Gilles Stoltz"
            ],
            "ref": "http://jmlr.org/papers/volume15/mannor14a/mannor14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Graphical Models With Hubs",
            "abstract": "We consider the problem of learning a high-dimensional graphical model in which there are a few hub nodes that are   densely-connected to many other nodes. Many authors have studied the use of an  penalty in order to learn a sparse graph in the high-dimensional setting. However, the  penalty implicitly assumes that each edge is equally likely and independent of all other edges. We propose a general framework to accommodate more realistic networks with hub nodes, using a convex formulation that involves a row-column overlap norm penalty. We apply this general framework to three widely- used probabilistic graphical models: the Gaussian graphical model, the covariance graph model, and the binary Ising model. An alternating direction method of multipliers algorithm is used to solve the corresponding convex optimization problems. On synthetic data, we demonstrate that our proposed framework outperforms competitors that do not explicitly model hub nodes. We illustrate our proposal on a webpage data set and a gene expression data set.",
            "keywords": [
                "Gaussian graphical model",
                "covariance graph",
                "binary network",
                "lasso",
                "hub",
                "alternating direction method of multipliersc 2014 Kean Ming Tan",
                "Palma London",
                "Karthik Mohan",
                "Su-In Lee",
                "Maryam Fazel"
            ],
            "author": [
                "Kean Ming Tan",
                "Palma London",
                "Karthik Mohan",
                "Su-In Lee",
                "Maryam Fazel",
                "Daniela Witten"
            ],
            "ref": "http://jmlr.org/papers/volume15/tan14b/tan14b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Inconsistency of Pitman-Yor Process Mixtures for the Number of Components",
            "abstract": "In many applications, a finite mixture is a natural model, but it can be difficult to choose an appropriate number of components. To circumvent this choice, investigators are increasingly turning to Dirichlet process mixtures (DPMs), and Pitman--Yor process mixtures (PYMs), more generally. While these models may be well-suited for Bayesian density estimation, many investigators are using them for inferences about the number of components, by considering the posterior on the number of components represented in the observed data. We show that this posterior is not consistent---that is, on data from a finite mixture, it does not concentrate at the true number of components. This result applies to a large class of nonparametric mixtures, including DPMs and PYMs, over a wide variety of families of component distributions, including essentially all discrete families, as well as continuous exponential families satisfying mild regularity conditions (such as multivariate Gaussians).",
            "keywords": [
                "consistency",
                "Dirichlet process mixture",
                "number of components",
                "finite mixture"
            ],
            "author": [
                "Jeffrey W. Miller",
                "Matthew T. Harrison"
            ],
            "ref": "http://jmlr.org/papers/volume15/miller14a/miller14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Contextual Policy Search",
            "abstract": "We consider the problem of learning skills that are versatilely applicable. One popular approach for learning such skills is contextual policy search in which the individual tasks are represented as context vectors. We are interested in settings in which the agent is able to actively select the tasks that it examines during the learning process. We argue that there is a better way than selecting each task equally often because some tasks might be easier to learn at the beginning and the knowledge that the agent can extract from these tasks can be transferred to similar but more difficult tasks. The methods that we propose for addressing the task-selection problem model the learning process as a non-stationary multi-armed bandit problem with custom intrinsic reward heuristics so that the estimated learning progress will be maximized. This approach does neither make any assumptions about the underlying contextual policy search algorithm nor about the policy representation. We present empirical results on an artificial benchmark problem and a ball throwing problem with a simulated Mitsubishi PA-10 robot arm which show that active context selection can improve the learning of skills considerably.",
            "keywords": [
                "reinforcement learning",
                "policy search",
                "movement primitives",
                "active learning"
            ],
            "author": [
                "Alexander Fabisch",
                "Jan Hendrik Metzen"
            ],
            "ref": "http://jmlr.org/papers/volume15/fabisch14a/fabisch14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Matrix Completion with the Trace Norm: Learning, Bounding, and Transducing",
            "abstract": "Trace-norm regularization is a widely-used and successful approach for collaborative filtering and matrix completion. However, previous learning guarantees require strong assumptions, such as a uniform distribution over the matrix entries. In this paper, we bridge this gap by providing such guarantees, under much milder assumptions which correspond to matrix completion as performed in practice. In fact, we claim that previous difficulties partially stemmed from a mismatch between the standard learning-theoretic modeling of matrix completion, and its practical application. Our results also shed some light on the issue of matrix completion with bounded models, which enforce predictions to lie within a certain range. In particular, we provide experimental and theoretical evidence that such models lead to a modest yet significant improvement.",
            "keywords": [
                "collaborative filtering",
                "matrix completion",
                "trace-norm regularization",
                "trans-     ductive learning"
            ],
            "author": [
                "Ohad Shamir",
                "Shai Shalev-Shwartz"
            ],
            "ref": "http://jmlr.org/papers/volume15/shamir14a/shamir14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Analysis of Metric Graph Reconstruction",
            "abstract": "A metric graph is a 1-dimensional stratified metric space consisting of vertices and edges or loops glued together. Metric graphs can be naturally used to represent and model data that take the form of noisy filamentary structures, such as street maps, neurons, networks of rivers and galaxies. We consider the statistical problem of reconstructing the topology of a metric graph embedded in  from a random sample. We derive lower and upper bounds on the minimax risk for the noiseless case and tubular noise case. The upper bound is based on the reconstruction algorithm given in Aanjaneya et al. (2012).",
            "keywords": [
                "metric graph",
                "filament",
                "reconstruction",
                "manifold learning"
            ],
            "author": [
                "Fabrizio Lecci",
                "Aless",
                "ro Rinaldo",
                "Larry Wasserman"
            ],
            "ref": "http://jmlr.org/papers/volume15/lecci14a/lecci14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Gesture Recognition Toolkit",
            "abstract": "The Gesture Recognition Toolkit is a cross-platform open-source C++ library designed to make real-time machine learning and gesture recognition more accessible for non-specialists. Emphasis is placed on ease of use, with a consistent, minimalist design that promotes accessibility while supporting flexibility and customization for advanced users. The toolkit features a broad range of classification and regression algorithms and has extensive support for building real-time systems. This includes algorithms for signal processing, feature extraction and automatic gesture spotting.",
            "keywords": [
                "gesture recognition",
                "machine learning"
            ],
            "author": [
                "Nicholas Gillian",
                "Joseph A. Paradiso"
            ],
            "ref": "http://jmlr.org/papers/volume15/gillian14a/gillian14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convolutional Nets and Watershed Cuts for Real-Time Semantic Labeling of RGBD Videos",
            "abstract": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. Using a frame by frame labeling, we obtain nearly state-of-the-art performance on the NYU-v2 depth data set with an accuracy of 64.5%. We then show that the labeling can be further improved by exploiting the temporal consistency in the video sequence of the scene. To that goal, we present a method producing temporally consistent superpixels from a streaming video. Among the different methods producing superpixel segmentations of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time by using a minimum spanning tree. In a framework exploiting minimum spanning trees all along, we propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real-time applications. We illustrate the labeling of indoor scenes in video sequences that could be processed in real-time using appropriate hardware such as an FPGA.",
            "keywords": [
                "deep learning",
                "optimization",
                "convolutional networks",
                "superpixels",
                "depth in-     formationc 2014 Camille Couprie"
            ],
            "author": [
                "Camille Couprie",
                "Cl\\'{e}ment Farabet",
                "Laurent Najman",
                "Yann LeCun"
            ],
            "ref": "http://jmlr.org/papers/volume15/couprie14a/couprie14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Bayes-Optimality of F-Measure Maximizers",
            "abstract": "The F-measure, which has originally been introduced in information retrieval, is nowadays routinely used as a performance metric for problems such as binary classification, multi-label classification, and structured output prediction. Optimizing this measure is a statistically and computationally challenging problem, since no closed-form solution exists. Adopting a decision-theoretic perspective, this article provides a formal and experimental analysis of different approaches for maximizing the F-measure. We start with a Bayes-risk analysis of related loss functions, such as Hamming loss and subset zero-one loss, showing that optimizing such losses as a surrogate of the F-measure leads to a high worst-case regret. Subsequently, we perform a similar type of analysis for F-measure maximizing algorithms, showing that such algorithms are approximate, while relying on additional assumptions regarding the statistical distribution of the binary response variables. Furthermore, we present a new algorithm which is not only computationally efficient but also Bayes-optimal, regardless of the underlying distribution. To this end, the algorithm requires only a quadratic (with respect to the number of binary responses) number of parameters of the joint distribution. We illustrate the practical performance of all analyzed methods by means of experiments with multi-label classification problems.",
            "keywords": [
                "F-measure",
                "Bayes-optimal predictions",
                "regret",
                "statistical decision theory",
                "multi-label classification"
            ],
            "author": [
                "Willem Waegeman",
                "Krzysztof Dembczy{\\'n}ski",
                "Arkadiusz Jachnik",
                "Weiwei Cheng",
                "Eyke Hüllermeier"
            ],
            "ref": "http://jmlr.org/papers/volume15/waegeman14a/waegeman14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SPMF: A Java Open-Source Pattern Mining Library",
            "abstract": "We present SPMF, an open-source data mining library offering implementations of more than 55 data mining algorithms. SPMF is a cross-platform library implemented in Java, specialized for discovering patterns in transaction and sequence databases such as frequent itemsets, association rules and sequential patterns. The source code can be integrated in other Java programs. Moreover, SPMF offers a command line interface and a simple graphical interface for quick testing. The source code is available under the GNU General Public License, version 3. The website of the project offers several resources such as documentation with examples of how to run each algorithm, a developer's guide, performance comparisons of algorithms, data sets, an active forum, a FAQ and a mailing list.",
            "keywords": [
                "data mining",
                "library",
                "frequent pattern mining",
                "sequence database",
                "transaction     database"
            ],
            "author": [
                "Philippe Fournier-Viger",
                "Antonio Gomariz",
                "Ted Gueniche",
                "Azadeh Soltani",
                "Cheng-Wei Wu",
                "Vincent S. Tseng"
            ],
            "ref": "http://jmlr.org/papers/volume15/fournierviger14a/fournierviger14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Learning and Planning with Compressed Predictive States",
            "abstract": "Predictive state representations (PSRs) offer an expressive framework for modelling partially observable systems. By compactly representing systems as functions of observable quantities, the PSR learning approach avoids using local-minima prone expectation-maximization and instead employs a globally optimal moment-based algorithm. Moreover, since PSRs do not require a predetermined latent state structure as an input, they offer an attractive framework for model-based reinforcement learning when agents must plan without a priori access to a system model. Unfortunately, the expressiveness of PSRs comes with significant computational cost, and this cost is a major factor inhibiting the use of PSRs in applications. In order to alleviate this shortcoming, we introduce the notion of compressed PSRs (CPSRs). The CPSR learning approach combines recent advancements in dimensionality reduction, incremental matrix decomposition, and compressed sensing. We show how this approach provides a principled avenue for learning accurate approximations of PSRs, drastically reducing the computational costs associated with learning while also providing effective regularization. Going further, we propose a planning framework which exploits these learned models. And we show that this approach facilitates model-learning and planning in large complex partially observable domains, a task that is infeasible without the principled use of compression.",
            "keywords": [
                "predictive state representation",
                "reinforcement learning",
                "dimensionality re-     duction"
            ],
            "author": [
                "William Hamilton",
                "Mahdi Milani Fard",
                "Joelle Pineau"
            ],
            "ref": "http://jmlr.org/papers/volume15/hamilton14a/hamilton14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Revisiting Stein's Paradox: Multi-Task Averaging",
            "abstract": "We present a multi-task learning approach to jointly estimate the means of multiple independent distributions from samples. The proposed multi-task averaging (MTA) algorithm results in a convex combination of the individual task's sample averages. We derive the optimal amount of regularization for the two task case for the minimum risk estimator and a minimax estimator, and show that the optimal amount of regularization can be practically estimated without cross-validation. We extend the practical estimators to an arbitrary number of tasks. Simulations and real data experiments demonstrate the advantage of the proposed MTA estimators over standard averaging and James-Stein estimation.",
            "keywords": [
                "multi-task learning",
                "James-Stein"
            ],
            "author": [
                "Sergey Feldman",
                "Maya R. Gupta",
                "Bela A. Frigyik"
            ],
            "ref": "http://jmlr.org/papers/volume15/feldman14a/feldman14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Objective Reinforcement Learning using Sets of Pareto Dominating Policies",
            "abstract": "Many real-world problems involve the optimization of multiple, possibly conflicting objectives. Multi-objective reinforcement learning (MORL) is a generalization of standard reinforcement learning where the scalar reward signal is extended to multiple feedback signals, in essence, one for each objective. MORL is the process of learning policies that optimize multiple criteria simultaneously. In this paper, we present a novel temporal difference learning algorithm that integrates the Pareto dominance relation into a reinforcement learning approach. This algorithm is a multi-policy algorithm that learns a set of Pareto dominating policies in a single run. We name this algorithm Pareto Q-learning and it is applicable in episodic environments with deterministic as well as stochastic transition functions. A crucial aspect of Pareto -learning is the updating mechanism that bootstraps sets of -vectors. One of our main contributions in this paper is a mechanism that separates the expected immediate reward vector from the set of expected future discounted reward vectors. This decomposition allows us to update the sets and to exploit the learned policies consistently throughout the state space. To balance exploration and exploitation during learning, we also propose three set evaluation mechanisms. These three mechanisms evaluate the sets of vectors to accommodate for standard action selection strategies, such as -greedy. More precisely, these mechanisms use multi-objective evaluation principles such as the hypervolume measure, the cardinality indicator and the Pareto dominance relation to select the most promising actions. We experimentally validate the algorithm on multiple environments with two and three objectives and we demonstrate that Pareto -learning outperforms current state-of-the-art MORL algorithms with respect to the hypervolume of the obtained policies. We note that (1) Pareto -learning is able to learn the entire Pareto front under the usual assumption that each state-action pair is sufficiently sampled, while (2) not being biased by the shape of the Pareto front. Furthermore, (3) the set evaluation mechanisms provide indicative measures for local action selection and (4) the learned policies can be retrieved throughout the state and action space.",
            "keywords": [
                "multiple criteria analysis",
                "multi-objective",
                "reinforcement learning",
                "Pareto     sets"
            ],
            "author": [
                "Kristof Van Moffaert",
                "Ann Nowé"
            ],
            "ref": "http://jmlr.org/papers/volume15/vanmoffaert14a/vanmoffaert14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Seeded Graph Matching for Correlated Erdos-Renyi Graphs",
            "abstract": "Graph matching is an important problem in machine learning and pattern recognition. Herein, we present theoretical and practical results on the consistency of graph matching for estimating a latent alignment function between the vertex sets of two graphs, as well as subsequent algorithmic implications when the latent alignment is partially observed. In the correlated ErdÅs-RÃ©nyi graph setting, we prove that graph matching provides a strongly consistent estimate of the latent alignment in the presence of even modest correlation. We then investigate a tractable, restricted-focus version of graph matching, which is only concerned with adjacency involving vertices in a partial observation of the latent alignment; we prove that a logarithmic number of vertices whose alignment is known is sufficient for this restricted-focus version of graph matching to yield a strongly consistent estimate of the latent alignment of the remaining vertices. We show how Frank-Wolfe methodology for approximate graph matching, when there is a partially observed latent alignment, inherently incorporates this restricted-focus graph matching. Lastly, we illustrate the relationship between seeded graph matching and restricted-focus graph matching by means of an illuminating example from human connectomics.",
            "keywords": [
                "graph matching"
            ],
            "author": [
                "Vince Lyzinski",
                "Donniell E. Fishkind",
                "Carey E. Priebe"
            ],
            "ref": "http://jmlr.org/papers/volume15/lyzinski14a/lyzinski14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "New Results for Random Walk Learning",
            "abstract": "In a very strong positive result for passive learning algorithms, Bshouty et al. showed that DNF expressions are efficiently learnable in the uniform random walk model. It is natural to ask whether the more expressive class of thresholds of parities (TOP) can also be learned efficiently in this model, since both DNF and TOP are efficiently uniform-learnable from queries. However, the time bounds of the algorithms of Bshouty et al. are exponential for TOP. We present a new approach to weak parity learning that leads to quasi-efficient uniform random walk learnability of TOP. We also introduce a more general random walk model and give two positive results in this new model: DNF is efficiently learnable and juntas are efficiently agnostically learnable.",
            "keywords": [
                "computational learning theory",
                "Fourier analysis of Boolean functions",
                "random     walks",
                "DNF learning"
            ],
            "author": [
                "Jeffrey C. Jackson",
                "Karl Wimmer"
            ],
            "ref": "http://jmlr.org/papers/volume15/jackson14a/jackson14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Transfer Learning Decision Forests for Gesture Recognition",
            "abstract": "Decision forests are an increasingly popular tool in computer vision problems. Their advantages include high computational efficiency, state-of-the-art accuracy and multi-class support. In this paper, we present a novel method for transfer learning which uses decision forests, and we apply it to recognize gestures and characters. We introduce two mechanisms into the decision forest framework in order to transfer knowledge from the source tasks to a given target task. The first one is mixed information gain, which is a data-based regularizer. The second one is label propagation, which infers the manifold structure of the feature space. We show that both of them are important to achieve higher accuracy. Our experiments demonstrate improvements over traditional decision forests in the ChaLearn Gesture Challenge and MNIST data set. They also compare favorably against other state-of-the-art classifiers.",
            "keywords": [
                "decision forests",
                "transfer learning"
            ],
            "author": [
                "Norberto A. Goussies",
                "Sebastián Ubalde",
                "Marta Mejail"
            ],
            "ref": "http://jmlr.org/papers/volume15/goussies14a/goussies14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Semi-Supervised Eigenvectors for Large-Scale Locally-Biased Learning",
            "abstract": "",
            "keywords": [
                "semi-supervised learning",
                "spectral clustering",
                "kernel methods",
                "large-scale     machine learning",
                "local spectral methods"
            ],
            "author": [
                "Toke J. Hansen",
                "Michael W. Mahoney"
            ],
            "ref": "http://jmlr.org/papers/volume15/hansen14a/hansen14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits",
            "abstract": "BayesOpt is a library with state-of-the-art Bayesian optimization methods to solve nonlinear optimization, stochastic bandits or sequential experimental design problems. Bayesian optimization characterized for being sample efficient as it builds a posterior distribution to capture the evidence and prior knowledge of the target function. Built in standard C++, the library is extremely efficient while being portable and flexible. It includes a common interface for C, C++, Python, Matlab and Octave.",
            "keywords": [
                "Bayesian optimization",
                "efficient global optimization",
                "sequential model-based     optimization",
                "sequential experimental design"
            ],
            "author": [
                "Ruben Martinez-Cantin"
            ],
            "ref": "http://jmlr.org/papers/volume15/martinezcantin14a/martinezcantin14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Order-Independent Constraint-Based Causal Structure Learning",
            "abstract": "We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al., 1993, 2000; Richardson, 1996; Colombo et al., 2012; Claassen et al., 2013). The first step of all these algorithms consists of the adjacency search of the PC-algorithm. The PC-algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low- dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modifications of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modifications are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modifications in simulation studies and on a yeast gene expression data set. We show that our modifications yield similar performance in low- dimensional settings and improved performance in high- dimensional settings. All software is implemented in the R-package pcalg.",
            "keywords": [
                "directed acyclic graph",
                "PC-algorithm",
                "FCI-algorithm",
                "CCD-algorithm",
                "order-     dependence",
                "consistency"
            ],
            "author": [
                "Diego Colombo",
                "Marloes H. Maathuis"
            ],
            "ref": "http://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Effective Sampling and Learning for Mallows Models with Pairwise-Preference Data",
            "abstract": "Learning preference distributions is a critical problem in many areas (e.g., recommender systems, IR, social choice). However, many existing learning and inference methods impose restrictive assumptions on the form of user preferences that can be admitted as evidence. We relax these restrictions by considering as data arbitrary pairwise comparisons of alternatives, which represent the fundamental building blocks of ordinal rankings. We develop the first algorithms for learning Mallows models (and mixtures thereof) from pairwise comparison data. At the heart of our technique is a new algorithm, the generalized repeated insertion model (GRIM), which allows sampling from arbitrary ranking distributions, and conditional Mallows models in particular. While we show that sampling from a Mallows model with pairwise evidence is computationally difficult in general, we develop approximate samplers that are exact for many important special cases--and have provable bounds with pairwise evidence--and derive algorithms for evaluating log-likelihood, learning Mallows mixtures, and non-parametric estimation. Experiments on real-world data sets demonstrate the effectiveness of our approach. (Some parts of this paper appeared in: T. Lu and C. Boutilier, Learning Mallows Models with Pairwise Preferences, Proceedings of the Twenty- Eighth International Conference on Machine Learning (ICML 2011), pp.145-152, Bellevue, WA (2011).)",
            "keywords": [
                "preference learning",
                "ranking",
                "incomplete data",
                "Mallows models"
            ],
            "author": [
                "Tyler Lu",
                "Craig Boutilier"
            ],
            "ref": "http://jmlr.org/papers/volume15/lu14a/lu14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Hierarchical Clustering",
            "abstract": "One of the most widely used techniques for data clustering is agglomerative clustering. Such algorithms have been long used across many different fields ranging from computational biology to social sciences to computer vision in part because their output is easy to interpret. Unfortunately, it is well known, however, that many of the classic agglomerative clustering algorithms are not robust to noise. In this paper we propose and analyze a new robust algorithm for bottom-up agglomerative clustering. We show that our algorithm can be used to cluster accurately in cases where the data satisfies a number of natural properties and where the traditional agglomerative algorithms fail. We also show how to adapt our algorithm to the inductive setting where our given data is only a small random sample of the entire data set. Experimental evaluations on synthetic and real world data sets show that our algorithm achieves better performance than other hierarchical algorithms in the presence of noise.",
            "keywords": [
                "unsupervised learning",
                "clustering",
                "agglomerative algorithms"
            ],
            "author": [
                "Maria-Florina Balcan",
                "Yingyu Liang",
                "Pramod Gupta"
            ],
            "ref": "http://jmlr.org/papers/volume15/balcan14a/balcan14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parallelizing Exploration-Exploitation Tradeoffs in Gaussian Process Bandit Optimization",
            "abstract": "How can we take advantage of opportunities for experimental parallelization in exploration-exploitation tradeoffs? In many experimental scenarios, it is often desirable to execute experiments simultaneously or in batches, rather than only performing one at a time. Additionally, observations may be both noisy and expensive. We introduce Gaussian Process Batch Upper Confidence Bound (GP-BUCB), an upper confidence bound-based algorithm, which models the reward function as a sample from a Gaussian process and which can select batches of experiments to run in parallel. We prove a general regret bound for GP-BUCB, as well as the surprising result that for some common kernels, the asymptotic average regret can be made independent of the batch size. The GP-BUCB algorithm is also applicable in the related case of a delay between initiation of an experiment and observation of its results, for which the same regret bounds hold. We also introduce Gaussian Process Adaptive Upper Confidence Bound (GP-AUCB), a variant of GP-BUCB which can exploit parallelism in an adaptive manner. We evaluate GP-BUCB and GP-AUCB on several simulated and real data sets. These experiments show that GP-BUCB and GP-AUCB are competitive with state-of-the-art heuristics. (A previous version of this work appeared in the Proceedings of the 29th International Conference on Machine Learning, 2012.)",
            "keywords": [
                "Gaussian process",
                "upper confidence bound",
                "batch",
                "active learning"
            ],
            "author": [
                "Thomas Desautels",
                "Andreas Krause",
                "Joel W. Burdick"
            ],
            "ref": "http://jmlr.org/papers/volume15/desautels14a/desautels14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Imitation Learning: Formal and Practical Reductions to I.I.D. Learning",
            "abstract": "In standard passive imitation learning, the goal is to learn a policy that performs as well as a target policy by passively observing full execution trajectories of it. Unfortunately, generating such trajectories can require substantial expert effort and be impractical in some cases. In this paper, we consider active imitation learning with the goal of reducing this effort by querying the expert about the desired action at individual states, which are selected based on answers to past queries and the learner's interactions with an environment simulator. We introduce a new approach based on reducing active imitation learning to active i.i.d. learning, which can leverage progress in the i.i.d. setting. Our first contribution is to analyze reductions for both non-stationary and stationary policies, showing for the first time that the label complexity (number of queries) of active imitation learning can be less than that of passive learning. Our second contribution is to introduce a practical algorithm inspired by the reductions, which is shown to be highly effective in five test domains compared to a number of alternatives.",
            "keywords": [
                "imitation learning",
                "active learning",
                "active imitation learning"
            ],
            "author": [
                "Kshitij Judah",
                "Alan P. Fern",
                "Thomas G. Dietterich",
                "Prasad Tadepalli"
            ],
            "ref": "http://jmlr.org/papers/volume15/judah14a/judah14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling",
            "abstract": "It has become increasingly popular to obtain machine learning labels through commercial crowdsourcing services. The crowdsourcing workers or annotators are paid for each label they provide, but the task requester usually has only a limited amount of the budget. Since the data instances have different levels of labeling difficulty and the workers have different reliability for the labeling task, it is desirable to wisely allocate the budget among all the instances and workers such that the overall labeling quality is maximized. In this paper, we formulate the budget allocation problem as a Bayesian Markov decision process (MDP), which simultaneously conducts learning and decision making. The optimal allocation policy can be obtained by using the dynamic programming (DP) recurrence. However, DP quickly becomes computationally intractable when the size of the problem increases. To solve this challenge, we propose a computationally efficient approximate policy which is called optimistic knowledge gradient. Our method applies to both pull crowdsourcing marketplaces with homogeneous workers and push marketplaces with heterogeneous workers. It can also incorporate the contextual information of instances when they are available. The experiments on both simulated and real data show that our policy achieves a higher labeling quality than other existing policies at the same budget level.",
            "keywords": [
                "crowdsourcing",
                "budget allocation",
                "Markov decision process",
                "dynamic pro-     gramming"
            ],
            "author": [
                "Xi Chen",
                "Qihang Lin",
                "Dengyong Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume16/chen15a/chen15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition",
            "abstract": "In multi-response regression, pursuit of two different types of structures is essential to battle the curse of dimensionality. In this paper, we seek a sparsest decomposition representation of a parameter matrix in terms of a sum of sparse and low rank matrices, among many overcomplete decompositions. On this basis, we propose a constrained method subject to two nonconvex constraints, respectively for sparseness and low-rank properties. Computationally, obtaining an exact global optimizer is rather challenging. To overcome the difficulty, we use an alternating directions method solving a low-rank subproblem and a sparseness subproblem alternatively, where we derive an exact solution to the low-rank subproblem, as well as an exact solution in a special case and an approximated solution generally through a surrogate of the -constraint and difference convex programming, for the sparse subproblem. Theoretically, we establish convergence rates of a global minimizer in the Hellinger-distance, providing an insight into why pursuit of two different types of decomposed structures is expected to deliver higher estimation accuracy than its counterparts based on either sparseness alone or low-rank approximation alone. Numerical examples are given to illustrate these aspects, in addition to an application to facial imagine recognition and multiple time series analysis.",
            "keywords": [
                "blockwise decent",
                "nonconvex minimization",
                "matrix decomposition"
            ],
            "author": [
                "Qi Yan",
                "Jieping Ye",
                "Xiaotong Shen"
            ],
            "ref": "http://jmlr.org/papers/volume16/yan15a/yan15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Topological Data Analysis using Persistence Landscapes",
            "abstract": "We define a new topological summary for data that we call the persistence landscape. Since this summary lies in a vector space, it is easy to combine with tools from statistics and machine learning, in contrast to the standard topological summaries. Viewed as a random variable with values in a Banach space, this summary obeys a strong law of large numbers and a central limit theorem. We show how a number of standard statistical tests can be used for statistical inference using this summary. We also prove that this summary is stable and that it can be used to provide lower bounds for the bottleneck and Wasserstein distances.",
            "keywords": [
                "topological data analysis",
                "statistical topology",
                "persistent homology",
                "topolog-     ical summary"
            ],
            "author": [
                "Peter Bubenik"
            ],
            "ref": "http://jmlr.org/papers/volume16/bubenik15a/bubenik15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Links Between Multiplicity Automata, Observable Operator Models and Predictive State Representations -- a Unified Learning Framework",
            "abstract": "",
            "keywords": [
                "multiplicity automata",
                "hidden Markov models",
                "observable operator models",
                "predictive state representations"
            ],
            "author": [
                "Michael Thon",
                "Herbert Jaeger"
            ],
            "ref": "http://jmlr.org/papers/volume16/thon15a/thon15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SAMOA: Scalable Advanced Massive Online Analysis",
            "abstract": "SAMOA (Scalable Advanced Massive Online Analysis) is a platform for mining big data streams. It provides a collection of distributed streaming algorithms for the most common data mining and machine learning tasks such as classification, clustering, and regression, as well as programming abstractions to develop new algorithms. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as Storm, S4, and Samza. SAMOA is written in Java, is open source, and is available at samoa-project.net under the Apache Software License version 2.0.",
            "keywords": [
                "data streams",
                "distributed systems",
                "classification",
                "clustering",
                "regression",
                "tool-     box"
            ],
            "author": [
                "Gianmarco De Francisci Morales",
                "Albert Bifet"
            ],
            "ref": "http://jmlr.org/papers/volume16/morales15a/morales15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Learning via Sequential Complexities",
            "abstract": "We consider the problem of sequential prediction and provide tools to study the minimax value of the associated game. Classical statistical learning theory provides several useful complexity measures to study learning with i.i.d. data. Our proposed sequential complexities can be seen as extensions of these measures to the sequential setting. The developed theory is shown to yield precise learning guarantees for the problem of sequential prediction. In particular, we show necessary and sufficient conditions for online learnability in the setting of supervised learning. Several examples show the utility of our framework: we can establish learnability without having to exhibit an explicit online learning algorithm.",
            "keywords": [
                "online learning",
                "sequential complexities"
            ],
            "author": [
                "Alexander Rakhlin",
                "Karthik Sridharan",
                "Ambuj Tewari"
            ],
            "ref": "http://jmlr.org/papers/volume16/rakhlin15a/rakhlin15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-layered Gesture Recognition with Kinect",
            "abstract": "This paper proposes a novel multi-layered gesture recognition method with Kinect. We explore the essential linguistic characters of gestures: the components concurrent character and the sequential organization character, in a multi-layered framework, which extracts features from both the segmented semantic units and the whole gesture sequence and then sequentially classifies the motion, location and shape components. In the first layer, an improved principle motion is applied to model the motion component. In the second layer, a particle-based descriptor and a weighted dynamic time warping are proposed for the location component classification. In the last layer, the spatial path warping is further proposed to classify the shape component represented by unclosed shape context. The proposed method can obtain relatively high performance for one-shot learning gesture recognition on the ChaLearn Gesture Dataset comprising more than 50, 000 gesture sequences recorded with Kinect.",
            "keywords": [
                "gesture recognition",
                "Kinect",
                "linguistic characters",
                "multi-layered classification",
                "principle motion"
            ],
            "author": [
                "Feng Jiang",
                "Shengping Zhang",
                "Shen Wu",
                "Yang Gao",
                "Debin Zhao"
            ],
            "ref": "http://jmlr.org/papers/volume16/jiang15a/jiang15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multimodal Gesture Recognition via Multiple Hypotheses Rescoring",
            "abstract": "We present a new framework for multimodal gesture recognition that is based on a multiple hypotheses rescoring fusion scheme. We specifically deal with a demanding Kinect-based multimodal data set, introduced in a recent gesture recognition challenge (ChaLearn 2013), where multiple subjects freely perform multimodal gestures. We employ multiple modalities, that is, visual cues, such as skeleton data, color and depth images, as well as audio, and we extract feature descriptors of the hands' movement, handshape, and audio spectral properties. Using a common hidden Markov model framework we build single-stream gesture models based on which we can generate multiple single stream-based hypotheses for an unknown gesture sequence. By multimodally rescoring these hypotheses via constrained decoding and a weighted combination scheme, we end up with a multimodally-selected best hypothesis. This is further refined by means of parallel fusion of the monomodal gesture models applied at a segmental level. In this setup, accurate gesture modeling is proven to be critical and is facilitated by an activity detection system that is also presented. The overall approach achieves 93.3% gesture recognition accuracy in the ChaLearn Kinect-based multimodal data set, significantly outperforming all recently published approaches on the same challenging multimodal gesture recognition task, providing a relative error rate reduction of at least 47.6%.",
            "keywords": [
                "multimodal gesture recognition",
                "HMMs",
                "speech recognition",
                "multimodal     fusion"
            ],
            "author": [
                "Vassilis Pitsikalis",
                "Athanasios Katsamanis",
                "Stavros Theodorakis",
                "Petros Maragos"
            ],
            "ref": "http://jmlr.org/papers/volume16/pitsikalis15a/pitsikalis15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Asynchronous Parallel Stochastic Coordinate Descent Algorithm",
            "abstract": "We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate () on general convex functions. Near-linear speedup on a multicore system can be expected if the number of processors is  in unconstrained optimization and  in the separable- constrained case, where  is the number of variables. We describe results from implementation on 40-core processors.",
            "keywords": [
                "asynchronous parallel optimization"
            ],
            "author": [
                "Ji Liu",
                "Stephen J. Wright",
                "Christopher Ré",
                "Victor Bittorf",
                "Srikrishna Sridhar"
            ],
            "ref": "http://jmlr.org/papers/volume16/liu15a/liu15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Composite Self-Concordant Minimization",
            "abstract": "We propose a variable metric framework for minimizing the sum of a self-concordant function and a possibly non-smooth convex function, endowed with an easily computable proximal operator. We theoretically establish the convergence of our framework without relying on the usual Lipschitz gradient assumption on the smooth part. An important highlight of our work is a new set of analytic step-size selection and correction procedures based on the structure of the problem. We describe concrete algorithmic instances of our framework for several interesting applications and demonstrate them numerically on both synthetic and real data.",
            "keywords": [],
            "author": [
                "Quoc Tran-Dinh",
                "Anastasios Kyrillidis",
                "Volkan Cevher"
            ],
            "ref": "http://jmlr.org/papers/volume16/trandihn15a/trandihn15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Network Granger Causality with Inherent Grouping Structure",
            "abstract": "The problem of estimating high-dimensional network models arises naturally in the analysis of many biological and socio-economic systems. In this work, we aim to learn a network structure from temporal panel data, employing the framework of Granger causal models under the assumptions of sparsity of its edges and inherent grouping structure among its nodes. To that end, we introduce a group lasso regression regularization framework, and also examine a thresholded variant to address the issue of group misspecification. Further, the norm consistency and variable selection consistency of the estimates are established, the latter under the novel concept of direction consistency. The performance of the proposed methodology is assessed through an extensive set of simulation studies and comparisons with existing techniques. The study is illustrated on two motivating examples coming from functional genomics and financial econometrics.",
            "keywords": [
                "Granger causality",
                "high dimensional networks",
                "panel vector autoregression     model",
                "group lasso"
            ],
            "author": [
                "Sumanta Basu",
                "Ali Shojaie",
                "George Michailidis"
            ],
            "ref": "http://jmlr.org/papers/volume16/basu15a/basu15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Iterative and Active Graph Clustering Using Trace Norm Minimization Without Cluster Size Constraints",
            "abstract": "This paper investigates graph clustering under the planted partition model in the presence of small clusters. Traditional results dictate that for an algorithm to provably correctly recover the underlying clusters, all clusters must be sufficiently large---in particular, the cluster sizes need to be , where  is the number of nodes of the graph. We show that this is not really a restriction: by a refined analysis of a convex-optimization-based recovery approach, we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones. Based on this result, we further devise an iterative algorithm to provably recover almost all clusters via a âpeeling strategyâ: we recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the partial observation setting, in which only a (chosen) part of the graph is observed. The peeling strategy gives rise to an active learning algorithm, in which edges adjacent to smaller clusters are queried more often after large clusters are learned (and removed). We expect that the idea of iterative peeling---that is, sequentially identifying a subset of the clusters and reducing the problem to a smaller one---is useful more broadly beyond the specific implementations (based on convex optimization) used in this paper.",
            "keywords": [
                "graph clustering",
                "community detection",
                "active clustering",
                "convex optimiza-     tion",
                "planted partition model"
            ],
            "author": [
                "Nir Ailon",
                "Yudong Chen",
                "Huan Xu"
            ],
            "ref": "http://jmlr.org/papers/volume16/ailon15a/ailon15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Classification Module for Genetic Programming Algorithms in JCLEC",
            "abstract": "JCLEC-Classification is a usable and extensible open source library for genetic programming classification algorithms. It houses implementations of rule-based methods for classification based on genetic programming, supporting multiple model representations and providing to users the tools to implement any classifier easily. The software is written in Java and it is available from jclec.sourceforge.net/classification under the GPL license.",
            "keywords": [
                "classification",
                "evolutionary algorithms",
                "genetic programming"
            ],
            "author": [
                "Alberto Cano",
                "José María Luna",
                "Amelia Zafra",
                "Sebastián Ventura"
            ],
            "ref": "http://jmlr.org/papers/volume16/cano15a/cano15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "AD3: Alternating Directions Dual Decomposition for MAP Inference in Graphical Models",
            "abstract": "We present AD, a new algorithm for approximate maximum a posteriori (MAP) inference on factor graphs, based on the alternating directions method of multipliers. Like other dual decomposition algorithms, AD has a modular architecture, where local subproblems are solved independently, and their solutions are gathered to compute a global update. The key characteristic of AD is that each local subproblem has a quadratic regularizer, leading to faster convergence, both theoretically and in practice. We provide closed-form solutions for these AD subproblems for binary pairwise factors and factors imposing first-order logic constraints. For arbitrary factors (large or combinatorial), we introduce an active set method which requires only an oracle for computing a local MAP configuration, making AD applicable to a wide range of problems. Experiments on synthetic and real-world problems show that AD compares favorably with the state-of-the-art.",
            "keywords": [
                "MAP inference",
                "graphical models",
                "dual decomposition"
            ],
            "author": [
                "André F. T. Martins",
                "Mário A. T. Figueiredo",
                "Pedro M. Q. Aguiar",
                "Noah A. Smith",
                "Eric P. Xing"
            ],
            "ref": "http://jmlr.org/papers/volume16/martins15a/martins15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Introducing CURRENNT: The Munich Open-Source CUDA RecurREnt Neural Network Toolkit",
            "abstract": "In this article, we introduce CURRENNT, an open-source parallel implementation of deep recurrent neural networks (RNNs) supporting graphics processing units (GPUs) through NVIDIA's Computed Unified Device Architecture (CUDA). CURRENNT supports uni- and bidirectional RNNs with Long Short-Term Memory (LSTM) memory cells which overcome the vanishing gradient problem. To our knowledge, CURRENNT is the first publicly available parallel implementation of deep LSTM-RNNs. Benchmarks are given on a noisy speech recognition task from the 2013 2nd CHiME Speech Separation and Recognition Challenge, where LSTM-RNNs have been shown to deliver best performance. In the result, double digit speedups in bidirectional LSTM training are achieved with respect to a reference single-threaded CPU implementation. CURRENNT is available under the GNU General Public License from http://sourceforge.net/p/currennt.",
            "keywords": [
                "parallel computing",
                "deep neural networks",
                "recurrent neural networks"
            ],
            "author": [
                "Felix Weninger"
            ],
            "ref": "http://jmlr.org/papers/volume16/weninger15a/weninger15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The flare Package for High Dimensional Linear Regression and Precision Matrix Estimation in R",
            "abstract": "This paper describes an R package named flare, which implements a family of new high dimensional regression methods (LAD Lasso, SQRT Lasso,  Lasso, and Dantzig selector) and their extensions to sparse precision matrix estimation (TIGER and CLIME). These methods exploit different nonsmooth loss functions to gain modeling flexibility, estimation robustness, and tuning insensitiveness. The developed solver is based on the alternating direction method of multipliers (ADMM). The package flare is coded in double precision C, and called from R by a user-friendly interface. The memory usage is optimized by using the sparse matrix output. The experiments show that flare is efficient and can scale up to large problems.",
            "keywords": [
                "sparse linear regression",
                "sparse precision matrix estimation",
                "alternating di-     rection method of multipliers",
                "robustness"
            ],
            "author": [
                "Xingguo Li",
                "Tuo Zhao",
                "Xiaoming Yuan",
                "Han Liu"
            ],
            "ref": "http://jmlr.org/papers/volume16/li15a/li15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized M-estimators with Nonconvexity: Statistical and Algorithmic Theory for Local Optima",
            "abstract": "We provide novel theoretical results regarding local optima of regularized -estimators, allowing for nonconvexity in both loss and penalty functions. Under restricted strong convexity on the loss and suitable regularity conditions on the penalty, we prove that any stationary point of the composite objective function will lie within statistical precision of the underlying parameter vector. Our theory covers many nonconvex objective functions of interest, including the corrected Lasso for errors-in-variables linear models; regression for generalized linear models with nonconvex penalties such as SCAD, MCP, and capped-; and high-dimensional graphical model estimation. We quantify statistical accuracy by providing bounds on the -, -, and prediction error between stationary points and the population-level optimum. We also propose a simple modification of composite gradient descent that may be used to obtain a near-global optimum within statistical precision  in  steps, which is the fastest possible rate of any first-order method. We provide simulation studies illustrating the sharpness of our theoretical results.",
            "keywords": [
                "high-dimensional statistics",
                "M -estimation",
                "model selection",
                "nonconvex opti-     mization"
            ],
            "author": [
                "Po-Ling Loh",
                "Martin J. Wainwright"
            ],
            "ref": "http://jmlr.org/papers/volume16/loh15a/loh15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Hierarchical Kernel Learning",
            "abstract": "This paper generalizes the framework of Hierarchical Kernel Learning (HKL) and illustrates its utility in the domain of rule learning. HKL involves Multiple Kernel Learning over a set of given base kernels assumed to be embedded on a directed acyclic graph. This paper proposes a two-fold generalization of HKL: the first is employing a generic  block-norm regularizer () that alleviates a key limitation of the HKL formulation. The second is a generalization to the case of multi-class, multi-label and more generally, multi-task applications. The main technical contribution of this work is the derivation of a highly specialized partial dual of the proposed generalized HKL formulation and an efficient mirror descent based active set algorithm for solving it. Importantly, the generic regularizer enables the proposed formulation to be employed in the Rule Ensemble Learning (REL) where the goal is to construct an ensemble of conjunctive propositional rules. Experiments on benchmark REL data sets illustrate the efficacy of the proposed generalizations.",
            "keywords": [
                "multiple kernel learning",
                "mixed-norm regularization",
                "multi-task learning",
                "rule     ensemble learning"
            ],
            "author": [
                "Pratik Jawanpuria",
                "Jagarlapudi Saketha Nath",
                "Ganesh Ramakrishnan"
            ],
            "ref": "http://jmlr.org/papers/volume16/jawanpuria15a/jawanpuria15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Discrete Restricted Boltzmann Machines",
            "abstract": "We describe discrete restricted Boltzmann machines: probabilistic graphical models with bipartite interactions between visible and hidden discrete variables. Examples are binary restricted Boltzmann machines and discrete naÃ¯ve Bayes models. We detail the inference functions and distributed representations arising in these models in terms of configurations of projected products of simplices and normal fans of products of simplices. We bound the number of hidden variables, depending on the cardinalities of their state spaces, for which these models can approximate any probability distribution on their visible states to any given accuracy. In addition, we use algebraic methods and coding theory to compute their dimension.",
            "keywords": [
                "restricted Boltzmann machine"
            ],
            "author": [
                "Guido Montúfar",
                "Jason Morton"
            ],
            "ref": "http://jmlr.org/papers/volume16/montufar15a/montufar15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Evolving GPU Machine Code",
            "abstract": "Parallel Graphics Processing Unit (GPU) implementations of GP have appeared in the literature using three main methodologies: (i) compilation, which generates the individuals in GPU code and requires compilation; (ii) pseudo-assembly, which generates the individuals in an intermediary assembly code and also requires compilation; and (iii) interpretation, which interprets the codes. This paper proposes a new methodology that uses the concepts of quantum computing and directly handles the GPU machine code instructions. Our methodology utilizes a probabilistic representation of an individual to improve the global search capability. In addition, the evolution in machine code eliminates both the overhead of compiling the code and the cost of parsing the program during evaluation. We obtained up to 2.74 trillion GP operations per second for the 20-bit Boolean Multiplexer benchmark. We also compared our approach with the other three GPU-based acceleration methodologies implemented for quantum-inspired linear GP. Significant gains in performance were obtained.",
            "keywords": [
                "genetic programming",
                "graphics processing units"
            ],
            "author": [
                "Cleomar Pereira da Silva",
                "Douglas Mota Dias",
                "Cristiana Bentes",
                "Marco Aur\\'{e}lio Cavalcanti Pacheco",
                "Le",
                "ro Fontoura Cupertino"
            ],
            "ref": "http://jmlr.org/papers/volume16/dasilva15a/dasilva15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Compression Technique for Analyzing Disagreement-Based Active Learning",
            "abstract": "We introduce a new and improved characterization of the label complexity of disagreement-based active learning, in which the leading quantity is the version space compression set size. This quantity is defined as the size of the smallest subset of the training data that induces the same version space. We show various applications of the new characterization, including a tight analysis of CAL and refined label complexity bounds for linear separators under mixtures of Gaussians and axis-aligned rectangles under product densities. The version space compression set size, as well as the new characterization of the label complexity, can be naturally extended to agnostic learning problems, for which we show new speedup results for two well known active learning algorithms.",
            "keywords": [
                "active learning",
                "selective sampling",
                "sequential design",
                "statistical learning theory",
                "PAC     learning",
                "sample complexity"
            ],
            "author": [
                "Yair Wiener",
                "Steve Hanneke",
                "Ran El-Yaniv"
            ],
            "ref": "http://jmlr.org/papers/volume16/wiener15a/wiener15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Response-Based Approachability with Applications to Generalized No-Regret Problems",
            "abstract": "Blackwell's theory of approachability provides fundamental results for repeated games with vector-valued payoffs, which have been usefully applied in the theory of learning in games, and in devising online learning algorithms in the adversarial setup. A target set  is approachable by a player (the agent) in such a game if he can ensure that the average payoff vector converges to , no matter what the opponent does. Blackwell provided two equivalent conditions for a convex set to be approachable. Standard approachability algorithms rely on the primal condition, which is a geometric separation condition, and essentially require to compute at each stage a projection direction from a certain point to . Here we introduce an approachability algorithm that relies on Blackwell's dual condition, which requires the agent to have a feasible  response to each mixed action of the opponent, namely a mixed action such that the expected payoff vector belongs to . Thus, rather than projections, the proposed algorithm relies on computing the response to a certain action of the opponent at each stage. We demonstrate the utility of the proposed approach by applying it to certain generalizations of the classical regret minimization problem, which incorporate side constraints, reward-to-cost criteria, and so-called global cost functions. In these extensions, computation of the projection is generally complex while the response is readily obtainable.",
            "keywords": [
                "approachability"
            ],
            "author": [
                "Andrey Bernstein",
                "Nahum Shimkin"
            ],
            "ref": "http://jmlr.org/papers/volume16/bernstein15a/bernstein15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Strong Consistency of the Prototype Based Clustering in Probabilistic Space",
            "abstract": "In this paper we formulate in general terms an approach to prove strong consistency of the Empirical Risk Minimisation inductive principle applied to the prototype or distance based clustering. This approach was motivated by the Divisive Information- Theoretic Feature Clustering model in probabilistic space with Kullback-Leibler divergence, which may be regarded as a special case within the Clustering Minimisation framework.",
            "keywords": [
                "clustering",
                "probabilistic space"
            ],
            "author": [
                "Vladimir Nikulin"
            ],
            "ref": "http://jmlr.org/papers/volume16/nikulin15a/nikulin15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm",
            "abstract": "We propose an extensive analysis of the behavior of majority votes in binary classification. In particular, we introduce a risk bound for majority votes, called the C-bound, that takes into account the average quality of the voters and their average disagreement. We also propose an extensive PAC-Bayesian analysis that shows how the C-bound can be estimated from various observations contained in the training data. The analysis intends to be self-contained and can be used as introductory material to PAC-Bayesian statistical learning theory. It starts from a general PAC-Bayesian perspective and ends with uncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback- Leibler divergence and others allow kernel functions to be used as voters (via the sample compression setting). Finally, out of the analysis, we propose the MinCq learning algorithm that basically minimizes the C-bound. MinCq reduces to a simple quadratic program. Aside from being theoretically grounded, MinCq achieves state-of-the-art performance, as shown in our extensive empirical comparison with both AdaBoost and the Support Vector Machine.",
            "keywords": [
                "majority vote",
                "ensemble methods",
                "learning theory",
                "PAC-Bayesian theory"
            ],
            "author": [
                "Pascal Germain",
                "Alexandre Lacasse",
                "Francois Laviolette",
                "Mario March",
                "Jean-Francis Roy"
            ],
            "ref": "http://jmlr.org/papers/volume16/germain15a/germain15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Statistical Perspective on Algorithmic Leveraging",
            "abstract": "One popular method for dealing with large-scale data sets is sampling. For example, by using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. This method has been successful in improving computational efficiency of algorithms for matrix problems such as least-squares approximation, least absolute deviations approximation, and low-rank matrix approximation. Existing work has focused on algorithmic issues such as worst-case running times and numerical issues associated with providing high-quality implementations, but none of it addresses statistical aspects of this method. In this paper, we provide a simple yet effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model with a fixed number of predictors. In particular, for several versions of leverage-based sampling, we derive results for the bias and variance, both conditional and unconditional on the observed data. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominates the other. This result is particularly striking, given the well-known result that, from the algorithmic perspective of worst-case analysis, leverage-based sampling provides uniformly superior worst-case algorithmic results, when compared with uniform sampling. Based on these theoretical results, we propose and analyze two new leveraging algorithms: one constructs a smaller least-squares problem with \"shrinkage\" leverage scores (SLEV), and the other solves a smaller and unweighted (or biased) least-squares problem (LEVUNW). A detailed empirical evaluation of existing leverage-based methods as well as these two new methods is carried out on both synthetic and real data sets. The empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage- based algorithms and that the new algorithms achieve improved performance. For example, with the same computation reduction as in the original algorithmic leveraging approach, our proposed SLEV typically leads to improved biases and variances both unconditionally and conditionally (on the observed data), and our proposed LEVUNW typically yields improved unconditional biases and variances.",
            "keywords": [],
            "author": [
                "Ping Ma",
                "Michael W. Mahoney",
                "Bin Yu"
            ],
            "ref": "http://jmlr.org/papers/volume16/ma15a/ma15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Matrix Completion and Robust Factorization",
            "abstract": "If learning methods are to scale to the massive sizes of modern data sets, it is essential for the field of machine learning to embrace parallel and distributed computing. Inspired by the recent development of matrix factorization methods with rich theory but poor computational complexity and by the relative ease of mapping matrices onto distributed architectures, we introduce a scalable divide-and-conquer framework for noisy matrix factorization. We present a thorough theoretical analysis of this framework in which we characterize the statistical errors introduced by the \"divide\" step and control their magnitude in the \"conquer\" step, so that the overall algorithm enjoys high-probability estimation guarantees comparable to those of its base algorithm. We also present experiments in collaborative filtering and video background modeling that demonstrate the near-linear to superlinear speed-ups attainable with this approach.",
            "keywords": [
                "collaborative filtering",
                "divide-and-conquer",
                "matrix completion",
                "matrix fac-      torization",
                "parallel and distributed algorithms",
                "randomized algorithms",
                "robust matrix fac-      torization"
            ],
            "author": [
                "Lester Mackey",
                "Ameet Talwalkar",
                "Michael I. Jordan"
            ],
            "ref": "http://jmlr.org/papers/volume16/mackey15a/mackey15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning with the Maximum Correntropy Criterion Induced Losses for Regression",
            "abstract": "Within the statistical learning framework, this paper studies the regression model associated with the correntropy induced losses. The correntropy, as a similarity measure, has been frequently employed in signal processing and pattern recognition. Motivated by its empirical successes, this paper aims at presenting some theoretical understanding towards the maximum correntropy criterion in regression problems. Our focus in this paper is two-fold: first, we are concerned with the connections between the regression model associated with the correntropy induced loss and the least squares regression model. Second, we study its convergence property. A learning theory analysis which is centered around the above two aspects is conducted. From our analysis, we see that the scale parameter in the loss function balances the convergence rates of the regression model and its robustness. We then make some efforts to sketch a general view on robust loss functions when being applied into the learning for regression problems. Numerical experiments are also implemented to verify the effectiveness of the model.",
            "keywords": [
                "correntropy",
                "the maximum correntropy criterion",
                "robust regression",
                "robust     loss function",
                "least squares regression"
            ],
            "author": [
                "Yunlong Feng",
                "Xiaolin Huang",
                "Lei Shi",
                "Yuning Yang",
                "Johan A.K. Suykens"
            ],
            "ref": "http://jmlr.org/papers/volume16/feng15a/feng15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Joint Estimation of Multiple Precision Matrices with Common Structures",
            "abstract": "Estimation of inverse covariance matrices, known as precision matrices, is important in various areas of statistical analysis. In this article, we consider estimation of multiple precision matrices sharing some common structures. In this setting, estimating each precision matrix separately can be suboptimal as it ignores potential common structures. This article proposes a new approach to parameterize each precision matrix as a sum of common and unique components and estimate multiple precision matrices in a constrained  minimization framework. We establish both estimation and selection consistency of the proposed estimator in the high dimensional setting. The proposed estimator achieves a faster convergence rate for the common structure in certain cases. Our numerical examples demonstrate that our new estimator can perform better than several existing methods in terms of the entropy loss and Frobenius loss. An application to a glioblastoma cancer data set reveals some interesting gene networks across multiple cancer subtypes.",
            "keywords": [
                "covariance matrix",
                "graphical model",
                "high dimension",
                "joint estimation"
            ],
            "author": [
                "Wonyul Lee",
                "Yufeng Liu"
            ],
            "ref": "http://jmlr.org/papers/volume16/lee15a/lee15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lasso Screening Rules via Dual Polytope Projection",
            "abstract": "Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large- scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have  components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose an efficient and effective screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there is currently no exact screening rule for group Lasso. We have evaluated our screening rule using synthetic and real data sets. Results show that our rule is more effective in identifying inactive predictors than existing state-of-the-art screening rules for Lasso.",
            "keywords": [
                "lasso",
                "safe screening",
                "sparse regularization",
                "polytope projection",
                "dual formu-     lation"
            ],
            "author": [
                "Jie Wang",
                "Peter Wonka",
                "Jieping Ye"
            ],
            "ref": "http://jmlr.org/papers/volume16/wang15a/wang15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Cross-Validation via Sequential Testing",
            "abstract": "With the increasing size of today's data sets, finding the right parameter configuration in model selection via cross-validation can be an extremely time-consuming task. In this paper we propose an improved cross-validation procedure which uses nonparametric testing coupled with sequential analysis to determine the best parameter set on linearly increasing subsets of the data. By eliminating underperforming candidates quickly and keeping promising candidates as long as possible, the method speeds up the computation while preserving the power of the full cross-validation. Theoretical considerations underline the statistical power of our procedure. The experimental evaluation shows that our method reduces the computation time by a factor of up to 120 compared to a full cross-validation with a negligible impact on the accuracy.",
            "keywords": [
                "cross-validation",
                "statistical testing"
            ],
            "author": [
                "Tammo Krueger",
                "Danny Panknin",
                "Mikio Braun"
            ],
            "ref": "http://jmlr.org/papers/volume16/krueger15a/krueger15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning the Structure and Parameters of Large-Population Graphical Games from Behavioral Data",
            "abstract": "We consider learning, from strictly behavioral data, the structure and parameters of linear influence games (LIGs), a class of parametric graphical games introduced by Irfan and Ortiz (2014). LIGs facilitate causal strategic inference (CSI): Making inferences from causal interventions on stable behavior in strategic settings. Applications include the identification of the most influential individuals in large (social) networks. Such tasks can also support policy-making analysis. Motivated by the computational work on LIGs, we cast the learning problem as maximum-likelihood estimation (MLE) of a generative model defined by pure-strategy Nash equilibria (PSNE). Our simple formulation uncovers the fundamental interplay between goodness-of-fit and model complexity: good models capture equilibrium behavior within the data while controlling the true number of equilibria, including those unobserved. We provide a generalization bound establishing the sample complexity for MLE in our framework. We propose several algorithms including convex loss minimization (CLM) and sigmoidal approximations. We prove that the number of exact PSNE in LIGs is small, with high probability; thus, CLM is sound. We illustrate our approach on synthetic data and real-world U.S. congressional voting records. We briefly discuss our learning framework's generality and potential applicability to general graphical games.",
            "keywords": [
                "linear influence games",
                "graphical games",
                "structure and parameter learning"
            ],
            "author": [
                "Jean Honorio",
                "Luis Ortiz"
            ],
            "ref": "http://jmlr.org/papers/volume16/honorio15a/honorio15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Identification of Overcomplete Dictionaries",
            "abstract": "This paper presents the first theoretical results showing that stable identification of overcomplete -coherent dictionaries  is locally possible from training signals with sparsity levels  up to the order  and signal to noise ratios up to . In particular the dictionary is recoverable as the local maximum of a new maximization criterion that generalizes the K-means criterion. For this maximization criterion results for asymptotic exact recovery for sparsity levels up to  and stable recovery for sparsity levels up to  as well as signal to noise ratios up to  are provided. These asymptotic results translate to finite sample size recovery results with high probability as long as the sample size  scales as , where the recovery precision  can go down to the asymptotically achievable precision. Further, to actually find the local maxima of the new criterion, a very simple Iterative Thresholding and K (signed) Means algorithm (ITKM), which has complexity  in each iteration, is presented and its local efficiency is demonstrated in several experiments.",
            "keywords": [
                "dictionary learning",
                "dictionary identification",
                "sparse coding",
                "sparse compo-     nent analysis",
                "vector quantization",
                "K-means",
                "finite sample size",
                "sample complexity",
                "maxi-     mization criterion"
            ],
            "author": [
                "Karin Schnass"
            ],
            "ref": "http://jmlr.org/papers/volume16/schnass15a/schnass15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Encog: Library of Interchangeable Machine Learning Models for Java and C#",
            "abstract": "This paper introduces the Encog library for Java and C#, a scalable, adaptable, multi-platform machine learning framework that was first released in 2008. Encog allows a variety of machine learning models to be applied to data sets using regression, classification, and clustering. Various supported machine learning models can be used interchangeably with minimal recoding. Encog uses efficient multithreaded code to reduce training time by exploiting modern multicore processors. The current version of Encog can be downloaded from www.encog.org.",
            "keywords": [
                "Java"
            ],
            "author": [
                "Jeff Heaton"
            ],
            "ref": "http://jmlr.org/papers/volume16/heaton15a/heaton15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Perturbed Message Passing for Constraint Satisfaction Problems",
            "abstract": "We introduce an efficient message passing scheme for solving Constraint Satisfaction Problems (CSPs), which uses stochastic perturbation of Belief Propagation (BP) and Survey Propagation (SP) messages to bypass decimation and directly produce a single satisfying assignment. Our first CSP solver, called Perturbed Belief Propagation, smoothly interpolates two well-known inference procedures; it starts as BP and ends as a Gibbs sampler, which produces a single sample from the set of solutions. Moreover we apply a similar perturbation scheme to SP to produce another CSP solver, Perturbed Survey Propagation. Experimental results on random and real-world CSPs show that Perturbed BP is often more successful and at the same time tens to hundreds of times more efficient than standard BP guided decimation. Perturbed BP also compares favorably with state-of-the-art SP-guided decimation, which has a computational complexity that generally scales exponentially worse than our method (w.r.t. the cardinality of variable domains and constraints). Furthermore, our experiments with random satisfiability and coloring problems demonstrate that Perturbed SP can outperform SP-guided decimation, making it the best incomplete random CSP-solver in difficult regimes.",
            "keywords": [
                "constraint satisfaction problem",
                "message passing",
                "belief propagation",
                "survey     propagation",
                "Gibbs sampling"
            ],
            "author": [
                "Siamak Ravanbakhsh",
                "Russell  Greiner"
            ],
            "ref": "http://jmlr.org/papers/volume16/ravanbakhsh15a/ravanbakhsh15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Sparse Low-Threshold Linear Classifiers",
            "abstract": "We consider the problem of learning a non-negative linear classifier with a -norm of at most , and a fixed threshold, under the hinge-loss. This problem generalizes the problem of learning a -monotone disjunction. We prove that we can learn efficiently in this setting, at a rate which is linear in both  and the size of the threshold, and that this is the best possible rate. We provide an efficient online learning algorithm that achieves the optimal rate, and show that in the batch case, empirical risk minimization achieves this rate as well. The rates we show are tighter than the uniform convergence rate, which grows with .",
            "keywords": [
                "linear classifiers",
                "monotone disjunctions",
                "online learning",
                "empirical risk min-     imization"
            ],
            "author": [
                "Sivan Sabato",
                "Shai Shalev-Shwartz",
                "Nathan Srebro",
                "Daniel Hsu",
                "Tong Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume16/sabato15a/sabato15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rationality, Optimism and Guarantees in General Reinforcement Learning",
            "abstract": "In this article, we present a top-down theoretical study of general reinforcement learning agents. We begin with rational agents with unlimited resources and then move to a setting where an agent can only maintain a limited number of hypotheses and optimizes plans over a horizon much shorter than what the agent designer actually wants. We axiomatize what is rational in such a setting in a manner that enables optimism, which is important to achieve systematic explorative behavior. Then, within the class of agents deemed rational, we achieve convergence and finite-error bounds. Such results are desirable since they imply that the agent learns well from its experiences, but the bounds do not directly guarantee good performance and can be achieved by agents doing things one should obviously not. Good performance cannot in fact be guaranteed for any agent in fully general settings. Our approach is to design agents that learn well from experience and act rationally. We introduce a framework for general reinforcement learning agents based on rationality axioms for a decision function and an hypothesis- generating function designed so as to achieve guarantees on the number errors. We will consistently use an optimistic decision function but the hypothesis-generating function needs to change depending on what is known/assumed. We investigate a number of natural situations having either a frequentist or Bayesian flavor, deterministic or stochastic environments and either finite or countable hypothesis class. Further, to achieve sufficiently good bounds as to hold promise for practical success we introduce a notion of a class of environments being generated by a set of laws. None of the above has previously been done for fully general reinforcement learning environments.",
            "keywords": [
                "reinforcement learning",
                "rationality",
                "optimism",
                "optimality"
            ],
            "author": [
                "Peter Sunehag",
                "Marcus Hutter"
            ],
            "ref": "http://jmlr.org/papers/volume16/sunehag15a/sunehag15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Algebraic Combinatorial Approach for Low-Rank Matrix Completion",
            "abstract": "We present a novel algebraic combinatorial view on low-rank matrix completion based on studying relations between a few entries with tools from algebraic geometry and matroid theory. The intrinsic locality of the approach allows for the treatment of single entries in a closed theoretical and practical framework. More specifically, apart from introducing an algebraic combinatorial theory of low-rank matrix completion, we present probability-one algorithms to decide whether a particular entry of the matrix can be completed. We also describe methods to complete that entry from a few others, and to estimate the error which is incurred by any method completing that entry. Furthermore, we show how known results on matrix completion and their sampling assumptions can be related to our new perspective and interpreted in terms of a completability phase transition.",
            "keywords": [
                "Low-rank matrix completion",
                "entry-wise completion",
                "matrix reconstruction"
            ],
            "author": [
                "Franz J.Király",
                "Louis Theran",
                "Ryota Tomioka"
            ],
            "ref": "http://jmlr.org/papers/volume16/kiraly15a/kiraly15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Comprehensive Survey on Safe Reinforcement Learning",
            "abstract": "Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted finite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.",
            "keywords": [
                "reinforcement learning",
                "risk sensitivity",
                "safe exploration"
            ],
            "author": [
                "Javier García",
                "Fern",
                "o Fernández"
            ],
            "ref": "http://jmlr.org/papers/volume16/garcia15a/garcia15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Second-Order Non-Stationary Online Learning for Regression",
            "abstract": "The goal of a learner in standard online learning, is to have the cumulative loss not much larger compared with the best- performing function from some fixed class. Numerous algorithms were shown to have this gap arbitrarily close to zero, compared with the best function that is chosen off-line. Nevertheless, many real-world applications, such as adaptive filtering, are non-stationary in nature, and the best prediction function may drift over time. We introduce two novel algorithms for online regression, designed to work well in non-stationary environment. Our first algorithm performs adaptive resets to forget the history, while the second is last-step min-max optimal in context of a drift. We analyze both algorithms in the worst-case regret framework and show that they maintain an average loss close to that of the best slowly changing sequence of linear functions, as long as the cumulative drift is sublinear. In addition, in the stationary case, when no drift occurs, our algorithms suffer logarithmic regret, as for previous algorithms. Our bounds improve over existing ones, and simulations demonstrate the usefulness of these algorithms compared with other state-of-the-art approaches.",
            "keywords": [
                "online learning",
                "regret bounds"
            ],
            "author": [
                "Edward Moroshko",
                "Nina Vaits",
                "Koby Crammer"
            ],
            "ref": "http://jmlr.org/papers/volume16/moroshko15a/moroshko15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Finite Sample Analysis of the Naive Bayes Classifier",
            "abstract": "We revisit, from a statistical learning perspective, the classical decision-theoretic problem of weighted expert voting. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Naive Bayes weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. We derive optimality results for our estimates and also establish some structural characterizations. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. Several challenging open problems are posed, and experimental results are provided to illustrate the theory.",
            "keywords": [
                "experts",
                "hypothesis testing",
                "Chernoff-Stein lemma",
                "Neyman-Pearson lemma",
                "naive Bayes"
            ],
            "author": [
                "Daniel Berend",
                "Aryeh Kontorovich"
            ],
            "ref": "http://jmlr.org/papers/volume16/berend15a/berend15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Flexible High-Dimensional Classification Machines and Their Asymptotic Properties",
            "abstract": "Classification is an important topic in statistics and machine learning with great potential in many real applications. In this paper, we investigate two popular large-margin classification methods, Support Vector Machine (SVM) and Distance Weighted Discrimination (DWD), under two contexts: the high-dimensional, low-sample size data and the imbalanced data. A unified family of classification machines, the FLexible Assortment MachinE (FLAME) is proposed, within which DWD and SVM are special cases. The FLAME family helps to identify the similarities and differences between SVM and DWD. It is well known that many classifiers overfit the data in the high-dimensional setting; and others are sensitive to the imbalanced data, that is, the class with a larger sample size overly influences the classifier and pushes the decision boundary towards the minority class. SVM is resistant to the imbalanced data issue, but it overfits high- dimensional data sets by showing the undesired data-piling phenomenon. The DWD method was proposed to improve SVM in the high-dimensional setting, but its decision boundary is sensitive to the imbalanced ratio of sample sizes. Our FLAME family helps to understand an intrinsic connection between SVM and DWD, and provides a trade-off between sensitivity to the imbalanced data and overfitting the high-dimensional data. Several asymptotic properties of the FLAME classifiers are studied. Simulations and real data applications are investigated to illustrate theoretical findings.",
            "keywords": [
                "classification",
                "Fisher consistency",
                "high-dimensional low-sample size asymp-     totics",
                "imbalanced data"
            ],
            "author": [
                "Xingye Qiao",
                "Lingsong Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume16/qiao15a/qiao15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "RLPy: A Value-Function-Based Reinforcement Learning Framework for Education and Research",
            "abstract": "RLPy is an object-oriented reinforcement learning software package with a focus on value-function-based methods using linear function approximation and discrete actions. The framework was designed for both educational and research purposes. It provides a rich library of fine-grained, easily exchangeable components for learning agents (e.g., policies or representations of value functions), facilitating recently increased specialization in reinforcement learning. RLPy is written in Python to allow fast prototyping, but is also suitable for large-scale experiments through its built-in support for optimized numerical libraries and parallelization. Code profiling, domain visualizations, and data analysis are integrated in a self-contained package available under the Modified BSD License at github.com/rlpy/rlpy. All of these properties allow users to compare various reinforcement learning algorithms with little effort.",
            "keywords": [
                "reinforcement learning",
                "value-function",
                "empirical evaluation"
            ],
            "author": [
                "Alborz Geramifard",
                "Christoph Dann",
                "Robert H. Klein",
                "William Dabney",
                "Jonathan P. How"
            ],
            "ref": "http://jmlr.org/papers/volume16/geramifard15a/geramifard15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Calibrated Multivariate Regression with Application to Neural Semantic Basis Discovery",
            "abstract": "We propose a calibrated multivariate regression method named CMR for fitting high dimensional multivariate regression models. Compared with existing methods, CMR calibrates regularization for each regression task with respect to its noise level so that it simultaneously attains improved finite-sample performance and tuning insensitiveness. Theoretically, we provide sufficient conditions under which CMR achieves the optimal rate of convergence in parameter estimation. Computationally, we propose an efficient smoothed proximal gradient algorithm with a worst- case numerical rate of convergence , where  is a pre-specified accuracy of the objective function value. We conduct thorough numerical simulations to illustrate that CMR consistently outperforms other high dimensional multivariate regression methods. We also apply CMR to solve a brain activity prediction problem and find that it is as competitive as a handcrafted model created by human experts. The R package camel implementing the proposed method is available on the Comprehensive R Archive Network cran.r-project.org/web/ packages/camel.",
            "keywords": [
                "calibration",
                "multivariate regression",
                "high dimension",
                "sparsity",
                "low Rank"
            ],
            "author": [
                "Han Liu",
                "Lie Wang",
                "Tuo Zhao"
            ],
            "ref": "http://jmlr.org/papers/volume16/liu15b/liu15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximate Modified Policy Iteration and its Application to the Game of Tetris",
            "abstract": "Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of the well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analysis that unify those for approximate policy and value iteration. We develop the finite-sample analysis of these algorithms, which highlights the influence of their parameters. In the classification-based version of the algorithm (CBMPI), the analysis shows that MPI's main parameter controls the balance between the estimation error of the classifier and the overall value function approximation. We illustrate and evaluate the behavior of these new algorithms in the Mountain Car and Tetris problems. Remarkably, in Tetris, CBMPI outperforms the existing DP approaches by a large margin, and competes with the current state-of-the-art methods while using fewer samples.",
            "keywords": [],
            "author": [
                "Bruno Scherrer",
                "Mohammad Ghavamzadeh",
                "Victor Gabillon",
                "Boris Lesner",
                "Matthieu Geist"
            ],
            "ref": "http://jmlr.org/papers/volume16/scherrer15a/scherrer15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Preface to this Special Issue",
            "abstract": "",
            "keywords": [],
            "author": [
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "http://jmlr.org/papers/volume16/gammerman15a/gammerman15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "V-Matrix Method of Solving Statistical Inference Problems",
            "abstract": "This paper presents direct settings and rigorous solutions of the main Statistical Inference problems. It shows that rigorous solutions require solving multidimensional Fredholm integral equations of the first kind in the situation where not only the right-hand side of the equation is an approximation, but the operator in the equation is also defined approximately. Using Stefanuyk-Vapnik theory for solving such ill-posed operator equations, constructive methods of empirical inference are introduced. These methods are based on a new concept called -matrix. This matrix captures geometric properties of the observation data that are ignored by classical statistical methods.",
            "keywords": [
                "conditional probability",
                "regression",
                "density ratio",
                "ill-posed problem",
                "mutual     information"
            ],
            "author": [
                "Vladimir Vapnik",
                "Rauf Izmailov"
            ],
            "ref": "http://jmlr.org/papers/volume16/vapnik15a/vapnik15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Batch Learning from Logged Bandit Feedback through Counterfactual Risk Minimization",
            "abstract": "We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem (Bottou et al., 2013) through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. In analogy to the Structural Risk Minimization principle of Wapnik and Tscherwonenkis (1979), these constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method--- called Policy Optimizer for Exponential Models (POEM)---for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. The effectiveness and efficiency of POEM is evaluated on several simulated multi- label classification problems, as well as on a real-world information retrieval problem. The empirical results show that the CRM objective implemented in POEM provides improved robustness and generalization performance compared to the state- of-the-art.",
            "keywords": [
                "empirical risk minimization",
                "bandit feedback",
                "importance sampling",
                "propen-     sity score matching"
            ],
            "author": [
                "Adith Swaminathan",
                "Thorsten Joachims"
            ],
            "ref": "http://jmlr.org/papers/volume16/swaminathan15a/swaminathan15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Estimation of Low Rank Density Matrices",
            "abstract": "The density matrices are positively semi-definite Hermitian matrices of unit trace that describe the state of a quantum system. The goal of the paper is to develop minimax lower bounds on error rates of estimation of low rank density matrices in trace regression models used in quantum state tomography (in particular, in the case of Pauli measurements) with explicit dependence of the bounds on the rank and other complexity parameters. Such bounds are established for several statistically relevant distances, including quantum versions of Kullback-Leibler divergence (relative entropy distance) and of Hellinger distance (so called Bures distance), and Schatten -norm distances. Sharp upper bounds and oracle inequalities for least squares estimator with von Neumann entropy penalization are obtained showing that minimax lower bounds are attained (up to logarithmic factors) for these distances.",
            "keywords": [
                "quantum state tomography",
                "low rank density matrix"
            ],
            "author": [
                "Vladimir Koltchinskii",
                "Dong Xia"
            ],
            "ref": "http://jmlr.org/papers/volume16/koltchinskii15a/koltchinskii15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Rates in Statistical and Online Learning",
            "abstract": "The speed with which a learning algorithm converges as it is presented with more data is a central problem in machine learning --- a fast rate of convergence means less data is needed for the same level of performance. The pursuit of fast rates in online and statistical learning has led to the discovery of many conditions in learning theory under which fast learning is possible. We show that most of these conditions are special cases of a single, unifying condition, that comes in two forms: the central condition for `proper' learning algorithms that always output a hypothesis in the given model, and stochastic mixability for online algorithms that may make predictions outside of the model. We show that under surprisingly weak assumptions both conditions are, in a certain sense, equivalent. The central condition has a re-interpretation in terms of convexity of a set of pseudoprobabilities, linking it to density estimation under misspecification. For bounded losses, we show how the central condition enables a direct proof of fast rates and we prove its equivalence to the Bernstein condition, itself a generalization of the Tsybakov margin condition, both of which have played a central role in obtaining fast rates in statistical learning. Yet, while the Bernstein condition is two-sided, the central condition is one-sided, making it more suitable to deal with unbounded losses. In its stochastic mixability form, our condition generalizes both a stochastic exp-concavity condition identified by Juditsky, Rigollet and Tsybakov and Vovk's notion of mixability. Our unifying conditions thus provide a substantial step towards a characterization of fast rates in statistical learning, similar to how classical mixability characterizes constant regret in the sequential prediction with expert advice setting.",
            "keywords": [
                "statistical learning theory",
                "fast rates",
                "Tsybakov margin condition",
                "mixability"
            ],
            "author": [
                "Tim van Erven",
                "Peter D. Grünwald",
                "Nishant A. Mehta",
                "Mark D. Reid",
                "Robert C. Williamson"
            ],
            "ref": "http://jmlr.org/papers/volume16/vanerven15a/vanerven15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Asymptotic Normality of an Estimate of a Regression Functional",
            "abstract": "An estimate of the second moment of the regression function is introduced. Its asymptotic normality is proved such that the asymptotic variance depends neither on the dimension of the observation vector, nor on the smoothness properties of the regression function. The asymptotic variance is given explicitly.",
            "keywords": [
                "nonparametric estimation",
                "regression functional",
                "central limit theorem"
            ],
            "author": [
                "László Györfi",
                "Harro Walk"
            ],
            "ref": "http://jmlr.org/papers/volume16/gyorfi15a/gyorfi15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sharp Oracle Bounds for Monotone and Convex Regression Through Aggregation",
            "abstract": "We derive oracle inequalities for the problems of isotonic and convex regression using the combination of -aggregation procedure and sparsity pattern aggregation. This improves upon the previous results including the oracle inequalities for the constrained least squares estimator. One of the improvements is that our oracle inequalities are sharp, i.e., with leading constant 1. It allows us to obtain bounds for the minimax regret thus accounting for model misspecification, which was not possible based on the previous results. Another improvement is that we obtain oracle inequalities both with high probability and in expectation.",
            "keywords": [
                "aggregation",
                "shape constraints",
                "isotonic regression",
                "convex regression",
                "minimax    regret",
                "sharp oracle inequalities"
            ],
            "author": [
                "Pierre C. Bellec",
                "Alexandre B. Tsybakov"
            ],
            "ref": "http://jmlr.org/papers/volume16/bellec15a/bellec15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exceptional Rotations of Random Graphs: A VC Theory",
            "abstract": "In this paper we explore maximal deviations of large random structures from their typical behavior. We introduce a model for a high-dimensional random graph process and ask analogous questions to those of Vapnik and Chervonenkis for deviations of averages: how \"rich\" does the process have to be so that one sees atypical behavior. In particular, we study a natural process of ErdÅs-RÃ©nyi random graphs indexed by unit vectors in . We investigate the deviations of the process with respect to three fundamental properties: clique number, chromatic number, and connectivity. In all cases we establish upper and lower bounds for the minimal dimension  that guarantees the existence of \"exceptional directions\" in which the random graph behaves atypically with respect to the property. For each of the three properties, four theorems are established, to describe upper and lower bounds for the threshold dimension in the subcritical and supercritical regimes.",
            "keywords": [
                "random graphs",
                "VC theory",
                "clique number",
                "chromatic number",
                "connectivityc 2015 Louigi Addario-Berry",
                "Shankar Bhamidi"
            ],
            "author": [
                "Louigi Addario-Berry",
                "Shankar Bhamidi",
                "Sébastien Bubeck",
                "Luc Devroye",
                "Gábor Lugosi",
                "Roberto Imbuzeiro Oliveira"
            ],
            "ref": "http://jmlr.org/papers/volume16/addarioberry15a/addarioberry15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Semi-Supervised Interpolation in an Anticausal Learning Scenario",
            "abstract": "According to a recently stated 'independence postulate', the distribution  contains no information about the conditional  while  may contain information about . Since semi- supervised learning (SSL) attempts to exploit information from  to assist in predicting  from , it should only work in anticausal direction, i.e., when  is the cause and  is the effect. In causal direction, when  is the cause and  the effect, unlabelled -values should be useless. To shed light on this asymmetry, we study a deterministic causal relation  as recently assayed in Information-Geometric Causal Inference (IGCI). Within this model, we discuss two options to formalize the independence of  and  as an orthogonality of vectors in appropriate inner product spaces. We prove that unlabelled data help for the problem of interpolating a monotonically increasing function if and only if the orthogonality conditions are violated -- which we only expect for the anticausal direction. Here, performance of SSL and its supervised baseline analogue is measured in terms of two different loss functions: first, the mean squared error and second the surprise in a Bayesian prediction scenario.",
            "keywords": [
                "semi-supervised learning",
                "anticausal learning",
                "independence of cause and     mechanism",
                "information geometry"
            ],
            "author": [
                "Dominik Janzing",
                "Bernhard Schölkopf"
            ],
            "ref": "http://jmlr.org/papers/volume16/janzing15a/janzing15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Towards an Axiomatic Approach to Hierarchical Clustering of Measures",
            "abstract": "We propose some axioms for hierarchical clustering of probability measures and investigate their ramifications. The basic idea is to let the user stipulate the clusters for some elementary measures. This is done without the need of any notion of metric, similarity or dissimilarity. Our main results then show that for each suitable choice of user-defined clustering on elementary measures we obtain a unique notion of clustering on a large set of distributions satisfying a set of additivity and continuity axioms. We illustrate the developed theory by numerous examples including some with and some without a density.",
            "keywords": [
                "axiomatic clustering",
                "hierarchical clustering",
                "infinite samples clustering",
                "den-     sity level set clustering"
            ],
            "author": [
                "Philipp Thomann",
                "Ingo Steinwart",
                "Nico Schmid"
            ],
            "ref": "http://jmlr.org/papers/volume16/thomann15a/thomann15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Predicting a Switching Sequence of Graph Labelings",
            "abstract": "We study the problem of predicting online the labeling of a graph. We consider a novel setting for this problem in which, in addition to observing vertices and labels on the graph, we also observe a sequence of just vertices on a second graph. A latent labeling of the second graph selects one of  labelings to be active on the first graph. We propose a polynomial time algorithm for online prediction in this setting and derive a mistake bound for the algorithm. The bound is controlled by the geometric cut of the observed and latent labelings, as well as the resistance diameters of the graphs. When specialized to multitask prediction and online switching problems the bound gives new and sharper results under certain conditions.",
            "keywords": [
                "online learning over graphs",
                "kernel methods",
                "matrix winnow"
            ],
            "author": [
                "Mark Herbster",
                "Stephen Pasteris",
                "Massimiliano Pontil"
            ],
            "ref": "http://jmlr.org/papers/volume16/herbster15a/herbster15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Using Privileged Information: Similarity Control and Knowledge Transfer",
            "abstract": "This paper describes a new paradigm of machine learning, in which Intelligent Teacher is involved. During training stage, Intelligent Teacher provides Student with information that contains, along with classification of each example, additional privileged information (for example, explanation) of this example. The paper describes two mechanisms that can be used for significantly accelerating the speed of Student's learning using privileged information: (1) correction of Student's concepts of similarity between examples, and (2) direct Teacher-Student knowledge transfer.",
            "keywords": [
                "intelligent teacher",
                "privileged information",
                "similarity control",
                "knowledge     transfer",
                "knowledge representation",
                "frames",
                "support vector machines"
            ],
            "author": [
                "Vladimir Vapnik",
                "Rauf Izmailov"
            ],
            "ref": "http://jmlr.org/papers/volume16/vapnik15b/vapnik15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Alexey Chervonenkis's Bibliography: Introductory Comments",
            "abstract": "",
            "keywords": [],
            "author": [
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "http://jmlr.org/papers/volume16/gammerman15b/gammerman15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Alexey Chervonenkis's Bibliography",
            "abstract": "",
            "keywords": [],
            "author": [
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "http://jmlr.org/papers/volume16/gammerman15c/gammerman15c.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Photonic Delay Systems as Machine Learning Implementations",
            "abstract": "Nonlinear photonic delay systems present interesting implementation platforms for machine learning models. They can be extremely fast, offer great degrees of parallelism and potentially consume far less power than digital processors. So far they have been successfully employed for signal processing using the Reservoir Computing paradigm. In this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encoding of such systems. We perform physical experiments that demonstrate that the obtained input encodings work well in reality, and we show that optimized systems perform significantly better than the common Reservoir Computing approach. The results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro-inspired analog computers.",
            "keywords": [
                "recurrent neural networks",
                "optical computing",
                "machine learning modelsc 2015 Michiel Hermans"
            ],
            "author": [
                "Michiel Hermans",
                "Miguel C. Soriano",
                "Joni Dambre",
                "Peter Bienstman",
                "Ingo Fischer"
            ],
            "ref": "http://jmlr.org/papers/volume16/hermans15a/hermans15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Linearly Constrained Minimum Variance Beamforming",
            "abstract": "Beamforming is a widely used technique for source localization in signal processing and neuroimaging. A number of vector- beamformers have been introduced to localize neuronal activity by using magnetoencephalography (MEG) data in the literature. However, the existing theoretical analyses on these beamformers have been limited to simple cases, where no more than two sources are allowed in the associated model and the theoretical sensor covariance is also assumed known. The information about the effects of the MEG spatial and temporal dimensions on the consistency of vector-beamforming is incomplete. In the present study, we consider a class of vector-beamformers defined by thresholding the sensor covariance matrix, which include the standard vector-beamformer as a special case. A general asymptotic theory is developed for these vector-beamformers, which shows the extent of effects to which the MEG spatial and temporal dimensions on estimating the neuronal activity index. The performances of the proposed beamformers are assessed by simulation studies. Superior performances of the proposed beamformers are obtained when the signal-to-noise ratio is low. We apply the proposed procedure to real MEG data sets derived from five sessions of a human face-perception experiment, finding several highly active areas in the brain. A good agreement between these findings and the known neurophysiology of the MEG response to human face perception is shown.",
            "keywords": [
                "MEG neuroimaging",
                "vector-beamforming",
                "sparse covariance estimation"
            ],
            "author": [
                "Jian Zhang",
                "Chao Liu"
            ],
            "ref": "http://jmlr.org/papers/volume16/zhang15b/zhang15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Constraint-based Causal Discovery from Multiple Interventions over Overlapping Variable Sets",
            "abstract": "Scientific practice typically involves repeatedly studying a system, each time trying to unravel a different perspective. In each study, the scientist may take measurements under different experimental conditions (interventions, manipulations, perturbations) and measure different sets of quantities (variables). The result is a collection of heterogeneous data sets coming from different data distributions. In this work, we present algorithm COmbINE, which accepts a collection of data sets over overlapping variable sets under different experimental conditions; COmbINE then outputs a summary of all causal models indicating the invariant and variant structural characteristics of all models that simultaneously fit all of the input data sets. COmbINE converts estimated dependencies and independencies in the data into path constraints on the data- generating causal model and encodes them as a SAT instance. The algorithm is sound and complete in the sample limit. To account for conflicting constraints arising from statistical errors, we introduce a general method for sorting constraints in order of confidence, computed as a function of their corresponding p-values. In our empirical evaluation, COmbINE outperforms in terms of efficiency the only pre-existing similar algorithm; the latter additionally admits feedback cycles, but does not admit conflicting constraints which hinders the applicability on real data. As a proof-of-concept, COmbINE is employed to co- analyze 4 real, mass-cytometry data sets measuring phosphorylated protein concentrations of overlapping protein sets under 3 different interventions.",
            "keywords": [
                "causality",
                "causal discovery",
                "graphical models",
                "maximal ancestral graphs",
                "semi-Markov causal models",
                "randomized experiments"
            ],
            "author": [
                "Sofia Triantafillou",
                "Ioannis Tsamardinos"
            ],
            "ref": "http://jmlr.org/papers/volume16/triantafillou15a/triantafillou15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Existence and Uniqueness of Proper Scoring Rules",
            "abstract": "To discuss the existence and uniqueness of proper scoring rules one needs to extend the associated entropy functions as sublinear functions to the conic hull of the prediction set. In some natural function spaces, such as the Lebesgue -spaces over , the positive cones have empty interior. Entropy functions defined on such cones have directional derivatives only, which typically exist on large subspaces and behave similarly to gradients. Certain entropies may be further extended continuously to open cones in normed spaces containing signed densities. The extended densities are Gateaux differentiable except on a negligible set and have everywhere continuous subgradients due to the supporting hyperplane theorem. We introduce the necessary framework from analysis and algebra that allows us to give an affirmative answer to the titular question of the paper. As a result of this, we give a formal sense in which entropy functions have uniquely associated proper scoring rules. We illustrate our framework by studying the derivatives and subgradients of the following three prototypical entropies: Shannon entropy, Hyvarinen entropy, and quadratic entropy.",
            "keywords": [
                "proper scoring rules",
                "entropy",
                "characterisation",
                "existence",
                "uniqueness",
                "quasi-     interior",
                "directional derivative"
            ],
            "author": [
                "Evgeni Y. Ovcharov"
            ],
            "ref": "http://jmlr.org/papers/volume16/ovcharov15a/ovcharov15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Strategy for Stratified Monte Carlo Sampling",
            "abstract": "We consider the problem of stratified sampling for Monte Carlo integration of a random variable. We model this problem in a -armed bandit, where the arms represent the  strata. The goal is to estimate the integral mean, that is a weighted average of the mean values of the arms. The learner is allowed to sample the variable  times, but it can decide on-line which stratum to sample next. We propose an UCB-type strategy that samples the arms according to an upper bound on their estimated standard deviations. We compare its performance to an ideal sample allocation that knows the standard deviations of the arms. For sub-Gaussian arm distributions, we provide bounds on the total regret: a distribution-dependent bound of order  (The notation  means that there exist , such that  for  large enough. Moreover,  means that  for  large enough.) that depends on a measure of the disparity  of the per stratum variances and a distribution-free bound  that does not. We give similar, but somewhat sharper bounds on a proxy of the regret. The problem- independent bound for this proxy matches its recent minimax lower bound in terms of  up to a  factor.",
            "keywords": [
                "adaptive sampling",
                "bandit theory",
                "stratified Monte Carlo",
                "minimax strategies"
            ],
            "author": [
                "Alexandra Carpentier",
                "Remi Munos",
                "András Antos"
            ],
            "ref": "http://jmlr.org/papers/volume16/carpentier15a/carpentier15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Concave Penalized Estimation of Sparse Gaussian Bayesian Networks",
            "abstract": "We develop a penalized likelihood estimation framework to learn the structure of Gaussian Bayesian networks from observational data. In contrast to recent methods which accelerate the learning problem by restricting the search space, our main contribution is a fast algorithm for score-based structure learning which does not restrict the search space in any way and works on high-dimensional data sets with thousands of variables. Our use of concave regularization, as opposed to the more popular  (e.g. BIC) penalty, is new. Moreover, we provide theoretical guarantees which generalize existing asymptotic results when the underlying distribution is Gaussian. Most notably, our framework does not require the existence of a so-called faithful DAG representation, and as a result, the theory must handle the inherent nonidentifiability of the estimation problem in a novel way. Finally, as a matter of independent interest, we provide a comprehensive comparison of our approach to several standard structure learning methods using open-source packages developed for the R language. Based on these experiments, we show that our algorithm obtains higher sensitivity with comparable false discovery rates for high- dimensional data and scales efficiently as the number of nodes increases. In particular, the total runtime for our method to generate a solution path of 20 estimates for DAGs with 8000 nodes is around one hour.",
            "keywords": [
                "Bayesian networks",
                "concave penalization",
                "directed acyclic graphs",
                "coordinate    descent"
            ],
            "author": [
                "Bryon Aragam",
                "Qing Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume16/aragam15a/aragam15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Achievability of Asymptotic Minimax Regret by Horizon-Dependent and Horizon-Independent Strategies",
            "abstract": "The normalized maximum likelihood distribution achieves minimax coding (log-loss) regret given a fixed sample size, or horizon, . It generally requires that  be known in advance. Furthermore, extracting the sequential predictions from the normalized maximum likelihood distribution is computationally infeasible for most statistical models. Several computationally feasible alternative strategies have been devised. We characterize the achievability of asymptotic minimaxity by horizon-dependent and horizon-independent strategies. We prove that no horizon-independent strategy can be asymptotically minimax in the multinomial case. A weaker result is given in the general case subject to a condition on the horizon-dependence of the normalized maximum likelihood. Motivated by these negative results, we demonstrate that an easily implementable Bayes mixture based on a conjugate Dirichlet prior with a simple dependency on  achieves asymptotic minimaxity for all sequences, simplifying earlier similar proposals. Our numerical experiments for the Bernoulli model demonstrate improved finite- sample performance by a number of novel horizon-dependent and horizon-independent algorithms.",
            "keywords": [
                "on-line learning",
                "prediction of individual sequences",
                "normalized maximum     likelihood",
                "asymptotic minimax regret"
            ],
            "author": [
                "Kazuho Watanabe",
                "Teemu Roos"
            ],
            "ref": "http://jmlr.org/papers/volume16/watanabe15a/watanabe15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiclass Learnability and the ERM Principle",
            "abstract": "We study the sample complexity of multiclass prediction in several learning settings. For the PAC setting our analysis reveals a surprising phenomenon: In sharp contrast to binary classification, we show that there exist multiclass hypothesis classes for which some Empirical Risk Minimizers (ERM learners) have lower sample complexity than others. Furthermore, there are classes that are learnable by some ERM learners, while other ERM learners will fail to learn them. We propose a principle for designing good ERM learners, and use this principle to prove tight bounds on the sample complexity of learning symmetric multiclass hypothesis classes---classes that are invariant under permutations of label names. We further provide a characterization of mistake and regret bounds for multiclass learning in the online setting and the bandit setting, using new generalizations of Littlestone's dimension.",
            "keywords": [
                "multiclass",
                "sample complexity"
            ],
            "author": [
                "Amit Daniely",
                "Sivan Sabato",
                "Shai Ben-David",
                "Shai Shalev-Shwartz"
            ],
            "ref": "http://jmlr.org/papers/volume16/daniely15a/daniely15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Geometry and Expressive Power of Conditional Restricted Boltzmann Machines",
            "abstract": "Conditional restricted Boltzmann machines are undirected stochastic neural networks with a layer of input and output units connected bipartitely to a layer of hidden units. These networks define models of conditional probability distributions on the states of the output units given the states of the input units, parameterized by interaction weights and biases. We address the representational power of these models, proving results on their ability to represent conditional Markov random fields and conditional distributions with restricted supports, the minimal size of universal approximators, the maximal model approximation errors, and on the dimension of the set of representable conditional distributions. We contribute new tools for investigating conditional probability models, which allow us to improve the results that can be derived from existing work on restricted Boltzmann machine probability models.",
            "keywords": [
                "conditional restricted Boltzmann machine",
                "universal approximation",
                "Kullback-     Leibler approximation error"
            ],
            "author": [
                "Guido Montúfar",
                "Nihat Ay",
                "Keyan Ghazi-Zahedi"
            ],
            "ref": "http://jmlr.org/papers/volume16/montufar15b/montufar15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "From Dependency to Causality: A Machine Learning Approach",
            "abstract": "The relationship between statistical dependency and causality lies at the heart of all statistical approaches to causal inference. Recent results in the ChaLearn cause-effect pair challenge have shown that causal directionality can be inferred with good accuracy also in Markov indistinguishable configurations thanks to data driven approaches. This paper proposes a supervised machine learning approach to infer the existence of a directed causal link between two variables in multivariate settings with  variables. The approach relies on the asymmetry of some conditional (in)dependence relations between the members of the Markov blankets of two variables causally connected. Our results show that supervised learning methods may be successfully used to extract causal information on the basis of asymmetric statistical descriptors also for  variate distributions.",
            "keywords": [
                "causal inference",
                "information theory"
            ],
            "author": [
                "Gianluca Bontempi",
                "Maxime Flauder"
            ],
            "ref": "http://jmlr.org/papers/volume16/bontempi15a/bontempi15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Libra Toolkit for Probabilistic Models",
            "abstract": "The Libra Toolkit is a collection of algorithms for learning and inference with discrete probabilistic models, including Bayesian networks, Markov networks, dependency networks, and sum-product networks. Compared to other toolkits, Libra places a greater emphasis on learning the structure of tractable models in which exact inference is efficient. It also includes a variety of algorithms for learning graphical models in which inference is potentially intractable, and for performing exact and approximate inference. Libra is released under a 2-clause BSD license to encourage broad use in academia and industry.",
            "keywords": [
                "probabilistic graphical models",
                "structure learning"
            ],
            "author": [
                "Daniel Lowd",
                "Amirmohammad Rooshenas"
            ],
            "ref": "http://jmlr.org/papers/volume16/lowd15a/lowd15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Complexity of Equivalence and Learning for Multiplicity Tree Automata",
            "abstract": "",
            "keywords": [
                "exact learning",
                "query complexity",
                "multiplicity tree automata",
                "Hankel matri-     ces",
                "DAG representations of trees"
            ],
            "author": [
                "Ines Marusic",
                "James Worrell"
            ],
            "ref": "http://jmlr.org/papers/volume16/marusic15a/marusic15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Nonparametric Covariance Regression",
            "abstract": "Capturing predictor-dependent correlations amongst the elements of a multivariate response vector is fundamental to numerous applied domains, including neuroscience, epidemiology, and finance. Although there is a rich literature on methods for allowing the variance in a univariate regression model to vary with predictors, relatively little has been done in the multivariate case. As a motivating example, we consider the Google Flu Trends data set, which provides indirect measurements of influenza incidence at a large set of locations over time (our predictor). To accurately characterize temporally evolving influenza incidence across regions, it is important to develop statistical methods for a time-varying covariance matrix. Importantly, the locations provide a redundant set of measurements and do not yield a sparse nor static spatial dependence structure. We propose to reduce dimensionality and induce a flexible Bayesian nonparametric covariance regression model by relating these location-specific trajectories to a lower-dimensional subspace through a latent factor model with predictor-dependent factor loadings. These loadings are in terms of a collection of basis functions that vary nonparametrically over the predictor space. Such low-rank approximations are in contrast to sparse precision assumptions, and are appropriate in a wide range of applications. Our formulation aims to address three challenges: scaling to large  domains, coping with missing values, and allowing an irregular grid of observations. The model is shown to be highly flexible, while leading to a computationally feasible implementation via Gibbs sampling. The ability to scale to large  domains and cope with missing values is fundamental in analyzing the Google Flu Trends data.",
            "keywords": [
                "covariance regression",
                "dictionary learning",
                "Gaussian process",
                "latent factor     model",
                "nonparametric Bayes"
            ],
            "author": [
                "Emily B. Fox",
                "David B. Dunson"
            ],
            "ref": "http://jmlr.org/papers/volume16/fox15a/fox15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A General Framework for Fast Stagewise Algorithms",
            "abstract": "",
            "keywords": [
                "forward stagewise regression",
                "lasso"
            ],
            "author": [
                "Ryan J. Tibshirani"
            ],
            "ref": "http://jmlr.org/papers/volume16/tibshirani15a/tibshirani15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Counting and Exploring Sizes of Markov Equivalence Classes of Directed Acyclic Graphs",
            "abstract": "",
            "keywords": [
                "directed acyclic graphs",
                "Markov equivalence class",
                "size distribution"
            ],
            "author": [
                "Yangbo He",
                "Jinzhu Jia",
                "Bin Yu"
            ],
            "ref": "http://jmlr.org/papers/volume16/he15a/he15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "pyGPs -- A Python Library for Gaussian Process Regression and Classification",
            "abstract": "We introduce pyGPs, an object-oriented implementation of Gaussian processes (gps) for machine learning. The library provides a wide range of functionalities reaching from simple gp specification via mean and covariance and gp inference to more complex implementations of hyperparameter optimization, sparse approximations, and graph based learning. Using Python we focus on usability for both \"users\" and \"researchers\". Our main goal is to offer a  user- friendly and flexible implementation of gps for machine learning.",
            "keywords": [
                "Gaussian processes",
                "Python"
            ],
            "author": [
                "Marion Neumann",
                "Shan Huang",
                "Daniel E. Marthaler",
                "Kristian Kersting"
            ],
            "ref": "http://jmlr.org/papers/volume16/neumann15a/neumann15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Derivative Estimation Based on Difference Sequence via Locally Weighted Least Squares Regression",
            "abstract": "A new method is proposed for estimating derivatives of a nonparametric regression function. By applying Taylor expansion technique to a derived symmetric difference sequence, we obtain a sequence of approximate linear regression representation in which the derivative is just the intercept term. Using locally weighted least squares, we estimate the derivative in the linear regression model. The estimator has less bias in both valleys and peaks of the true derivative function. For the special case of a domain with equispaced design points, the asymptotic bias and variance are derived; consistency and asymptotic normality are established. In simulations our estimators have less bias and mean square error than its main competitors, especially second order derivative estimator.",
            "keywords": [
                "nonparametric derivative estimation",
                "locally weighted least squares",
                "bias-     correction",
                "symmetric difference sequence"
            ],
            "author": [
                "WenWu Wang",
                "Lu Lin"
            ],
            "ref": "http://jmlr.org/papers/volume16/wang15b/wang15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "When Are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity",
            "abstract": "Overcomplete latent representations have been very popular for unsupervised feature learning in recent years. In this paper, we specify which overcomplete models can be identified given observable moments of a certain order. We consider probabilistic admixture or topic models in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary. While general overcomplete topic models are not identifiable, we establish generic identifiability under a constraint, referred to as topic persistence. Our sufficient conditions for identifiability involve a novel set of \"higher order\" expansion conditions on the topic-word matrix or the population structure of the model. This set of higher-order expansion conditions allow for overcomplete models, and require the existence of a perfect matching from latent topics to higher order observed words. We establish that random structured topic models are identifiable w.h.p. in the overcomplete regime. Our identifiability results allows for general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework. Our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of Tucker decompositions, but is more general than the Candecomp/Parafac (CP) decomposition.",
            "keywords": [],
            "author": [
                "Animashree An",
                "kumar",
                "Daniel Hsu",
                "Majid Janzamin",
                "Sham Kakade"
            ],
            "ref": "http://jmlr.org/papers/volume16/anandkumar15a/anandkumar15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Absent Data Generating Classifier for Imbalanced Class Sizes",
            "abstract": "We propose an algorithm for two-class classification problems when the training data are imbalanced. This means the number of training instances in one of the classes is so low that the conventional classification algorithms become ineffective in detecting the minority class. We present a modification of the kernel Fisher discriminant analysis such that the imbalanced nature of the problem is explicitly addressed in the new algorithm formulation. The new algorithm exploits the properties of the existing minority points to learn the effects of other minority data points, had they actually existed. The algorithm proceeds iteratively by employing the learned properties and conditional sampling in such a way that it generates sufficient artificial data points for the minority set, thus enhancing the detection probability of the minority class. Implementing the proposed method on a number of simulated and real data sets, we show that our proposed method performs competitively compared to a set of alternative state-of-the-art imbalanced classification algorithms.",
            "keywords": [
                "kernel Fisher discriminant analysis",
                "imbalanced data"
            ],
            "author": [
                "Arash Pourhabib",
                "Bani K. Mallick",
                "Yu Ding"
            ],
            "ref": "http://jmlr.org/papers/volume16/pourhabib15a/pourhabib15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Decision Boundary for Discrete Bayesian Network Classifiers",
            "abstract": "Bayesian network classifiers are a powerful machine learning tool. In order to evaluate the expressive power of these models, we compute families of polynomials that sign-represent decision functions induced by Bayesian network classifiers. We prove that those families are linear combinations of products of Lagrange basis polynomials. In absence of -structures in the predictor sub-graph, we are also able to prove that this family of polynomials does indeed characterize the specific classifier considered. We then use this representation to bound the number of decision functions representable by Bayesian network classifiers with a given structure.",
            "keywords": [
                "Bayesian networks",
                "supervised classification",
                "decision boundary",
                "polynomial     threshold function"
            ],
            "author": [
                "Gherardo Var",
                "o",
                "Concha Bielza",
                "Pedro Larranaga"
            ],
            "ref": "http://jmlr.org/papers/volume16/varando15a/varando15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A View of Margin Losses as Regularizers of Probability Estimates",
            "abstract": "Regularization is commonly used in classifier design, to assure good generalization. Classical regularization enforces a cost on classifier complexity, by constraining parameters. This is usually combined with a margin loss, which favors large-margin decision rules. A novel and unified view of this architecture is proposed, by showing that margin losses act as regularizers of posterior class probabilities, in a way that amplifies classical parameter regularization. The problem of controlling the regularization strength of a margin loss is considered, using a decomposition of the loss in terms of a link and a binding function. The link function is shown to be responsible for the regularization strength of the loss, while the binding function determines its outlier robustness. A large class of losses is then categorized into equivalence classes of identical regularization strength or outlier robustness. It is shown that losses in the same regularization class can be parameterized so as to have tunable regularization strength. This parameterization is finally used to derive boosting algorithms with loss regularization (BoostLR). Three classes of tunable regularization losses are considered in detail. Canonical losses can implement all regularization behaviors but have no flexibility in terms of outlier modeling. Shrinkage losses support equally parameterized link and binding functions, leading to boosting algorithms that implement the popular shrinkage procedure. This offers a new explanation for shrinkage as a special case of loss-based regularization. Finally, -tunable losses enable the independent parameterization of link and binding functions, leading to boosting algorithms of great flexibility. This is illustrated by the derivation of an algorithm that generalizes both AdaBoost and LogitBoost, behaving as either one when that best suits the data to classify. Various experiments provide evidence of the benefits of probability regularization for both classification and estimation of posterior class probabilities.",
            "keywords": [
                "classification",
                "margin losses",
                "regularization",
                "boosting",
                "probability elicitation",
                "generalization",
                "loss functions",
                "link functions",
                "binding functions"
            ],
            "author": [
                "Hamed Masnadi-Shirazi",
                "Nuno Vasconcelos"
            ],
            "ref": "http://jmlr.org/papers/volume16/masnadi15a/masnadi15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Tensor Methods for Learning Latent Variable Models",
            "abstract": "We introduce an online tensor decomposition based approach for two latent variable modeling problems namely, (1) community detection, in which we learn the latent communities that the social actors in social networks belong to, and (2) topic modeling, in which we infer hidden topics of text articles. We consider decomposition of moment tensors using stochastic gradient descent. We conduct optimization of multilinear operations in SGD and avoid directly forming the tensors, to save computational and storage costs. We present optimized algorithm in two platforms. Our GPU-based implementation exploits the parallelism of SIMD architectures to allow for maximum speed-up by a careful optimization of storage and data transfer, whereas our CPU-based implementation uses efficient sparse matrix computations and is suitable for large sparse data sets. For the community detection problem, we demonstrate accuracy and computational efficiency on Facebook, Yelp and DBLP data sets, and for the topic modeling problem, we also demonstrate good performance on the New York Times data set. We compare our results to the state-of-the-art algorithms such as the variational method, and report a gain of accuracy and a gain of several orders of magnitude in the execution time.",
            "keywords": [
                "mixed membership stochastic blockmodel",
                "topic modeling",
                "tensor method",
                "stochastic gradient descent",
                "parallel implementation"
            ],
            "author": [
                "Furong Huang",
                "U. N. Niranjan",
                "Mohammad Umar Hakeem",
                "Animashree An",
                "kumar"
            ],
            "ref": "http://jmlr.org/papers/volume16/huang15a/huang15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Bayesian Estimation in Random Covariate Design with a Rescaled Gaussian Process Prior",
            "abstract": "In Bayesian nonparametric models, Gaussian processes provide a popular prior choice for regression function estimation. Existing literature on the theoretical investigation of the resulting posterior distribution almost exclusively assume a fixed design for covariates. The only random design result we are aware of (van der Vaart and van Zanten, 2011) assumes the assigned Gaussian process to be supported on the smoothness class specified by the true function with probability one. This is a fairly restrictive assumption as it essentially rules out the Gaussian process prior with a squared exponential kernel when modeling rougher functions. In this article, we show that an appropriate rescaling of the above Gaussian process leads to a rate-optimal posterior distribution even when the covariates are independently realized from a known density on a compact set. The proofs are based on deriving sharp concentration inequalities for frequentist kernel estimators; the results might be of independent interest.",
            "keywords": [
                "Bayesian",
                "convergence rate",
                "Gaussian process",
                "nonparametric regression",
                "random design"
            ],
            "author": [
                "Debdeep Pati",
                "Anirban Bhattacharya",
                "Guang Cheng"
            ],
            "ref": "http://jmlr.org/papers/volume16/pati15a/pati15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "CEKA: A Tool for Mining the Wisdom of Crowds",
            "abstract": "CEKA is a software package for developers and researchers to mine the wisdom of crowds. It makes the entire knowledge discovery procedure much easier, including analyzing qualities of workers, simulating labeling behaviors, inferring true class labels of instances, filtering and correcting mislabeled instances (noise), building learning models and evaluating them. It integrates a set of state-of-the-art inference algorithms, a set of general noise handling algorithms, and abundant functions for model training and evaluation. CEKA is written in Java with core classes being compatible with the well-known machine learning tool WEKA, which makes the utilization of the functions in WEKA much easier.",
            "keywords": [
                "crowdsourcing",
                "learning from crowds",
                "multiple noisy labeling",
                "inference",
                "noise     handling"
            ],
            "author": [
                "Jing Zhang",
                "Victor S. Sheng",
                "Bryce A. Nicholson",
                "Xindong Wu"
            ],
            "ref": "http://jmlr.org/papers/volume16/zhang15a/zhang15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear Dimensionality Reduction: Survey, Insights, and Generalizations",
            "abstract": "Linear dimensionality reduction methods are a cornerstone of analyzing high dimensional data, due to their simple geometric interpretations and typically attractive computational properties. These methods capture many data features of interest, such as covariance, dynamical structure, correlation between data sets, input-output relationships, and margin between data classes. Methods have been developed with a variety of names and motivations in many fields, and perhaps as a result the connections between all these methods have not been highlighted. Here we survey methods from this disparate literature as optimization programs over matrix manifolds. We discuss principal component analysis, factor analysis, linear multidimensional scaling, Fisher's linear discriminant analysis, canonical correlations analysis, maximum autocorrelation factors, slow feature analysis, sufficient dimensionality reduction, undercomplete independent component analysis, linear regression, distance metric learning, and more. This optimization framework gives insight to some rarely discussed shortcomings of well-known methods, such as the suboptimality of certain eigenvector solutions. Modern techniques for optimization over matrix manifolds enable a generic linear dimensionality reduction solver, which accepts as input data and an objective to be optimized, and returns, as output, an optimal low-dimensional projection of the data. This simple optimization framework further allows straightforward generalizations and novel variants of classical methods, which we demonstrate here by creating an orthogonal-projection canonical correlations analysis. More broadly, this survey and generic solver suggest that linear dimensionality reduction can move toward becoming a blackbox, objective-agnostic numerical technology.",
            "keywords": [
                "dimensionality reduction",
                "eigenvector problems"
            ],
            "author": [
                "John P. Cunningham",
                "Zoubin Ghahramani"
            ],
            "ref": "http://jmlr.org/papers/volume16/cunningham15a/cunningham15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Randomized Causation Coefficient",
            "abstract": "We are interested in learning causal relationships between pairs of random variables, purely from observational data. To effectively address this task, the state-of-the-art relies on strong assumptions on the mechanisms mapping causes to effects, such as invertibility or the existence of additive noise, which only hold in limited situations. On the contrary, this short paper proposes to learn how to perform causal inference directly from data, without the need of feature engineering. In particular, we pose causality as a kernel mean embedding classification problem, where inputs are samples from arbitrary probability distributions on pairs of random variables, and labels are types of causal relationships. We validate the performance of our method on synthetic and real-world data against the state-of-the-art. Moreover, we submitted our algorithm to the ChaLearn's \"Fast Causation Coefficient Challenge\" competition, with which we won the fastest code prize and ranked third in the overall leaderboard.",
            "keywords": [
                "causality",
                "cause-effect inference",
                "kernel mean embeddings"
            ],
            "author": [
                "David Lopez-Paz",
                "Krikamol Mu",
                "et",
                "Benjamin Recht"
            ],
            "ref": "http://jmlr.org/papers/volume16/lopezpaz15a/lopezpaz15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimality of Poisson Processes Intensity Learning with Gaussian Processes",
            "abstract": "In this paper we provide theoretical support for the so-called \"Sigmoidal Gaussian Cox Process\" approach to learning the intensity of an inhomogeneous Poisson process on a -dimensional domain. This method was proposed by Adams, Murray and MacKay (ICML, 2009), who developed a tractable computational approach and showed in simulation and real data experiments that it can work quite satisfactorily. The results presented in the present paper provide theoretical underpinning of the method. In particular, we show how to tune the priors on the hyper parameters of the model in order for the procedure to automatically adapt to the degree of smoothness of the unknown intensity, and to achieve optimal convergence rates.",
            "keywords": [
                "inhomogeneous Poisson process",
                "Bayesian intensity learning",
                "Gaussian pro-     cess prior",
                "optimal rates"
            ],
            "author": [
                "Alisa Kirichenko",
                "Harry van Zanten"
            ],
            "ref": "http://jmlr.org/papers/volume16/kirichenko15a/kirichenko15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Combination of Feature Engineering and Ranking Models for Paper-Author Identification in KDD Cup 2013",
            "abstract": "This paper describes the winning solution of team National Taiwan University for track 1 of KDD Cup 2013. The track 1 in KDD Cup 2013 considers the paper-author identification problem, which is to identify whether a paper is truly written by an author. First, we conduct feature engineering to transform the various types of provided text information into 97 features. Second, we train classification and ranking models using these features. Last, we combine our individual models to boost the performance by using results on the internal validation set and the official Valid set. Some effective post-processing techniques have also been proposed. Our solution achieves 0.98259 MAP score and ranks the first place on the private leaderboard of the Test set.",
            "keywords": [
                "paper-author identification"
            ],
            "author": [
                "Chun-Liang Li",
                "Yu-Chuan Su",
                "Ting-Wei Lin",
                "Cheng-Hao Tsai",
                "Wei-Cheng Chang",
                "Kuan-Hao Huang",
                "Tzu-Ming Kuo",
                "Shan-Wei Lin",
                "Young-San Lin",
                "Yu-Chen Lu",
                "Chun-Pai Yang",
                "Cheng-Xia Chang",
                "Wei-Sheng Chin",
                "Yu-Chin Juan",
                "Hsiao-Yu Tung",
                "Jui-Pin Wang",
                "Cheng-Kuang Wei",
                "Felix Wu",
                "Tu-Chun Yin",
                "Tong Yu",
                "Yong Zhuang",
                "Shou-de Lin",
                "Hsuan-Tien Lin",
                "Chih-Jen Lin"
            ],
            "ref": "http://jmlr.org/papers/volume16/li15b/li15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Completing Any Low-rank Matrix, Provably",
            "abstract": "Matrix completion, i.e., the exact and provable recovery of a low-rank matrix from a small subset of its elements, is currently only known to be possible if the matrix satisfies a restrictive structural constraint---known as incoherence---on its row and column spaces. In these cases, the subset of elements is assumed to be sampled uniformly at random. In this paper, we show that any rank- -by- matrix can be exactly recovered from as few as  randomly chosen elements, provided this random choice is made according to a specific biased distribution suitably dependent on the coherence structure of the matrix: the probability of any element being sampled should be at least a constant times the sum of the leverage scores of the corresponding row and column. Moreover, we prove that this specific form of sampling is nearly necessary, in a natural precise sense; this implies that many other perhaps more intuitive sampling schemes fail. We further establish three ways to use the above result for the setting when leverage scores are not known a priori. (a) We describe a provably-correct sampling strategy for the case when only the column space is incoherent and no assumption or knowledge of the row space is required. (b) We propose a two-phase sampling procedure for general matrices that first samples to estimate leverage scores followed by sampling for exact recovery. These two approaches assume control over the sampling procedure. (c) By using our main theorem in a reverse direction, we provide an analysis showing the advantages of the (empirically successful) weighted nuclear/trace-norm minimization approach over the vanilla un- weighted formulation given non-uniformly distributed observed elements. This approach does not require controlled sampling or knowledge of the leverage scores.",
            "keywords": [
                "matrix completion",
                "coherence",
                "leverage score",
                "nuclear norm"
            ],
            "author": [
                "Yudong Chen",
                "Srinadh Bhojanapalli",
                "Sujay Sanghavi",
                "Rachel Ward"
            ],
            "ref": "http://jmlr.org/papers/volume16/chen15b/chen15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Eigenwords: Spectral Word Embeddings",
            "abstract": "Spectral learning algorithms have recently become popular in data-rich domains, driven in part by recent advances in large scale randomized SVD, and in spectral estimation of Hidden Markov Models. Extensions of these methods lead to statistical estimation algorithms which are not only fast, scalable, and useful on real data sets, but are also provably correct. Following this line of research, we propose four fast and scalable spectral algorithms for learning word embeddings -- low dimensional real vectors (called Eigenwords) that capture the \"meaning\" of words from their context. All the proposed algorithms harness the multi-view nature of text data i.e. the left and right context of each word, are fast to train and have strong theoretical properties. Some of the variants also have lower sample complexity and hence higher statistical power for rare words. We provide theory which establishes relationships between these algorithms and optimality criteria for the estimates they provide. We also perform thorough qualitative and quantitative evaluation of Eigenwords showing that simple linear approaches give performance comparable to or superior than the state-of-the-art non-linear deep learning based methods.",
            "keywords": [
                "spectral learning",
                "CCA",
                "word embeddings"
            ],
            "author": [
                "Paramveer S. Dhillon",
                "Dean P.  Foster",
                "Lyle H. Ungar"
            ],
            "ref": "http://jmlr.org/papers/volume16/dhillon15a/dhillon15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Discrete Reproducing Kernel Hilbert Spaces: Sampling and Distribution of Dirac-masses",
            "abstract": "We study reproducing kernels, and associated reproducing kernel Hilbert spaces (RKHSs)  over infinite, discrete and countable sets . In this setting we analyze in detail the distributions of the corresponding Dirac point-masses of . Illustrations include certain models from neural networks: An Extreme Learning Machine (ELM) is a neural network-configuration in which a hidden layer of weights are randomly sampled, and where the object is then to compute resulting output. For RKHSs  of functions defined on a prescribed countable infinite discrete set , we characterize those which contain the Dirac masses  for all points  in . Further examples and applications where this question plays an important role are: (i) discrete Brownian motion-Hilbert spaces, i.e., discrete versions of the Cameron-Martin Hilbert space; (ii) energy-Hilbert spaces corresponding to graph-Laplacians where the set  of vertices is then equipped with a resistance metric; and finally (iii) the study of Gaussian free fields.",
            "keywords": [
                "Gaussian reproducing kernel Hilbert spaces",
                "sampling in discrete systems",
                "resistance metric",
                "graph Laplacians"
            ],
            "author": [
                "Palle Jorgensen",
                "Feng Tian"
            ],
            "ref": "http://jmlr.org/papers/volume16/jorgensen15a/jorgensen15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Direct Estimation of High Dimensional Stationary Vector Autoregressions",
            "abstract": "The vector autoregressive (VAR) model is a powerful tool in learning complex time series and has been exploited in many fields. The VAR model poses some unique challenges to researchers: On one hand, the dimensionality, introduced by incorporating multiple numbers of time series and adding the order of the vector autoregression, is usually much higher than the time series length; On the other hand, the temporal dependence structure naturally present in the VAR model gives rise to extra difficulties in data analysis. The regular way in cracking the VAR model is via \"least squares\" and usually involves adding different penalty terms (e.g., ridge or lasso penalty) in handling high dimensionality. In this manuscript, we propose an alternative way in estimating the VAR model. The main idea is, via exploiting the temporal dependence structure, formulating the estimating problem to a linear program. There is instant advantage of the proposed approach over the lasso-type estimators: The estimation equation can be decomposed to multiple sub-equations and accordingly can be solved efficiently using parallel computing. Besides that, we also bring new theoretical insights into the VAR model analysis. So far the theoretical results developed in high dimensions (e.g., Song and Bickel, 2011 and Kock and Callot, 2015) are based on stringent assumptions that are not transparent. Our results, on the other hand, show that the spectral norms of the transition matrices play an important role in estimation accuracy and build estimation and prediction consistency accordingly. Moreover, we provide some experiments on both synthetic and real-world equity data. We show that there are empirical advantages of our method over the lasso-type estimators in parameter estimation and forecasting.",
            "keywords": [
                "transition matrix",
                "multivariate time series",
                "vector autoregressive model",
                "double asymptotic framework"
            ],
            "author": [
                "Fang Han",
                "Huanran Lu",
                "Han Liu"
            ],
            "ref": "http://jmlr.org/papers/volume16/han15a/han15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Global Convergence of Online Limited Memory BFGS",
            "abstract": "Global convergence of an online (stochastic) limited memory version of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi- Newton method for solving optimization problems with stochastic objectives that arise in large scale machine learning is established. Lower and upper bounds on the Hessian eigenvalues of the sample functions are shown to suffice to guarantee that the curvature approximation matrices have bounded determinants and traces, which, in turn, permits establishing convergence to optimal arguments with probability 1. Experimental evaluation on a search engine advertising problem showcase reductions in convergence time relative to stochastic gradient descent algorithms.",
            "keywords": [
                "quasi-Newton methods",
                "large-scale optimization"
            ],
            "author": [
                "Aryan Mokhtari",
                "Alej",
                "ro Ribeiro"
            ],
            "ref": "http://jmlr.org/papers/volume16/mokhtari15a/mokhtari15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Semi-Supervised Linear Regression in Covariate Shift Problems",
            "abstract": "Semi-supervised learning approaches are trained using the full training (labeled) data and available testing (unlabeled) data. Demonstrations of the value of training with unlabeled data typically depend on a smoothness assumption relating the conditional expectation to high density regions of the marginal distribution and an inherent missing completely at random assumption for the labeling. So-called covariate shift poses a challenge for many existing semi-supervised or supervised learning techniques. Covariate shift models allow the marginal distributions of the labeled and unlabeled feature data to differ, but the conditional distribution of the response given the feature data is the same. An example of this occurs when a complete labeled data sample and then an unlabeled sample are obtained sequentially, as it would likely follow that the distributions of the feature data are quite different between samples. The value of using unlabeled data during training for the elastic net is justified geometrically in such practical covariate shift problems. The approach works by obtaining adjusted coefficients for unlabeled prediction which recalibrate the supervised elastic net to compromise: (i) maintaining elastic net predictions on the labeled data with (ii) shrinking unlabeled predictions to zero. Our approach is shown to dominate linear supervised alternatives on unlabeled response predictions when the unlabeled feature data are concentrated on a low dimensional manifold away from the labeled data and the true coefficient vector emphasizes directions away from this manifold. Large variance of the supervised predictions on the unlabeled set is reduced more than the increase in squared bias when the unlabeled responses are expected to be small, so an improved compromise within the bias-variance tradeoff is the rationale for this performance improvement. Performance is validated on simulated and real data.",
            "keywords": [
                "joint optimization",
                "semi-supervised regression"
            ],
            "author": [
                "Kenneth Joseph Ryan",
                "Mark Vere Culp"
            ],
            "ref": "http://jmlr.org/papers/volume16/ryan15a/ryan15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Plug-and-Play Dual-Tree Algorithm Runtime Analysis",
            "abstract": "Numerous machine learning algorithms contain pairwise statistical problems at their core---that is, tasks that require computations over all pairs of input points if implemented naively. Often, tree structures are used to solve these problems efficiently. Dual-tree algorithms can efficiently solve or approximate many of these problems. Using cover trees, rigorous worst-case runtime guarantees have been proven for some of these algorithms. In this paper, we present a problem- independent runtime guarantee for any dual-tree algorithm using the cover tree, separating out the problem- dependent and the problem-independent elements. This allows us to just plug in bounds for the problem-dependent elements to get runtime guarantees for dual-tree algorithms for any pairwise statistical problem without re-deriving the entire proof. We demonstrate this plug-and-play procedure for nearest-neighbor search and approximate kernel density estimation to get improved runtime guarantees. Under mild assumptions, we also present the first linear runtime guarantee for dual-tree based range search.",
            "keywords": [
                "dual-tree algorithms",
                "adaptive runtime analysis",
                "cover tree",
                "expansion con-     stant",
                "nearest neighbor search",
                "kernel density estimation"
            ],
            "author": [
                "Ryan R. Curtin",
                "Dongryeol Lee",
                "William B. March",
                "Parikshit Ram"
            ],
            "ref": "http://jmlr.org/papers/volume16/curtin15a/curtin15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with Minimax Optimal Rates",
            "abstract": "We study a decomposition-based scalable approach to kernel ridge regression, and show that it achieves minimax optimal convergence rates under relatively mild conditions. The method is simple to describe: it randomly partitions a dataset of size  into  subsets of equal size, computes an independent kernel ridge regression estimator for each subset using a careful choice of the regularization parameter, then averages the local solutions into a global predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing kernel ridge regression on all  samples. Our two main theorems establish that despite the computational speed-up, statistical optimality is retained: as long as  is not too large, the partition-based estimator achieves the statistical minimax rate over all estimators using the set of  samples. As concrete examples, our theory guarantees that the number of subsets  may grow nearly linearly for finite-rank or Gaussian kernels and polynomially in  for Sobolev spaces, which in turn allows for substantial reductions in computational cost. We conclude with experiments on both simulated data and a music-prediction task that complement our theoretical results, exhibiting the computational and statistical benefits of our approach.",
            "keywords": [
                "kernel ridge regression",
                "divide and conquer"
            ],
            "author": [
                "Yuchen Zhang",
                "John Duchi",
                "Martin Wainwright"
            ],
            "ref": "http://jmlr.org/papers/volume16/zhang15d/zhang15d.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Theory of Randomized Kaczmarz Algorithm",
            "abstract": "A relaxed randomized Kaczmarz algorithm is investigated in a least squares regression setting by a learning theory approach. When the sampling values are accurate and the regression function (conditional means) is linear, such an algorithm has been well studied in the community of non-uniform sampling. In this paper, we are mainly interested in the different case of either noisy random measurements or a nonlinear regression function. In this case, we show that relaxation is needed. A necessary and sufficient condition on the sequence of relaxation parameters or step sizes for the convergence of the algorithm in expectation is presented. Moreover, polynomial rates of convergence, both in expectation and in probability, are provided explicitly. As a result, the almost sure convergence of the algorithm is proved by applying the Borel-Cantelli Lemma.",
            "keywords": [
                "learning theory",
                "relaxed randomized Kaczmarz algorithm",
                "online learning",
                "space of homogeneous linear functions"
            ],
            "author": [
                "Junhong Lin",
                "Ding-Xuan Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume16/lin15a/lin15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares",
            "abstract": "The matrix-completion problem has attracted a lot of attention, largely as a result of the celebrated Netflix competition. Two popular approaches for solving the problem are nuclear-norm- regularized matrix approximation (Candes and Tao, 2009; Mazumder et al., 2010), and maximum-margin matrix factorization (Srebro et al., 2005). These two procedures are in some cases solving equivalent problems, but with quite different algorithms. In this article we bring the two approaches together, leading to an efficient algorithm for large matrix factorization and completion that outperforms both of these. We develop a software package softImpute in R for implementing our approaches, and a distributed version for very large matrices using the Spark cluster programming environment",
            "keywords": [
                "matrix completion",
                "alternating least squares",
                "svd"
            ],
            "author": [
                "Trevor Hastie",
                "Rahul Mazumder",
                "Jason D. Lee",
                "Reza Zadeh"
            ],
            "ref": "http://jmlr.org/papers/volume16/hastie15a/hastie15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Inductive Bias of Dropout",
            "abstract": "Dropout is a simple but effective technique for learning in neural networks and other settings. A sound theoretical understanding of dropout is needed to determine when dropout should be applied and how to use it most effectively. In this paper we continue the exploration of dropout as a regularizer pioneered by Wager et al. We focus on linear classification where a convex proxy to the misclassification loss (i.e. the logistic loss used in logistic regression) is minimized. We show:",
            "keywords": [
                "dropout",
                "inductive bias",
                "learning theory",
                "regularization"
            ],
            "author": [
                "David P. Helmbold",
                "Philip M. Long"
            ],
            "ref": "http://jmlr.org/papers/volume16/helmbold15a/helmbold15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Agnostic Learning of Disjunctions on Symmetric Distributions",
            "abstract": "",
            "keywords": [
                "agnostic learning",
                "symmetric distribution",
                "polynomial approximation",
                "regres-     sion",
                "disjunction",
                "conjunction",
                "DNF"
            ],
            "author": [
                "Vitaly Feldman",
                "Pravesh Kothari"
            ],
            "ref": "http://jmlr.org/papers/volume16/feldman15a/feldman15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Sample Complexity of Learning Linear Predictors with the Squared Loss",
            "abstract": "We provide a tight sample complexity bound for learning bounded- norm linear predictors with respect to the squared loss. Our focus is on an agnostic PAC-style setting, where no assumptions are made on the data distribution beyond boundedness. This contrasts with existing results in the literature, which rely on other distributional assumptions, refer to specific parameter settings, or use other performance measures.",
            "keywords": [
                "sample complexity",
                "squared loss",
                "linear predictors"
            ],
            "author": [
                "Ohad Shamir"
            ],
            "ref": "http://jmlr.org/papers/volume16/shamir15a/shamir15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Analysis of Active Learning",
            "abstract": "This work establishes distribution-free upper and lower bounds on the minimax label complexity of active learning with general hypothesis classes, under various noise models. The results reveal a number of surprising facts. In particular, under the noise model of Tsybakov (2004), the minimax label complexity of active learning with a VC class is always asymptotically smaller than that of passive learning, and is typically significantly smaller than the best previously-published upper bounds in the active learning literature. In high-noise regimes, it turns out that all active learning problems of a given VC dimension have roughly the same minimax label complexity, which contrasts with well-known results for bounded noise. In low-noise regimes, we find that the label complexity is well-characterized by a simple combinatorial complexity measure we call the star number. Interestingly, we find that almost all of the complexity measures previously explored in the active learning literature have worst-case values exactly equal to the star number. We also propose new active learning strategies that nearly achieve these minimax label complexities.",
            "keywords": [
                "active learning",
                "selective sampling",
                "sequential design",
                "adaptive sampling",
                "statistical learning theory",
                "margin condition",
                "Tsybakov noise",
                "sample complexity"
            ],
            "author": [
                "Steve Hanneke",
                "Liu Yang"
            ],
            "ref": "http://jmlr.org/papers/volume16/hanneke15a/hanneke15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Rates for Persistence Diagram Estimation in Topological Data Analysis",
            "abstract": "Computational topology has recently seen an important development toward data analysis, giving birth to the field of topological data analysis. Topological persistence, or persistent homology, appears as a fundamental tool in this field. In this paper, we study topological persistence in general metric spaces, with a statistical approach. We show that the use of persistent homology can be naturally considered in general statistical frameworks and that persistence diagrams can be used as statistics with interesting convergence properties. Some numerical experiments are performed in various contexts to illustrate our results.",
            "keywords": [
                "persistent homology",
                "convergence rates"
            ],
            "author": [
                "Frédéric Chazal",
                "Marc Glisse",
                "Catherine Labruère",
                "Bertrand Michel"
            ],
            "ref": "http://jmlr.org/papers/volume16/chazal15a/chazal15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Supervised Learning via Euler's Elastica Models",
            "abstract": "This paper investigates the Euler's elastica (EE) model for high-dimensional supervised learning problems in a function approximation framework. In 1744 Euler introduced the elastica energy for a 2D curve on modeling torsion-free thin elastic rods. Together with its degenerate form of total variation (TV), Euler's elastica has been successfully applied to low- dimensional data processing such as image denoising and image inpainting in the last two decades. Our motivation is to apply Euler's elastica to high-dimensional supervised learning problems. To this end, a supervised learning problem is modeled as an energy functional minimization under a new geometric regularization scheme, where the energy is composed of a squared loss and an elastica penalty. The elastica penalty aims at regularizing the approximated function by heavily penalizing large gradients and high curvature values on all level curves. We take a computational PDE approach to minimize the energy functional. By using variational principles, the energy minimization problem is transformed into an Euler-Lagrange PDE. However, this PDE is usually high-dimensional and can not be directly handled by common low-dimensional solvers. To circumvent this difficulty, we use radial basis functions (RBF) to approximate the target function, which reduces the optimization problem to finding the linear coefficients of these basis functions. Some theoretical properties of this new model, including the existence and uniqueness of solutions and universal consistency, are analyzed. Extensive experiments have demonstrated the effectiveness of the proposed model for binary classification, multi-class classification, and regression tasks.",
            "keywords": [
                "supervised learning"
            ],
            "author": [
                "Tong Lin",
                "Hanlin Xue",
                "Ling Wang",
                "Bo Huang",
                "Hongbin Zha"
            ],
            "ref": "http://jmlr.org/papers/volume16/lin15b/lin15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning to Identify Concise Regular Expressions that Describe Email Campaigns",
            "abstract": "This paper addresses the problem of inferring a regular expression from a given set of strings that resembles, as closely as possible, the regular expression that a human expert would have written to identify the language. This is motivated by our goal of automating the task of postmasters who use regular expressions to describe and blacklist email spam campaigns. Training data contains batches of messages and corresponding regular expressions that an expert postmaster feels confident to blacklist. We model this task as a two-stage learning problem with structured output spaces and appropriate loss functions. We derive decoders and the resulting optimization problems which can be solved using standard cutting plane methods. We report on a case study conducted with an email service provider.",
            "keywords": [
                "applications of machine learning",
                "learning with structured output spaces",
                "supervised learning",
                "regular expressions"
            ],
            "author": [
                "Paul Prasse",
                "Christoph Sawade",
                "Niels L",
                "wehr",
                "Tobias Scheffer"
            ],
            "ref": "http://jmlr.org/papers/volume16/prasse15a/prasse15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Condition for Perfect Dimensionality Recovery by Variational Bayesian PCA",
            "abstract": "Having shown its good performance in many applications, variational Bayesian (VB) learning is known to be one of the best tractable approximations to Bayesian learning. However, its performance was not well understood theoretically. In this paper, we clarify the behavior of VB learning in probabilistic PCA (or fully-observed matrix factorization). More specifically, we establish a necessary and sufficient condition for perfect dimensionality (or rank) recovery in the large-scale limit when the matrix size goes to infinity. Our result theoretically guarantees the performance of VB-PCA. At the same time, it also reveals the conservative nature of VB learning--- it offers a low false positive rate at the expense of low sensitivity. By contrasting with an alternative dimensionality selection method, we characterize VB learning in PCA. In our analysis, we obtain bounds of the noise variance estimator, and a new and simple analytic-form solution for the other parameters, which themselves are useful for implementation of VB-PCA.",
            "keywords": [
                "variational Bayesian learning",
                "matrix factorization",
                "principal component     analysis",
                "automatic relevance determination"
            ],
            "author": [
                "Shinichi Nakajima",
                "Ryota Tomioka",
                "Masashi Sugiyama",
                "S. Derin Babacan"
            ],
            "ref": "http://jmlr.org/papers/volume16/nakajima15a/nakajima15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graphical Models via Univariate Exponential Family Distributions",
            "abstract": "Undirected graphical models, or Markov networks, are a popular class of statistical models, used in a wide variety of applications. Popular instances of this class include Gaussian graphical models and Ising models. In many settings, however, it might not be clear which subclass of graphical models to use, particularly for non-Gaussian and non-categorical data. In this paper, we consider a general sub-class of graphical models where the node-wise conditional distributions arise from exponential families. This allows us to derive multivariate graphical model distributions from univariate exponential family distributions, such as the Poisson, negative binomial, and exponential distributions. Our key contributions include a class of M-estimators to fit these graphical model distributions; and rigorous statistical analysis showing that these M-estimators recover the true graphical model structure exactly, with high probability. We provide examples of genomic and proteomic networks learned via instances of our class of graphical models derived from Poisson and exponential distributions.",
            "keywords": [
                "graphical models",
                "model selection"
            ],
            "author": [
                "Eunho Yang",
                "Pradeep Ravikumar",
                "Genevera I. Allen",
                "Zhandong Liu"
            ],
            "ref": "http://jmlr.org/papers/volume16/yang15a/yang15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Marginalizing Stacked Linear Denoising Autoencoders",
            "abstract": "Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. They have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized Stacked Linear Denoising Autoencoder (mSLDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSLDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters --- in fact, the linear formulation gives rise to a closed-form solution. Consequently, mSLDA, which can be implemented in only 20 lines of MATLAB, is about two orders of magnitude faster than a corresponding SDA. Furthermore, the representations learnt by mSLDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.",
            "keywords": [
                "domain adaption",
                "fast representation learning",
                "noise marginalization"
            ],
            "author": [
                "Minmin Chen",
                "Kilian Q. Weinberger",
                "Zhixiang (Eddie) Xu",
                "Fei Sha"
            ],
            "ref": "http://jmlr.org/papers/volume16/chen15c/chen15c.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PAC Optimal MDP Planning with Application to Invasive Species Management",
            "abstract": "In a simulator-defined MDP, the Markovian dynamics and rewards are provided in the form of a simulator from which samples can be drawn. This paper studies MDP planning algorithms that attempt to minimize the number of simulator calls before terminating and outputting a policy that is approximately optimal with high probability. The paper introduces two heuristics for efficient exploration and an improved confidence interval that enables earlier termination with probabilistic guarantees. We prove that the heuristics and the confidence interval are sound and produce with high probability an approximately optimal policy in polynomial time. Experiments on two benchmark problems and two instances of an invasive species management problem show that the improved confidence intervals and the new search heuristics yield reductions of between 8% and 47% in the number of simulator calls required to reach near- optimal policies.",
            "keywords": [
                "invasive species management",
                "Markov decision processes",
                "MDP planning",
                "Good-     Turing estimate"
            ],
            "author": [
                "Majid Alkaee Taleghan",
                "Thomas G. Dietterich",
                "Mark Crowley",
                "Kim Hall",
                "H. Jo Albers"
            ],
            "ref": "http://jmlr.org/papers/volume16/taleghan15a/taleghan15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "partykit: A Modular Toolkit for Recursive Partytioning in R",
            "abstract": "The R package partykit provides a flexible toolkit for learning, representing, summarizing, and visualizing a wide range of tree- structured regression and classification models. The functionality encompasses: (a) basic infrastructure for representing trees (inferred by any algorithm) so that unified print/plot/predict  methods are available; (b) dedicated methods for trees with constant fits in the leaves (or terminal nodes) along with suitable coercion functions to create such trees (e.g., by rpart, RWeka, PMML); (c) a reimplementation of conditional inference trees (ctree, originally provided in the party package); (d) an extended reimplementation of model-based recursive     partitioning (mob, also originally in party) along with dedicated methods for trees with parametric models in the leaves. Here, a brief overview of the package and its design is given while more detailed discussions of items (a)—(d) are available in vignettes accompanying the package.",
            "keywords": [
                "recursive partitioning",
                "regression trees",
                "classification trees",
                "statistical learning"
            ],
            "author": [
                "Torsten Hothorn",
                "Achim Zeileis"
            ],
            "ref": "http://jmlr.org/papers/volume16/hothorn15a/hothorn15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Complexity of Best-Arm Identification in Multi-Armed Bandit Models",
            "abstract": "The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the  best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when  under general assumptions. In the specific case of two armed- bandits, we derive refined lower bounds in both the fixed- confidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed- budget setting may be smaller than the complexity of the fixed- confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest : a deviation lemma for self-normalized sums (Lemma 7) and a novel change of measure inequality for bandit models (Lemma 1).",
            "keywords": [
                "multi-armed bandit",
                "best-arm identification",
                "pure exploration",
                "information-     theoretic divergences"
            ],
            "author": [
                "Emilie Kaufmann",
                "Olivier Cappé",
                "Aurélien Garivier"
            ],
            "ref": "http://jmlr.org/papers/volume17/kaufman16a/kaufman16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiscale Dictionary Learning: Non-Asymptotic Bounds and Robustness",
            "abstract": "",
            "keywords": [
                "dictionary learning",
                "multi-resolution analysis",
                "manifold learning",
                "robustness",
                "sparsityc 2016 Mauro Maggioni",
                "Stanislav Minsker"
            ],
            "author": [
                "Mauro Maggioni",
                "Stanislav Minsker",
                "Nate Strawn"
            ],
            "ref": "http://jmlr.org/papers/volume17/maggioni16a/maggioni16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistent Algorithms for Clustering Time Series",
            "abstract": "The problem of clustering is considered for the case where every point is a time series. The time series are either given in one batch (offline setting), or they are allowed to grow with time and new time series can be added along the way (online setting). We propose a natural notion of consistency for this problem, and show that there are simple, computationally efficient algorithms that are asymptotically consistent under extremely weak assumptions on the distributions that generate the data. The notion of consistency is as follows. A clustering algorithm is called consistent if it places two time series into the same cluster if and only if the distribution that generates them is the same. In the considered framework the time series are allowed to be highly dependent, and the dependence can have arbitrary form. If the number of clusters is known, the only assumption we make is that the (marginal) distribution of each time series is stationary ergodic. No parametric, memory or mixing assumptions are made. When the number of clusters is unknown, stronger assumptions are provably necessary, but it is still possible to devise nonparametric algorithms that are consistent under very general conditions. The theoretical findings of this work are illustrated with experiments on both synthetic and real data.",
            "keywords": [
                "clustering",
                "time series",
                "ergodicity"
            ],
            "author": [
                "Azadeh Khaleghi",
                "Daniil Ryabko",
                "Jérémie Mary",
                "Philippe Preux"
            ],
            "ref": "http://jmlr.org/papers/volume17/khaleghi16a/khaleghi16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Rotation Ensembles",
            "abstract": "In machine learning, ensemble methods combine the predictions of multiple base learners to construct more accurate aggregate predictions. Established supervised learning algorithms inject randomness into the construction of the individual base learners in an effort to promote diversity within the resulting ensembles. An undesirable side effect of this approach is that it generally also reduces the accuracy of the base learners. In this paper, we introduce a method that is simple to implement yet general and effective in improving ensemble diversity with only modest impact on the accuracy of the individual base learners. By randomly rotating the feature space prior to inducing the base learners, we achieve favorable aggregate predictions on standard data sets compared to state of the art ensemble methods, most notably for tree-based ensembles, which are particularly sensitive to rotation.",
            "keywords": [
                "feature rotation",
                "ensemble diversity"
            ],
            "author": [
                "Rico Blaser",
                "Piotr Fryzlewicz"
            ],
            "ref": "http://jmlr.org/papers/volume17/blaser16a/blaser16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Should We Really Use Post-Hoc Tests Based on Mean-Ranks?",
            "abstract": "The statistical comparison of multiple algorithms over multiple data sets is fundamental in machine learning. This is typically carried out by the Friedman test. When the Friedman test rejects the null hypothesis, multiple comparisons are carried out to establish which are the significant differences among algorithms. The multiple comparisons are usually performed using the mean-ranks test. The aim of this technical note is to discuss the inconsistencies of the mean-ranks post-hoc test with the goal of discouraging its use in machine learning as well as in medicine, psychology, etc.. We show that the outcome of the mean-ranks test depends on the pool of algorithms originally included in the experiment. In other words, the outcome of the comparison between algorithms  and  depends also on the performance of the other algorithms included in the original experiment. This can lead to paradoxical situations. For instance the difference between  and  could be declared significant if the pool comprises algorithms  and not significant if the pool comprises algorithms . To overcome these issues, we suggest instead to perform the multiple comparison using a test whose outcome only depends on the two algorithms being compared, such as the sign-test or the Wilcoxon signed-rank test.",
            "keywords": [
                "statistical comparison",
                "Friedman test"
            ],
            "author": [
                "Alessio Benavoli",
                "Giorgio Corani",
                "Francesca Mangili"
            ],
            "ref": "http://jmlr.org/papers/volume17/benavoli16a/benavoli16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Rates in Permutation Estimation for Feature Matching",
            "abstract": "The problem of matching two sets of features appears in various tasks of computer vision and can be often formalized as a problem of permutation estimation. We address this problem from a statistical point of view and provide a theoretical analysis of the accuracy of several natural estimators. To this end, the minimax rate of separation is investigated and its expression is obtained as a function of the sample size, noise level and dimension of the features. We consider the cases of homoscedastic and heteroscedastic noise and establish, in each case, tight upper bounds on the separation distance of several estimators. These upper bounds are shown to be unimprovable both in the homoscedastic and heteroscedastic settings. Interestingly, these bounds demonstrate that a phase transition occurs when the dimension  of the features is of the order of the logarithm of the number of features . For , the rate is dimension free and equals , where  is the noise level. In contrast, when  is larger than  for some constant , the minimax rate increases with  and is of the order of . We also discuss the computational aspects of the estimators and provide empirical evidence of their consistency on synthetic data. Finally, we show that our results extend to more general matching criteria.",
            "keywords": [
                "permutation estimation",
                "minimax rate of separation"
            ],
            "author": [
                "Olivier Collier",
                "Arnak S. Dalalyan"
            ],
            "ref": "http://jmlr.org/papers/volume17/collier16a/collier16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics",
            "abstract": "Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data sets is computationally expensive. Both the calculation of the acceptance probability and the creation of informed proposals usually require an iteration through the whole data set. The recently proposed stochastic gradient Langevin dynamics (SGLD) method circumvents this problem by generating proposals which are only based on a subset of the data, by skipping the accept-reject step and by using decreasing step-sizes sequence . We provide in this article a rigorous mathematical framework for analysing this algorithm. We prove that, under verifiable assumptions, the algorithm is consistent, satisfies a central limit theorem (CLT) and its asymptotic bias-variance decomposition can be characterized by an explicit functional of the step-sizes sequence . We leverage this analysis to give practical recommendations for the notoriously difficult tuning of this algorithm: it is asymptotically optimal to use a step-size sequence of the type , leading to an algorithm whose mean squared error (MSE) decreases at rate .",
            "keywords": [
                "Markov chain Monte Carlo",
                "Langevin dynamics"
            ],
            "author": [
                "Yee Whye Teh",
                "Alexandre H. Thiery",
                "Sebastian J. Vollmer"
            ],
            "ref": "http://jmlr.org/papers/volume17/teh16a/teh16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Knowledge Matters: Importance of Prior Information for Optimization",
            "abstract": "We explored the effect of introducing prior knowledge into the intermediate level of deep supervised neural networks on two tasks. On a task we designed, all black-box state-of-the-art machine learning algorithms which we tested, failed to generalize well. We motivate our work from the hypothesis that, there is a training barrier involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals by using a form of supervision or guidance using a curriculum. Our results provide a positive evidence in favor of this hypothesis. In our experiments, we trained a two- tiered MLP architecture on a dataset for which each input image contains three sprites, and the binary target class is  if all of three shapes belong to the same category and otherwise the class is . In terms of generalization, black-box machine learning algorithms could not perform better than chance on this task. Standard deep supervised neural networks also failed to generalize. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allowed us to solve the task efficiently. We obtained much better than chance, but imperfect results by exploring different architectures and optimization variants. This observation might be an indication of optimization difficulty when the neural network trained without hints on this task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with the hypotheses on cultural learning inspired by the observations of training of neural networks sometimes getting stuck, even though good solutions exist, both in terms of training and generalization error.",
            "keywords": [
                "deep learning",
                "neural networks",
                "optimization",
                "evolution of culture",
                "curriculum     learning"
            ],
            "author": [
                "Çağlar Gülçehre",
                "Yoshua Bengio"
            ],
            "ref": "http://jmlr.org/papers/volume17/gulchere16a/gulchere16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Harry: A Tool for Measuring String Similarity",
            "abstract": "Comparing strings and assessing their similarity is a basic operation in many application domains of machine learning, such as in information retrieval, natural language processing and bioinformatics. The practitioner can choose from a large variety of available similarity measures for this task, each emphasizing different aspects of the string data. In this article, we present Harry, a small tool specifically designed for measuring the similarity of strings. Harry implements over 20 similarity measures, including common string distances and string kernels, such as the Levenshtein distance and the Subsequence kernel. The tool has been designed with efficiency in mind and allows for multi-threaded as well as distributed computing, enabling the analysis of large data sets of strings. Harry supports common data formats and thus can interface with analysis environments, such as Matlab, Pylab and Weka.",
            "keywords": [
                "string kernels",
                "string distances"
            ],
            "author": [
                "Konrad Rieck",
                "Christian Wressnegger"
            ],
            "ref": "http://jmlr.org/papers/volume17/rieck16a/rieck16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Herded Gibbs Sampling",
            "abstract": "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an  convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.",
            "keywords": [
                "Gibbs sampling",
                "herding"
            ],
            "author": [
                "Yutian Chen",
                "Luke Bornn",
                "Nando de Freitas",
                "Mareija Eskelin",
                "Jing Fang",
                "Max Welling"
            ],
            "ref": "http://jmlr.org/papers/volume17/chen16a/chen16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Complexity of Representation and Inference in Compositional Models with Part Sharing",
            "abstract": "This paper performs a complexity analysis of a class of serial and parallel compositional models of multiple objects and shows that they enable efficient representation and rapid inference. Compositional models are generative and represent objects in a hierarchically distributed manner in terms of parts and subparts, which are constructed recursively by part-subpart compositions. Parts are represented more coarsely at higher level of the hierarchy, so that the upper levels give coarse summary descriptions (e.g., there is a horse in the image) while the lower levels represents the details (e.g., the positions of the legs of the horse). This hierarchically distributed representation obeys the executive summary principle, meaning that a high level executive only requires a coarse summary description and can, if necessary, get more details by consulting lower level executives. The parts and subparts are organized in terms of hierarchical dictionaries which enables part sharing between different objects allowing efficient representation of many objects. The first main contribution of this paper is to show that compositional models can be mapped onto a parallel visual architecture similar to that used by bio- inspired visual models such as deep convolutional networks but more explicit in terms of representation, hence enabling part detection as well as object detection, and suitable for complexity analysis. Inference algorithms can be run on this architecture to exploit the gains caused by part sharing and executive summary. Effectively, this compositional architecture enables us to perform exact inference simultaneously over a large class of generative models of objects. The second contribution is an analysis of the complexity of compositional models in terms of computation time (for serial computers) and numbers of nodes (e.g., \"neurons\") for parallel computers. In particular, we compute the complexity gains by part sharing and executive summary and their dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level of the hierarchy, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can enable linear processing on parallel computers.",
            "keywords": [],
            "author": [
                "Alan Yuille",
                "Roozbeh Mottaghi"
            ],
            "ref": "http://jmlr.org/papers/volume17/yuille16a/yuille16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Noisy Sparse Subspace Clustering",
            "abstract": "This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabeled input data points, which are assumed to be in a union of low-dimensional subspaces. We show that a modified version of SSC is provably effective in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to more practical settings and provides justification to the success of SSC in a class of real applications.",
            "keywords": [
                "Subspace clustering",
                "robustness",
                "stability",
                "compressive sensing"
            ],
            "author": [
                "Yu-Xiang Wang",
                "Huan Xu"
            ],
            "ref": "http://jmlr.org/papers/volume17/13-354/13-354.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning the Variance of the Reward-To-Go",
            "abstract": "In Markov decision processes (MDPs), the variance of the reward- to-go is a natural measure of uncertainty about the long term performance of a policy, and is important in domains such as finance, resource allocation, and process control. Currently however, there is no tractable procedure for calculating it in large scale MDPs. This is in contrast to the case of the expected reward-to-go, also known as the value function, for which effective simulation-based algorithms are known, and have been used successfully in various domains. In this paper we extend temporal difference (TD) learning algorithms to estimating the variance of the reward-to- go for a fixed policy. We propose variants of both TD(0) and LSTD() with linear function approximation, prove their convergence, and demonstrate their utility in an option pricing problem. Our results show a dramatic improvement in terms of sample efficiency over standard Monte-Carlo methods, which are currently the state-of-the-art.",
            "keywords": [
                "Reinforcement learning",
                "Markov decision processes",
                "variance estimation",
                "simulation"
            ],
            "author": [
                "Aviv Tamar",
                "Dotan Di Castro",
                "Shie Mannor"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-335/14-335.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convex Calibration Dimension for Multiclass Loss Matrices",
            "abstract": "We study consistency properties of surrogate loss functions for general multiclass learning problems, defined by a general multiclass loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be calibrated with respect to a loss matrix in this setting. We then introduce the notion of convex calibration dimension of a multiclass loss matrix, which measures the smallest \"size\" of a prediction space in which it is possible to design a convex surrogate that is calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, we apply our framework to study various subset ranking losses, and use the convex calibration dimension as a tool to show both the existence and non-existence of various types of convex calibrated surrogates for these losses. Our results strengthen recent results of Duchi et al. (2010) and CalauzÃ¨nes et al. (2012) on the non-existence of certain types of convex calibrated surrogates in subset ranking. We anticipate the convex calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems.",
            "keywords": [
                "Statistical consistency",
                "multiclass loss",
                "loss matrix",
                "surrogate loss",
                "convex     surrogates",
                "calibrated surrogates",
                "classification calibration"
            ],
            "author": [
                "Harish G. Ramaswamy",
                "Shivani Agarwal"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-316/14-316.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "LLORMA: Local Low-Rank Matrix Approximation",
            "abstract": "Matrix approximation is a common tool in recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is low-rank. In this paper, we propose, analyze, and experiment with two procedures, one parallel and the other global, for constructing local matrix approximations. The two approaches approximate the observed matrix as a weighted sum of low-rank matrices. These matrices are limited to a local region of the observed matrix. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks.",
            "keywords": [
                "Matrix approximation",
                "non-parametric methods",
                "kernel smoothing",
                "collabo-     rative Filtering"
            ],
            "author": [
                "Joonseok Lee",
                "Seungyeon Kim",
                "Guy Lebanon",
                "Yoram Singer",
                "Samy Bengio"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-301/14-301.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Consistent Information Criterion for Support Vector Machines in Diverging Model Spaces",
            "abstract": "Information criteria have been popularly used in model selection and proved to possess nice theoretical properties. For classification, Claeskens et al. (2880) proposed support vector machine information criterion for feature selection and provided encouraging numerical evidence. Yet no theoretical justification was given there. This work aims to fill the gap and to provide some theoretical justifications for support vector machine information criterion in both fixed and diverging model spaces. We first derive a uniform convergence rate for the support vector machine solution and then show that a modification of the support vector machine information criterion achieves model selection consistency even when the number of features diverges at an exponential rate of the sample size. This consistency result can be further applied to selecting the optimal tuning parameter for various penalized support vector machine methods. Finite-sample performance of the proposed information criterion is investigated using Monte Carlo studies and one real-world gene selection problem.",
            "keywords": [
                "Bayesian Information Criterion",
                "Diverging Model Spaces",
                "Feature Selection"
            ],
            "author": [
                "Xiang Zhang",
                "Yichao Wu",
                "Lan Wang",
                "Runze Li"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-231/14-231.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Extremal Mechanisms for Local Differential Privacy",
            "abstract": "Local differential privacy has recently surfaced as a strong measure of privacy in contexts where personal information remains private even from data analysts. Working in a setting where both the data providers and data analysts want to maximize the utility of statistical analyses performed on the released data, we study the fundamental trade-off between local differential privacy and utility. This trade-off is formulated as a constrained optimization problem: maximize utility subject to local differential privacy constraints. We introduce a combinatorial family of extremal privatization mechanisms, which we call staircase mechanisms, and show that it contains the optimal privatization mechanisms for a broad class of information theoretic utilities such as mutual information and -divergences. We further prove that for any utility function and any privacy level, solving the privacy-utility maximization problem is equivalent to solving a finite-dimensional linear program, the outcome of which is the optimal staircase mechanism. However, solving this linear program can be computationally expensive since it has a number of variables that is exponential in the size of the alphabet the data lives in. To account for this, we show that two simple privatization mechanisms, the binary and randomized response mechanisms, are universally optimal in the low and high privacy regimes, and well approximate the intermediate regime.",
            "keywords": [
                "local differential privacy",
                "privacy-preserving machine learning algorithms",
                "information theoretic utilities",
                "f -divergences",
                "mutual information",
                "statistical inference",
                "hy-     pothesis testing"
            ],
            "author": [
                "Peter Kairouz",
                "Sewoong Oh",
                "Pramod Viswanath"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-135/15-135.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Loss Minimization and Parameter Estimation with Heavy Tails",
            "abstract": "This work studies applications and generalizations of a simple estimation technique that provides exponential concentration under heavy-tailed distributions, assuming only bounded low- order moments. We show that the technique can be used for approximate minimization of smooth and strongly convex losses, and specifically for least squares linear regression. For instance, our -dimensional estimator requires just  random samples to obtain a constant factor approximation to the optimal least squares loss with probability , without requiring the covariates or noise to be bounded or subgaussian. We provide further applications to sparse linear regression and low-rank covariance matrix estimation with similar allowances on the noise and covariate distributions. The core technique is a generalization of the median-of-means estimator to arbitrary metric spaces.",
            "keywords": [
                "Heavy-tailed distributions",
                "unbounded losses",
                "linear regression"
            ],
            "author": [
                "Daniel Hsu",
                "Sivan Sabato"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-273/14-273.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analysis of Classification-based Policy Iteration Algorithms",
            "abstract": "We introduce a variant of the classification-based approach to policy iteration which uses a cost-sensitive loss function weighting each classification mistake by its actual regret, that is, the difference between the action- value of the greedy action and of the action chosen by the classifier. For this algorithm, we provide a full finite-sample analysis. Our results state a performance bound in terms of the number of policy improvement steps, the number of rollouts used in each iteration, the capacity of the considered policy space (classifier), and a capacity measure which indicates how well the policy space can approximate policies that are greedy with respect to any of its members. The analysis reveals a tradeoff between the estimation and approximation errors in this classification-based policy iteration setting. Furthermore it confirms the intuition that classification-based policy iteration algorithms could be favorably compared to value-based approaches when the policies can be approximated more easily than their corresponding value functions. We also study the consistency of the algorithm when there exists a sequence of policy spaces with increasing capacity.",
            "keywords": [
                "reinforcement learning",
                "policy iteration",
                "classification-based approach to      policy iteration"
            ],
            "author": [
                "Alessandro Lazaric",
                "Mohammad Ghavamzadeh",
                "R{\\'e}mi Munos"
            ],
            "ref": "http://jmlr.org/papers/volume17/10-364/10-364.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MEKA: A Multi-label/Multi-target Extension to WEKA",
            "abstract": "Multi-label classification has rapidly attracted interest in the machine learning literature, and there are now a large number and considerable variety of methods for this type of learning. We present MEKA: an open-source Java framework based on the well-known WEKA library. MEKA provides interfaces to facilitate practical application, and a wealth of multi-label classifiers, evaluation metrics, and tools for multi-label experiments and development. It supports multi-label and multi-target data, including in incremental and semi- supervised contexts.",
            "keywords": [
                "classification",
                "learning",
                "multi-label",
                "multi-target"
            ],
            "author": [
                "Jesse Read",
                "Peter Reutemann",
                "Bernhard Pfahringer",
                "Geoff Holmes"
            ],
            "ref": "http://jmlr.org/papers/volume17/12-164/12-164.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gradients Weights improve Regression and Classification",
            "abstract": "In regression problems over , the unknown function  often varies more in some coordinates than in others. We show that weighting each coordinate  according to an estimate of the variation of  along coordinate  -- e.g. the  norm of the th-directional derivative of  -- is an efficient way to significantly improve the performance of distance-based regressors such as kernel and -NN regressors. The approach, termed Gradient Weighting (GW), consists of a first pass regression estimate  which serves to evaluate the directional derivatives of , and a second-pass regression estimate on the re-weighted data. The GW approach can be instantiated for both regression and classification, and is grounded in strong theoretical principles having to do with the way regression bias and variance are affected by a generic feature-weighting scheme. These theoretical principles provide further technical foundation for some existing feature-weighting heuristics that have proved successful in practice. We propose a simple estimator of these derivative norms and prove its consistency. The proposed estimator computes efficiently and easily extends to run online. We then derive a classification version of the GW approach which evaluates on real-worlds datasets with as much success as its regression counterpart.",
            "keywords": [
                "Nonparametric learning",
                "feature selection",
                "feature weighting",
                "nonparametric sparsity"
            ],
            "author": [
                "Samory Kpotufe",
                "Abdeslam Boularias",
                "Thomas Schultz",
                "Kyoungok Kim"
            ],
            "ref": "http://jmlr.org/papers/volume17/13-351/13-351.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Closer Look at Adaptive Regret",
            "abstract": "For the prediction with expert advice setting, we consider methods to construct algorithms that have low adaptive regret. The adaptive regret of an algorithm on a time interval  is the loss of the algorithm minus the loss of the best expert over that interval. Adaptive regret measures how well the algorithm approximates the best expert locally, and so is different from, although closely related to, both the classical regret, measured over an initial time interval , and the tracking regret, where the algorithm is compared to a good sequence of experts over . We investigate two existing intuitive methods for deriving algorithms with low adaptive regret, one based on specialist experts and the other based on restarts. Quite surprisingly, we show that both methods lead to the same algorithm, namely Fixed Share, which is known for its tracking regret. We provide a thorough analysis of the adaptive regret of Fixed Share. We obtain the exact worst-case adaptive regret for Fixed Share, from which the classical tracking bounds follow. We prove that Fixed Share is optimal for adaptive regret: the worst-case adaptive regret of any algorithm is at least that of an instance of Fixed Share.",
            "keywords": [
                "online learning",
                "adaptive regret",
                "Fixed Share"
            ],
            "author": [
                "Dmitry Adamskiy",
                "Wouter M. Koolen",
                "Alexey Chernov",
                "Vladimir Vovk"
            ],
            "ref": "http://jmlr.org/papers/volume17/13-533/13-533.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Using Anti-Training with Sacrificial Data",
            "abstract": "Traditionally the machine-learning community has viewed the No Free Lunch (NFL) theorems for search and optimization as a limitation. We review, analyze, and unify the NFL theorem with the perspectives of \"blind\" search and meta-learning to arrive at necessary conditions for improving black-box optimization. We survey meta-learning literature to determine when and how meta- learning can benefit machine learning. Then, we generalize meta- learning in the context of the NFL theorems, to arrive at a novel technique called anti-training with sacrificial data (ATSD). Our technique applies at the meta level to arrive at domain specific algorithms. We also show how to generate sacrificial data. An extensive case study is presented along with simulated annealing results to demonstrate the efficacy of the ATSD method.",
            "keywords": [
                "Machine Learning",
                "Optimization",
                "Meta Optimization",
                "No Free Lunch",
                "Anti-                                     Training"
            ],
            "author": [
                "Michael L. Valenzuela",
                "Jerzy W. Rozenblit"
            ],
            "ref": "http://jmlr.org/papers/volume17/13-589/13-589.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unifying Framework in Vector-valued Reproducing Kernel Hilbert Spaces for Manifold Regularization and Co-Regularized Multi-view Learning",
            "abstract": "This paper presents a general vector-valued reproducing kernel Hilbert spaces (RKHS) framework for the problem of learning an unknown functional dependency between a structured input space and a structured output space. Our formulation encompasses both Vector-valued Manifold Regularization and Co-regularized Multi- view Learning, providing in particular a unifying framework linking these two important learning approaches. In the case of the least square loss function, we provide a closed form solution, which is obtained by solving a system of linear equations. In the case of Support Vector Machine (SVM) classification, our formulation generalizes in particular both the binary Laplacian SVM to the multi-class, multi-view settings and the multi-class Simplex Cone SVM to the semi-supervised, multi-view settings. The solution is obtained by solving a single quadratic optimization problem, as in standard SVM, via the Sequential Minimal Optimization (SMO) approach. Empirical results obtained on the task of object recognition, using several challenging data sets, demonstrate the competitiveness of our algorithms compared with other state-of-the-art methods.",
            "keywords": [
                "kernel methods",
                "vector-valued RKHS",
                "multi-view learning",
                "multi-modality     learning",
                "multi-kernel learning",
                "manifold regularization"
            ],
            "author": [
                "Hà Quang Minh",
                "Loris Bazzani",
                "Vittorio Murino"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-036/14-036.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantifying Uncertainty in Random Forests via Confidence Intervals and Hypothesis Tests",
            "abstract": "This work develops formal statistical inference procedures for predictions generated by supervised learning ensembles. Ensemble methods based on bootstrapping, such as bagging and random forests, have improved the predictive accuracy of individual trees, but fail to provide a framework in which distributional results can be easily determined. Instead of aggregating full bootstrap samples, we consider predicting by averaging over trees built on subsamples of the training set and demonstrate that the resulting estimator takes the form of a U-statistic. As such, predictions for individual feature vectors are asymptotically normal, allowing for confidence intervals to accompany predictions. In practice, a subset of subsamples is used for computational speed; here our estimators take the form of incomplete U-statistics and equivalent results are derived. We further demonstrate that this setup provides a framework for testing the significance of features. Moreover, the internal estimation method we develop allows us to estimate the variance parameters and perform these inference procedures at no additional computational cost. Simulations and illustrations on a real data set are provided.",
            "keywords": [
                "trees",
                "u-statistics",
                "bagging",
                "subbagging"
            ],
            "author": [
                "Lucas Mentch",
                "Giles Hooker"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-168/14-168.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical-Computational Tradeoffs in Planted Problems and Submatrix Localization with a Growing Number of Clusters and Submatrices",
            "abstract": "",
            "keywords": [
                "planted partition",
                "planted clique",
                "planted coloring",
                "submatrix localization",
                "graph clus-     tering",
                "bi-clustering",
                "minimax recovery",
                "computational hardness"
            ],
            "author": [
                "Yudong Chen",
                "Jiaming Xu"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-330/14-330.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-linear Causal Inference using Gaussianity Measures",
            "abstract": "We provide theoretical and empirical evidence for a type of asymmetry between causes and effects that is present when these are related via linear models contaminated with additive non- Gaussian noise. Assuming that the causes and the effects have the same distribution, we show that the distribution of the residuals of a linear fit in the anti-causal direction is closer to a Gaussian than the distribution of the residuals in the causal direction. This Gaussianization effect is characterized by reduction of the magnitude of the high-order cumulants and by an increment of the differential entropy of the residuals. The problem of non-linear causal inference is addressed by performing an embedding in an expanded feature space, in which the relation between causes and effects can be assumed to be linear. The effectiveness of a method to discriminate between causes and effects based on this type of asymmetry is illustrated in a variety of experiments using different measures of Gaussianity. The proposed method is shown to be competitive with state-of-the-art techniques for causal inference.",
            "keywords": [
                "causal inference",
                "Gaussianity of the residuals"
            ],
            "author": [
                "Daniel Hern{\\'a}ndez-Lobato",
                "Pablo Morales-Mombiela",
                "David Lopez-Paz",
                "Alberto Su{\\'a}rez"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-375/14-375.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistent Distribution-Free $K$-Sample and Independence Tests for Univariate Random Variables",
            "abstract": "A popular approach for testing if two univariate random variables are statistically independent consists of partitioning the sample space into bins, and evaluating a test statistic on the binned data. The partition size matters, and the optimal partition size is data dependent. While for detecting simple relationships coarse partitions may be best, for detecting complex relationships a great gain in power can be achieved by considering finer partitions. We suggest novel consistent distribution-free tests that are based on summation or maximization aggregation of scores over all partitions of a fixed size. We show that our test statistics based on summation can serve as good estimators of the mutual information. Moreover, we suggest regularized tests that aggregate over all partition sizes, and prove those are consistent too. We provide polynomial-time algorithms, which are critical for computing the suggested test statistics efficiently. We show that the power of the regularized tests is excellent compared to existing tests, and almost as powerful as the tests based on the optimal (yet unknown in practice) partition size, in simulations as well as on a real data example.",
            "keywords": [
                "bivariate distribution",
                "nonparametric test",
                "statistical independence",
                "mutual informa-     tion",
                "two-sample test"
            ],
            "author": [
                "Ruth Heller",
                "Yair Heller",
                "Shachar Kaufman",
                "Barak Brill",
                "Malka Gorfine"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-441/14-441.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Gibbs Sampler for Learning DAGs",
            "abstract": "We propose a Gibbs sampler for structure learning in directed acyclic graph (DAG) models. The standard Markov chain Monte Carlo algorithms used for learning DAGs are random-walk Metropolis-Hastings samplers. These samplers are guaranteed to converge asymptotically but often mix slowly when exploring the large graph spaces that arise in structure learning. In each step, the sampler we propose draws entire sets of parents for multiple nodes from the appropriate conditional distribution. This provides an efficient way to make large moves in graph space, permitting faster mixing whilst retaining asymptotic guarantees of convergence. The conditional distribution is related to variable selection with candidate parents playing the role of covariates or inputs. We empirically examine the performance of the sampler using several simulated and real data examples. The proposed method gives robust results in diverse settings, outperforming several existing Bayesian and frequentist methods. In addition, our empirical results shed some light on the relative merits of Bayesian and constraint- based methods for structure learning.",
            "keywords": [
                "structure learning",
                "DAGs",
                "Bayesian networks",
                "Gibbs sampling",
                "Markov chain     Monte Carlo"
            ],
            "author": [
                "Robert J. B. Goudie",
                "Sach Mukherjee"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-486/14-486.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning",
            "abstract": "Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution . These methods rely on a singular value decomposition of a matrix , called the empirical Hankel matrix, that records the frequencies of (some of) the observed strings . The accuracy of the learned distribution depends both on the quantity of information embedded in  and on the distance between  and its mean . Existing concentration bounds seem to indicate that the concentration over  gets looser with its dimensions, suggesting that it might be necessary to bound the dimensions of  for learning. We prove new dimension-free concentration bounds for classical Hankel matrices and several variants, based on prefixes or factors of strings, that are useful for learning. Experiments demonstrate that these bounds are tight and that they significantly improve existing (dimension-dependent) bounds. One consequence of these results is that the spectral learning approach remains consistent even if all the observations are recorded within the empirical matrix.",
            "keywords": [
                "Hankel matrices",
                "Matrix Bernstein bounds",
                "Probabilistic Grammatical In-     ference",
                "Rational series"
            ],
            "author": [
                "François Denis",
                "Mattias Gybels",
                "Amaury Habrard"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-501/14-501.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distinguishing Cause from Effect Using Observational Data: Methods and Benchmarks",
            "abstract": "The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether  causes  or, alternatively,  causes , given joint observations of two variables . An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of no confounding, no feedback loops, and no selection bias, such bivariate causal discovery problems are challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: methods based on Additive Noise Models (ANMs) and Information Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs that consists of data for 100 different cause-effect pairs selected from 37 data sets from various domains (e.g., meteorology, biology, medicine, engineering, economy, etc.) and motivate our decisions regarding the ground truth causal directions of all pairs. We evaluate the performance of several bivariate causal discovery methods on these real-world benchmark data and in addition on artificially simulated data. Our empirical results on real-world data indicate that certain methods are indeed able to distinguish cause from effect using only purely observational data, although more benchmark data would be needed to obtain statistically significant conclusions. One of the best performing methods overall is the method based on Additive Noise Models that has originally been proposed by Hoyer et al. (2009), which obtains an accuracy of 63  10 % and an AUC of 0.74  0.05 on the real-world benchmark. As the main theoretical contribution of this work we prove the consistency of that method.",
            "keywords": [],
            "author": [
                "Joris M. Mooij",
                "Jonas Peters",
                "Dominik Janzing",
                "Jakob Zscheischler",
                "Bernhard Schölkopf"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-518/14-518.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-task Sparse Structure Learning with Gaussian Copula Models",
            "abstract": "Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously. While sometimes the underlying task relationship structure is known, often the structure needs to be estimated from data at hand. In this paper, we present a novel family of models for MTL, applicable to regression and classification problems, capable of learning the structure of tasks relationship. In particular, we consider a joint estimation problem of the tasks relationship structure and the individual task parameters, which is solved using alternating minimization. The task relationship revealed by structure learning is founded on recent advances in Gaussian graphical models endowed with sparse estimators of the precision (inverse covariance) matrix. An extension to include flexible Gaussian copula models that relaxes the Gaussian marginal assumption is also proposed. We illustrate the effectiveness of the proposed model on a variety of synthetic and benchmark data sets for regression and classification. We also consider the problem of combining Earth System Model (ESM) outputs for better projections of future climate, with focus on projections of temperature by combining ESMs in South and North America, and show that the proposed model outperforms several existing methods for the problem.",
            "keywords": [
                "multi-task learning",
                "structure learning",
                "Gaussian copula",
                "probabilistic graph-     ical model"
            ],
            "author": [
                "André R. Gonçalves",
                "Fernando J. Von Zuben",
                "Arindam Banerjee"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-215/15-215.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MLlib: Machine Learning in Apache Spark",
            "abstract": "Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open- source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.",
            "keywords": [],
            "author": [
                "Xiangrui Meng",
                "Joseph Bradley",
                "Burak Yavuz",
                "Evan Sparks",
                "Shivaram Venkataraman",
                "Davies Liu",
                "Jeremy Freeman",
                "DB Tsai",
                "Manish Amde",
                "Sean Owen",
                "Doris Xin",
                "Reynold Xin",
                "Michael J. Franklin",
                "Reza Zadeh",
                "Matei Zaharia",
                "Ameet Talwalkar"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-237/15-237.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "OLPS: A Toolbox for On-Line Portfolio Selection",
            "abstract": "On-line portfolio selection is a practical financial engineering problem, which aims to sequentially allocate capital among a set of assets in order to maximize long-term return. In recent years, a variety of machine learning algorithms have been proposed to address this challenging problem, but no comprehensive open-source toolbox has been released for various reasons. This article presents the first open-source toolbox for \"On-Line Portfolio Selection\" (OLPS), which implements a collection of classical and state-of-the-art strategies powered by machine learning algorithms. We hope that OLPS can facilitate the development of new learning methods and enable the performance benchmarking and comparisons of different strategies. OLPS is an open-source project released under Apache License (version 2.0), which is available at github.com/OLPS/OLPS or OLPS.stevenhoi.org.",
            "keywords": [
                "On-line portfolio selection",
                "online learning",
                "trading system"
            ],
            "author": [
                "Bin Li",
                "Doyen Sahoo",
                "Steven C.H. Hoi"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-317/15-317.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bounded p-norm Approximation of Max-Convolution for Sub-Quadratic Bayesian Inference on Additive Factors",
            "abstract": "Max-convolution is an important problem closely resembling standard convolution; as such, max-convolution occurs frequently across many fields. Here we extend the method with fastest known worst-case runtime, which can be applied to nonnegative vectors by numerically approximating the Chebyshev norm , and use this approach to derive two numerically stable methods based on the idea of computing -norms via fast convolution: The first method proposed, with runtime in  (which is less than  for any vectors that can be practically realized), uses the -norm as a direct approximation of the Chebyshev norm. The second approach proposed, with runtime in  (although in practice both perform similarly), uses a novel null space projection method, which extracts information from a sequence of -norms to estimate the maximum value in the vector (this is equivalent to querying a small number of moments from a distribution of bounded support in order to estimate the maximum). The -norm approaches are compared to one another and are shown to compute an approximation of the Viterbi path in a hidden Markov model where the transition matrix is a Toeplitz matrix; the runtime of approximating the Viterbi path is thus reduced from  steps to  steps in practice, and is demonstrated by inferring the U.S. unemployment rate from the S&P 500 stock index.",
            "keywords": [
                "Bayesian inference",
                "maximum a posteriori",
                "fast Fourier transform",
                "max-convolution",
                "p-norm",
                "Lp space",
                "hidden Markov model",
                "null space projection"
            ],
            "author": [
                "Julianus Pfeuffer",
                "Oliver Serang"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-319/15-319.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Learn Neural Networks",
            "abstract": "In this paper, we propose a novel model for high-dimensional data, called the Hybrid Orthogonal Projection and Estimation (HOPE) model, which combines a linear orthogonal projection and a finite mixture model under a unified generative modeling framework. The HOPE model itself can be learned unsupervised from unlabelled data based on the maximum likelihood estimation as well as discriminatively from labelled data. More interestingly, we have shown the proposed HOPE models are closely related to neural networks (NNs) in a sense that each hidden layer can be reformulated as a HOPE model. As a result, the HOPE framework can be used as a novel tool to probe why and how NNs work, more importantly, to learn NNs in either supervised or unsupervised ways. In this work, we have investigated the HOPE framework to learn NNs for several standard tasks, including image recognition on MNIST and speech recognition on TIMIT. Experimental results have shown that the HOPE framework yields significant performance gains over the current state-of-the-art methods in various types of NN learning problems, including unsupervised feature learning, supervised or semi-supervised learning.",
            "keywords": [
                "Orthogonal Projection",
                "PCA",
                "Mixture Models",
                "Neural Networks",
                "Unsuper-     vised Learning"
            ],
            "author": [
                "Shiliang Zhang",
                "Hui Jiang",
                "Lirong Dai"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-335/15-335.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Optimal Sample Complexity of PAC Learning",
            "abstract": "This work establishes a new upper bound on the number of samples sufficient for PAC learning in the realizable case. The bound matches known lower bounds up to numerical constant factors. This solves a long-standing open problem on the sample complexity of PAC learning. The technique and analysis build on a recent breakthrough by Hans Simon.",
            "keywords": [
                "sample complexity",
                "PAC learning",
                "statistical learning theory",
                "minimax anal-     ysis"
            ],
            "author": [
                "Steve Hanneke"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-389/15-389.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "End-to-End Training of Deep Visuomotor Policies",
            "abstract": "Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.",
            "keywords": [
                "Reinforcement Learning",
                "Optimal Control",
                "Vision"
            ],
            "author": [
                "Sergey Levine",
                "Chelsea Finn",
                "Trevor Darrell",
                "Pieter Abbeel"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-522/15-522.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Quantile Regression in Reproducing Kernel Hilbert Spaces with the Data Sparsity Constraint",
            "abstract": "For spline regressions, it is well known that the choice of knots is crucial for the performance of the estimator. As a general learning framework covering the smoothing splines, learning in a Reproducing Kernel Hilbert Space (RKHS) has a similar issue. However, the selection of training data points for kernel functions in the RKHS representation has not been carefully studied in the literature. In this paper we study quantile regression as an example of learning in a RKHS. In this case, the regular squared norm penalty does not perform training data selection. We propose a data sparsity constraint that imposes thresholding on the kernel function coefficients to achieve a sparse kernel function representation. We demonstrate that the proposed data sparsity method can have competitive prediction performance for certain situations, and have comparable performance in other cases compared to that of the traditional squared norm penalty. Therefore, the data sparsity method can serve as a competitive alternative to the squared norm penalty method. Some theoretical properties of our proposed method using the data sparsity constraint are obtained. Both simulated and real data sets are used to demonstrate the usefulness of our data sparsity constraint.",
            "keywords": [
                "kernel learning",
                "Rademacher complexity",
                "regression",
                "smoothing"
            ],
            "author": [
                "Chong Zhang",
                "Yufeng Liu",
                "Yichao Wu"
            ],
            "ref": "http://jmlr.org/papers/volume17/zhang16a/zhang16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "BayesPy: Variational Bayesian Inference in Python",
            "abstract": "BayesPy is an open-source Python software package for performing variational Bayesian inference. It is based on the variational message passing framework and supports conjugate exponential family models. By removing the tedious task of implementing the variational Bayesian update equations, the user can construct models faster and in a less error-prone way. Simple syntax, flexible model construction and efficient inference make BayesPy suitable for both average and expert Bayesian users. It also supports some advanced methods such as stochastic and collapsed variational inference.",
            "keywords": [
                "variational Bayes",
                "probabilistic programming"
            ],
            "author": [
                "Jaakko Luttinen"
            ],
            "ref": "http://jmlr.org/papers/volume17/luttinen16a/luttinen16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes",
            "abstract": "The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximised over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximising an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from i.i.d. observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the non-linear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain or partially missing inputs. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data.",
            "keywords": [
                "Gaussian processes",
                "variational inference",
                "latent variable models",
                "dynamical     systems"
            ],
            "author": [
                "Andreas C. Damianou",
                "Michalis K. Titsias",
                "Neil D. Lawrence"
            ],
            "ref": "http://jmlr.org/papers/volume17/damianou16a/damianou16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Estimation of the Gradient Lines of a Density and the Consistency of the Mean-Shift Algorithm",
            "abstract": "We consider the problem of estimating the gradient lines of a density, which can be used to cluster points sampled from that density, for example via the mean-shift algorithm of Fukunaga and Hostetler (1975). We prove general convergence bounds that we then specialize to kernel density estimation.",
            "keywords": [
                "mean-shift",
                "gradient lines",
                "density estimation"
            ],
            "author": [
                "Ery Arias-Castro",
                "David Mason",
                "Bruno Pelletier"
            ],
            "ref": "http://jmlr.org/papers/volume17/ariascastro16a/ariascastro16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Learning of Bayesian Network Classifiers",
            "abstract": "Ever increasing data quantity makes ever more urgent the need for highly scalable learners that have good classification performance. Therefore, an out-of-core learner with excellent time and space complexity, along with high expressivity (that is, capacity to learn very complex multivariate probability distributions) is extremely desirable. This paper presents such a learner. We propose an extension to the -dependence Bayesian classifier (KDB) that discriminatively selects a sub- model of a full KDB classifier. It requires only one additional pass through the training data, making it a three-pass learner. Our extensive experimental evaluation on  large data sets reveals that this out-of-core algorithm achieves competitive classification performance, and substantially better training and classification time than state-of-the-art in-core learners such as random forest and linear and non-linear logistic regression.",
            "keywords": [
                "scalable Bayesian classification",
                "feature selection",
                "out-of-core learning"
            ],
            "author": [
                "Ana M. Martínez",
                "Geoffrey I. Webb",
                "Shenglei Chen",
                "Nayyar A. Zaidi"
            ],
            "ref": "http://jmlr.org/papers/volume17/martinez16a/martinez16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unified View on Multi-class Support Vector Classification",
            "abstract": "",
            "keywords": [
                "support vector machines",
                "multi-class classification"
            ],
            "author": [
                "{\\\"U}rün Do\\u{g}an",
                "Tobias Glasmachers",
                "Christian Igel"
            ],
            "ref": "http://jmlr.org/papers/volume17/11-229/11-229.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Addressing Environment Non-Stationarity by Repeating Q-learning Updates",
            "abstract": "",
            "keywords": [
                "reinforcement learning",
                "Q-learning",
                "multi-agent learning"
            ],
            "author": [
                "Sherief Abdallah",
                "Michael Kaisers"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-037/14-037.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large Scale Online Kernel Learning",
            "abstract": "In this paper, we present a new framework for large scale online kernel learning, making kernel methods efficient and scalable for large-scale online learning applications. Unlike the regular budget online kernel learning scheme that usually uses some budget maintenance strategies to bound the number of support vectors, our framework explores a completely different approach of kernel functional approximation techniques to make the subsequent online learning task efficient and scalable. Specifically, we present two different online kernel machine learning algorithms: (i) Fourier Online Gradient Descent (FOGD) algorithm that applies the random Fourier features for approximating kernel functions; and (ii) NystrÃ¶m Online Gradient Descent (NOGD) algorithm that applies the NystrÃ¶m method to approximate large kernel matrices. We explore these two approaches to tackle three online learning tasks: binary classification, multi-class classification, and regression. The encouraging results of our experiments on large-scale datasets validate the effectiveness and efficiency of the proposed algorithms, making them potentially more practical than the family of existing budget online kernel learning approaches.",
            "keywords": [
                "online learning",
                "kernel approximation"
            ],
            "author": [
                "Jing Lu",
                "Steven C.H. Hoi",
                "Jialei Wang",
                "Peilin Zhao",
                "Zhi-Yong Liu"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-148/14-148.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Mean Shrinkage Estimators",
            "abstract": "A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel mean, is central to kernel methods in that it is used by many classical algorithms such as kernel principal component analysis, and it also forms the core inference step of modern kernel methods that rely on embedding probability distributions in RKHSs. Given a finite sample, an empirical average has been used commonly as a standard estimator of the true kernel mean. Despite a widespread use of this estimator, we show that it can be improved thanks to the well-known Stein phenomenon. We propose a new family of estimators called kernel mean shrinkage estimators (KMSEs), which benefit from both theoretical justifications and good empirical performance. The results demonstrate that the proposed estimators outperform the standard one, especially in a \"large , small \" paradigm.",
            "keywords": [
                "covariance operator",
                "James-Stein estimators",
                "kernel methods",
                "kernel mean",
                "shrinkage estimators",
                "Stein effect"
            ],
            "author": [
                "Krikamol Mu",
                "et",
                "Bharath Sriperumbudur",
                "Kenji Fukumizu",
                "Arthur Gretton",
                "Bernhard Schölkopf"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-195/14-195.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SPSD Matrix Approximation vis Column Selection: Theories, Algorithms, and Extensions",
            "abstract": "Symmetric positive semidefinite (SPSD) matrix approximation is an important problem with applications in kernel methods. However, existing SPSD matrix approximation methods such as the NystrÃ¶m method only have weak error bounds. In this paper we conduct in-depth studies of an SPSD matrix approximation model and establish strong relative-error bounds. We call it the prototype model for it has more efficient and effective extensions, and some of its extensions have high scalability. Though the prototype model itself is not suitable for large- scale data, it is still useful to study its properties, on which the analysis of its extensions relies. This paper offers novel theoretical analysis, efficient algorithms, and a highly accurate extension. First, we establish a lower error bound for the prototype model and improve the error bound of an existing column selection algorithm to match the lower bound. In this way, we obtain the first optimal column selection algorithm for the prototype model. We also prove that the prototype model is exact under certain conditions. Second, we develop a simple column selection algorithm with a provable error bound. Third, we propose a so-called spectral shifting model to make the approximation more accurate when the eigenvalues of the matrix decay slowly, and the improvement is theoretically quantified. The spectral shifting method can also be applied to improve other SPSD matrix approximation models.",
            "keywords": [
                "Matrix approximation",
                "matrix factorization",
                "kernel methods"
            ],
            "author": [
                "Shusen Wang",
                "Luo Luo",
                "Zhihua Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-199/14-199.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Differentially Private Data Releasing for Smooth Queries",
            "abstract": "In the past few years, differential privacy has become a standard concept in the area of privacy. One of the most important problems in this field is to answer queries while preserving differential privacy. In spite of extensive studies, most existing work on differentially private query answering assumes the data are discrete (i.e., in ) and focuses on queries induced by \\emph{Boolean} functions. In real applications however, continuous data are at least as common as binary data. Thus, in this work we explore a less studied topic, namely, differential privately query answering for continuous data with continuous function. As a first step towards the continuous case, we study a natural class of linear queries on continuous data which we refer to as smooth queries. A linear query is said to be -smooth if it is specified by a function defined on  whose partial derivatives up to order  are all bounded. We develop two -differentially private mechanisms which are able to answer all smooth queries. The first mechanism outputs a summary of the database and can then give answers to the queries. The second mechanism is an improvement of the first one and it outputs a synthetic database. The two mechanisms both achieve an accuracy of . Here we assume that the dimension  is a constant. It turns out that even in this parameter setting (which is almost trivial in the discrete case), using existing discrete mechanisms to answer the smooth queries is difficult and requires more noise. Our mechanisms are based on -approximation of (transformed) smooth functions by low-degree even trigonometric polynomials with uniformly bounded coefficients. We also develop practically efficient variants of the mechanisms with promising experimental results.",
            "keywords": [],
            "author": [
                "Ziteng Wang",
                "Chi Jin",
                "Kai Fan",
                "Jiaqi Zhang",
                "Junliang Huang",
                "Yiqiao Zhong",
                "Liwei Wang"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-388/14-388.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Subspace Learning with Partial Information",
            "abstract": "The goal of subspace learning is to find a -dimensional subspace of , such that the expected squared distance between instance vectors and the subspace is as small as possible. In this paper we study subspace learning in a partial information setting, in which the learner can only observe  attributes from each instance vector. We propose several efficient algorithms for this task, and analyze their sample complexity.",
            "keywords": [
                "principal components analysis",
                "budgeted learning",
                "statistical learning",
                "learning with     partial information"
            ],
            "author": [
                "Alon Gonen",
                "Dan Rosenbaum",
                "Yonina C. Eldar",
                "Shai Shalev-Shwartz"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-443/14-443.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Lasso and group-Lasso for functional Poisson regression",
            "abstract": "High dimensional Poisson regression has become a standard framework for the analysis of massive counts datasets. In this work we estimate the intensity function of the Poisson regression model by using a dictionary approach, which generalizes the classical basis approach, combined with a Lasso or a group-Lasso procedure. Selection depends on penalty weights that need to be calibrated. Standard methodologies developed in the Gaussian framework can not be directly applied to Poisson models due to heteroscedasticity. Here we provide data-driven weights for the Lasso and the group-Lasso derived from concentration inequalities adapted to the Poisson case. We show that the associated Lasso and group-Lasso procedures satisfy fast and slow oracle inequalities. Simulations are used to assess the empirical performance of our procedure, and an original application to the analysis of Next Generation Sequencing data is provided.",
            "keywords": [
                "Functional Poisson regression",
                "adaptive lasso",
                "adaptive group-lasso",
                "calibra-     tion",
                "concentrationIntroductionPoisson functional regression has become a standard framework for image or spectra anal-ysis"
            ],
            "author": [
                "Stéphane Ivanoff",
                "Franck Picard",
                "Vincent Rivoirard"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-021/15-021.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Inference through a Witness Protection Program",
            "abstract": "One of the most fundamental problems in causal inference is the estimation of a causal effect when treatment and outcome are confounded. This is difficult in an observational study, because one has no direct evidence that all confounders have been adjusted for. We introduce a novel approach for estimating causal effects that exploits observational conditional independencies to suggest âweakâ paths in an unknown causal graph. The widely used faithfulness condition of Spirtes et al. is relaxed to allow for varying degrees of âpath cancellationsâ that imply conditional independencies but do not rule out the existence of confounding causal paths. The output is a posterior distribution over bounds on the average causal effect via a linear programming approach and Bayesian inference. We claim this approach should be used in regular practice as a complement to other tools in observational studies.",
            "keywords": [
                "Causal inference",
                "instrumental variables",
                "Bayesian inference"
            ],
            "author": [
                "Ricardo Silva",
                "Robin Evans"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-130/15-130.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structure Discovery in Bayesian Networks by Sampling Partial Orders",
            "abstract": "We present methods based on Metropolis-coupled Markov chain Monte Carlo (MC3) and annealed importance sampling (AIS) for estimating the posterior distribution of Bayesian networks. The methods draw samples from an appropriate distribution of partial orders on the nodes, continued by sampling directed acyclic graphs (DAGs) conditionally on the sampled partial orders. We show that the computations needed for the sampling algorithms are feasible as long as the encountered partial orders have relatively few down-sets. While the algorithms assume suitable modularity properties of the priors, arbitrary priors can be handled by dividing the importance weight of each sampled DAG by the number of topological sorts it has---we give a practical dynamic programming algorithm to compute these numbers. Our empirical results demonstrate that the presented partial-order- based samplers are superior to previous Markov chain Monte Carlo methods, which sample DAGs either directly or via linear orders on the nodes. The results also suggest that the convergence rate of the estimators based on AIS are competitive to those of MC3. Thus AIS is the preferred method, as it enables easier large- scale parallelization and, in addition, supplies good probabilistic lower bound guarantees for the marginal likelihood of the model.",
            "keywords": [
                "annealed importance sampling",
                "directed acyclic graph",
                "fast zeta transform",
                "linear extension"
            ],
            "author": [
                "Teppo Niinim\\\"{a}ki",
                "Pekka Parviainen",
                "Mikko Koivisto"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-140/15-140.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence",
            "abstract": "Data in the form of pairwise comparisons arises in many domains, including preference elicitation, sporting competitions, and peer grading among others. We consider parametric ordinal models for such pairwise comparison data involving a latent vector  that represents the âqualitiesâ of the  items being compared; this class of models includes the two most widely used parametric models---the Bradley-Terry-Luce (BTL) and the Thurstone models. Working within a standard minimax framework, we provide tight upper and lower bounds on the optimal error in estimating the quality score vector  under this class of models. The bounds depend on the topology of the comparison graph induced by the subset of pairs being compared, via the spectrum of the Laplacian of the comparison graph. Thus, in settings where the subset of pairs may be chosen, our results provide principled guidelines for making this choice. Finally, we compare these error rates to those under cardinal measurement models and show that the error rates in the ordinal and cardinal settings have identical scalings apart from constant pre- factors.",
            "keywords": [],
            "author": [
                "Nihar B. Shah",
                "Sivaraman Balakrishnan",
                "Joseph Bradley",
                "Abhay Parekh",
                "Kannan Ramch",
                "ran",
                "Martin J. Wainwright"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-189/15-189.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Probabilistic Low-Rank Matrix Completion from Quantized Measurements",
            "abstract": "We consider the recovery of a low rank real-valued matrix  given a subset of noisy discrete (or quantized) measurements. Such problems arise in several applications such as collaborative filtering, learning and content analytics, and sensor network localization. We consider constrained maximum likelihood estimation of , under a constraint on the entry- wise infinity-norm of  and an exact rank constraint. We provide upper bounds on the Frobenius norm of matrix estimation error under this model. Previous theoretical investigations have focused on binary (1-bit) quantizers, and been based on convex relaxation of the rank. Compared to the existing binary results, our performance upper bound has faster convergence rate with matrix dimensions when the fraction of revealed observations is fixed. We also propose a globally convergent optimization algorithm based on low rank factorization of  and validate the method on synthetic and real data, with improved performance over previous methods.",
            "keywords": [
                "constrained maximum likelihood",
                "quantization",
                "matrix completion",
                "collabo-     rative filtering"
            ],
            "author": [
                "Sonia A. Bhaskar"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-273/15-273.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DSA: Decentralized Double Stochastic Averaging Gradient Algorithm",
            "abstract": "This paper considers optimization problems where nodes of a network have access to summands of a global objective. Each of these local objectives is further assumed to be an average of a finite set of functions. The motivation for this setup is to solve large scale machine learning problems where elements of the training set are distributed to multiple computational elements. The decentralized double stochastic averaging gradient (DSA) algorithm is proposed as a solution alternative that relies on: (i) The use of local stochastic averaging gradients. (ii) Determination of descent steps as differences of consecutive stochastic averaging gradients. Strong convexity of local functions and Lipschitz continuity of local gradients is shown to guarantee linear convergence of the sequence generated by DSA in expectation. Local iterates are further shown to approach the optimal argument for almost all realizations. The expected linear convergence of DSA is in contrast to the sublinear rate characteristic of existing methods for decentralized stochastic optimization. Numerical experiments on a logistic regression problem illustrate reductions in convergence time and number of feature vectors processed until convergence relative to these other alternatives.",
            "keywords": [
                "decentralized optimization",
                "stochastic optimization",
                "stochastic averaging gradient",
                "linear convergence",
                "large-scale optimization"
            ],
            "author": [
                "Aryan Mokhtari",
                "Alejandro Ribeiro"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-292/15-292.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Statistical Performance of Collaborative Inference",
            "abstract": "The statistical analysis of massive and complex data sets will require the development of algorithms that depend on distributed computing and collaborative inference. Inspired by this, we propose a collaborative framework that aims to estimate the unknown mean  of a random variable . In the model we present, a certain number of calculation units, distributed across a communication network represented by a graph, participate in the estimation of  by sequentially receiving independent data from  while exchanging messages via a stochastic matrix  defined over the graph. We give precise conditions on the matrix  under which the statistical precision of the individual units is comparable to that of a (gold standard) virtual centralized estimate, even though each unit does not have access to all of the data. We show in particular the fundamental role played by both the non-trivial eigenvalues of  and the Ramanujan class of expander graphs, which provide remarkable performance for moderate algorithmic cost.",
            "keywords": [
                "distributed computing",
                "collaborative estimation",
                "stochastic matrix",
                "graph     theory",
                "complexity"
            ],
            "author": [
                "Gérard Biau",
                "Kevin Bleakley",
                "Benoît Cadre"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-346/15-346.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence of an Alternating Maximization Procedure",
            "abstract": "We derive two convergence results for a sequential alternating maximization procedure to approximate the maximizer of random functionals such as the realized log likelihood in MLE estimation. We manage to show that the sequence attains the same deviation properties as shown for the profile M-estimator by Andresen and Spokoiny (2013), that means a finite sample Wilks and Fisher theorem. Further under slightly stronger smoothness constraints on the random functional we can show nearly linear convergence to the global maximizer if the starting point for the procedure is well chosen.",
            "keywords": [
                "alternating maximization",
                "alternating minimization",
                "profile maximum likeli-     hood",
                "EM-algorithm",
                "M-estimation",
                "local linear approximation",
                "local concentration",
                "semi-     parametricc 2016 Andreas Andresen"
            ],
            "author": [
                "Andreas Andresen",
                "Vladimir Spokoiny"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-392/15-392.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "StructED: Risk Minimization in Structured Prediction",
            "abstract": "Structured tasks are distinctive: each task has its own measure of performance, such as the word error rate in speech recognition, the BLEU score in machine translation, the NDCG score in information retrieval, or the intersection-over-union score in visual object segmentation. This paper presents StructED, a software package for learning structured prediction models with training methods that aimed at optimizing the task measure of performance. The package was written in Java and released under the MIT license. It can be downloaded from  adiyoss.github.io/StructED.",
            "keywords": [
                "structured prediction",
                "structural SVM",
                "CRF"
            ],
            "author": [
                "Yossi Adi",
                "Joseph Keshet"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-531/15-531.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches",
            "abstract": "We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.",
            "keywords": [
                "stereo",
                "matching cost",
                "similarity learning",
                "supervised learning"
            ],
            "author": [
                "Jure Žbontar",
                "Yann LeCun"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-535/15-535.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Policy Gradient and Actor-Critic Algorithms",
            "abstract": "Policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate. Many conventional policy gradient methods use Monte-Carlo techniques to estimate this gradient. The policy is improved by adjusting the parameters in the direction of the gradient estimate. Since Monte-Carlo methods tend to have high variance, a large number of samples is required to attain accurate estimates, resulting in slow convergence. In this paper, we first propose a Bayesian framework for policy gradient, based on modeling the policy gradient as a Gaussian process. This reduces the number of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates, namely, the gradient covariance, are provided at little extra cost. Since the proposed Bayesian framework considers system trajectories as its basic observable unit, it does not require the dynamics within trajectories to be of any particular form, and thus, can be easily extended to partially observable problems. On the downside, it cannot take advantage of the Markov property when the system is Markovian. To address this issue, we proceed to supplement our Bayesian policy gradient framework with a new actor-critic learning model in which a Bayesian class of non- parametric critics, based on Gaussian process temporal difference learning, is used. Such critics model the action- value function as a Gaussian process, allowing Bayes' rule to be used in computing the posterior distribution over action-value functions, conditioned on the observed data. Appropriate choices of the policy parameterization and of the prior covariance (kernel) between action-values allow us to obtain closed-form expressions for the posterior distribution of the gradient of the expected return with respect to the policy parameters. We perform detailed experimental comparisons of the proposed Bayesian policy gradient and actor-critic algorithms with classic Monte-Carlo based policy gradient methods, as well as with each other, on a number of reinforcement learning problems.",
            "keywords": [
                "reinforcement learning",
                "policy gradient methods",
                "actor-critic algorithms",
                "Bayesian inference"
            ],
            "author": [
                "Mohammad Ghavamzadeh",
                "Yaakov Engel",
                "Michal Valko"
            ],
            "ref": "http://jmlr.org/papers/volume17/10-245/10-245.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Practical Kernel-Based Reinforcement Learning",
            "abstract": "Kernel-based reinforcement learning (KBRL) stands out among approximate reinforcement learning algorithms for its strong theoretical guarantees. By casting the learning problem as a local kernel approximation, KBRL provides a way of computing a decision policy which converges to a unique solution and is statistically consistent. Unfortunately, the model constructed by KBRL grows with the number of sample transitions, resulting in a computational cost that precludes its application to large-scale or on-line domains. In this paper we introduce an algorithm that turns KBRL into a practical reinforcement learning tool. Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a transition probability matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix, potentially much smaller than the original, which retains some fundamental properties of its precursor. KBSF exploits such an insight to compress the information contained in KBRL's model into an approximator of fixed size. This makes it possible to build an approximation considering both the difficulty of the problem and the associated computational cost. KBSF's computational complexity is linear in the number of sample transitions, which is the best one can do without discarding data. Moreover, the algorithm's simple mechanics allow for a fully incremental implementation that makes the amount of memory used independent of the number of sample transitions. The result is a kernel-based reinforcement learning algorithm that can be applied to large-scale problems in both off-line and on-line regimes. We derive upper bounds for the distance between the value functions computed by KBRL and KBSF using the same data. We also prove that it is possible to control the magnitude of the variables appearing in our bounds, which means that, given enough computational resources, we can make KBSF's value function as close as desired to the value function that would be computed by KBRL using the same set of sample transitions. The potential of our algorithm is demonstrated in an extensive empirical study in which KBSF is applied to difficult tasks based on real-world data. Not only does KBSF solve problems that had never been solved before, but it also significantly outperforms other state-of-the-art reinforcement learning algorithms on the tasks studied.",
            "keywords": [
                "reinforcement learning",
                "dynamic programming",
                "Markov decision processes",
                "kernel-based approximation"
            ],
            "author": [
                "André M.S. Barreto",
                "Doina Precup",
                "Joelle Pineau"
            ],
            "ref": "http://jmlr.org/papers/volume17/13-134/13-134.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Information-Theoretic Analysis of Thompson Sampling",
            "abstract": "We provide an information-theoretic analysis of Thompson sampling that applies across a broad range of online optimization problems in which a decision-maker must learn from partial feedback. This analysis inherits the simplicity and elegance of information theory and leads to regret bounds that scale with the entropy of the optimal-action distribution. This strengthens preexisting results and yields new insight into how information improves performance.",
            "keywords": [
                "Thompson sampling",
                "online optimization"
            ],
            "author": [
                "Daniel Russo",
                "Benjamin Van Roy"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-087/14-087.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Compressed Gaussian Process for Manifold Regression",
            "abstract": "Nonparametric regression for large numbers of features () is an increasingly important problem. If the sample size  is massive, a common strategy is to partition the feature space, and then separately apply simple models to each partition set. This is not ideal when  is modest relative to , and we propose an alternative approach relying on random compression of the feature vector combined with Gaussian process regression. The proposed approach is particularly motivated by the setting in which the response is conditionally independent of the features given the projection to a low dimensional manifold. Conditionally on the random compression matrix and a smoothness parameter, the posterior distribution for the regression surface and posterior predictive distributions are available analytically. Running the analysis in parallel for many random compression matrices and smoothness parameters, model averaging is used to combine the results. The algorithm can be implemented rapidly even in very large  and moderately large  nonparametric regression, has strong theoretical justification, and is found to yield state of the art predictive performance.",
            "keywords": [],
            "author": [
                "Rajarshi Guhaniyogi",
                "David B. Dunson"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-230/14-230.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Characterization of a Class of Fisher-Consistent Loss Functions and its Application to Boosting",
            "abstract": "Accurate classification of categorical outcomes is essential in a wide range of applications. Due to computational issues with minimizing the empirical 0/1 loss, Fisher consistent losses have been proposed as viable proxies. However, even with smooth losses, direct minimization remains a daunting task. To approximate such a minimizer, various boosting algorithms have been suggested. For example, with exponential loss, the AdaBoost algorithm (Freund and Schapire, 1995) is widely used for two- class problems and has been extended to the multi-class setting (Zhu et al., 2009). Alternative loss functions, such as the logistic and the hinge losses, and their corresponding boosting algorithms have also been proposed (Zou et al., 2008; Wang, 2012).  In this paper we demonstrate that a broad class of losses, including non-convex functions, achieve Fisher consistency, and in addition can be used for explicit estimation of the conditional class probabilities. Furthermore, we provide a generic boosting algorithm that is not loss-specific. Extensive simulation results suggest that the proposed boosting algorithms could outperform existing methods with properly chosen losses and bags of weak learners.",
            "keywords": [
                "Boosting",
                "Fisher-Consistency",
                "Multiclass Classification"
            ],
            "author": [
                "Matey Neykov",
                "Jun S. Liu",
                "Tianxi Cai"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-306/14-306.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exact Inference on Gaussian Graphical Models of Arbitrary Topology using Path-Sums",
            "abstract": "We present the path-sum formulation for exact statistical inference of marginals on Gaussian graphical models of arbitrary topology. The path-sum formulation gives the covariance between each pair of variables as a branched continued fraction of finite depth and breadth. Our method originates from the closed- form resummation of infinite families of terms of the walk-sum representation of the covariance matrix. We prove that the path- sum formulation always exists for models whose covariance matrix is positive definite: i.e. it is valid for both walk-summable and non-walk-summable graphical models of arbitrary topology. We show that for graphical models on trees the path-sum formulation is equivalent to Gaussian belief propagation. We also recover, as a corollary, an existing result that uses determinants to calculate the covariance matrix. We show that the path-sum formulation formulation is valid for arbitrary partitions of the inverse covariance matrix. We give detailed examples demonstrating our results.",
            "keywords": [
                "Gaussian graphical models",
                "belief propagation",
                "path-sum",
                "walk-sum",
                "graphs     of arbitrary topology",
                "block matricesc 2016 Pierre-Louis Giscard",
                "Zheng Choo"
            ],
            "author": [
                "P.-L. Giscard",
                "Z. Choo",
                "S. J. Thwaite",
                "D. Jaksch"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-445/14-445.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Challenges in multimodal gesture recognition",
            "abstract": "This paper surveys the state of the art on multimodal gesture recognition and introduces the JMLR special topic on gesture recognition 2011-2015. We began right at the start of the \\kinect revolution when inexpensive infrared cameras providing image depth recordings became available. We published papers using this technology and other more conventional methods, including regular video cameras, to record data, thus providing a good overview of uses of machine learning and computer vision using multimodal data in this area of application. Notably, we organized a series of challenges and made available several datasets we recorded for that purpose, including tens of thousands of videos, which are available to conduct further research. We also overview recent state of the art works on gesture recognition based on a proposed taxonomy for gesture recognition, discussing challenges and future lines of research.",
            "keywords": [
                "Gesture Recognition",
                "Time Series Analysis",
                "Multimodal Data Analysis",
                "Computer     Vision",
                "Pattern Recognition",
                "Wearable sensors",
                "Infrared Cameras"
            ],
            "author": [
                "Sergio Escalera",
                "Vassilis Athitsos",
                "Isabelle Guyon"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-468/14-468.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
            "abstract": "In this paper we introduce the idea of improving the performance of parametric temporal-difference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, we show that varying the emphasis of linear TD()'s updates in a particular way causes its expected update to become stable under off-policy training. The only prior model-free TD methods to achieve this with per- step computation linear in the number of function approximation parameters are the gradient-TD family of methods including TDC, GTD(), and GQ). Compared to these methods, our emphatic TD() is simpler and easier to use; it has only one learned parameter vector and one step-size parameter. Our treatment includes general state- dependent discounting and bootstrapping functions, and a way of specifying varying degrees of interest in accurately valuing different states.",
            "keywords": [
                "Temporal-difference learning",
                "Off-policy learning",
                "Function approximation",
                "Stability"
            ],
            "author": [
                "Richard S. Sutton",
                "A. Rupam Mahmood",
                "Martha White"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-488/14-488.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Algorithms for Second-Price Auctions with Reserve",
            "abstract": "Second-price auctions with reserve play a critical role in the revenue of modern search engine and popular online sites since the revenue of these companies often directly depends on the outcome of such auctions. The choice of the reserve price is the main mechanism through which the auction revenue can be influenced in these electronic markets. We cast the problem of selecting the reserve price to optimize revenue as a learning problem and present a full theoretical analysis dealing with the complex properties of the corresponding loss function. We further give novel algorithms for solving this problem and report the results of several experiments in both synthetic and real-world data demonstrating their effectiveness.",
            "keywords": [
                "Learning Theory",
                "Auctions"
            ],
            "author": [
                "Mehryar Mohri",
                "Andres Munoz Medina"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-499/14-499.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Coordinate Descent Method for Learning with Big Data",
            "abstract": "In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent method for solving loss minimization problems with big data. We initially partition the coordinates (features) and assign each partition to a different node of a cluster. At every iteration, each node picks a random subset of the coordinates from those it owns, independently from the other computers, and in parallel computes and applies updates to the selected coordinates based on a simple closed-form formula. We give bounds on the number of iterations sufficient to approximately solve the problem with high probability, and show how it depends on the data and on the partitioning. We perform numerical experiments with a LASSO instance described by a 3TB matrix.",
            "keywords": [
                "stochastic methods",
                "parallel coordinate descent",
                "distributed algorithms"
            ],
            "author": [
                "Peter Richtárik",
                "Martin Takáč"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-001/15-001.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scaling-up Empirical Risk Minimization: Optimization of Incomplete $U$-statistics",
            "abstract": "In a wide range of statistical learning problems such as ranking, clustering or metric learning among others, the risk is accurately estimated by -statistics of degree , i.e. functionals of the training data with low variance that take the form of averages over -tuples. From a computational perspective, the calculation of such statistics is highly expensive even for a moderate sample size , as it requires averaging  terms. This makes learning procedures relying on the optimization of such data functionals hardly feasible in practice. It is the major goal of this paper to show that, strikingly, such empirical risks can be replaced by drastically computationally simpler Monte-Carlo estimates based on  terms only, usually referred to as incomplete -statistics, without damaging the  learning rate of Empirical Risk Minimization (ERM) procedures. For this purpose, we establish uniform deviation results describing the error made when approximating a -process by its incomplete version under appropriate complexity assumptions. Extensions to model selection, fast rate situations and various sampling techniques are also considered, as well as an application to stochastic gradient descent for ERM. Finally, numerical examples are displayed in order to provide strong empirical evidence that the approach we promote largely surpasses more naive subsampling techniques.",
            "keywords": [
                "big data",
                "empirical risk minimization",
                "U-processes",
                "rate bound analysis",
                "sampling design"
            ],
            "author": [
                "Stephan Clémençon",
                "Igor Colin",
                "Aurélien Bellet"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-012/15-012.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Iterative Regularization for Learning with Convex Loss Functions",
            "abstract": "We consider the problem of supervised learning with convex loss functions and propose a new form of iterative regularization based on the subgradient method. Unlike other regularization approaches, in iterative regularization no constraint or penalization is considered, and generalization is achieved by (early) stopping an empirical iteration. We consider a nonparametric setting, in the framework of reproducing kernel Hilbert spaces, and prove consistency and finite sample bounds on the excess risk under general regularity conditions. Our study provides a new class of efficient regularized learning algorithms and gives insights on the interplay between statistics and optimization in machine learning.",
            "keywords": [],
            "author": [
                "Junhong Lin",
                "Lorenzo Rosasco",
                "Ding-Xuan Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-115/15-115.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Latent Space Inference of Internet-Scale Networks",
            "abstract": "The rise of Internet-scale networks, such as web graphs and social media with hundreds of millions to billions of nodes, presents new scientific opportunities, such as overlapping community detection to discover the structure of the Internet, or to analyze trends in online social behavior. However, many existing probabilistic network models are difficult or impossible to deploy at these massive scales. We propose a scalable approach for modeling and inferring latent spaces in Internet-scale networks, with an eye towards overlapping community detection as a key application. By applying a succinct representation of networks as a bag of triangular motifs, developing a parsimonious statistical model, deriving an efficient stochastic variational inference algorithm, and implementing it as a distributed cluster program via the Petuum parameter server system, we demonstrate overlapping community detection on real networks with up to 100 million nodes and 1000 communities on 5 machines in under 40 hours. Compared to other state-of-the-art probabilistic network approaches, our method is several orders of magnitude faster, with competitive or improved accuracy at overlapping community detection.",
            "keywords": [
                "probabilistic network models",
                "triangular modeling",
                "stochastic variational     inference",
                "distributed computation"
            ],
            "author": [
                "Qirong Ho",
                "Junming Yin",
                "Eric P. Xing"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-142/15-142.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Patient Risk Stratification with Time-Varying Parameters: A Multitask Learning Approach",
            "abstract": "The proliferation of electronic health records (EHRs) frames opportunities for using machine learning to build models that help healthcare providers improve patient outcomes. However, building useful risk stratification models presents many technical challenges including the large number of factors (both intrinsic and extrinsic) influencing a patient's risk of an adverse outcome and the inherent evolution of that risk over time. We address these challenges in the context of learning a risk stratification model for predicting which patients are at risk of acquiring a Clostridium difficile infection (CDI). We take a novel data-centric approach, leveraging the contents of EHRs from nearly 50,000 hospital admissions. We show how, by adapting techniques from multitask learning, we can learn models for patient risk stratification with unprecedented classification performance. Our model, based on thousands of variables, both time-varying and time-invariant, changes over the course of a patient admission. Applied to a held out set of approximately 25,000 patient admissions, we achieve an area under the receiver operating characteristic curve of 0.81 (95% CI 0.78-0.84). The model has been integrated into the health record system at a large hospital in the US, and can be used to produce daily risk estimates for each inpatient. While more complex than traditional risk stratification methods, the widespread development and use of such data-driven models could ultimately enable cost-effective, targeted prevention strategies that lead to better patient outcomes.",
            "keywords": [
                "risk stratification",
                "time-varying coefficients",
                "multitask learning",
                "Clostridium     difficile"
            ],
            "author": [
                "Jenna Wiens",
                "John Guttag",
                "Eric Horvitz"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-177/15-177.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiplicative Multitask Feature Learning",
            "abstract": "We investigate a general framework of multiplicative multitask feature learning which decomposes individual task's model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods can be proved to be special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the task- specific component for all these regularizers, leading to a better understanding of the shrinkage effects of different regularizers. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. An efficient blockwise coordinate descent algorithm is developed suitable for solving the entire family of formulations with rigorous convergence analysis. Simulation studies have identified the statistical properties of data that would be in favor of the new formulations. Extensive empirical studies on various classification and regression benchmark data sets have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks.",
            "keywords": [],
            "author": [
                "Xin Wang",
                "Jinbo Bi",
                "Shipeng Yu",
                "Jiangwen Sun",
                "Minghu Song"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-234/15-234.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Benefit of Multitask Representation Learning",
            "abstract": "We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks.",
            "keywords": [
                "learning-to-learn",
                "multitask learning",
                "representation learning",
                "statistical     learning theory"
            ],
            "author": [
                "Andreas Maurer",
                "Massimiliano Pontil",
                "Bernardino Romera-Paredes"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-242/15-242.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model-free Variable Selection in Reproducing Kernel Hilbert Space",
            "abstract": "Variable selection is popular in high-dimensional data analysis to identify the truly informative variables. Many variable selection methods have been developed under various model assumptions. Whereas success has been widely reported in literature, their performances largely depend on validity of the assumed models, such as the linear or additive models. This article introduces a model-free variable selection method via learning the gradient functions. The idea is based on the equivalence between whether a variable is informative and whether its corresponding gradient function is substantially non-zero. The proposed variable selection method is then formulated in a framework of learning gradients in a flexible reproducing kernel Hilbert space. The key advantage of the proposed method is that it requires no explicit model assumption and allows for general variable effects. Its asymptotic estimation and selection consistencies are studied, which establish the convergence rate of the estimated sparse gradients and assure that the truly informative variables are correctly identified in probability. The effectiveness of the proposed method is also supported by a variety of simulated examples and two real-life examples.",
            "keywords": [
                "group Lasso",
                "high-dimensional data",
                "kernel regression",
                "learning gradients"
            ],
            "author": [
                "Lei Yang",
                "Shaogao Lv",
                "Junhui Wang"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-390/15-390.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lenient Learning in Independent-Learner Stochastic Cooperative Games",
            "abstract": "We introduce the Lenient Multiagent Reinforcement Learning 2 (LMRL2) algorithm for independent-learner stochastic cooperative games. LMRL2 is designed to overcome a pathology called relative overgeneralization, and to do so while still performing well in games with stochastic transitions, stochastic rewards, and miscoordination. We discuss the existing literature, then compare LMRL2 against other algorithms drawn from the literature which can be used for games of this kind: traditional (âDistributedâ) Q-learning, Hysteretic Q-learning, WoLF-PHC, SOoN, and (for repeated games only) FMQ. The results show that LMRL2 is very effective in both of our measures (complete and correct policies), and is found in the top rank more often than any other technique. LMRL2 is also easy to tune: though it has many available parameters, almost all of them stay at default settings. Generally the algorithm is optimally tuned with a single parameter, if any. We then examine and discuss a number of side-issues and options for LMRL2.",
            "keywords": [
                "multiagent learning",
                "reinforcement learning",
                "game theory",
                "lenient learning"
            ],
            "author": [
                "Ermo Wei",
                "Sean Luke"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-417/15-417.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structure-Leveraged Methods in Breast Cancer Risk Prediction",
            "abstract": "Predicting breast cancer risk has long been a goal of medical research in the pursuit of precision medicine. The goal of this study is to develop novel penalized methods to improve breast cancer risk prediction by leveraging structure information in electronic health records. We conducted a retrospective case- control study, garnering 49 mammography descriptors and 77 high- frequency/low-penetrance single-nucleotide polymorphisms (SNPs) from an existing personalized medicine data repository. Structured mammography reports and breast imaging features have long been part of a standard electronic health record (EHR), and genetic markers likely will be in the near future. Lasso and its variants are widely used approaches to integrated learning and feature selection, and our methodological contribution is to incorporate the dependence structure among the features into these approaches. More specifically, we propose a new methodology by combining group penalty and  () fusion penalty to improve breast cancer risk prediction, taking into account structure information in mammography descriptors and SNPs. We demonstrate that our method provides benefits that are both statistically significant and potentially significant to people's lives.",
            "keywords": [],
            "author": [
                "Jun Fan",
                "Yirong Wu",
                "Ming Yuan",
                "David Page",
                "Jie Liu",
                "Irene M. Ong",
                "Peggy Peissig",
                "Elizabeth Burnside"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-444/15-444.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "L1-Regularized Least Squares for Support Recovery of High Dimensional Single Index Models with Gaussian Designs",
            "abstract": "It is known that for a certain class of single index models (SIMs) , support recovery is impossible when  and a model complexity adjusted sample size is below a critical threshold. Recently, optimal algorithms based on Sliced Inverse Regression (SIR) were suggested. These algorithms work provably under the assumption that the design  comes from an i.i.d. Gaussian distribution. In the present paper we analyze algorithms based on covariance screening and least squares with  penalization (i.e. LASSO) and demonstrate that they can also enjoy optimal (up to a scalar) rescaled sample size in terms of support recovery, albeit under slightly different assumptions on  and  compared to the SIR based algorithms. Furthermore, we show more generally, that LASSO succeeds in recovering the signed support of  if , and the covariance  satisfies the irrepresentable condition. Our work extends existing results on the support recovery of LASSO for the linear model, to a more general class of SIMs.",
            "keywords": [
                "Single index models",
                "Sparsity",
                "Support recovery",
                "High-dimensional statistics"
            ],
            "author": [
                "Matey Neykov",
                "Jun S. Liu",
                "Tianxi Cai"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-006/16-006.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral Ranking using Seriation",
            "abstract": "We describe a seriation algorithm for ranking a set of items given pairwise comparisons between these items. Intuitively, the algorithm assigns similar rankings to items that compare similarly with all others. It does so by constructing a similarity matrix from pairwise comparisons, using seriation methods to reorder this matrix and construct a ranking. We first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order. We then show that ranking reconstruction is still exact when some pairwise comparisons are corrupted or missing, and that seriation based spectral ranking is more robust to noise than classical scoring methods. Finally, we bound the ranking error when only a random subset of the comparions are observed. An additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems. Experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods.",
            "keywords": [
                "Ranking",
                "seriation"
            ],
            "author": [
                "Fajwel Fogel",
                "Alexandre d'Aspremont",
                "Milan Vojnovic"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-035/16-035.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparsity and Error Analysis of Empirical Feature-Based Regularization Schemes",
            "abstract": "We consider a learning algorithm generated by a regularization scheme with a concave regularizer for the purpose of achieving sparsity and good learning rates in a least squares regression setting. The regularization is induced for linear combinations of empirical features, constructed in the literatures of kernel principal component analysis and kernel projection machines, based on kernels and samples. In addition to the separability of the involved optimization problem caused by the empirical features, we carry out sparsity and error analysis, giving bounds in the norm of the reproducing kernel Hilbert space, based on a priori conditions which do not require assumptions on sparsity in terms of any basis or system. In particular, we show that as the concave exponent  of the concave regularizer increases to , the learning ability of the algorithm improves. Some numerical simulations for both artificial and real MHC-peptide binding data involving the  regularizer and the SCAD penalty are presented to demonstrate the sparsity and error analysis.",
            "keywords": [
                "Sparsity",
                "concave regularizer",
                "reproducing kernel Hilbert space",
                "regularization     with empirical features"
            ],
            "author": [
                "Xin Guo",
                "Jun Fan",
                "Ding-Xuan Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume17/11-207/11-207.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimating Diffusion Networks: Recovery Conditions, Sample Complexity and Soft-thresholding Algorithm",
            "abstract": "Information spreads across social and technological networks, but often the network structures are hidden from us and we only observe the traces left by the diffusion  processes, called cascades. Can we recover the hidden network structures from these observed cascades? What kind of cascades and how many cascades  do we need? Are there some network structures which are more difficult than others to recover? Can we design efficient inference algorithms with provable  guarantees? Despite the increasing availability of cascade data and methods for inferring networks from these data, a thorough theoretical understanding of the above questions  remains largely unexplored in the literature. In this paper, we investigate the network structure inference problem for a general family of continuous- time diffusion models  using an -regularized likelihood maximization framework.   We show that, as long as the cascade sampling process satisfies a natural incoherence condition, our framework can recover the correct network structure with high probability if we observe  cascades, where  is the maximum number of parents of a node and  is the total number of nodes. Moreover, we develop a  simple and efficient soft-thresholding network inference algorithm which demonstrate the match between our theoretical prediction and empirical results. In practice, this new algorithm also outperforms other alternatives in terms of the accuracy of recovering hidden diffusion networks.",
            "keywords": [],
            "author": [
                "Manuel Gomez-Rodriguez",
                "Le Song",
                "Hadi Daneshm",
                "Bernhard Schölkopf"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-430/14-430.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rounding-based Moves for Semi-Metric Labeling",
            "abstract": "Semi-metric labeling is a special case of energy minimization for pairwise Markov random fields. The energy function consists of arbitrary unary potentials, and pairwise potentials that are proportional to a given semi-metric distance function over the label set. Popular methods for solving semi-metric labeling include (i) move-making algorithms, which iteratively solve a minimum -cut problem; and (ii) the linear programming ( LP) relaxation based approach. In order to convert the fractional solution of the LP relaxation to an integer solution, several randomized rounding procedures have been developed in the literature. We consider a large class of parallel rounding procedures, and design move-making algorithms that closely mimic them. We prove that the multiplicative bound of a move-making algorithm exactly matches the approximation factor of the corresponding rounding procedure for any arbitrary distance function. Our analysis includes all known results for move- making algorithms as special cases.",
            "keywords": [
                "semi-metric labeling",
                "move-making algorithms",
                "linear programming relaxation"
            ],
            "author": [
                "M. Pawan Kumar",
                "Puneet K. Dokania"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-454/14-454.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rate Optimal Denoising of Simultaneously Sparse and Low Rank Matrices",
            "abstract": "We study minimax rates for denoising simultaneously sparse and low rank matrices in high dimensions. We show that an iterative thresholding algorithm achieves (near) optimal rates adaptively under mild conditions for a large class of loss functions. Numerical experiments on synthetic datasets also demonstrate the competitive performance of the proposed method.",
            "keywords": [
                "Denoising",
                "High dimensionality",
                "Low rank matrices",
                "Minimax rates",
                "Simul-     taneously structured matrices",
                "Sparse SVD"
            ],
            "author": [
                "Dan Yang",
                "Zongming Ma",
                "Andreas Buja"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-134/15-134.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hierarchical Relative Entropy Policy Search",
            "abstract": "Many reinforcement learning (RL) tasks, especially in robotics, consist of multiple sub-tasks that are strongly structured. Such task structures can be exploited by incorporating hierarchical policies that consist of gating networks and sub-policies. However, this concept has only been partially explored for real world settings and complete methods, derived from first principles, are needed. Real world settings are challenging due to large and continuous state-action spaces that are prohibitive for exhaustive sampling methods. We define the problem of learning sub-policies in continuous state action spaces as finding a hierarchical policy that is composed of a high-level gating policy to select the low-level sub-policies for execution by the agent. In order to efficiently share experience with all sub-policies, also called inter-policy learning, we treat these sub-policies as latent variables which allows for distribution of the update information between the sub-policies. We present three different variants of our algorithm, designed to be suitable for a wide variety of real world robot learning tasks and evaluate our algorithms in two real robot learning scenarios as well as several simulations and comparisons.",
            "keywords": [
                "Reinforcement Learning",
                "Policy Search",
                "Hierarchical Learning",
                "Robot Learning",
                "Mo-      tor Skill Learning",
                "Robust Learning",
                "Structured Learning",
                "Temporal Correlation",
                "HiREPS"
            ],
            "author": [
                "Christian Daniel",
                "Gerhard Neumann",
                "Oliver Kroemer",
                "Jan Peters"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-188/15-188.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convex Regression with Interpretable Sharp Partitions",
            "abstract": "We consider the problem of predicting an outcome variable on the basis of a small number of covariates, using an interpretable yet non-additive model. We propose convex regression with interpretable sharp partitions (CRISP) for this task. CRISP partitions the covariate space into blocks in a data- adaptive way, and fits a mean model within each block. Unlike other partitioning methods, CRISP is fit using a non-greedy approach by solving a convex optimization problem, resulting in low- variance fits. We explore the properties of CRISP, and evaluate its performance in a simulation study and on a housing price data set.",
            "keywords": [
                "convex optimization",
                "interpretability",
                "non-additivity",
                "non-parametric regres-     sion"
            ],
            "author": [
                "Ashley Petersen",
                "Noah Simon",
                "Daniela Witten"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-344/15-344.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "JCLAL: A Java Framework for Active Learning",
            "abstract": "Active Learning has become an important area of research owing to the increasing number of real-world problems which contain labelled and unlabelled examples at the same time. JCLAL is a Java Class Library for Active Learning which has an architecture that follows strong principles of object-oriented design. It is easy to use, and it allows the developers to adapt, modify and extend the framework according to their needs. The library offers a variety of active learning methods that have been proposed in the literature. The software is available under the GPL license.",
            "keywords": [
                "active learning",
                "framework",
                "java language"
            ],
            "author": [
                "Oscar Reyes",
                "Eduardo Pérez",
                "María del Carmen Rodríguez-Hernández",
                "Habib M. Fardoun",
                "Sebastián  Ventura"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-347/15-347.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Integrated Common Sense Learning and Planning in POMDPs",
            "abstract": "We formulate a new variant of the problem of planning in an unknown environment, for which we can provide algorithms with reasonable theoretical guarantees in spite of large state spaces and time horizons, partial observability, and complex dynamics. In this variant, an agent is given a collection of example traces produced by a reference policy, which may, for example, capture the agent's past behavior. The agent is (only) asked to find policies that are supported by regularities in the dynamics that are observable on these example traces. We describe an efficient algorithm that uses such common sense knowledge reflected in the example traces to construct decision tree policies for goal-oriented factored POMDPs. More precisely, our algorithm (provably) succeeds at finding a policy for a given input goal when (1) there is a CNF that is almost always observed satisfied on the traces of the POMDP, capturing a sufficient approximation of its dynamics and (2) for a decision tree policy of bounded complexity, there exist small- space resolution proofs that the goal is achieved on each branch using the aforementioned CNF capturing the common sense rules. Such a CNF always exists for noisy STRIPS domains, for example. Our results thus essentially establish that the possession of a suitable exploration policy for collecting the necessary examples is the fundamental obstacle to learning to act in such environments.",
            "keywords": [
                "Partially Observed Markov Decision Process",
                "Decision Tree Policies",
                "PAC-     Semantics",
                "Noisy STRIPS"
            ],
            "author": [
                "Brendan Juba"
            ],
            "ref": "http://jmlr.org/papers/volume17/13-584/13-584.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cells in Multidimensional Recurrent Neural Networks",
            "abstract": "The transcription of handwritten text on images is one task in machine learning and one solution to solve it is using multi- dimensional recurrent neural networks (MDRNN) with connectionist temporal classification (CTC). The RNNs can contain special units, the long short-term memory (LSTM) cells. They are able to learn long term dependencies but they get unstable when the dimension is chosen greater than one. We defined some useful and necessary properties for the one-dimensional LSTM cell and extend them in the multi-dimensional case. Thereby we introduce several new cells with better stability. We present a method to design cells using the theory of linear shift invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT and Rimes database, where we can improve the recognition rate compared to the LSTM cell. So each application where the LSTM cells in MDRNNs are used could be improved by substituting them by the new developed cells.",
            "keywords": [],
            "author": [
                "Gundram Leifert",
                "Tobias Strau{\\ss}",
                "Tobias Gr{ü}ning",
                "Welf Wustlich",
                "Roger Labahn"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-203/14-203.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Taxonomy Adaptation in Large-scale Classification",
            "abstract": "In this paper, we study flat and hierarchical classification strategies in the context of large-scale taxonomies. Addressing the problem from a learning-theoretic point of view, we first propose a multi-class, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. Based on this bound, we also propose a technique for modifying a given taxonomy through pruning, that leads to a lower value of the upper bound as compared to the original taxonomy. We then present another method for hierarchy pruning by studying approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.",
            "keywords": [
                "Large-scale classification",
                "Hierarchical classification",
                "Taxonomy adaptation",
                "Rademacher complexity"
            ],
            "author": [
                "Rohit Babbar",
                "Ioannis Partalas",
                "Eric Gaussier",
                "Massih-Reza Amini",
                "Cécile Amblard"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-207/14-207.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "How to Center Deep Boltzmann Machines",
            "abstract": "This work analyzes centered Restricted Boltzmann Machines (RBMs) and centered Deep Boltzmann Machines (DBMs), where centering is done by subtracting offset values from visible and hidden variables. We show analytically that (i) centered and normal Boltzmann Machines (BMs) and thus RBMs and DBMs are different parameterizations of the same model class, such that any normal BM/RBM/DBM can be transformed to an equivalent centered BM/RBM/DBM and vice versa, and that this equivalence generalizes to artificial neural networks in general, (ii) the expected performance of centered binary BMs/RBMs/DBMs is invariant under simultaneous flip of data and offsets, for any offset value in the range of zero to one, (iii) centering can be reformulated as a different update rule for normal BMs/RBMs/DBMs, and (iv) using the enhanced gradient is equivalent to setting the offset values to the average over model and data mean. Furthermore, we present numerical simulations suggesting that (i) optimal generative performance is achieved by subtracting mean values from visible as well as hidden variables, (ii) centered binary RBMs/DBMs reach significantly higher log-likelihood values than normal binary RBMs/DBMs, (iii) centering variants whose offsets depend on the model mean, like the enhanced gradient, suffer from severe divergence problems, (iv) learning is stabilized if an exponentially moving average over the batch means is used for the offset values instead of the current batch mean, which also prevents the enhanced gradient from severe divergence, (v) on a similar level of log-likelihood values centered binary RBMs/DBMs have smaller weights and bigger bias parameters than normal binary RBMs/DBMs, (vi) centering leads to an update direction that is closer to the natural gradient, which is extremely efficient for training as we show for small binary RBMs, (vii) centering eliminates the need for greedy layer-wise pre-training of DBMs, which often even deteriorates the results independently of whether centering is used or not, and (ix) centering is also beneficial for auto encoders.",
            "keywords": [
                "centering",
                "restricted Boltzmann machine",
                "deep Boltzmann machine",
                "gener-     ative model",
                "artificial neural network",
                "auto encoder",
                "enhanced gradient",
                "natural gradient",
                "stochastic maximum likelihood",
                "contrastive divergence"
            ],
            "author": [
                "Jan Melchior",
                "Asja Fischer",
                "Laurenz Wiskott"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-237/14-237.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Control Function Instrumental Variable Estimation of Nonlinear Causal Effect Models",
            "abstract": "The instrumental variable method consistently estimates the effect of a treatment when there is unmeasured confounding and a valid instrumental variable. A valid instrumental variable is a variable that is independent of unmeasured confounders and affects the treatment but does not have a direct effect on the outcome beyond its effect on the treatment. Two commonly used estimators for using an instrumental variable to estimate a treatment effect are the two stage least squares estimator and the control function estimator. For linear causal effect models, these two estimators are equivalent, but for nonlinear causal effect models, the estimators are different. We provide a systematic comparison of these two estimators for nonlinear causal effect models and develop an approach to combing the two estimators that generally performs better than either one alone. We show that the control function estimator is a two stage least squares estimator with an augmented set of instrumental variables. If these augmented instrumental variables are valid, then the control function estimator can be much more efficient than usual two stage least squares without the augmented instrumental variables while if the augmented instrumental variables are not valid, then the control function estimator may be inconsistent while the usual two stage least squares remains consistent. We apply the Hausman test to test whether the augmented instrumental variables are valid and construct a pretest estimator based on this test. The pretest estimator is shown to work well in a simulation study. An application to the effect of exposure to violence on time preference is considered.",
            "keywords": [
                "Causal Inference",
                "Control Function Estimator",
                "Endogenous Variable",
                "Instru-     mental Variable Method",
                "Two Stage Least Squares Estimator"
            ],
            "author": [
                "Zijian Guo",
                "Dylan S. Small"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-379/14-379.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structure Learning in Bayesian Networks of a Moderate Size by Efficient Sampling",
            "abstract": "We study the Bayesian model averaging approach to learning Bayesian network structures (DAGs) from data. We develop new algorithms including the first algorithm that is able to efficiently sample DAGs of a moderate size (with up to about 25 variables) according to the exact structure posterior. The DAG samples can then be used to construct estimators for the posterior of any feature. We theoretically prove good properties of our estimators and empirically show that our estimators considerably outperform the estimators from the previous state- of-the-art methods.",
            "keywords": [
                "Bayesian model averaging",
                "Bayesian networks",
                "DAG sampling",
                "dynamic program-     ming",
                "order sampling"
            ],
            "author": [
                "Ru He",
                "Jin Tian",
                "Huaiqing Wu"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-497/14-497.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral Methods Meet EM: A Provably Optimal Algorithm for Crowdsourcing",
            "abstract": "Crowdsourcing is a popular paradigm for effectively collecting labels at low cost. The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.",
            "keywords": [
                "crowdsourcing",
                "spectral methods",
                "EM",
                "Dawid-Skene model",
                "non-convex op-     timization"
            ],
            "author": [
                "Yuchen Zhang",
                "Xi Chen",
                "Dengyong Zhou",
                "Michael I. Jordan"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-511/14-511.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Leave-One-Out Cross-Validation Approximations for Gaussian Latent Variable Models",
            "abstract": "The future predictive performance of a Bayesian model can be estimated using Bayesian cross-validation. In this article, we consider Gaussian latent variable models where the integration over the latent values is approximated using the Laplace method or expectation propagation (EP). We study the properties of several Bayesian leave-one-out (LOO) cross-validation approximations that in most cases can be computed with a small additional cost after forming the posterior approximation given the full data. Our main objective is to assess the accuracy of the approximative LOO cross-validation estimators. That is, for each method (Laplace and EP) we compare the approximate fast computation with the exact brute force LOO computation. Secondarily, we evaluate the accuracy of the Laplace and EP approximations themselves against a ground truth established through extensive Markov chain Monte Carlo simulation. Our empirical results show that the approach based upon a Gaussian approximation to the LOO marginal distribution (the so-called cavity distribution) gives the most accurate and reliable results among the fast methods.",
            "keywords": [
                "predictive performance",
                "leave-one-out cross-validation",
                "Gaussian latent vari-     able model",
                "Laplace approximation"
            ],
            "author": [
                "Aki Vehtari",
                "Tommi Mononen",
                "Ville Tolvanen",
                "Tuomas Sivula",
                "Ole Winther"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-540/14-540.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "e-PAL: An Active Learning Approach to the Multi-Objective Optimization Problem",
            "abstract": "In many fields one encounters the challenge of identifying out of a pool of possible designs those that simultaneously optimize multiple objectives. In many applications an exhaustive search for the Pareto-optimal set is infeasible. To address this challenge, we propose the -Pareto Active Learning (-PAL) algorithm which adaptively samples the design space to predict a set of Pareto-optimal solutions that cover the true Pareto front of the design space with some granularity regulated by a parameter . Key features of -PAL include (1) modeling the objectives as draws from a Gaussian process distribution to capture structure and accommodate noisy evaluation; (2) a method to carefully choose the next design to evaluate to maximize progress; and (3) the ability to control prediction accuracy and sampling cost. We provide theoretical bounds on -PAL's sampling cost required to achieve a desired accuracy. Further, we perform an experimental evaluation on three real-world data sets that demonstrate -PAL's effectiveness; in comparison to the state-of-the-art active learning algorithm PAL, -PAL reduces the amount of computations and the number of samples from the design space required to meet the user's desired level of accuracy. In addition, we show that -PAL improves significantly over a state-of-the-art multi- objective optimization method, saving in most cases 30\\% to 70\\% evaluations to achieve the same accuracy.",
            "keywords": [
                "multi-objective optimization",
                "active learning",
                "pareto optimality",
                "Bayesian     optimization"
            ],
            "author": [
                "Marcela Zuluaga",
                "Andreas Krause",
                "Markus P{ü}schel"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-047/15-047.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Trend Filtering on Graphs",
            "abstract": "We introduce a family of adaptive estimators on graphs, based on penalizing the  norm of discrete graph differences. This generalizes the idea of trend filtering (Kim et al., 2009; Tibshirani, 2014), used for univariate nonparametric regression, to graphs. Analogous to the univariate case, graph trend filtering exhibits a level of local adaptivity unmatched by the usual -based graph smoothers. It is also defined by a convex minimization problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We demonstrate the merits of graph trend filtering through both examples and theory.",
            "keywords": [
                "trend filtering",
                "graph smoothing",
                "total variation denoising",
                "fused lasso"
            ],
            "author": [
                "Yu-Xiang Wang",
                "James Sharpnack",
                "Alexander J. Smola",
                "Ryan J. Tibshirani"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-147/15-147.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Task Learning for Straggler Avoiding Predictive Job Scheduling",
            "abstract": "Parallel processing frameworks (Dean and Ghemawat, 2004) accelerate jobs by breaking them into tasks that execute in parallel. However, slow running or straggler tasks can run up to 8 times slower than the median task on a production cluster (Ananthanarayanan et al., 2013), leading to delayed job completion and inefficient use of resources. Existing straggler mitigation techniques wait to detect stragglers and then relaunch them, delaying straggler detection and wasting resources. We built Wrangler (Yadwadkar et al., 2014), a system that predicts when stragglers are going to occur and makes scheduling decisions to avoid such situations. To capture node and workload variability, Wrangler built separate models for every node and workload, requiring the time-consuming collection of substantial training data. In this paper, we propose multi- task learning formulations that share information between the various models, allowing us to use less training data and bring training time down from 4 hours to 40 minutes. Unlike naive multi-task learning formulations, our formulations capture the shared structure in our data, improving generalization performance on limited data. Finally, we extend these formulations using group sparsity inducing norms to automatically discover the similarities between tasks and improve interpretability.",
            "keywords": [],
            "author": [
                "Neeraja J. Yadwadkar",
                "Bharath Hariharan",
                "Joseph E. Gonzalez",
                "R",
                "y Katz"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-149/15-149.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation",
            "abstract": "Despite tremendous progress in computer vision, there has not been an attempt to apply machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's Picture Archiving and Communication System. With natural language processing, we mine a collection of 216K representative two-dimensional images selected by clinicians for diagnostic reference and match the images with their descriptions in an automated manner. We then employ a weakly supervised approach using all of our available data to build models for generating approximate interpretations of patient images. Finally, we demonstrate a more strictly supervised approach to detect the presence and absence of a number of frequent disease types, providing more specific interpretations of patient scans. A relatively small amount of data is used for this part, due to the challenge in gathering quality labels from large raw text data. Our work shows the feasibility of large-scale learning and prediction in electronic patient records available in most modern clinical institutions. It also demonstrates the trade-offs to consider in designing machine learning systems for analyzing large medical data.",
            "keywords": [
                "Deep learning",
                "Convolutional Neural Networks",
                "Topic Models",
                "Natural Lan-     guage Processing"
            ],
            "author": [
                "Hoo-Chang Shin",
                "Le Lu",
                "Lauren Kim",
                "Ari Seff",
                "Jianhua Yao",
                "Ronald M. Summers"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-176/15-176.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distribution-Matching Embedding for Visual Domain Adaptation",
            "abstract": "Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Distribution-Matching Embedding approach: An unsupervised domain adaptation method that overcomes this issue by mapping the data to a latent space where the distance between the empirical distributions of the source and target examples is minimized. In other words, we seek to extract the information that is invariant across the source and target data. In particular, we study two different distances to compare the source and target distributions: the Maximum Mean Discrepancy and the Hellinger distance. Furthermore, we show that our approach allows us to learn either a linear embedding, or a nonlinear one. We demonstrate the benefits of our approach on the tasks of visual object recognition, text categorization, and WiFi localization.",
            "keywords": [
                "Domain Adaptation",
                "Maximum Mean Discrepancy",
                "Hellinger Distance",
                "Distribution     Matching"
            ],
            "author": [
                "Mahsa Baktashmotlagh",
                "Mehrtash Har",
                "i",
                "Mathieu Salzmann"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-207/15-207.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Monotonic Calibrated Interpolated Look-Up Tables",
            "abstract": "Real-world machine learning applications may have requirements beyond accuracy, such as fast evaluation times and interpretability. In particular, guaranteed monotonicity of the learned function with respect to some of the inputs can be critical for user confidence. We propose meeting these goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables. We extend the structural risk minimization framework of lattice regression to monotonic functions by adding linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, at the cost of making the objective non-convex. We address large- scale learning through parallelization, mini-batching, and random sampling of additive regularizer terms. Case studies on real-world problems with up to sixteen features and up to hundreds of millions of training samples demonstrate the proposed monotonic functions can achieve state-of-the-art accuracy in practice while providing greater transparency to users.",
            "keywords": [
                "interpretability",
                "interpolation",
                "look-up tables"
            ],
            "author": [
                "Maya Gupta",
                "Andrew Cotter",
                "Jan Pfeifer",
                "Konstantin Voevodski",
                "Kevin Canini",
                "Alexander Mangylov",
                "Wojciech Moczydlowski",
                "Alexander van Esbroeck"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-243/15-243.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Are Random Forests Truly the Best Classifiers?",
            "abstract": "The JMLR study Do we need hundreds of classifiers to solve real world classification problems? benchmarks 179 classifiers in 17 families on 121 data sets from the UCI repository and claims that âthe random forest is clearly the best family of classifierâ. In this response, we show that the study's results are biased by the lack of a held-out test set and the exclusion of trials with errors. Further, the study's own statistical tests indicate that random forests do not have significantly higher percent accuracy than support vector machines and neural networks, calling into question the conclusion that random forests are the best classifiers.",
            "keywords": [
                "classification",
                "benchmarking",
                "random forests",
                "support vector machines"
            ],
            "author": [
                "Michael Wainberg",
                "Babak Alipanahi",
                "Brendan J. Frey"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-374/15-374.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Adaptive Estimation of Nonparametric Hidden Markov Models",
            "abstract": "We consider stationary hidden Markov models with finite state space and nonparametric modeling of the emission distributions. It has remained unknown until very recently that such models are identifiable. In this paper, we propose a new penalized least- squares estimator for the emission distributions which is statistically optimal and practically tractable. We prove a non asymptotic oracle inequality for our nonparametric estimator of the emission distributions. A consequence is that this new estimator is rate minimax adaptive up to a logarithmic term. Our methodology is based on projections of the emission distributions onto nested subspaces of increasing complexity. The popular spectral estimators are unable to achieve the optimal rate but may be used as initial points in our procedure. Simulations are given that show the improvement obtained when applying the least-squares minimization consecutively to the spectral estimation.",
            "keywords": [
                "nonparametric estimation",
                "hidden Markov models",
                "minimax adaptive esti-     mation",
                "oracle inequality"
            ],
            "author": [
                "Yohann De Castro",
                "{\\'E}lisabeth Gassiat",
                "Claire Lacour"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-381/15-381.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Decrypting “Cryptogenic” Epilepsy: Semi-supervised Hierarchical Conditional Random Fields For Detecting Cortical Lesions In MRI-Negative Patients",
            "abstract": "Focal cortical dysplasia (FCD) is the most common cause of pediatric epilepsy and the third most common cause in adults with treatment-resistant epilepsy. Surgical resection of the lesion is the most effective treatment to stop seizures. Technical advances in MRI have revolutionized the diagnosis of FCD, leading to high success rates for resective surgery. However, 45% of histologically confirmed FCD patients have normal MRIs (MRI-negative). Without a visible lesion, the success rate of surgery drops from 66% to 29%. In this work, we cast the problem of detecting potential FCD lesions using MRI scans of MRI-negative patients in an image segmentation framework based on hierarchical conditional random fields (HCRF). We use surface based morphometry to model the cortical surface as a two-dimensional surface which is then segmented at multiple scales to extract superpixels of different sizes. Each superpixel is assigned an outlier score by comparing it to a control population. The lesion is detected by fusing the outlier probabilities across multiple scales using a tree- structured HCRF. The proposed method achieves a higher detection rate, with superior recall and precision on a sample of twenty MRI-negative FCD patients as compared to a baseline across four morphological features and their combinations.",
            "keywords": [],
            "author": [
                "Bilal Ahmed",
                "Thomas Thesen",
                "Karen E. Blackmon",
                "Ruben Kuzniekcy",
                "Orrin Devinsky",
                "Carla E. Brodley"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-428/15-428.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fused Lasso Approach in Regression Coefficients Clustering -- Learning Parameter Heterogeneity in Data Integration",
            "abstract": "As data sets of related studies become more easily accessible, combining data sets of similar studies is often undertaken in practice to achieve a larger sample size and higher power. A major challenge arising from data integration pertains to data heterogeneity in terms of study population, study design, or study coordination. Ignoring such heterogeneity in data analysis may result in biased estimation and misleading inference. Traditional techniques of remedy to data heterogeneity include the use of interactions and random effects, which are inferior to achieving desirable statistical power or providing a meaningful interpretation, especially when a large number of smaller data sets are combined. In this paper, we propose a regularized fusion method that allows us to identify and merge inter-study homogeneous parameter clusters in regression analysis, without the use of hypothesis testing approach. Using the fused lasso, we establish a computationally efficient procedure to deal with large-scale integrated data. Incorporating the estimated parameter ordering in the fused lasso facilitates computing speed with no loss of statistical power. We conduct extensive simulation studies and provide an application example to demonstrate the performance of the new method with a comparison to the conventional methods.",
            "keywords": [
                "Fused lasso",
                "Data integration",
                "Extended BIC"
            ],
            "author": [
                "Lu Tang",
                "Peter X.K. Song"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-598/15-598.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The LRP Toolbox for Artificial Neural Networks",
            "abstract": "The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. With the LRP Toolbox we provide platform-agnostic implementations for explaining the predictions of pre-trained state of the art Caffe networks and stand-alone implementations for fully connected Neural Network models. The implementations for Matlab and python shall serve as a playing field to familiarize oneself with the LRP algorithm and are implemented with readability and transparency in mind. Models and data can be imported and exported using raw text formats, Matlab's .mat files and the .npy format for numpy or plain text.",
            "keywords": [
                "layer-wise relevance propagation",
                "explaining classifiers",
                "deep learning",
                "artifi-     cial neural networks"
            ],
            "author": [
                "Sebastian Lapuschkin",
                "Alexander Binder",
                "Grégoire Montavon",
                "Klaus-Robert Müller",
                "Wojciech Samek"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-618/15-618.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Equivalence of Graphical Lasso and Thresholding for Sparse Graphs",
            "abstract": "This paper is concerned with the problem of finding a sparse graph capturing the conditional dependence between the entries of a Gaussian random vector, where the only available information is a sample correlation matrix. A popular approach to address this problem is the graphical lasso technique, which employs a sparsity-promoting regularization term. This paper derives a simple condition under which the computationally- expensive graphical lasso behaves the same as the simple heuristic method of thresholding. This condition depends only on the solution of graphical lasso and makes no direct use of the sample correlation matrix or the regularization coefficient. It is proved that this condition is always satisfied if the solution of graphical lasso is close to its first-order Taylor approximation or equivalently the regularization term is relatively large. This condition is tested on several random problems, and it is shown that graphical lasso and the thresholding method lead to highly similar results in the case where a sparse graph is sought. We also conduct two case studies on brain connectivity networks of twenty subjects based on fMRI data and the topology identification of electrical circuits to support the findings of this work on the similarity of graphical lasso and thresholding.",
            "keywords": [
                "Graphical Lasso",
                "Graphical Models",
                "Sparse Graphs",
                "Brain Connectivity     Networks"
            ],
            "author": [
                "Somayeh Sojoudi"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-013/16-013.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Network That Learns Strassen Multiplication",
            "abstract": "We study neural networks whose only non-linear components are multipliers, to test a new training rule in a context where the precise representation of data is paramount. These networks are challenged to discover the rules of matrix multiplication, given many examples. By limiting the number of multipliers, the network is forced to discover the Strassen multiplication rules. This is the mathematical equivalent of finding low rank decompositions of the  matrix multiplication tensor, . We train these networks with the conservative learning rule, which makes minimal changes to the weights so as to give the correct output for each input at the time the input-output pair is received. Conservative learning needs a few thousand examples to find the rank 7 decomposition of , and  for the rank 23 decomposition of  (the lowest known). High precision is critical, especially for , to discriminate between true decompositions and âborder approximations\".",
            "keywords": [
                "sum-product networks",
                "Strassen multiplication"
            ],
            "author": [
                "Veit Elser"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-074/16-074.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Revisiting the Nyström Method for Improved Large-scale Machine Learning",
            "abstract": "We reconsider randomized algorithms for the low-rank approximation of symmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods; they characterize the effects of common data preprocessing steps on the performance of these algorithms; and they point to important differences between uniform sampling and nonuniform sampling methods based on leverage scores. In addition, our empirical results illustrate that existing theory is so weak that it does not provide even a qualitative guide to practice. Thus, we complement our empirical results with a suite of worst- case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds---e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error---and they point to future directions to make these algorithms useful in even larger-scale machine learning applications.",
            "keywords": [],
            "author": [
                "Alex Gittens",
                "Michael W. Mahoney"
            ],
            "ref": "http://jmlr.org/papers/volume17/gittens16a/gittens16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improving Structure MCMC for Bayesian Networks through Markov Blanket Resampling",
            "abstract": "Algorithms for inferring the structure of Bayesian networks from data have become an increasingly popular method for uncovering the direct and indirect influences among variables in complex systems. A Bayesian approach to structure learning uses posterior probabilities to quantify the strength with which the data and prior knowledge jointly support each possible graph feature. Existing Markov Chain Monte Carlo (MCMC) algorithms for estimating these posterior probabilities are slow in mixing and convergence, especially for large networks. We present a novel Markov blanket resampling (MBR) scheme that intermittently reconstructs the Markov blanket of nodes, thus allowing the sampler to more effectively traverse low-probability regions between local maxima. As we can derive the complementary forward and backward directions of the MBR proposal distribution, the Metropolis-Hastings algorithm can be used to account for any asymmetries in these proposals. Experiments across a range of network sizes show that the MBR scheme outperforms other state- of-the-art algorithms, both in terms of learning performance and convergence rate. In particular, MBR achieves better learning performance than the other algorithms when the number of observations is relatively small and faster convergence when the number of variables in the network is large.",
            "keywords": [
                "probabilistic graphical models",
                "directed acyclic graph",
                "Bayesian inference"
            ],
            "author": [
                "Chengwei Su",
                "Mark E. Borsuk"
            ],
            "ref": "http://jmlr.org/papers/volume17/su16a/su16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Volumetric Spanners: An Efficient Exploration Basis for Learning",
            "abstract": "Numerous learning problems that contain exploration, such as experiment design, multi-arm bandits, online routing, search result aggregation and many more, have been studied extensively in isolation. In this paper we consider a generic and efficiently computable method for action space exploration based on convex geometry. We define a novel geometric notion of an exploration mechanism with low variance called volumetric spanners, and give efficient algorithms to construct such spanners. We describe applications of this mechanism to the problem of optimal experiment design and the general framework for decision making under uncertainty of bandit linear optimization. For the latter we give efficient and near-optimal regret algorithm over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self- concordant barrier for the underlying set.",
            "keywords": [
                "barycentric spanner",
                "volumetric spanner",
                "linear bandits"
            ],
            "author": [
                "Elad Hazan",
                "Zohar Karnin"
            ],
            "ref": "http://jmlr.org/papers/volume17/hazan16a/hazan16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels",
            "abstract": "We consider the problem of improving the efficiency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large data sets. These approximate feature maps arise as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use  Quasi-Monte Carlo (QMC) approximations instead, where the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy based on theoretical characterizations of the integration error with respect to a given sequence. We then propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization. Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of classical and adaptive QMC techniques for this problem.",
            "keywords": [],
            "author": [
                "Haim Avron",
                "Vikas Sindhwani",
                "Jiyan Yang",
                "Michael W. Mahoney"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-538/14-538.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Dependent Multi-output Gaussian Process Dynamical Systems",
            "abstract": "This paper presents a dependent multi-output Gaussian process (GP) for modeling complex dynamical systems. The outputs are dependent in this model, which is largely different from previous GP dynamical systems. We adopt convolved multi-output GPs to model the outputs, which are provided with a flexible multi-output covariance function. We adapt the variational inference method with inducing points for learning the model. Conjugate gradient based optimization is used to solve parameters involved by maximizing the variational lower bound of the marginal likelihood. The proposed model has superiority on modeling dynamical systems under the more reasonable assumption and the fully Bayesian learning framework. Further, it can be flexibly extended to handle regression problems. We evaluate the model on both synthetic and real-world data including motion capture data, traffic flow data and robot inverse dynamics data. Various evaluation methods are taken on the experiments to demonstrate the effectiveness of our model, and encouraging results are observed.",
            "keywords": [
                "Gaussian process",
                "variational inference",
                "dynamical system"
            ],
            "author": [
                "Jing Zhao",
                "Shiliang Sun"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-423/14-423.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiple Output Regression with Latent Noise",
            "abstract": "In high-dimensional data, structured noise caused by observed and unobserved factors affecting multiple target variables simultaneously, imposes a serious challenge for modeling, by masking the often weak signal. Therefore, (1) explaining away the structured noise in multiple-output regression is of paramount importance. Additionally, (2) assumptions about the correlation structure of the regression weights are needed. We note that both can be formulated in a natural way in a latent variable model, in which both the interesting signal and the noise are mediated through the same latent factors. Under this assumption, the signal model then borrows strength from the noise model by encouraging similar effects on correlated targets. We introduce a hyperparameter for the latent signal-to-noise ratio which turns out to be important for modelling weak signals, and an ordered infinite-dimensional shrinkage prior that resolves the rotational unidentifiability in reduced-rank regression models. Simulations and prediction experiments with metabolite, gene expression, FMRI measurement, and macroeconomic time series data show that our model equals or exceeds the state-of-the-art performance and, in particular, outperforms the standard approach of assuming independent noise and signal models.",
            "keywords": [],
            "author": [
                "Jussi Gillberg",
                "Pekka Marttinen",
                "Matti Pirinen",
                "Antti J. Kangas",
                "Pasi Soininen",
                "Mehreen Ali",
                "Aki S. Havulinna",
                "Marjo-Riitta Järvelin",
                "Mika Ala-Korpela",
                "Samuel Kaski"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-436/14-436.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Constrained Dantzig Selector with Enhanced Consistency",
            "abstract": "The Dantzig selector has received popularity for many applications such as compressed sensing and sparse modeling, thanks to its computational efficiency as a linear programming problem and its nice sampling properties. Existing results show that it can recover sparse signals mimicking the accuracy of the ideal procedure, up to a logarithmic factor of the dimensionality. Such a factor has been shown to hold for many regularization methods. An important question is whether this factor can be reduced to a logarithmic factor of the sample size in ultra-high dimensions under mild regularity conditions. To provide an affirmative answer, in this paper we suggest the constrained Dantzig selector, which has more flexible constraints and parameter space. We prove that the suggested method can achieve convergence rates within a logarithmic factor of the sample size of the oracle rates and improved sparsity, under a fairly weak assumption on the signal strength. Such improvement is significant in ultra-high dimensions. This method can be implemented efficiently through sequential linear programming. Numerical studies confirm that the sample size needed for a certain level of accuracy in these problems can be much reduced.",
            "keywords": [
                "Sparse Modeling",
                "Compressed Sensing",
                "Ultra-high Dimensionality",
                "Dantzig     Selector",
                "Regularization Methods"
            ],
            "author": [
                "Yinfei Kong",
                "Zemin Zheng",
                "Jinchi Lv"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-513/14-513.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Lower and Upper Bounds in Smooth and Strongly Convex Optimization",
            "abstract": "We develop a novel framework to study smooth and strongly convex optimization algorithms. Focusing on quadratic functions we are able to examine optimization algorithms as a recursive application of linear operators. This, in turn, reveals a powerful connection between a class of optimization algorithms and the analytic theory of polynomials whereby new lower and upper bounds are derived. Whereas existing lower bounds for this setting are only valid when the dimensionality scales with the number of iterations, our lower bound holds in the natural regime where the dimensionality is fixed. Lastly, expressing it as an optimal solution for the corresponding optimization problem over polynomials, as formulated by our framework, we present a novel systematic derivation of Nesterov's well-known Accelerated Gradient Descent method. This rather natural interpretation of AGD contrasts with earlier ones which lacked a simple, yet solid, motivation.",
            "keywords": [
                "smooth and strongly convex optimization",
                "full gradient descent",
                "accelerated     gradient descent"
            ],
            "author": [
                "Yossi Arjevani",
                "Shai Shalev-Shwartz",
                "Ohad Shamir"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-106/15-106.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dual Control for Approximate Bayesian Reinforcement Learning",
            "abstract": "Control of non-episodic, finite-horizon dynamical systems with uncertain dynamics poses a tough and elementary case of the exploration-exploitation trade-off. Bayesian reinforcement learning, reasoning about the effect of actions and future observations, offers a principled solution, but is intractable. We review, then extend an old approximate approach from control theory---where the problem is known as dual control---in the context of modern regression methods, specifically generalized linear regression. Experiments on simulated systems show that this framework offers a useful approximation to the intractable aspects of Bayesian RL, producing structured exploration strategies that differ from standard RL approaches. We provide simple examples for the use of this framework in (approximate) Gaussian process regression and feedforward neural networks for the control of exploration.",
            "keywords": [
                "reinforcement learning",
                "control",
                "Gaussian processes",
                "filtering"
            ],
            "author": [
                "Edgar D. Klenske",
                "Philipp Hennig"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-162/15-162.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiple-Instance Learning from Distributions",
            "abstract": "We propose a new theoretical framework for analyzing the multiple-instance learning (MIL) setting. In MIL, training examples are provided to a learning algorithm in the form of labeled sets, or \"bags,\" of instances. Applications of MIL include 3-D quantitative structure--activity relationship prediction for drug discovery and content-based image retrieval for web search. The goal of an algorithm is to learn a function that correctly labels new bags or a function that correctly labels new instances. We propose that bags should be treated as latent distributions from which samples are observed. We show that it is possible to learn accurate instance- and bag-labeling functions in this setting as well as functions that correctly rank bags or instances under weak assumptions. Additionally, our theoretical results suggest that it is possible to learn to rank efficiently using traditional, well-studied \"supervised\" learning approaches. We perform an extensive empirical evaluation that supports the theoretical predictions entailed by the new framework. The proposed theoretical framework leads to a better understanding of the relationship between the MI and standard supervised learning settings, and it provides new methods for learning from MI data that are more accurate, more efficient, and have better understood theoretical properties than existing MI-specific algorithms.",
            "keywords": [
                "multiple-instance learning",
                "learning theory",
                "ranking"
            ],
            "author": [
                "Gary Doran",
                "Soumya Ray"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-171/15-171.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Online Convex Optimization Approach to Blackwell's Approachability",
            "abstract": "The problem of approachability in repeated games with vector payoffs was introduced by Blackwell in the 1950s, along with geometric conditions and corresponding approachability strategies that rely on computing a sequence of direction vectors in the payoff space. For convex target sets, these vectors are obtained as projections from the current average payoff vector to the set. A recent paper by Abernethy, Batlett and Hazan (2011) proposed a class of approachability algorithms that rely on Online Linear Programming for obtaining alternative sequences of direction vectors. This is first implemented for target sets that are convex cones, and then generalized to any convex set by embedding it in a higher-dimensional convex cone. In this paper we present a more direct formulation that relies on general Online Convex Optimization (OCO) algorithms, along with basic properties of the support function of convex sets. This leads to a general class of approachability algorithms, depending on the choice of the OCO algorithm and the used norms. Blackwell's original algorithm and its convergence are recovered when Follow The Leader (or a regularized version thereof) is used for the OCO algorithm.",
            "keywords": [
                "approachability",
                "online convex optimization"
            ],
            "author": [
                "Nahum Shimkin"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-339/15-339.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Well-Conditioned and Sparse Estimation of Covariance and Inverse Covariance Matrices Using a Joint Penalty",
            "abstract": "We develop a method for estimating well-conditioned and sparse covariance and inverse covariance matrices from a sample of vectors drawn from a sub-Gaussian distribution in high dimensional setting. The proposed estimators are obtained by minimizing the quadratic loss function and joint penalty of  norm and variance of its eigenvalues. In contrast to some of the existing methods of covariance and inverse covariance matrix estimation, where often the interest is to estimate a sparse matrix, the proposed method is flexible in estimating both a sparse and well-conditioned covariance matrix simultaneously. The proposed estimators are optimal in the sense that they achieve the mini-max rate of estimation in operator norm for the underlying class of covariance and inverse covariance matrices. We give a very fast algorithm for computation of these covariance and inverse covariance matrices which is easily scalable to large scale data analysis problems. The simulation study for varying sample sizes and variables shows that the proposed estimators performs better than several other estimators for various choices of structured covariance and inverse covariance matrices. We also use our proposed estimator for tumor tissues classification using gene expression data and compare its performance with some other classification methods.",
            "keywords": [
                "Sparsity",
                "Eigenvalue Penalty"
            ],
            "author": [
                "Ashwini Maurya"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-345/15-345.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "String and Membrane Gaussian Processes",
            "abstract": "In this paper we introduce a novel framework for making exact nonparametric Bayesian inference on latent functions that is particularly suitable for Big Data tasks. Firstly, we introduce a class of stochastic processes we refer to as string Gaussian processes (string GPs which are not to be mistaken for Gaussian processes operating on text). We construct string GPs so that their finite- dimensional marginals exhibit suitable local conditional independence structures, which allow for scalable, distributed, and flexible nonparametric Bayesian inference, without resorting to approximations, and while ensuring some mild global regularity constraints. Furthermore, string GP priors naturally cope with heterogeneous input data, and the gradient of the learned latent function is readily available for explanatory analysis. Secondly, we provide some theoretical results relating our approach to the standard GP paradigm. In particular, we prove that some string GPs are Gaussian processes, which provides a complementary global perspective on our framework. Finally, we derive a scalable and distributed MCMC scheme for supervised learning tasks under string GP priors. The proposed MCMC scheme has computational time complexity  and memory requirement , where  is the data size and  the dimension of the input space. We illustrate the efficacy of the proposed approach on several synthetic and real-world data sets, including a data set with  millions input points and  attributes.",
            "keywords": [
                "String Gaussian processes",
                "scalable Bayesian nonparametrics",
                "Gaussian processes",
                "nonstationary kernels",
                "reversible-jump MCMC"
            ],
            "author": [
                "Yves-Laurent Kom Samo",
                "Stephen J. Roberts"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-382/15-382.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Extracting PICO Sentences from Clinical Trial Reports using Supervised Distant Supervision",
            "abstract": "Systematic reviews underpin Evidence Based Medicine (EBM) by addressing precise clinical questions via comprehensive synthesis of all relevant published evidence. Authors of systematic reviews typically define a Population/Problem, Intervention, Comparator, and Outcome (a PICO criteria) of interest, and then retrieve, appraise and synthesize results from all reports of clinical trials that meet these criteria. Identifying PICO elements in the full-texts of trial reports is thus a critical yet time-consuming step in the systematic review process. We seek to expedite evidence synthesis by developing machine learning models to automatically extract sentences from articles relevant to PICO elements. Collecting a large corpus of training data for this task would be prohibitively expensive. Therefore, we derive distant supervision (DS) with which to train models using previously conducted reviews. DS entails heuristically deriving 'soft' labels from an available structured resource. However, we have access only to unstructured, free-text summaries of PICO elements for corresponding articles; we must derive from these the desired sentence-level annotations. To this end, we propose a novel method -- supervised distant supervision (SDS) -- that uses a small amount of direct supervision to better exploit a large corpus of distantly labeled instances by learning to pseudo-annotate articles using the available DS. We show that this approach tends to outperform existing methods with respect to automated PICO extraction.",
            "keywords": [],
            "author": [
                "Byron C. Wallace",
                "Joël Kuiper",
                "Aakash Sharma",
                "Mingxi (Brian) Zhu",
                "Iain J. Marshall"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-404/15-404.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cross-Corpora Unsupervised Learning of Trajectories in Autism Spectrum Disorders",
            "abstract": "",
            "keywords": [
                "Disease progression model"
            ],
            "author": [
                "Huseyin Melih Elibol",
                "Vincent Nguyen",
                "Scott Linderman",
                "Matthew Johnson",
                "Amna Hashmi",
                "Finale Doshi-Velez"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-431/15-431.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Refined Error Bounds for Several Learning Algorithms",
            "abstract": "This article studies the achievable guarantees on the error rates of certain learning algorithms, with particular focus on refining logarithmic factors. Many of the results are based on a general technique for obtaining bounds on the error rates of sample-consistent classifiers with monotonic error regions, in the realizable case. We prove bounds of this type expressed in terms of either the VC dimension or the sample compression size. This general technique also enables us to derive several new bounds on the error rates of general sample-consistent learning algorithms, as well as refined bounds on the label complexity of the CAL active learning algorithm. Additionally, we establish a simple necessary and sufficient condition for the existence of a distribution-free bound on the error rates of all sample- consistent learning rules, converging at a rate inversely proportional to the sample size. We also study learning in the presence of classification noise, deriving a new excess error rate guarantee for general VC classes under Tsybakov's noise condition, and establishing a simple and general necessary and sufficient condition for the minimax excess risk under bounded noise to converge at a rate inversely proportional to the sample size.",
            "keywords": [
                "sample complexity",
                "PAC learning",
                "statistical learning theory",
                "active learning"
            ],
            "author": [
                "Steve Hanneke"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-655/15-655.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Synergy of Monotonic Rules",
            "abstract": "",
            "keywords": [
                "conditional probability",
                "synergy",
                "ensemble learning",
                "intelligent teacher",
                "priv-     ileged information",
                "knowledge transfer",
                "support vector machines"
            ],
            "author": [
                "Vladimir Vapnik",
                "Rauf Izmailov"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-137/16-137.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic Differentiation",
            "abstract": "",
            "keywords": [
                "Riemannian optimization",
                "non-convex optimization",
                "manifold optimization",
                "projec-     tion matrices",
                "symmetric matrices",
                "rotation matrices"
            ],
            "author": [
                "James Townsend",
                "Niklas Koep",
                "Sebastian Weichwald"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-177/16-177.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "CrossCat: A Fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data",
            "abstract": "There is a widespread need for statistical methods that can analyze high-dimensional datasets without imposing restrictive or opaque modeling assumptions. This paper describes a domain- general data analysis method called CrossCat. CrossCat infers multiple non-overlapping views of the data, each consisting of a subset of the variables, and uses a separate nonparametric mixture to model each view. CrossCat is based on approximately Bayesian inference in a hierarchical, nonparametric model for data tables. This model consists of a Dirichlet process mixture over the columns of a data table in which each mixture component is itself an independent Dirichlet process mixture over the rows; the inner mixture components are simple parametric models whose form depends on the types of data in the table. CrossCat combines strengths of mixture modeling and Bayesian network structure learning. Like mixture modeling, CrossCat can model a broad class of distributions by positing latent variables, and produces representations that can be efficiently conditioned and sampled from for prediction. Like Bayesian networks, CrossCat represents the dependencies and independencies between variables, and thus remains accurate when there are multiple statistical signals. Inference is done via a scalable Gibbs sampling scheme; this paper shows that it works well in practice. This paper also includes empirical results on heterogeneous tabular data of up to 10 million cells, such as hospital cost and quality measures, voting records, unemployment rates, gene expression measurements, and images of handwritten digits. CrossCat infers structure that is consistent with accepted findings and common-sense knowledge in multiple domains and yields predictive accuracy competitive with generative, discriminative, and model-free alternatives.",
            "keywords": [
                "Bayesian nonparametrics",
                "Dirichlet processes",
                "Markov chain Monte Carlo",
                "multivari-     ate analysis",
                "structure learning",
                "unsupervised learning"
            ],
            "author": [
                "Vikash Mansinghka",
                "Patrick Shafto",
                "Eric Jonas",
                "Cap Petschulat",
                "Max Gasner",
                "Joshua B. Tenenbaum"
            ],
            "ref": "http://jmlr.org/papers/volume17/11-392/11-392.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Policy Iteration with Nonparametric Function Spaces",
            "abstract": "We study two regularization-based approximate policy iteration algorithms, namely REG-LSPI and REG-BRM, to solve reinforcement learning and planning problems in discounted Markov Decision Processes with large state and finite action spaces. The core of these algorithms are the regularized extensions of the Least- Squares Temporal Difference (LSTD) learning and Bellman Residual Minimization (BRM), which are used in the algorithms' policy evaluation steps. Regularization provides a convenient way to control the complexity of the function space to which the estimated value function belongs and as a result enables us to work with rich nonparametric function spaces. We derive efficient implementations of our methods when the function space is a reproducing kernel Hilbert space. We analyze the statistical properties of REG-LSPI and provide an upper bound on the policy evaluation error and the performance loss of the policy returned by this method. Our bound shows the dependence of the loss on the number of samples, the capacity of the function space, and some intrinsic properties of the underlying Markov Decision Process. The dependence of the policy evaluation bound on the number of samples is minimax optimal. This is the first work that provides such a strong guarantee for a nonparametric approximate policy iteration algorithm. (This work is an extension of the NIPS 2008 conference paper by Farahmand et al. (2009b).)",
            "keywords": [
                "reinforcement learning",
                "approximate policy iteration",
                "regularization",
                "non-     parametric method"
            ],
            "author": [
                "Amir-massoud Farahm",
                "",
                "Mohammad Ghavamzadeh",
                "Csaba Szepesvári",
                "Shie Mannor"
            ],
            "ref": "http://jmlr.org/papers/volume17/13-016/13-016.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse PCA via Covariance Thresholding",
            "abstract": "",
            "keywords": [],
            "author": [
                "Yash Deshp",
                "e",
                "Andrea Montanari"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-160/15-160.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large Scale Visual Recognition through Adaptation using Joint Representation and Multiple Instance Learning",
            "abstract": "A major barrier towards scaling visual recognition systems is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) trained used 1.2M+ labeled images have emerged as clear winners on object classification benchmarks. Unfortunately, only a small fraction of those labels are available with bounding box localization for training the detection task and even fewer pixel level annotations are available for semantic segmentation. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect scene-centric images with precisely localized labels. We develop methods for learning large scale recognition models which exploit joint training over both weak (image-level) and strong (bounding box) labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. We provide a novel formulation of a joint multiple instance learning method that includes examples from object-centric data with image-level labels when available, and also performs domain transfer learning to improve the underlying detector representation. We then show how to use our large scale detectors to produce pixel level annotations. Using our method, we produce a 7.6K category detector and release code and models at lsda.berkeley vision.org.",
            "keywords": [
                "Computer Vision",
                "Deep Learning",
                "Transfer Learning"
            ],
            "author": [
                "Judy Hoffman",
                "Deepak Pathak",
                "Eric Tzeng",
                "Jonathan Long",
                "Sergio Guadarrama",
                "Trevor Darrell",
                "Kate Saenko"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-223/15-223.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Covariance-based Clustering in Multivariate and Functional Data Analysis",
            "abstract": "In this paper we propose a new algorithm to perform clustering of multivariate and functional data. We study the case of two populations different in their covariances, rather than in their means. The algorithm relies on a proper quantification of distance between the estimated covariance operators of the populations, and subdivides data in two groups maximising the distance between their induced covariances. The naive implementation of such an algorithm is computationally forbidding, so we propose a heuristic formulation with a much lighter complexity and we study its convergence properties, along with its computational cost. We also propose to use an enhanced estimator for the estimation of discrete covariances of functional data, namely a linear shrinkage estimator, in order to improve the precision of the clustering. We establish the effectiveness of our algorithm through applications to both synthetic data and a real data set coming from a biomedical context, showing also how the use of shrinkage estimation may lead to substantially better results.",
            "keywords": [
                "Clustering",
                "covariance operator",
                "operator distance",
                "shrinkage estimation"
            ],
            "author": [
                "Francesca Ieva",
                "Anna Maria Paganoni",
                "Nicholas Tarabelloni"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-568/15-568.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MOCCA: Mirrored Convex/Concave Optimization for Nonconvex Composite Functions",
            "abstract": "Many optimization problems arising in high-dimensional statistics decompose naturally into a sum of several terms, where the individual terms are relatively simple but the composite objective function can only be optimized with iterative algorithms. In this paper, we are interested in optimization problems of the form , where  is a fixed linear transformation, while  and  are functions that may be nonconvex and/or nondifferentiable. In particular, if either of the terms are nonconvex, existing alternating minimization techniques may fail to converge; other types of existing approaches may instead be unable to handle nondifferentiability. We propose the MOCCA (mirrored convex/concave) algorithm, a primal/dual optimization approach that takes a local convex approximation to each term at every iteration. Inspired by optimization problems arising in computed tomography (CT) imaging, this algorithm can handle a range of nonconvex composite optimization problems, and offers theoretical guarantees for convergence when the overall problem is approximately convex (that is, any concavity in one term is balanced out by convexity in the other term). Empirical results show fast convergence for several structured signal recovery problems.",
            "keywords": [
                "MOCCA",
                "ADMM",
                "nonconvex",
                "penalized likelihood",
                "total variation"
            ],
            "author": [
                "Rina Foygel Barber",
                "Emil Y. Sidky"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-583/15-583.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "True Online Temporal-Difference Learning",
            "abstract": "The temporal-difference methods TD() and Sarsa() form a core part of modern reinforcement learning. Their appeal comes from their good performance, low computational cost, and their simple interpretation, given by their forward view. Recently, new versions of these methods were introduced, called true online TD() and true online Sarsa(), respectively (van Seijen & Sutton, 2014). Algorithmically, these true online methods only make two small changes to the update rules of the regular methods, and the extra computational cost is negligible in most cases. However, they follow the ideas underlying the forward view much more closely. In particular, they maintain an exact equivalence with the forward view at all times, whereas the traditional versions only approximate it for small step-sizes. We hypothesize that these true online methods not only have better theoretical properties, but also dominate the regular methods empirically. In this article, we put this hypothesis to the test by performing an extensive empirical comparison. Specifically, we compare the performance of true online TD()/Sarsa() with regular TD()/Sarsa() on random MRPs, a real-world myoelectric prosthetic arm, and a domain from the Arcade Learning Environment. We use linear function approximation with tabular, binary, and non-binary features. Our results suggest that the true online methods indeed dominate the regular methods. Across all domains/representations the learning speed of the true online methods are often better, but never worse than that of the regular methods. An additional advantage is that no choice between traces has to be made for the true online methods. Besides the empirical results, we provide an in-dept analysis of the theory behind true online temporal-difference learning. In addition, we show that new true online temporal- difference methods can be derived by making changes to the online forward view and then rewriting the update equations.",
            "keywords": [
                "temporal-difference learning",
                "eligibility traces",
                "forward-view equivalencec 2016 Harm van Seijen"
            ],
            "author": [
                "Harm van Seijen",
                "A. Rupam Mahmood",
                "Patrick M. Pilarski",
                "Marlos C. Machado",
                "Richard S. Sutton"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-599/15-599.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Penalized Maximum Likelihood Estimation of Multi-layered Gaussian Graphical Models",
            "abstract": "Analyzing multi-layered graphical models provides insight into understanding the conditional relationships among nodes within layers after adjusting for and quantifying the effects of nodes from other layers. We obtain the penalized maximum likelihood estimator for Gaussian multi-layered graphical models, based on a computational approach involving screening of variables, iterative estimation of the directed edges between layers and undirected edges within layers and a final refitting and stability selection step that provides improved performance in finite sample settings. We establish the consistency of the estimator in a high-dimensional setting. To obtain this result, we develop a strategy that leverages the biconvexity of the likelihood function to ensure convergence of the developed iterative algorithm to a stationary point, as well as careful uniform error control of the estimates over iterations. The performance of the maximum likelihood estimator is illustrated on synthetic data.",
            "keywords": [
                "graphical models",
                "penalized likelihood",
                "block coordinate descent",
                "conver-     gence"
            ],
            "author": [
                "Jiahe Lin",
                "Sumanta Basu",
                "Moulinath Banerjee",
                "George Michailidis"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-004/16-004.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Network Community Detection with Continuous Optimization of Conductance and Weighted Kernel K-Means",
            "abstract": "Local network community detection is the task of finding a single community of nodes concentrated around few given seed nodes in a localized way. Conductance is a popular objective function used in many algorithms for local community detection. This paper studies a continuous relaxation of conductance. We show that continuous optimization of this objective still leads to discrete communities. We investigate the relation of conductance with weighted kernel k-means for a single community, which leads to the introduction of a new objective function, -conductance. Conductance is obtained by setting  to . Two algorithms, EMc and PGDc, are proposed to locally optimize -conductance and automatically tune the parameter . They are based on expectation maximization and projected gradient descent, respectively. We prove locality and give performance guarantees for EMc and PGDc for a class of dense and well separated communities centered around the seeds. Experiments are conducted on networks with ground-truth communities, comparing to state-of-the-art graph diffusion algorithms for conductance optimization. On large graphs, results indicate that EMc and PGDc stay localized and produce communities most similar to the ground, while graph diffusion algorithms generate large communities of lower quality. (Source code of the algorithms used in the paper is available   online.)",
            "keywords": [
                "community detection",
                "conductance"
            ],
            "author": [
                "Twan van Laarhoven",
                "Elena Marchiori"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-043/16-043.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Megaman: Scalable Manifold Learning in Python",
            "abstract": "Manifold Learning (ML) is a class of algorithms seeking a low-dimensional non-linear representation of high-dimensional data. Thus, ML algorithms are most applicable to high- dimensional data and require large sample sizes to accurately estimate the manifold. Despite this, most existing manifold learning implementations are not particularly scalable. Here we present a Python package that implements a variety of manifold learning algorithms in a modular and scalable fashion, using fast approximate neighbors searches and fast sparse eigendecompositions. The package incorporates theoretical advances in manifold learning, such as the unbiased Laplacian estimator introduced by Coifman and Lafon (2006) and the estimation of the embedding distortion by the Riemannian metric method introduced by Perrault-Joncas and Meila (2013). In benchmarks, even on a single-core desktop computer, our code embeds millions of data points in minutes, and takes just 200 minutes to embed the main sample of galaxy spectra from the Sloan Digital Sky Survey--- consisting of 0.6 million samples in 3750-dimensions---a task which has not previously been possible.",
            "keywords": [
                "manifold learning",
                "dimension reduction",
                "Riemannian metric",
                "graph embed-     ding",
                "scalable methods"
            ],
            "author": [
                "James McQueen",
                "Marina Meilă",
                "Jacob VanderPlas",
                "Zhongyue Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-109/16-109.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Estimation and Model Combination in A Bandit Problem with Covariates",
            "abstract": "Multi-armed bandit problem is an important optimization game that requires an exploration-exploitation tradeoff to achieve optimal total reward. Motivated from industrial applications such as online advertising and clinical research, we consider a setting where the rewards of bandit machines are associated with covariates, and the accurate estimation of the corresponding mean reward functions plays an important role in the performance of allocation rules. Under a flexible problem setup, we establish asymptotic strong consistency and perform a finite- time regret analysis for a sequential randomized allocation strategy based on kernel estimation. In addition, since many nonparametric and parametric methods in supervised learning may be applied to estimating the mean reward functions but guidance on how to choose among them is generally unavailable, we propose a model combining allocation strategy for adaptive performance. Simulations and a real data evaluation are conducted to illustrate the performance of the proposed allocation strategy.",
            "keywords": [
                "contextual bandit problem",
                "exploration-exploitation tradeoff",
                "nonparametric     regression",
                "regret bound"
            ],
            "author": [
                "Wei Qian",
                "Yuhong Yang"
            ],
            "ref": "http://jmlr.org/papers/volume17/13-210/13-210.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A General Framework for Consistency of Principal Component Analysis",
            "abstract": "A general asymptotic framework is developed for studying consistency properties of principal component analysis (PCA). Our framework includes several previously studied domains of asymptotics as special cases and allows one to investigate interesting connections and transitions among the various domains. More importantly, it enables us to investigate asymptotic scenarios that have not been considered before, and gain new insights into the consistency, subspace consistency and strong inconsistency regions of PCA and the boundaries among them. We also establish the corresponding convergence rate within each region. Under general spike covariance models, the dimension (or number of variables) discourages the consistency of PCA, while the sample size and spike information (the relative size of the population eigenvalues) encourage PCA consistency. Our framework nicely illustrates the relationship among these three types of information in terms of dimension, sample size and spike size, and rigorously characterizes how their relationships affect PCA consistency.",
            "keywords": [
                "High dimension low sample size",
                "PCA",
                "Random matrix"
            ],
            "author": [
                "Dan Shen",
                "Haipeng Shen",
                "J. S. Marron"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-229/14-229.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Conditional Independencies under the Algorithmic Independence of Conditionals",
            "abstract": "In this paper we analyze the relationship between faithfulness and the more recent condition of algorithmic Independence of Conditionals (IC) with respect to the Conditional Independencies (CIs) they allow. Both conditions have been extensively used for causal inference by refuting factorizations for which the condition does not hold. Violation of faithfulness happens when there are CIs that do not follow from the Markov condition. For those CIs, non-trivial constraints among some parameters of the Conditional Probability Distributions (CPDs) must hold. When such a constraint is defined over parameters of different CPDs, we prove that IC is also violated unless the parameters have a simple description. To understand which non-Markovian CIs are permitted we define a new condition closely related to IC: the Independence from Product Constraints (IPC). The condition reflects that CIs might be the result of specific parameterizations of individual CPDs but not from constraints on parameters of different CPDs. In that sense it is more restrictive than IC: parameters may have a simple description. On the other hand, IC also excludes other forms of algorithmic dependencies between CPDs. Finally, we prove that on top of the CIs permitted by the Markov condition (faithfulness), IPC allows non-minimality, deterministic relations and what we called proportional CPDs. These are the only cases in which a CI follows from a specific parameterization of a single CPD.",
            "keywords": [
                "faithfulness",
                "causality",
                "independence of conditionals"
            ],
            "author": [
                "Jan Lemeire"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-450/14-450.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Theory for Distribution Regression",
            "abstract": "We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the  one-stage sampled minimax optimal rate (Caponnetto and De Vito, 2007; Steinwart et al., 2009). This result answers a -year-old open question, establishing the consistency of the classical set kernel (Haussler, 1999; GÃ¤rtner et al., 2002) in regression. We also cover consistency for more recent kernels on distributions, including those due to Christmann and Steinwart (2010).",
            "keywords": [],
            "author": [
                "Zoltán Szabó",
                "Bharath K. Sriperumbudur",
                "Barnabás Póczos",
                "Arthur Gretton"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-510/14-510.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights",
            "abstract": "We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.",
            "keywords": [],
            "author": [
                "Weijie Su",
                "Stephen Boyd",
                "Emmanuel J. Candès"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-084/15-084.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Importance Weighting Without Importance Weights: An Efficient Algorithm for Combinatorial Semi-Bandits",
            "abstract": "We propose a sample-efficient alternative for importance weighting for situations where one only has sample access to the probability distribution that generates the observations. Our new method, called Geometric Resampling (GR), is described and analyzed in the context of online combinatorial optimization under semi-bandit feedback, where a learner sequentially selects its actions from a combinatorial decision set so as to minimize its cumulative loss. In particular, we show that the well-known Follow-the-Perturbed-Leader (FPL) prediction method coupled with Geometric Resampling yields the first computationally efficient reduction from offline to online optimization in this setting. We provide a thorough theoretical analysis for the resulting algorithm, showing that its performance is on par with previous, inefficient solutions. Our main contribution is showing that, despite the relatively large variance induced by the GR procedure, our performance guarantees hold with high probability rather than only in expectation. As a side result, we also improve the best known regret bounds for FPL in online combinatorial optimization with full feedback, closing the perceived performance gap between FPL and exponential weights in this setting. (A preliminary version of this paper was published as Neu and BartÃ³k (2013). Parts of this work were completed while Gergely Neu was with the SequeL team at INRIA Lille -- Nord Europe, France and GÃ¡bor BartÃ³k was with the Department of Computer Science at ETH ZÃ¼rich.)",
            "keywords": [
                "online learning",
                "combinatorial optimization",
                "bandit problems",
                "semi-bandit feedback",
                "follow the perturbed leader"
            ],
            "author": [
                "Gergely Neu",
                "Gábor Bartók"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-091/15-091.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "New Perspectives on k-Support and Cluster Norms",
            "abstract": "We study a regularizer which is defined as a parameterized infimum of quadratics, and which we call the box-norm. We show that the -support norm, a regularizer proposed by Argyriou et al. (2012) for sparse vector prediction problems, belongs to this family, and the box-norm can be generated as a perturbation of the former. We derive an improved algorithm to compute the proximity operator of the squared box-norm, and we provide a method to compute the norm. We extend the norms to matrices, introducing the spectral -support norm and spectral box-norm. We note that the spectral box-norm is essentially equivalent to the cluster norm, a multitask learning regularizer introduced by Jacob et al. (2009a), and which in turn can be interpreted as a perturbation of the spectral -support norm. Centering the norm is important for multitask learning and we also provide a method to use centered versions of the norms as regularizers. Numerical experiments indicate that the spectral -support and box-norms and their centered variants provide state of the art performance in matrix completion and multitask learning problems respectively.",
            "keywords": [
                "Convex optimization",
                "matrix completion",
                "multitask learning",
                "spectral regu-     larization"
            ],
            "author": [
                "Andrew M. McDonald",
                "Massimiliano Pontil",
                "Dimitris Stamos"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-151/15-151.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimum Density Hyperplanes",
            "abstract": "Associating distinct groups of objects (clusters) with contiguous regions of high probability density (high-density clusters), is central to many statistical and machine learning approaches to the classification of unlabelled data. We propose a novel hyperplane classifier for clustering and semi-supervised classification which is motivated by this objective. The proposed minimum density hyperplane minimises the integral of the empirical probability density function along it, thereby avoiding intersection with high density clusters. We show that the minimum density and the maximum margin hyperplanes are asymptotically equivalent, thus linking this approach to maximum margin clustering and semi-supervised support vector classifiers. We propose a projection pursuit formulation of the associated optimisation problem which allows us to find minimum density hyperplanes efficiently in practice, and evaluate its performance on a range of benchmark data sets. The proposed approach is found to be very competitive with state of the art methods for clustering and semi-supervised classification.",
            "keywords": [
                "low-density separation",
                "high-density clusters",
                "clustering",
                "semi-supervised     classification"
            ],
            "author": [
                "Nicos G. Pavlidis",
                "David P. Hofmeyr",
                "Sotiris K. Tasoulis"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-307/15-307.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Universal Approximation Results for the Temporal Restricted Boltzmann Machine and the Recurrent Temporal Restricted Boltzmann Machine",
            "abstract": "The Restricted Boltzmann Machine (RBM) has proved to be a powerful tool in machine learning, both on its own and as the building block for Deep Belief Networks (multi-layer generative graphical models). The RBM and Deep Belief Network have been shown to be universal approximators for probability distributions on binary vectors. In this paper we prove several similar universal approximation results for two variations of the Restricted Boltzmann Machine with time dependence, the Temporal Restricted Boltzmann Machine (TRBM) and the Recurrent Temporal Restricted Boltzmann Machine (RTRBM). We show that the TRBM is a universal approximator for Markov chains and generalize the theorem to sequences with longer time dependence. We then prove that the RTRBM is a universal approximator for stochastic processes with finite time dependence. We conclude with a discussion on efficiency and how the constructions developed could explain some previous experimental results.",
            "keywords": [
                "TRBM",
                "RTRBM",
                "machine learning"
            ],
            "author": [
                "Simon Odense",
                "Roderick Edwards"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-478/15-478.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exploration of the (Non-)Asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics",
            "abstract": "Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data sets is computationally infeasible. The recently proposed stochastic gradient Langevin dynamics (SGLD) method circumvents this problem in three ways: it generates proposed moves using only a subset of the data, it skips the Metropolis- Hastings accept-reject step, and it uses sequences of decreasing step sizes. In Teh et al. (2014), we provided the mathematical foundations for the decreasing step size SGLD, including consistency and a central limit theorem. However, in practice the SGLD is run for a relatively small number of iterations, and its step size is not decreased to zero. The present article investigates the behaviour of the SGLD with fixed step size. In particular we characterise the asymptotic bias explicitly, along with its dependence on the step size and the variance of the stochastic gradient. On that basis a modified SGLD which removes the asymptotic bias due to the variance of the stochastic gradients up to first order in the step size is derived. Moreover, we are able to obtain bounds on the finite-time bias, variance and mean squared error (MSE). The theory is illustrated with a Gaussian toy model for which the bias and the MSE for the estimation of moments can be obtained explicitly. For this toy model we study the gain of the SGLD over the standard Euler method in the limit of large data sets.",
            "keywords": [
                "Markov Chain Monte Carlo",
                "Langevin dynamics",
                "big data"
            ],
            "author": [
                "Sebastian J. Vollmer",
                "Konstantinos C. Zygalakis",
                "Yee Whye Teh"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-494/15-494.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A General Framework for Constrained Bayesian Optimization using Information-based Search",
            "abstract": "We present an information-theoretic framework for solving global black-box optimization problems that also have black-box constraints. Of particular interest to us is to efficiently solve problems with decoupled constraints, in which subsets of the objective and constraint functions may be evaluated independently. For example, when the objective is evaluated on a CPU and the constraints are evaluated independently on a GPU. These problems require an acquisition function that can be separated into the contributions of the individual function evaluations. We develop one such acquisition function and call it Predictive Entropy Search with Constraints (PESC). PESC is an approximation to the expected information gain criterion and it compares favorably to alternative approaches based on improvement in several synthetic and real- world problems. In addition to this, we consider problems with a mix of functions that are fast and slow to evaluate. These problems require balancing the amount of time spent in the meta- computation of PESC and in the actual evaluation of the target objective. We take a bounded rationality approach and develop a partial update for PESC which trades off accuracy against speed. We then propose a method for adaptively switching between the partial and full updates for PESC. This allows us to interpolate between versions of PESC that are efficient in terms of function evaluations and those that are efficient in terms of wall-clock time. Overall, we demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization.",
            "keywords": [
                "Bayesian optimization",
                "constraints"
            ],
            "author": [
                "Jos\\'{e} Miguel Hern\\'{a}ndez-Lobato",
                "Michael A. Gelbart",
                "Ryan P. Adams",
                "Matthew W. Hoffman",
                "Zoubin Ghahramani"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-616/15-616.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Estimation and Completion of Matrices with Biclustering Structures",
            "abstract": "Biclustering structures in data matrices were first formalized in a seminal paper by John Hartigan (Hartigan, 1972) where one seeks to cluster cases and variables simultaneously. Such structures are also prevalent in block modeling of networks. In this paper, we develop a theory for the estimation and completion of matrices with biclustering structures, where the data is a partially observed and noise contaminated matrix with a certain underlying biclustering structure. In particular, we show that a constrained least squares estimator achieves minimax rate-optimal performance in several of the most important scenarios. To this end, we derive unified high probability upper bounds for all sub-Gaussian data and also provide matching minimax lower bounds in both Gaussian and binary cases. Due to the close connection of graphon to stochastic block models, an immediate consequence of our general results is a minimax rate- optimal estimator for sparse graphons.",
            "keywords": [
                "Biclustering",
                "graphon",
                "matrix completion",
                "missing data",
                "stochastic block     models"
            ],
            "author": [
                "Chao Gao",
                "Yu Lu",
                "Zongming Ma",
                "Harrison H. Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-617/15-617.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Teaching Dimension of Linear Learners",
            "abstract": "Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization. This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression. We also exhibit optimal training sets that match these teaching dimensions. Our approach generalizes to other linear learners.",
            "keywords": [
                "Optimization based learner",
                "Karush-Kuhn-Tucker conditions"
            ],
            "author": [
                "Ji Liu",
                "Xiaojin Zhu"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-630/15-630.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Augmentable Gamma Belief Networks",
            "abstract": "To infer multilayer deep representations of high-dimensional discrete and nonnegative real vectors, we propose an augmentable gamma belief network (GBN) that factorizes each of its hidden layers into the product of a sparse connection weight matrix and the nonnegative real hidden units of the next layer. The GBN's hidden layers are jointly trained with an upward-downward Gibbs sampler that solves each layer with the same subroutine. The gamma-negative binomial process combined with a layer-wise training strategy allows inferring the width of each layer given a fixed budget on the width of the first layer. Example results illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the GBN can add more layers to improve its performance in both unsupervisedly extracting features and predicting heldout data. For exploratory data analysis, we extract trees and subnetworks from the learned deep network to visualize how the very specific factors discovered at the first hidden layer and the increasingly more general factors discovered at deeper hidden layers are related to each other, and we generate synthetic data by propagating random variables through the deep network from the top hidden layer back to the bottom data layer.",
            "keywords": [
                "Bayesian nonparametrics",
                "deep learning",
                "multilayer representation",
                "Poisson     factor analysis",
                "topic modeling"
            ],
            "author": [
                "Mingyuan Zhou",
                "Yulai Cong",
                "Bo Chen"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-633/15-633.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Estimation of Derivatives in Nonparametric Regression",
            "abstract": "We propose a simple framework for estimating derivatives without fitting the regression function in nonparametric regression. Unlike most existing methods that use the symmetric difference quotients, our method is constructed as a linear combination of observations. It is hence very flexible and applicable to both interior and boundary points, including most existing methods as special cases of ours. Within this framework, we define the variance-minimizing estimators for any order derivative of the regression function with a fixed bias-reduction level. For the equidistant design, we derive the asymptotic variance and bias of these estimators. We also show that our new method will, for the first time, achieve the asymptotically optimal convergence rate for difference-based estimators. Finally, we provide an effective criterion for selection of tuning parameters and demonstrate the usefulness of the proposed method through extensive simulation studies of the first- and second-order derivative estimators.",
            "keywords": [
                "Linear combination",
                "Nonparametric derivative estimation",
                "Nonparametric     regression",
                "Optimal sequence"
            ],
            "author": [
                "Wenlin Dai",
                "Tiejun Tong",
                "Marc G. Genton"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-640/15-640.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing",
            "abstract": "Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural no-free-lunch requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive- compatible mechanisms (that may or may not satisfy no-free- lunch), our mechanism makes the smallest possible payment to spammers. We further extend our results to a more general setting in which workers are required to provide a quantized confidence for each question. Interestingly, this unique mechanism takes a multiplicative form. The simplicity of the mechanism is an added benefit. In preliminary experiments involving over 900 worker-task pairs, we observe a significant drop in the error rates under this unique mechanism for the same or lower monetary expenditure.",
            "keywords": [
                "high-quality labels",
                "supervised learning",
                "crowdsourcing",
                "mechanism design"
            ],
            "author": [
                "Nihar B. Shah",
                "Dengyong Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-642/15-642.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Joint Structural Estimation of Multiple Graphical Models",
            "abstract": "Gaussian graphical models capture dependence relationships between random variables through the pattern of nonzero elements in the corresponding inverse covariance matrices. To date, there has been a large body of literature on both computational methods and analytical results on the estimation of a single graphical model. However, in many application domains, one has to estimate several related graphical models, a problem that has also received attention in the literature. The available approaches usually assume that all graphical models are globally related. On the other hand, in many settings different relationships between subsets of the node sets exist between different graphical models. We develop methodology that jointly estimates multiple Gaussian graphical models, assuming that there exists prior information on how they are structurally related. For many applications, such information is available from external data sources. The proposed method consists of first applying neighborhood selection with a group lasso penalty to obtain edge sets of the graphs, and a maximum likelihood refit for estimating the nonzero entries in the inverse covariance matrices. We establish consistency of the proposed method for sparse high-dimensional Gaussian graphical models and examine its performance using simulation experiments. Applications to a climate data set and a breast cancer data set are also discussed.",
            "keywords": [
                "Gaussian graphical model",
                "structured sparsity",
                "group lasso penalty",
                "consistency"
            ],
            "author": [
                "Jing Ma",
                "George Michailidis"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-656/15-656.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Support Vector Hazards Machine: A Counting Process Framework for Learning Risk Scores for Censored Outcomes",
            "abstract": "Learning risk scores to predict dichotomous or continuous outcomes using machine learning approaches has been studied extensively. However, how to learn risk scores for time-to-event outcomes subject to right censoring has received little attention until recently. Existing approaches rely on inverse probability weighting or rank-based regression, which may be inefficient. In this paper, we develop a new support vector hazards machine (SVHM) approach to predict censored outcomes. Our method is based on predicting the counting process associated with the time-to-event outcomes among subjects at risk via a series of support vector machines. Introducing counting processes to represent time-to-event data leads to a connection between support vector machines in supervised learning and hazards regression in standard survival analysis. To account for different at risk populations at observed event times, a time-varying offset is used in estimating risk scores. The resulting optimization is a convex quadratic programming problem that can easily incorporate non-linearity using kernel trick. We demonstrate an interesting link from the profiled empirical risk function of SVHM to the Cox partial likelihood. We then formally show that SVHM is optimal in discriminating covariate-specific hazard function from population average hazard function, and establish the consistency and learning rate of the predicted risk using the estimated risk scores. Simulation studies show improved prediction accuracy of the event times using SVHM compared to existing machine learning methods and standard conventional approaches. Finally, we analyze two real world biomedical study data where we use clinical markers and neuroimaging biomarkers to predict age-at- onset of a disease, and demonstrate superiority of SVHM in distinguishing high risk versus low risk subjects.",
            "keywords": [
                "support vector machine",
                "survival analysis",
                "risk bound",
                "risk prediction",
                "neu-     roimaging biomarkers",
                "early disease detectionc 2016 Yuanjia Wang",
                "Tianle Chen"
            ],
            "author": [
                "Yuanjia Wang",
                "Tianle Chen",
                "Donglin Zeng"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-007/16-007.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stable Graphical Models",
            "abstract": "Stable random variables are motivated by the central limit theorem for densities with (potentially) unbounded variance and can be thought of as natural generalizations of the Gaussian distribution to skewed and heavy-tailed phenomenon. In this paper, we introduce -stable graphical (-SG) models, a class of multivariate stable densities that can also be represented as Bayesian networks whose edges encode linear dependencies between random variables. One major hurdle to the extensive use of stable distributions is the lack of a closed- form analytical expression for their densities. This makes penalized maximum-likelihood based learning computationally demanding. We establish theoretically that the Bayesian information criterion (BIC) can asymptotically be reduced to the computationally more tractable minimum dispersion criterion (MDC) and develop StabLe, a structure learning algorithm based on MDC. We use simulated datasets for five benchmark network topologies to empirically demonstrate how  StabLe improves upon ordinary least squares (OLS) regression. We also apply StabLe to microarray gene expression data for lymphoblastoid cells from 727 individuals belonging to eight global population groups. We establish that StabLe improves test set performance relative to OLS via ten-fold cross-validation. Finally, we develop SGEX, a method for quantifying differential expression of genes between different population groups.",
            "keywords": [
                "Bayesian networks",
                "stable distributions",
                "linear regression",
                "structure learning",
                "gene expression"
            ],
            "author": [
                "Navodit Misra",
                "Ercan E. Kuruoglu"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-150/14-150.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bounding the Search Space for Global Optimization of Neural Networks Learning Error: An Interval Analysis Approach",
            "abstract": "Training a multilayer perceptron (MLP) with algorithms employing global search strategies has been an important research direction in the field of neural networks. Despite a number of significant results, an important matter concerning the bounds of the search region---typically defined as a box---where a global optimization method has to search for a potential global minimizer seems to be unresolved. The approach presented in this paper builds on interval analysis and attempts to define guaranteed bounds in the search space prior to applying a global search algorithm for training an MLP. These bounds depend on the machine precision and the term guaranteed denotes that the region defined surely encloses weight sets that are global minimizers of the neural network's error function. Although the solution set to the bounding problem for an MLP is in general non-convex, the paper presents the theoretical results that help deriving a box which is a convex set. This box is an outer approximation of the algebraic solutions to the interval equations resulting from the function implemented by the network nodes. An experimental study using well known benchmarks is presented in accordance with the theoretical results.",
            "keywords": [
                "neural network training",
                "bound constrained global optimization",
                "interval     analysis",
                "interval linear equations"
            ],
            "author": [
                "Stavros P. Adam",
                "George D. Magoulas",
                "Dimitrios A. Karras",
                "Michael N. Vrahatis"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-350/14-350.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "mlr: Machine Learning in R",
            "abstract": "The mlr package provides a generic, object- oriented, and extensible framework for classification, regression, survival analysis and clustering for the R language. It provides a unified interface to more than 160 basic learners and includes meta-algorithms and model selection techniques to improve and extend the functionality of basic learners with, e.g., hyperparameter tuning, feature selection, and ensemble construction. Parallel high-performance computing is natively supported. The package targets practitioners who want to quickly apply machine learning algorithms, as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment.",
            "keywords": [
                "machine learning",
                "hyperparameter tuning",
                "model selection",
                "feature selection",
                "benchmarking",
                "R",
                "visualization"
            ],
            "author": [
                "Bernd Bischl",
                "Michel Lang",
                "Lars Kotthoff",
                "Julia Schiffner",
                "Jakob Richter",
                "Erich Studerus",
                "Giuseppe Casalicchio",
                "Zachary M. Jones"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-066/15-066.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Feature-Level Domain Adaptation",
            "abstract": "Domain adaptation is the supervised learning setting in which the training and test data are sampled from different distributions: training data is sampled from a source domain, whilst test data is sampled from a target domain. This paper proposes and studies an approach, called feature-level domain adaptation (FLDA), that models the dependence between the two domains by means of a feature-level transfer model that is trained to describe the transfer from source to target domain. Subsequently, we train a domain-adapted classifier by minimizing the expected loss under the resulting transfer model. For linear classifiers and a large family of loss functions and transfer models, this expected loss can be computed or approximated analytically, and minimized efficiently. Our empirical evaluation of FLDA focuses on problems comprising binary and count data in which the transfer can be naturally modeled via a dropout distribution, which allows the classifier to adapt to differences in the marginal probability of features in the source and the target domain. Our experiments on several real- world problems show that FLDA performs on par with state- of- the-art domain-adaptation techniques.",
            "keywords": [
                "Domain adaptation",
                "transfer learning",
                "covariate shift"
            ],
            "author": [
                "Wouter M. Kouw",
                "Laurens J.P. van der Maaten",
                "Jesse H. Krijthe",
                "Marco Loog"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-206/15-206.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online PCA with Optimal Regret",
            "abstract": "",
            "keywords": [
                "online learning",
                "regret bounds",
                "expert setting",
                "k-sets",
                "PCA",
                "Gradient Descent"
            ],
            "author": [
                "Jiazhong Nie",
                "Wojciech Kotlowski",
                "Manfred K. Warmuth"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-320/15-320.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Computation of Gaussian Process Regression for Large Spatial Data Sets by Patching Local Gaussian Processes",
            "abstract": "This paper develops an efficient computational method for solving a Gaussian process (GP) regression for large spatial data sets using a collection of suitably defined local GP regressions. The conventional local GP approach first partitions a domain into multiple non-overlapping local regions, and then fits an independent GP regression for each local region using the training data belonging to the region. Two key issues with the local GP are (1) the prediction around the boundary of a local region is not as accurate as the prediction at interior of the local region, and (2) two local GP regressions for two neighboring local regions produce different predictions at the boundary of the two regions, creating undesirable discontinuity in the prediction. We address these issues by constraining the predictions of local GP regressions sharing a common boundary to satisfy the same boundary constraints, which in turn are estimated by the data. The boundary constrained local GP regressions are solved by a finite element method. Our approach shows competitive performance when compared with several state- of-the-art methods using two synthetic data sets and three real data sets.",
            "keywords": [
                "constrained Gaussian process regression",
                "kriging",
                "local regression",
                "boundary     value problem",
                "spatial prediction"
            ],
            "author": [
                "Chiwoo Park",
                "Jianhua Z. Huang"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-327/15-327.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "bandicoot: a Python Toolbox for Mobile Phone Metadata",
            "abstract": "bandicoot is an open-source Python toolbox to extract more than 1442 features from standard mobile phone metadata. bandicoot makes it easy for machine learning researchers and practitioners to load mobile phone data, to analyze and visualize them, and to extract robust features which can be used for various classification and clustering tasks. Emphasis is put on ease of use, consistency, and documentation. bandicoot has no dependencies and is distributed under MIT license.",
            "keywords": [
                "Python",
                "feature engineering",
                "mobile phone metadata",
                "CDR"
            ],
            "author": [
                "Yves-Alexandre de Montjoye",
                "Luc Rocher",
                "Alex Sandy Pentland"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-593/15-593.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Input Output Kernel Regression: Supervised and Semi-Supervised Structured Output Prediction with Operator-Valued Kernels",
            "abstract": "In this paper, we introduce a novel approach, called Input Output Kernel Regression (IOKR), for learning mappings between structured inputs and structured outputs. The approach belongs to the family of Output Kernel Regression methods devoted to regression in feature space endowed with some output kernel. In order to take into account structure in input data and benefit from kernels in the input space as well, we use the Reproducing Kernel Hilbert Space theory for vector-valued functions. We first recall the ridge solution for supervised learning and then study the regularized hinge loss-based solution used in Maximum Margin Regression. Both models are also developed in the context of semi-supervised setting. In addition we derive an extension of Generalized Cross Validation for model selection in the case of the least-square model. Finally we show the versatility of the IOKR framework on two different problems: link prediction seen as a structured output problem and multi-task regression seen as a multiple and interdependent output problem. Eventually, we present a set of detailed numerical results that shows the relevance of the method on these two tasks.",
            "keywords": [
                "structured output prediction",
                "output kernel regression",
                "vector-valued RKHS",
                "operator-valued kernel"
            ],
            "author": [
                "Céline Brouard",
                "Marie Szafranski",
                "Florence d'Alché-Buc"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-602/15-602.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Note on the Sample Complexity of the Er-SpUD Algorithm by Spielman, Wang and Wright for Exact Recovery of Sparsely Used Dictionaries",
            "abstract": "We consider the problem of recovering an invertible  matrix  and a sparse  random matrix  based on the observation of  (up to a scaling and permutation of columns of  and rows of ). Using only elementary tools from the theory of empirical processes we show that a version of the Er-SpUD algorithm by Spielman, Wang and Wright with high probability recovers  and  exactly, provided that , which is optimal up to the constant .",
            "keywords": [
                "sparse dictionaries",
                "Er-SpUD algorithm"
            ],
            "author": [
                "Radoslaw Adamczak"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-047/16-047.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Asymptotic Performance of Linear Echo State Neural Networks",
            "abstract": "In this article, a study of the mean-square error (MSE) performance of linear echo-state neural networks is performed, both for training and testing tasks. Considering the realistic setting of noise present at the network nodes, we derive deterministic equivalents for the aforementioned MSE in the limit where the number of input data  and network size  both grow large. Specializing then the network connectivity matrix to specific random settings, we further obtain simple formulas that provide new insights on the performance of such networks.",
            "keywords": [],
            "author": [
                "Romain Couillet",
                "Gilles Wainrib",
                "Harry Sevi",
                "Hafiz Tiomoko Ali"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-076/16-076.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Consistency of the Likelihood Maximization Vertex Nomination Scheme: Bridging the Gap Between Maximum Likelihood Estimation and Graph Matching",
            "abstract": "Given a graph in which a few vertices are deemed interesting a priori, the vertex nomination task is to order the remaining vertices into a nomination list such that there is a concentration of interesting vertices at the top of the list. Previous work has yielded several approaches to this problem, with theoretical results in the setting where the graph is drawn from a stochastic block model (SBM), including a vertex nomination analogue of the Bayes optimal classifier. In this paper, we prove that maximum likelihood (ML)-based vertex nomination is consistent, in the sense that the performance of the ML-based scheme asymptotically matches that of the Bayes optimal scheme. We prove theorems of this form both when model parameters are known and unknown. Additionally, we introduce and prove consistency of a related, more scalable restricted-focus ML vertex nomination scheme. Finally, we incorporate vertex and edge features into ML-based vertex nomination and briefly explore the empirical effectiveness of this approach.",
            "keywords": [
                "vertex nomination",
                "graph matching",
                "graph inference",
                "stochastic block model"
            ],
            "author": [
                "Vince Lyzinski",
                "Keith Levin",
                "Donniell E. Fishkind",
                "Carey E. Priebe"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-343/16-343.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Characteristic Kernels and Infinitely Divisible Distributions",
            "abstract": "We connect shift-invariant characteristic kernels to infinitely divisible distributions on . Characteristic kernels play an important role in machine learning applications with their kernel means to distinguish any two probability measures. The contribution of this paper is twofold. First, we show, using the Levy--Khintchine formula, that any shift- invariant kernel given by a bounded, continuous, and symmetric probability density function (pdf) of an infinitely divisible distribution on  is characteristic. We mention some closure properties of such characteristic kernels under addition, pointwise product, and convolution. Second, in developing various kernel mean algorithms, it is fundamental to compute the following values: (i) kernel mean values , , and (ii) kernel mean RKHS inner products , for probability measures . If , and kernel  are Gaussians, then the computation of (i) and (ii) results in Gaussian pdfs that are tractable. We generalize this Gaussian combination to more general cases in the class of infinitely divisible distributions. We then introduce a conjugate kernel and a convolution trick, so that the above (i) and (ii) have the same pdf form, expecting tractable computation at least in some cases. As specific instances, we explore -stable distributions and a rich class of generalized hyperbolic distributions, where the Laplace, Cauchy, and Student's  distributions are included.",
            "keywords": [
                "Characteristic Kernel",
                "Kernel Mean",
                "Infinitely Divisible Distribution",
                "Con-     jugate Kernel"
            ],
            "author": [
                "Yu Nishiyama",
                "Kenji Fukumizu"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-132/14-132.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency of Cheeger and Ratio Graph Cuts",
            "abstract": "This paper establishes the consistency of a family of graph-cut- based algorithms for clustering of data clouds. We consider point clouds obtained as samples of a ground-truth measure. We investigate approaches to clustering based on minimizing objective functionals defined on proximity graphs of the given sample. Our focus is on functionals based on graph cuts like the Cheeger and ratio cuts. We show that minimizers of these cuts converge as the sample size increases to a minimizer of a corresponding continuum cut (which partitions the ground truth measure). Moreover, we obtain sharp conditions on how the connectivity radius can be scaled with respect to the number of sample points for the consistency to hold. We provide results for two-way and for multiway cuts. Furthermore we provide numerical experiments that illustrate the results and explore the optimality of scaling in dimension two.",
            "keywords": [
                "data clustering",
                "balanced cut",
                "consistency"
            ],
            "author": [
                "Nicolás García Trillos",
                "Dejan Slep\\v{c}ev",
                "James von Brecht",
                "Thomas Laurent",
                "Xavier Bresson"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-490/14-490.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Jointly Informative Feature Selection Made Tractable by Gaussian Modeling",
            "abstract": "We address the problem of selecting groups of jointly informative, continuous, features in the context of classification and propose several novel criteria for performing this selection. The proposed class of methods is based on combining a Gaussian modeling of the feature responses with derived bounds on and approximations to their mutual information with the class label. Furthermore, specific algorithmic implementations of these criteria are presented which reduce the computational complexity of the proposed feature selection algorithms by up to two-orders of magnitude. Consequently we show that feature selection based on the joint mutual information of features and class label is in fact tractable; this runs contrary to prior works that largely depend on marginal quantities. An empirical evaluation using several types of classifiers on multiple data sets show that this class of methods outperforms state-of-the-art baselines, both in terms of speed and classification accuracy.",
            "keywords": [
                "feature selection",
                "mutual information",
                "entropy"
            ],
            "author": [
                "Leonidas Lefakis",
                "François Fleuret"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-026/15-026.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "fastFM: A Library for Factorization Machines",
            "abstract": "Factorization Machines (FM) are currently only used in a narrow range of applications and are not yet part of the standard machine learning toolbox, despite their great success in collaborative filtering and click-through rate prediction. However, Factorization Machines are a general model to deal with sparse and high dimensional features. Our Factorization Machine implementation (fastFM) provides easy access to many solvers and supports regression, classification and ranking tasks. Such an implementation simplifies the use of FM for a wide range of applications. Therefore, our implementation has the potential to improve understanding of the FM model and drive new development.",
            "keywords": [
                "Python",
                "MCMC",
                "matrix factorization"
            ],
            "author": [
                "Immanuel Bayer"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-355/15-355.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Factorized Self-Controlled Case Series Method: An Approach for Estimating the Effects of Many Drugs on Many Outcomes",
            "abstract": "We provide a hierarchical Bayesian model for estimating the effects of transient drug exposures on a collection of health outcomes, where the effects of all drugs on all outcomes are estimated simultaneously. The method possesses properties that allow it to handle important challenges of dealing with large- scale longitudinal observational databases. In particular, this model is a generalization of the self-controlled case series (SCCS) method, meaning that certain patient specific baseline rates never need to be estimated. Further, this model is formulated with layers of latent factors, which substantially reduces the number of parameters and helps with interpretability by illuminating latent classes of drugs and outcomes. We believe our work is the first to consider multivariate SCCS (in the sense of multiple outcomes) and is the first to couple latent factor analysis with SCCS. We demonstrate the approach by estimating the effects of various time-sensitive insulin treatments for diabetes.",
            "keywords": [
                "Bayesian Analysis",
                "Drug Safety",
                "Self-Controlled Case Series",
                "Matrix Factor-     ization"
            ],
            "author": [
                "Ramin Moghaddass",
                "Cynthia Rudin",
                "David Madigan"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-405/15-405.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Electronic Health Record Analysis via Deep Poisson Factor Models",
            "abstract": "Electronic Health Record (EHR) phenotyping utilizes patient data captured through normal medical practice, to identify features that may represent computational medical phenotypes. These features may be used to identify at-risk patients and improve prediction of patient morbidity and mortality. We present a novel deep multi-modality architecture for EHR analysis (applicable to joint analysis of multiple forms of EHR data), based on Poisson Factor Analysis (PFA) modules. Each modality, composed of observed counts, is represented as a Poisson distribution, parameterized in terms of hidden binary units. Information from different modalities is shared via a deep hierarchy of common hidden units. Activation of these binary units occurs with probability characterized as Bernoulli- Poisson link functions, instead of more traditional logistic link functions. In addition, we demonstrate that PFA modules can be adapted to discriminative modalities. To compute model parameters, we derive efficient Markov Chain Monte Carlo (MCMC) inference that scales efficiently, with significant computational gains when compared to related models based on logistic link functions. To explore the utility of these models, we apply them to a subset of patients from the Duke-Durham patient cohort. We identified a cohort of over 16,000 patients with Type 2 Diabetes Mellitus (T2DM) based on diagnosis codes and laboratory tests out of our patient population of over 240,000. Examining the common hidden units uniting the PFA modules, we identify patient features that represent medical concepts. Experiments indicate that our learned features are better able to predict mortality and morbidity than clinical features identified previously in a large-scale clinical trial.",
            "keywords": [],
            "author": [
                "Ricardo Henao",
                "James T. Lu",
                "Joseph E. Lucas",
                "Jeffrey Ferranti",
                "Lawrence Carin"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-429/15-429.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Low-Rank Doubly Stochastic Matrix Decomposition for Cluster Analysis",
            "abstract": "Cluster analysis by nonnegative low-rank approximations has experienced a remarkable progress in the past decade. However, the majority of such approximation approaches are still restricted to nonnegative matrix factorization (NMF) and suffer from the following two drawbacks: 1) they are unable to produce balanced partitions for large-scale manifold data which are common in real-world clustering tasks; 2) most existing NMF-type clustering methods cannot automatically determine the number of clusters. We propose a new low-rank learning method to address these two problems, which is beyond matrix factorization. Our method approximately decomposes a sparse input similarity in a normalized way and its objective can be used to learn both cluster assignments and the number of clusters. For efficient optimization, we use a relaxed formulation based on Data- Cluster-Data random walk, which is also shown to be equivalent to low-rank factorization of the doubly-stochastically normalized cluster incidence matrix. The probabilistic cluster assignments can thus be learned with a multiplicative majorization-minimization algorithm. Experimental results show that the new method is more accurate both in terms of clustering large-scale manifold data sets and of selecting the number of clusters.",
            "keywords": [
                "cluster analysis",
                "probabilistic relaxation",
                "doubly stochastic matrix",
                "manifold"
            ],
            "author": [
                "Zhirong Yang",
                "Jukka Cor",
                "er",
                "Erkki Oja"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-549/15-549.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A New Algorithm and Theory for Penalized Regression-based Clustering",
            "abstract": "Clustering is unsupervised and exploratory in nature. Yet, it can be performed through penalized regression with grouping pursuit, as demonstrated in Pan et al. (2013). In this paper, we develop a more efficient algorithm for scalable computation and a new theory of clustering consistency for the method. This algorithm, called DC-ADMM, combines difference of convex (DC) programming with the alternating direction method of multipliers (ADMM). This algorithm is shown to be more computationally efficient than the quadratic penalty based algorithm of Pan et al. (2013) because of the former's closed-form updating formulas. Numerically, we compare the DC- ADMM algorithm with the quadratic penalty algorithm to demonstrate its utility and scalability. Theoretically, we establish a finite-sample mis- clustering error bound for penalized regression based clustering with the  constrained regularization in a general setting. On this ground, we provide conditions for clustering consistency of the penalized clustering method. As an end product, we put R package prclust implementing PRclust with various loss and grouping penalty functions available on GitHub and CRAN.",
            "keywords": [],
            "author": [
                "Chong Wu",
                "Sunghoon Kwon",
                "Xiaotong Shen",
                "Wei Pan"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-553/15-553.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classification of Imbalanced Data with a Geometric Digraph Family",
            "abstract": "We use a geometric digraph family called class cover catch digraphs (CCCDs) to tackle the class imbalance problem in statistical classification. CCCDs provide graph theoretic solutions to the class cover problem and have been employed in classification. We assess the classification performance of CCCD classifiers by extensive Monte Carlo simulations, comparing them with other classifiers commonly used in the literature. In particular, we show that CCCD classifiers perform relatively well when one class is more frequent than the other in a two- class setting, an example of the class imbalance problem. We also point out the relationship between class imbalance and class overlapping problems, and their influence on the performance of CCCD classifiers and other classification methods as well as some state-of-the-art algorithms which are robust to class imbalance by construction. Experiments on both simulated and real data sets indicate that CCCD classifiers are robust to the class imbalance problem. CCCDs substantially undersample from the majority class while preserving the information on the discarded points during the undersampling process. Many state- of-the-art methods, however, keep this information by means of ensemble classifiers, but CCCDs yield only a single classifier with the same property, making it both appealing and fast.",
            "keywords": [
                "Class Cover Catch Digraphs",
                "Class Cover Problem",
                "Class Imbalance Problem",
                "Class Overlapping Problem",
                "Graph Domination",
                "Prototype Selection"
            ],
            "author": [
                "Artür Manukyan",
                "Elvan Ceyhan"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-604/15-604.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Variational Approach to Path Estimation and Parameter Inference of Hidden Diffusion Processes",
            "abstract": "We consider a hidden Markov model, where the signal process, given by a diffusion, is only indirectly observed through some noisy measurements. The article develops a variational method for approximating the hidden states of the signal process given the full set of observations. This, in particular, leads to systematic approximations of the smoothing densities of the signal process. The paper then demonstrates how an efficient inference scheme, based on this variational approach to the approximation of the hidden states, can be designed to estimate the unknown parameters of stochastic differential equations. Two examples at the end illustrate the efficacy and the accuracy of the presented method.",
            "keywords": [
                "Variational inference",
                "stochastic differential equations",
                "diffusion processes",
                "hidden Markov model"
            ],
            "author": [
                "Tobias Sutter",
                "Arnab Ganguly",
                "Heinz Koeppl"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-075/16-075.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "One-class classification of point patterns of extremes",
            "abstract": "Novelty detection or one-class classification starts from a model describing some type of `normal behaviour' and aims to classify deviations from this model as being either novelties or anomalies. In this paper the problem of novelty detection for point patterns  is treated where examples of anomalies are very sparse, or even absent. The latter complicates the tuning of hyperparameters in models commonly used for novelty detection, such as one-class support vector machines and hidden Markov models. To this end, the use of extreme value statistics is introduced to estimate explicitly a model for the abnormal class by means of extrapolation from a statistical model  for the normal class. We show how multiple types of information obtained from any available extreme instances of  can be combined to reduce the high false-alarm rate that is typically encountered when classes are strongly imbalanced, as often occurs in the one-class setting (whereby `abnormal' data are often scarce). The approach is illustrated using simulated data and then a real-life application is used as an exemplar, whereby accelerometry data from epileptic seizures are analysed - these are known to be extreme and rare with respect to normal accelerometer data.",
            "keywords": [],
            "author": [
                "Stijn Luca",
                "David A. Clifton",
                "Bart Vanrumste"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-112/16-112.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Influence of Momentum Acceleration on Online Learning",
            "abstract": "The article examines in some detail the convergence rate and mean-square-error performance of momentum stochastic gradient methods in the constant step-size and slow adaptation regime. The results establish that momentum methods are equivalent to the standard stochastic gradient method with a re-scaled (larger) step-size value. The size of the re-scaling is determined by the value of the momentum parameter. The equivalence result is established for all time instants and not only in steady-state. The analysis is carried out for general strongly convex and smooth risk functions, and is not limited to quadratic risks. One notable conclusion is that the well-known benefits of momentum constructions for deterministic optimization problems do not necessarily carry over to the adaptive online setting when small constant step-sizes are used to enable continuous adaptation and learning in the presence of persistent gradient noise. From simulations, the equivalence between momentum and standard stochastic gradient methods is also observed for non-differentiable and non-convex problems.",
            "keywords": [
                "Online Learning",
                "Stochastic Gradient",
                "Momentum Acceleration",
                "Heavy-ball     Method"
            ],
            "author": [
                "Kun Yuan",
                "Bicheng Ying",
                "Ali H. Sayed"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-157/16-157.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Learning Rates for Localized SVMs",
            "abstract": "One of the limiting factors of using support vector machines (SVMs) in large scale applications are their super-linear computational requirements in terms of the number of training samples. To address this issue, several approaches that train SVMs on many small chunks separately have been proposed in the literature. With the exception of random chunks, which is also known as divide-and-conquer kernel ridge regression, however, these approaches have only been empirically investigated. In this work we investigate a spatially oriented method to generate the chunks. For the resulting localized SVM that uses Gaussian kernels and the least squares loss we derive an oracle inequality, which in turn is used to deduce learning rates that are essentially minimax optimal under some standard smoothness assumptions on the regression function. In addition, we derive local learning rates that are based on the local smoothness of the regression function. We further introduce a data-dependent parameter selection method for our local SVM approach and show that this method achieves the same almost optimal learning rates. Finally, we present a few larger scale experiments for our localized SVM showing that it achieves essentially the same test error as a global SVM for a fraction of the computational requirements. In addition, it turns out that the computational requirements for the local SVMs are similar to those of a vanilla random chunk approach, while the achieved test errors are significantly better.",
            "keywords": [
                "least squares regression",
                "support vector machines"
            ],
            "author": [
                "Mona Meister",
                "Ingo Steinwart"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-023/14-023.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bipartite Ranking: a Risk-Theoretic Perspective",
            "abstract": "We present a systematic study of the bipartite ranking problem, with the aim of explicating its connections to the class- probability estimation problem. Our study focuses on the properties of the statistical risk for bipartite ranking with general losses, which is closely related to a generalised notion of the area under the ROC curve: we establish alternate representations of this risk, relate the Bayes-optimal risk to a class of probability divergences, and characterise the set of Bayes-optimal scorers for the risk. We further study properties of a generalised class of bipartite risks, based on the -norm push of Rudin (2009). Our analysis is based on the rich framework of proper losses, which are the central tool in the study of class-probability estimation. We show how this analytic tool makes transparent the generalisations of several existing results, such as the equivalence of the minimisers for four seemingly disparate risks from bipartite ranking and class- probability estimation. A novel practical implication of our analysis is the design of new families of losses for scenarios where accuracy at the head of ranked list is paramount, with comparable empirical performance to the -norm push.",
            "keywords": [
                "bipartite ranking",
                "class-probability estimation",
                "proper losses",
                "Bayes-optimality"
            ],
            "author": [
                "Aditya Krishna Menon",
                "Robert C. Williamson"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-265/14-265.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian group factor analysis with structured sparsity",
            "abstract": "Latent factor models are the canonical statistical tool for exploratory analyses of low-dimensional linear structure for a matrix of  features across  samples. We develop a structured Bayesian group factor analysis model that extends the factor model to multiple coupled observation matrices; in the case of two observations, this reduces to a Bayesian model of canonical correlation analysis. Here, we carefully define a structured Bayesian prior that encourages both element-wise and column-wise shrinkage and leads to desirable behavior on high- dimensional data. In particular, our model puts a structured prior on the joint factor loading matrix, regularizing at three levels, which enables element-wise sparsity and unsupervised recovery of latent factors corresponding to structured variance across arbitrary subsets of the observations. In addition, our structured prior allows for both dense and sparse latent factors so that covariation among either all features or only a subset of features can be recovered. We use fast parameter-expanded expectation-maximization for parameter estimation in this model. We validate our method on simulated data with substantial structure. We show results of our method applied to three high- dimensional data sets, comparing results against a number of state-of-the-art approaches. These results illustrate useful properties of our model, including i) recovering sparse signal in the presence of dense effects; ii) the ability to scale naturally to large numbers of observations; iii) flexible observation- and factor-specific regularization to recover factors with a wide variety of sparsity levels and percentage of variance explained; and iv) tractable inference that scales to modern genomic and text data sizes.",
            "keywords": [],
            "author": [
                "Shiwen Zhao",
                "Chuan Gao",
                "Sayan Mukherjee",
                "Barbara E Engelhardt"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-472/14-472.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Machine Learning in an Auction Environment",
            "abstract": "We consider a model of repeated online auctions in which an ad with an uncertain click-through rate faces a random distribution of competing bids in each auction and there is discounting of payoffs. We formulate the optimal solution to this explore/exploit problem as a dynamic programming problem and show that efficiency is maximized by making a bid for each advertiser equal to the advertiser's expected value for the advertising opportunity plus a term proportional to the variance in this value divided by the number of impressions the advertiser has received thus far. We then use this result to illustrate that the value of incorporating active exploration in an auction environment is exceedingly small.",
            "keywords": [
                "Auctions"
            ],
            "author": [
                "Patrick Hummel",
                "R. Preston McAfee"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-109/15-109.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Wavelet decompositions of Random Forests - smoothness analysis, sparse approximation and applications",
            "abstract": "In this paper we introduce, in the setting of machine learning, a generalization of wavelet analysis which is a popular approach to low dimensional structured signal analysis. The wavelet decomposition of a Random Forest provides a sparse approximation of any regression or classification high dimensional function at various levels of detail, with a concrete ordering of the Random Forest nodes: from `significant' elements to nodes capturing only `insignificant' noise. Motivated by function space theory, we use the wavelet decomposition to compute numerically a `weak- type' smoothness index that captures the complexity of the underlying function. As we show through extensive experimentation, this sparse representation facilitates a variety of applications such as improved regression for difficult datasets, a novel approach to feature importance, resilience to noisy or irrelevant features, compression of ensembles, etc.",
            "keywords": [
                "Random Forest",
                "Wavelets",
                "Besov spaces",
                "adaptive approximation"
            ],
            "author": [
                "Oren Elisha",
                "Shai Dekel"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-203/15-203.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Mutual Information Based Matching for Causal Inference with Observational Data",
            "abstract": "This paper presents an information theory-driven matching methodology for making causal inference from observational data. The paper adopts a âpotential outcomes frameworkâ view on evaluating the strength of cause-effect relationships: the population-wide average effects of binary treatments are estimated by comparing two groups of units -- the treated and untreated (control). To reduce the bias in such treatment effect estimation, one has to compose a control group in such a way that across the compared groups of units, treatment is independent of the units' covariates. This requirement gives rise to a subset selection / matching problem. This paper presents the models and algorithms that solve the matching problem by minimizing the mutual information (MI) between the covariates and the treatment variable. Such a formulation becomes tractable thanks to the derived optimality conditions that tackle the non-linearity of the sample-based MI function. Computational experiments with mixed integer-programming formulations and four matching algorithms demonstrate the utility of MI based matching for causal inference studies. The algorithmic developments culminate in a matching heuristic that allows for balancing the compared groups in polynomial (close to linear) time, thus allowing for treatment effect estimation with large data sets.",
            "keywords": [
                "Observational Causal Inference",
                "Mutual Information",
                "Matching",
                "Subset Se-     lection"
            ],
            "author": [
                "Lei Sun",
                "Alexander G. Nikolaev"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-420/15-420.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multivariate Spearman's $\\rho$ for Aggregating Ranks Using Copulas",
            "abstract": "We study the problem of rank aggregation: given a set of ranked lists, we want to form a consensus ranking. Furthermore, we consider the case of extreme lists: i.e., only the rank of the best or worst elements are known. We impute missing ranks and generalise Spearman's  to extreme ranks. Our main contribution is the derivation of a non-parametric estimator for rank aggregation based on multivariate extensions of Spearman's , which measures correlation between a set of ranked lists. Multivariate Spearman's  is defined using copulas, and we show that the geometric mean of normalised ranks maximises multivariate correlation. Motivated by this, we propose a weighted geometric mean approach for learning to rank which has a closed form least squares solution. When only the best (top-k) or worst (bottom-k) elements of a ranked list are known, we impute the missing ranks by the average value, allowing us to apply Spearman's . We discuss an optimistic and pessimistic imputation of missing values, which respectively maximise and minimise correlation, and show its effect on aggregating university rankings. Finally, we demonstrate good performance on the rank aggregation benchmarks MQ2007 and MQ2008.",
            "keywords": [],
            "author": [
                "Justin Bedő",
                "Cheng Soon Ong"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-625/15-625.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric Network Models for Link Prediction",
            "abstract": "",
            "keywords": [
                "Dirichlet process",
                "networks",
                "Bayesian nonparametrics",
                "Gibbs sampling"
            ],
            "author": [
                "Sinead A. Williamson"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-032/16-032.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Guarding against Spurious Discoveries in High Dimensions",
            "abstract": "Many data mining and statistical machine learning algorithms have been developed to select a subset of covariates to associate with a response variable. Spurious discoveries can easily arise in high-dimensional data analysis due to enormous possibilities of such selections. How can we know statistically our discoveries better than those by chance? In this paper, we define a measure of goodness of spurious fit, which shows how good a response variable can be fitted by an optimally selected subset of covariates under the null model, and propose a simple and effective LAMM algorithm to compute it. It coincides with the maximum spurious correlation for linear models and can be regarded as a generalized maximum spurious correlation. We derive the asymptotic distribution of such goodness of spurious fit for generalized linear models and  regression. Such an asymptotic distribution depends on the sample size, ambient dimension, the number of variables used in the fit, and the covariance information. It can be consistently estimated by multiplier bootstrapping and used as a benchmark to guard against spurious discoveries. It can also be applied to model selection, which considers only candidate models with goodness of fits better than those by spurious fits. The theory and method are convincingly illustrated by simulated examples and an application to the binary outcomes from German Neuroblastoma Trials.",
            "keywords": [
                "Bootstrap",
                "Gaussian approximation",
                "generalized linear models",
                "L1 regression",
                "model selection",
                "sparsity",
                "spurious correlation"
            ],
            "author": [
                "Jianqing Fan",
                "Wen-Xin Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-068/16-068.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Graphical Models for Multivariate Functional Data",
            "abstract": "Graphical models express conditional independence relationships among variables. Although methods for vector-valued data are well established, functional data graphical models remain underdeveloped. By functional data, we refer to data that are realizations of random functions varying over a continuum (e.g., images, signals). We introduce a notion of conditional independence between random functions, and construct a framework for Bayesian inference of undirected, decomposable graphs in the multivariate functional data context. This framework is based on extending Markov distributions and hyper Markov laws from random variables to random processes, providing a principled alternative to naive application of multivariate methods to discretized functional data. Markov properties facilitate the composition of likelihoods and priors according to the decomposition of a graph. Our focus is on Gaussian process graphical models using orthogonal basis expansions. We propose a hyper-inverse-Wishart-process prior for the covariance kernels of the infinite coefficient sequences of the basis expansion, and establish its existence and uniqueness. We also prove the strong hyper Markov property and the conjugacy of this prior under a finite rank condition of the prior kernel parameter. Stochastic search Markov chain Monte Carlo algorithms are developed for posterior inference, assessed through simulations, and applied to a study of brain activity and alcoholism.",
            "keywords": [
                "graphical model",
                "functional data analysis",
                "gaussian process",
                "model uncer-     tainty"
            ],
            "author": [
                "Hongxiao Zhu",
                "Nate Strawn",
                "David B. Dunson"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-164/16-164.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Neural Autoregressive Distribution Estimation",
            "abstract": "We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE.",
            "keywords": [
                "deep learning",
                "neural networks",
                "density modeling"
            ],
            "author": [
                "Benigno Uria",
                "Marc-Alexandre Côté",
                "Karol Gregor",
                "Iain Murray",
                "Hugo Larochelle"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-272/16-272.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ERRATA: On the Estimation of the Gradient Lines of a Density and the Consistency of the Mean-Shift Algorithm",
            "abstract": "ERRATA to the paper On the  Estimation of the Gradient Lines of a Density and the Consistency of  the Mean-Shift Algorithm.",
            "keywords": [],
            "author": [
                "Ery Arias-Castro",
                "David Mason",
                "Bruno Pelletier"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-527/16-527.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Modelling Interactions in High-dimensional Data with Backtracking",
            "abstract": "",
            "keywords": [
                "high-dimensional data",
                "interactions",
                "Lasso"
            ],
            "author": [
                "Rajen D. Shah"
            ],
            "ref": "http://jmlr.org/papers/volume17/13-515/13-515.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Choice of V for V-Fold Cross-Validation in Least-Squares Density Estimation",
            "abstract": "This paper studies -fold cross-validation for model selection in least-squares density estimation. The goal is to provide theoretical grounds for choosing  in order to minimize the least-squares loss of the selected estimator. We first prove a non-asymptotic oracle inequality for -fold cross-validation and its bias-corrected version (-fold penalization). In particular, this result implies that -fold penalization is asymptotically optimal in the nonparametric case. Then, we compute the variance of -fold cross-validation and related criteria, as well as the variance of key quantities for model selection performance. We show that these variances depend on  like , at least in some particular cases, suggesting that the performance increases much from  to  or , and then is almost constant. Overall, this can explain the common advice to take ---at least in our setting and when the computational power is limited---, as supported by some simulation experiments. An oracle inequality and exact formulas for the variance are also proved for Monte- Carlo cross-validation, also known as repeated cross-validation, where the parameter  is replaced by the number  of random splits of the data.",
            "keywords": [
                "V -fold cross-validation",
                "Monte-Carlo cross-validation",
                "leave-one-out",
                "leave-p-     out",
                "resampling penalties",
                "density estimation",
                "model selection"
            ],
            "author": [
                "Sylvain Arlot",
                "Matthieu Lerasle"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-296/14-296.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Towards More Efficient SPSD Matrix Approximation and CUR Matrix Decomposition",
            "abstract": "Symmetric positive semi-definite (SPSD) matrix approximation methods have been extensively used to speed up large-scale eigenvalue computation and kernel learning methods. The standard sketch based method, which we call the prototype model, produces relatively accurate approximations, but is inefficient on large square matrices. The NystrÃ¶m method is highly efficient, but can only achieve low accuracy. In this paper we propose a novel model that we call the fast SPSD matrix approximation model. The fast model is nearly as efficient as the NystrÃ¶m method and as accurate as the prototype model. We show that the fast model can potentially solve eigenvalue problems and kernel learning problems in linear time with respect to the matrix size  to achieve  relative-error, whereas both the prototype model and the NystrÃ¶m method cost at least quadratic time to attain comparable error bound. Empirical comparisons among the prototype model, the NystrÃ¶m method, and our fast model demonstrate the superiority of the fast model. We also contribute new understandings of the NystrÃ¶m method. The NystrÃ¶m method is a special instance of our fast model and is approximation to the prototype model. Our technique can be straightforwardly applied to make the CUR matrix decomposition more efficiently computed without much affecting the accuracy.",
            "keywords": [
                "Kernel approximation",
                "matrix factorization"
            ],
            "author": [
                "Shusen Wang",
                "Zhihua Zhang",
                "Tong Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-190/15-190.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Objective Markov Decision Processes for Data-Driven Decision Support",
            "abstract": "We present new methodology based on Multi-Objective Markov Decision Processes for developing sequential decision support systems from data. Our approach uses sequential decision-making data to provide support that is useful to many different decision-makers, each with different, potentially time-varying preference. To accomplish this, we develop an extension of fitted- iteration for multiple objectives that computes policies for all scalarization functions, i.e. preference functions, simultaneously from continuous-state, finite-horizon data. We identify and address several conceptual and computational challenges along the way, and we introduce a new solution concept that is appropriate when different actions have similar expected outcomes. Finally, we demonstrate an application of our method using data from the Clinical Antipsychotic Trials of Intervention Effectiveness and show that our approach offers decision-makers increased choice by a larger class of optimal policies.",
            "keywords": [
                "multi-objective optimization",
                "reinforcement learning",
                "Markov decision pro-     cesses",
                "clinical decision support"
            ],
            "author": [
                "Daniel J. Lizotte",
                "Eric B. Laber"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-252/15-252.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Measuring Dependence Powerfully and Equitably",
            "abstract": "Given a high-dimensional data set, we often wish to find the strongest relationships within it. A common strategy is to evaluate a measure of dependence on every variable pair and retain the highest-scoring pairs for follow-up. This strategy works well if the statistic used (a) has good power to detect non-trivial relationships, and (b) is equitable, meaning that for some measure of noise it assigns similar scores to equally noisy relationships regardless of relationship type (e.g., linear, exponential, periodic). In this paper, we define and theoretically characterize two new statistics that together yield an efficient approach for obtaining both power and equitability. To do this, we first introduce a new population measure of dependence and show three equivalent ways that it can be viewed, including as a canonical smoothing of mutual information. We then introduce an efficiently computable consistent estimator of our population measure of dependence, and we empirically establish its equitability on a large class of noisy functional relationships. This new statistic has better bias/variance properties and better runtime complexity than a previous heuristic approach. Next, we derive a second, related statistic whose computation is a trivial side-product of our algorithm and whose goal is powerful independence testing rather than equitability. We prove that this statistic yields a consistent independence test and show in simulations that the test has good power against independence. Taken together, our results suggest that these two statistics are a valuable pair of tools for exploratory data analysis.",
            "keywords": [],
            "author": [
                "Yakir A. Reshef",
                "David N. Reshef",
                "Hilary K. Finucane",
                "Pardis C. Sabeti",
                "Michael Mitzenmacher"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-308/15-308.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Neyman-Pearson Classification under High-Dimensional Settings",
            "abstract": "Most existing binary classification methods target on the optimization of the overall classification risk and may fail to serve some real-world applications such as cancer diagnosis, where users are more concerned with the risk of misclassifying one specific class than the other. Neyman-Pearson (NP) paradigm was introduced in this context as a novel statistical framework for handling asymmetric type I/II error priorities. It seeks classifiers with a minimal type II error and a constrained type I error under a user specified level. This article is the first attempt to construct classifiers with guaranteed theoretical performance under the NP paradigm in high-dimensional settings. Based on the fundamental Neyman-Pearson Lemma, we used a plug-in approach to construct NP-type classifiers for Naive Bayes models. The proposed classifiers satisfy the NP oracle inequalities, which are natural NP paradigm counterparts of the oracle inequalities in classical binary classification. Besides their desirable theoretical properties, we also demonstrated their numerical advantages in prioritized error control via both simulation and real data studies.",
            "keywords": [
                "classification",
                "high-dimension",
                "Naive Bayes"
            ],
            "author": [
                "Anqi Zhao",
                "Yang Feng",
                "Lie Wang",
                "Xin Tong"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-418/15-418.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Statistical Perspective on Randomized Sketching for Ordinary Least-Squares",
            "abstract": "We consider statistical as well as algorithmic aspects of solving large-scale least-squares (LS) problems using randomized sketching algorithms. For a LS problem with input data , sketching algorithms use a sketching matrix, , where . Then, rather than solving the LS problem using the full data , sketching algorithms solve the LS problem using only the sketched data . Prior work has typically adopted an algorithmic perspective, in that it has made no statistical assumptions on the input  and , and instead it has been assumed that the data  are fixed and worst-case (WC). Prior results show that, when using sketching matrices such as random projections and leverage-score sampling algorithms, with , the WC error is the same as solving the original problem, up to a small constant. From a statistical perspective, we typically consider the mean-squared error performance of randomized sketching algorithms, when data  are generated according to a statistical linear model , where  is a noise process. In this paper, we provide a rigorous comparison of both perspectives leading to insights on how they differ. To do this, we first develop a framework for assessing, in a unified manner, algorithmic and statistical aspects of randomized sketching methods. We then consider the statistical prediction efficiency (PE) and the statistical residual efficiency (RE) of the sketched LS estimator; and we use our framework to provide upper bounds for several types of random projection and random sampling sketching algorithms. Among other results, we show that the RE can be upper bounded when  while the PE typically requires the sample size  to be substantially larger. Lower bounds developed in subsequent results show that our upper bounds on PE can not be improved. (A preliminary version of this paper appeared as Raskutti and Mahoney (2014, 2015).)",
            "keywords": [
                "algorithmic leveraging",
                "randomized linear algebra",
                "sketching",
                "random pro-     jection",
                "statistical leverage"
            ],
            "author": [
                "Garvesh Raskutti",
                "Michael W. Mahoney"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-440/15-440.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Planar Ising Models",
            "abstract": "Inference and learning of graphical models are both well-studied problems in statistics and machine learning that have found many applications in science and engineering. However, exact inference is intractable in general graphical models, which suggests the problem of seeking the best approximation to a collection of random variables within some tractable family of graphical models. In this paper, we focus on the class of planar Ising models, for which exact inference is tractable using techniques of statistical physics. Based on these techniques and recent methods for planarity testing and planar embedding, we propose a greedy algorithm for learning the best planar Ising model to approximate an arbitrary collection of binary random variables (possibly from sample data). Given the set of all pairwise correlations among variables, we select a planar graph and optimal planar Ising model defined on this graph to best approximate that set of correlations. We demonstrate our method in simulations and for two applications: modeling senate voting records and identifying geo-chemical depth trends from Mars rover data.",
            "keywords": [
                "Ising models"
            ],
            "author": [
                "Jason K. Johnson",
                "Diane Oyen",
                "Michael Chertkov",
                "Praneeth Netrapalli"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-579/15-579.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Newton-Stein Method: An Optimization Method for GLMs via Stein's Lemma",
            "abstract": "We consider the problem of efficiently computing the maximum likelihood estimator in Generalized Linear Models (GLMs) when the number of observations is much larger than the number of coefficients (). In this regime, optimization algorithms can immensely benefit from approximate second order information. We propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a Stein-type lemma, which allows further improvements through sub-sampling and eigenvalue thresholding. Our algorithm enjoys fast convergence rates, resembling that of second order methods, with modest per-iteration cost. We provide its convergence analysis for the general case where the rows of the design matrix are samples from a sub-Gaussian distribution. We show that the convergence has two phases, a quadratic phase followed by a linear phase. Finally, we empirically demonstrate that our algorithm achieves the highest performance compared to various optimization algorithms on several data sets.",
            "keywords": [
                "Optimization",
                "Generalized Linear Models"
            ],
            "author": [
                "Murat A. Erdogdu"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-062/16-062.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Decision Process for Cost-Efficient Dynamic Ranking via Crowdsourcing",
            "abstract": "Rank aggregation based on pairwise comparisons over a set of items has a wide range of applications. Although considerable research has been devoted to the development of rank aggregation algorithms, one basic question is how to efficiently collect a large amount of high-quality pairwise comparisons for the ranking purpose. Because of the advent of many crowdsourcing services, a crowd of workers are often hired to conduct pairwise comparisons with a small monetary reward for each pair they compare. Since different workers have different levels of reliability and different pairs have different levels of ambiguity, it is desirable to wisely allocate the limited budget for comparisons among the pairs of items and workers so that the global ranking can be accurately inferred from the comparison results. To this end, we model the active sampling problem in crowdsourced ranking as a Bayesian Markov decision process, which dynamically selects item pairs and workers to improve the ranking accuracy under a budget constraint. We further develop a computationally efficient sampling policy based on knowledge gradient as well as a moment matching technique for posterior approximation. Experimental evaluations on both synthetic and real data show that the proposed policy achieves high ranking accuracy with a lower labeling cost.",
            "keywords": [
                "crowdsourced ranking",
                "Bayesian",
                "Markov decision process",
                "dynamic program-     ming",
                "knowledge gradient"
            ],
            "author": [
                "Xi Chen",
                "Kevin Jiao",
                "Qihang Lin"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-066/16-066.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Bayes Risk Lower Bounds",
            "abstract": "This paper provides a general technique for lower bounding the Bayes risk of statistical estimation, applicable to arbitrary loss functions and arbitrary prior distributions. A lower bound on the Bayes risk not only serves as a lower bound on the minimax risk, but also characterizes the fundamental limit of any estimator given the prior knowledge. Our bounds are based on the notion of -informativity (CsiszÃ¡r, 1972), which is a function of the underlying class of probability measures and the prior. Application of our bounds requires upper bounds on the -informativity, thus we derive new upper bounds on -informativity which often lead to tight Bayes risk lower bounds. Our technique leads to generalizations of a variety of classical minimax bounds (e.g., generalized Fano's inequality). Our Bayes risk lower bounds can be directly applied to several concrete estimation problems, including Gaussian location models, generalized linear models, and principal component analysis for spiked covariance models. To further demonstrate the applications of our Bayes risk lower bounds to machine learning problems, we present two new theoretical results: (1) a precise characterization of the minimax risk of learning spherical Gaussian mixture models under the smoothed analysis framework, and (2) lower bounds for the Bayes risk under a natural prior for both the prediction and estimation errors for high-dimensional sparse linear regression under an improper learning setting.",
            "keywords": [
                "Bayes risk",
                "Minimax risk",
                "f -divergence",
                "f -informativity"
            ],
            "author": [
                "Xi Chen",
                "Adityan",
                "Guntuboyina",
                "Yuchen Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-185/16-185.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Weak Convergence Properties of Constrained Emphatic Temporal-difference Learning with Constant and Slowly Diminishing Stepsize",
            "abstract": "We consider the emphatic temporal-difference (TD) algorithm, ETD(), for learning the value functions of stationary policies in a discounted, finite state and action Markov decision process. The ETD() algorithm was recently proposed by Sutton, Mahmood, and White (2016) to solve a long-standing divergence problem of the standard TD algorithm when it is applied to off- policy training, where data from an exploratory policy are used to evaluate other policies of interest. The almost sure convergence of ETD() has been proved in our recent work under general off-policy training conditions, but for a narrow range of diminishing stepsize. In this paper we present convergence results for constrained versions of ETD() with constant stepsize and with diminishing stepsize from a broad range. Our results characterize the asymptotic behavior of the trajectory of iterates produced by those algorithms, and are derived by combining key properties of ETD() with powerful convergence theorems from the weak convergence methods in stochastic approximation theory. For the case of constant stepsize, in addition to analyzing the behavior of the algorithms in the limit as the stepsize parameter approaches zero, we also analyze their behavior for a fixed stepsize and bound the deviations of their averaged iterates from the desired solution. These results are obtained by exploiting the weak Feller property of the Markov chains associated with the algorithms, and by using ergodic theorems for weak Feller Markov chains, in conjunction with the convergence results we get from the weak convergence methods. Besides ETD(), our analysis also applies to the off-policy TD() algorithm, when the divergence issue is avoided by setting  sufficiently large. It yields, for that case, new results on the asymptotic convergence properties of constrained off-policy TD() with constant or slowly diminishing stepsize.",
            "keywords": [
                "Markov decision processes",
                "approximate policy evaluation",
                "reinforcement     learning",
                "temporal-difference methods",
                "importance sampling",
                "stochastic approximation"
            ],
            "author": [
                "Huizhen Yu"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-242/16-242.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "RLScore: Regularized Least-Squares Learners",
            "abstract": "RLScore is a Python open source module for kernel based machine learning. The library provides implementations of several regularized least-squares (RLS) type of learners. RLS methods for regression and classification, ranking, greedy feature selection, multi-task and zero-shot learning, and unsupervised classification are included. Matrix algebra based computational short-cuts are used to ensure efficiency of both training and cross-validation. A simple API and extensive tutorials allow for easy use of RLScore.",
            "keywords": [
                "cross-validation",
                "feature selection",
                "kernel methods",
                "Kronecker product kernel",
                "pair-input learning",
                "python"
            ],
            "author": [
                "Tapio Pahikkala",
                "Antti Airola"
            ],
            "ref": "http://jmlr.org/papers/volume17/16-470/16-470.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stability and Generalization in Structured Prediction",
            "abstract": "Structured prediction models have been found to learn effectively from a few large examples--- sometimes even just one. Despite empirical evidence, canonical learning theory cannot guarantee  generalization in this setting because the error bounds decrease as a function of the number of examples.  We therefore propose new PAC-Bayesian generalization bounds for structured prediction that decrease  as a function of both the number of examples and the size of each example. Our analysis hinges on the  stability of joint inference and the smoothness of the data distribution. We apply our bounds to several  common learning scenarios, including max-margin and soft-max training of Markov random fields.  Under certain conditions, the resulting error bounds can be far more optimistic than previous results and  can even guarantee generalization from a single large example.",
            "keywords": [
                "structured prediction",
                "learning theory",
                "PAC-Bayes"
            ],
            "author": [
                "Ben London",
                "Bert Huang",
                "Lise Getoor"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-501/15-501.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Composite Multiclass Losses",
            "abstract": "We consider loss functions for multiclass prediction problems. We show when a multiclass loss can be expressed as a proper composite loss, which is the composition of a proper loss and a link function. We extend existing results for binary losses to multiclass losses. We subsume results on âclassification calibrationâ by relating it to properness. We determine the stationarity condition, Bregman representation, order- sensitivity, and quasi-convexity of multiclass proper losses. We then characterise the existence and uniqueness of the composite representation for multiclass losses. We show how the composite representation is related to other core properties of a loss: mixability, admissibility and (strong) convexity of multiclass losses which we characterise in terms of the Hessian of the Bayes risk. We show that the simple integral representation for binary proper losses can not be extended to multiclass losses but offer concrete guidance regarding how to design different loss functions. The conclusion drawn from these results is that the proper composite representation is a natural and convenient tool for the design of multiclass loss functions.",
            "keywords": [
                "Proper losses",
                "Multiclass losses",
                "Link Functions",
                "Convexity and quasi-convexity     of losses",
                "Margin losses",
                "Classification calibration",
                "Parametrisations and representations of loss     functions",
                "Admissibility",
                "Mixability",
                "Minimaxity"
            ],
            "author": [
                "Robert C. Williamson",
                "Elodie Vernet",
                "Mark D. Reid"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-294/14-294.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Latent Variable Models by Pairwise Cluster Comparison: Part I - Theory and Overview",
            "abstract": "Identification of latent variables that govern a problem and the relationships among them, given measurements in the observed world, are important for causal discovery. This identification can be accomplished by analyzing the constraints imposed by the latents in the measurements. We introduce the concept of pairwise cluster comparison (PCC) to identify causal relationships from clusters of data points and provide a two- stage algorithm called learning PCC (LPCC) that learns a latent variable model (LVM) using PCC. First, LPCC learns exogenous latents and latent colliders, as well as their observed descendants, by using pairwise comparisons between data clusters in the measurement space that may explain latent causes. Since in this first stage LPCC cannot distinguish endogenous latent non-colliders from their exogenous ancestors, a second stage is needed to extract the former, with their observed children, from the latter. If the true graph has no serial connections, LPCC returns the true graph, and if the true graph has a serial connection, LPCC returns a pattern of the true graph. LPCC's most important advantage is that it is not limited to linear or latent-tree models and makes only mild assumptions about the distribution. The paper is divided in two parts: Part I (this paper) provides the necessary preliminaries, theoretical foundation to PCC, and an overview of LPCC; Part II formally introduces the LPCC algorithm and experimentally evaluates its merit in different synthetic and real domains. The code for the LPCC algorithm and data sets used in the experiments reported in Part II are available online.",
            "keywords": [
                "causal discovery",
                "clustering",
                "learning latent variable model",
                "multiple indica-     tor model"
            ],
            "author": [
                "Nuaman Asbeh",
                "Boaz Lerner"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-401/14-401.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GenSVM: A Generalized Multiclass Support Vector Machine",
            "abstract": "Traditional extensions of the binary support vector machine (SVM) to multiclass problems are either heuristics or require solving a large dual optimization problem. Here, a generalized multiclass SVM is proposed called GenSVM. In this method classification boundaries for a -class problem are constructed in a -dimensional space using a simplex encoding. Additionally, several different weightings of the misclassification errors are incorporated in the loss function, such that it generalizes three existing multiclass SVMs through a single optimization problem. An iterative majorization algorithm is derived that solves the optimization problem without the need of a dual formulation. This algorithm has the advantage that it can use warm starts during cross validation and during a grid search, which significantly speeds up the training phase. Rigorous numerical experiments compare linear GenSVM with seven existing multiclass SVMs on both small and large data sets. These comparisons show that the proposed method is competitive with existing methods in both predictive accuracy and training time, and that it significantly outperforms several existing methods on these criteria.",
            "keywords": [
                "support vector machines",
                "SVM",
                "multiclass classification",
                "iterative majoriza-     tion",
                "MM algorithm"
            ],
            "author": [
                "Gerrit J.J. van den Burg",
                "Patrick J.F. Groenen"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-526/14-526.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Approximate Bayesian Inference for Outlier Detection under Informative Sampling",
            "abstract": "Government surveys of business establishments receive a large volume of submissions where a small subset contain errors. Analysts need a fast-computing algorithm to flag this subset due to a short time window between collection and reporting. We offer a computationally-scalable optimization method based on non-parametric mixtures of hierarchical Dirichlet processes that allows discovery of multiple industry-indexed local partitions linked to a set of global cluster centers. Outliers are nominated as those clusters containing few observations. We extend an existing approach with a new merge step that reduces sensitivity to hyperparameter settings. Survey data are typically acquired under an informative sampling design where the probability of inclusion depends on the surveyed response such that the distribution for the observed sample is different from the population. We extend the derivation of a penalized objective function to use a pseudo-posterior that incorporates sampling weights that undo the informative design. We provide a simulation study to demonstrate that our approach produces unbiased estimation for the outlying cluster under informative sampling. The method is applied for outlier nomination for the Current Employment Statistics survey conducted by the Bureau of Labor Statistics.",
            "keywords": [],
            "author": [
                "Terrance D. Savitsky"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-088/15-088.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximate Newton Methods for Policy Search in Markov Decision Processes",
            "abstract": "Approximate Newton methods are standard optimization tools which aim to maintain the benefits of Newton's method, such as a fast rate of convergence, while alleviating its drawbacks, such as computationally expensive calculation or estimation of the inverse Hessian. In this work we investigate approximate Newton methods for policy optimization in Markov decision processes (MDPs). We first analyse the structure of the Hessian of the total expected reward, which is a standard objective function for MDPs. We show that, like the gradient, the Hessian exhibits useful structure in the context of MDPs and we use this analysis to motivate two Gauss-Newton methods for MDPs. Like the Gauss- Newton method for non-linear least squares, these methods drop certain terms in the Hessian. The approximate Hessians possess desirable properties, such as negative definiteness, and we demonstrate several important performance guarantees including guaranteed ascent directions, invariance to affine transformation of the parameter space and convergence guarantees. We finally provide a unifying perspective of key policy search algorithms, demonstrating that our second Gauss- Newton algorithm is closely related to both the EM-algorithm and natural gradient ascent applied to MDPs, but performs significantly better in practice on a range of challenging domains.",
            "keywords": [
                "Markov decision processes",
                "reinforcement learning",
                "Newton method"
            ],
            "author": [
                "Thomas Furmston",
                "Guy Lever",
                "David Barber"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-414/15-414.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gains and Losses are Fundamentally Different in Regret Minimization: The Sparse Case",
            "abstract": "We demonstrate that, in the classical non-stochastic regret minimization problem with  decisions, gains and losses to be respectively maximized or minimized are fundamentally different. Indeed, by considering the additional sparsity assumption (at each stage, at most  decisions incur a nonzero outcome), we derive optimal regret bounds of different orders. Specifically, with gains, we obtain an optimal regret guarantee after  stages of order , so the classical dependency in the dimension is replaced by the sparsity size. With losses, we provide matching upper and lower bounds of order , which is decreasing in . Eventually, we also study the bandit setting, and obtain an upper bound of order  when outcomes are losses. This bound is proven to be optimal up to the logarithmic factor .",
            "keywords": [
                "regret minimization",
                "bandit"
            ],
            "author": [
                "Joon Kwon",
                "Vianney Perchet"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-503/15-503.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear Convergence of Randomized Feasible Descent Methods Under the Weak Strong Convexity Assumption",
            "abstract": "In this paper we generalize the framework of the Feasible Descent Method (FDM) to a Randomized (R-FDM) and a Randomized Coordinate-wise Feasible Descent Method (RC-FDM) framework. We show that many machine learning algorithms, including the famous SDCA algorithm for optimizing the SVM dual problem, or the stochastic coordinate descent method for the LASSO problem, fits into the framework of RC-FDM. We prove linear convergence for both R-FDM and RC-FDM under the weak strong convexity assumption. Moreover, we show that the duality gap converges linearly for RC-FDM, which implies that the duality gap also converges linearly for SDCA applied to the SVM dual problem.",
            "keywords": [
                "feasible descent method",
                "stochastic methods",
                "iteration complexity",
                "conver-     gence theory"
            ],
            "author": [
                "Chenxin Ma",
                "Rachael Tappenden",
                "Martin  Takáč"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-541/15-541.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Practical Scheme and Fast Algorithm to Tune the Lasso With Optimality Guarantees",
            "abstract": "We introduce a novel scheme for choosing the regularization parameter in high-dimensional linear  regression with Lasso. This scheme, inspired by Lepskiâs method for bandwidth selection in non-parametric  regression, is equipped with both optimal finite-sample guarantees and a fast algorithm. In particular, for any design matrix such that the Lasso has low sup-norm error under an âoracle choiceâ of the  regularization parameter, we show that our method matches the oracle performance up to a small constant factor,  and show that it can be implemented by performing simple tests along a single Lasso path. By applying the  Lasso to simulated and real data, we find that our novel scheme can be faster and more accurate than  standard schemes such as Cross-Validation.",
            "keywords": [
                "Lasso",
                "regularization parameter",
                "tuning parameter",
                "high-dimensional regres-     sion"
            ],
            "author": [
                "Michael Chichignoud",
                "Johannes Lederer",
                "Martin J. Wainwright"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-605/15-605.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Latent Variable Models by Pairwise Cluster Comparison: Part II - Algorithm and Evaluation",
            "abstract": "It is important for causal discovery to identify any latent variables that govern a problem and the relationships among them, given measurements in the observed world. In Part I of this paper, we were interested in learning a discrete latent variable model (LVM) and introduced the concept of pairwise cluster comparison (PCC) to identify causal relationships from clusters of data points and an overview of a two-stage algorithm for learning PCC (LPCC). First, LPCC learns exogenous latent variables and latent colliders, as well as their observed descendants, by using pairwise comparisons between data clusters in the measurement space that may explain latent causes. Second, LPCC identifies endogenous latent non- colliders with their observed children. In Part I, we showed that if the true graph has no serial connections, then LPCC returns the true graph, and if the true graph has a serial connection, then LPCC returns a pattern of the true graph. In this paper (Part II), we formally introduce the LPCC algorithm that implements the PCC concept. In addition, we thoroughly evaluate LPCC using simulated and real-world data sets in comparison to state-of-the-art algorithms. Besides using three real-world data sets, which have already been tested in learning an LVM, we also evaluate the algorithms using data sets that represent two original problems. The first problem is identifying young drivers' involvement in road accidents, and the second is identifying cellular subpopulations of the immune system from mass cytometry. The results of our evaluation show that LPCC improves in accuracy with the sample size, can learn large LVMs, and is accurate in learning compared to state-of- the-art algorithms. The code for the LPCC algorithm and data sets used in the experiments reported here are available online.",
            "keywords": [
                "learning latent variable models",
                "graphical models",
                "clustering"
            ],
            "author": [
                "Nuaman Asbeh",
                "Boaz Lerner"
            ],
            "ref": "http://jmlr.org/papers/volume17/14-402/14-402.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Characterization of Linkage-Based Hierarchical Clustering",
            "abstract": "The class of linkage-based algorithms is perhaps the most popular class of hierarchical algorithms. We identify two properties of hierarchical algorithms, and prove that linkage- based algorithms are the only ones that satisfy both of these properties. Our characterization clearly delineates the difference between linkage-based algorithms and other hierarchical methods. We formulate an intuitive notion of locality of a hierarchical algorithm that distinguishes between linkage-based and global hierarchical algorithms like bisecting -means, and prove that popular divisive hierarchical algorithms produce clusterings that cannot be produced by any linkage-based algorithm.",
            "keywords": [],
            "author": [
                "Margareta Ackerman",
                "Shai Ben-David"
            ],
            "ref": "http://jmlr.org/papers/volume17/11-198/11-198.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Integrative Analysis using Coupled Latent Variable Models for Individualizing Prognoses",
            "abstract": "Complex chronic diseases (e.g., autism, lupus, and Parkinson's) are remarkably heterogeneous across individuals. This heterogeneity makes treatment difficult for caregivers because they cannot accurately predict the way in which the disease will progress in order to guide treatment decisions. Therefore, tools that help to predict the trajectory of these complex chronic diseases can help to improve the quality of health care. To build such tools, we can leverage clinical markers that are collected at baseline when a patient first presents and longitudinally over time during follow-up visits. Because complex chronic diseases are typically systemic, the longitudinal markers often track disease progression in multiple organ systems. In this paper, our goal is to predict a function of time that models the future trajectory of a single target clinical marker tracking a disease process of interest. We want to make these predictions using the histories of many related clinical markers as input. Our proposed solution tackles several key challenges. First, we can easily handle irregularly and sparsely sampled markers, which are standard in clinical data. Second, the number of parameters and the computational complexity of learning our model grows linearly in the number of marker types included in the model. This makes our approach applicable to diseases where many different markers are recorded over time. Finally, our model accounts for latent factors influencing disease expression, whereas standard regression models rely on observed features alone to explain variability. Moreover, our approach can be applied dynamically in continous- time and updates its predictions as soon as any new data is available. We apply our approach to the problem of predicting lung disease trajectories in scleroderma, a complex autoimmune disease. We show that our model improves over state-of-the-art baselines in predictive accuracy and we provide a qualitative analysis of our model's output. Finally, the variability of disease presentation in scleroderma makes clinical trial recruitment challenging. We show that a prognostic tool that integrates multiple types of routinely collected longitudinal data can be used to identify individuals at greatest risk of rapid progression and to target trial recruitment.",
            "keywords": [
                "gaussian processes",
                "conditional random fields",
                "prediction of functional targets",
                "latent variable models",
                "disease trajectories"
            ],
            "author": [
                "Peter Schulam",
                "Suchi Saria"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-436/15-436.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Error Bound for L1-norm Support Vector Machine Coefficients in Ultra-high Dimension",
            "abstract": "Comparing with the standard -norm support vector machine (SVM), the -norm SVM enjoys the nice property of simultaneously preforming classification and feature selection. In this paper, we investigate the statistical performance of -norm SVM in ultra-high dimension, where the number of features  grows at an exponential rate of the sample size . Different from existing theory for SVM which has been mainly focused on the generalization error rates and empirical risk, we study the asymptotic behavior of the coefficients of -norm SVM. Our analysis reveals that the -norm SVM coefficients achieve near oracle rate, that is, with high probability, the  error bound of the estimated -norm SVM coefficients is of order , where  is the number of features with nonzero coefficients. Furthermore, we show that if the -norm SVM is used as an initial value for a recently proposed algorithm for solving non- convex penalized SVM (Zhang et al., 2016b), then in two iterative steps it is guaranteed to produce an estimator that possesses the oracle property in ultra-high dimension, which in particular implies that with probability approaching one the zero coefficients are estimated as exactly zero. Simulation studies demonstrate the fine performance of -norm SVM as a sparse classifier and its effectiveness to be utilized to solve non-convex penalized SVM problems in high dimension.",
            "keywords": [
                "feature selection"
            ],
            "author": [
                "Bo Peng",
                "Lan Wang",
                "Yichao Wu"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-654/15-654.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Blending Learning and Inference in Conditional Random Fields",
            "abstract": "Conditional random fields maximize the log-likelihood of training labels given the training data, e.g., objects given images. In many cases the training labels are structures that consist of a set of variables and the computational complexity for estimating their likelihood is exponential in the number of the variables. Learning algorithms relax this computational burden using approximate inference that is nested as a sub- procedure. In this paper we describe the objective function for nested learning and inference in conditional random fields. The devised objective maximizes the log-beliefs --- probability distributions over subsets of training variables that agree on their marginal probabilities. This objective is concave and consists of two types of variables that are related to the learning and inference tasks respectively. Importantly, we afterwards show how to blend the learning and inference procedure and effectively get to the identical optimum much faster. The proposed algorithm currently achieves the state-of- the-art in various computer vision applications.",
            "keywords": [],
            "author": [
                "Tamir Hazan",
                "Alexander G. Schwing",
                "Raquel Urtasun"
            ],
            "ref": "http://jmlr.org/papers/volume17/13-260/13-260.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Submodular Maximization",
            "abstract": "Many large-scale machine learning problems--clustering, non- parametric learning, kernel machines, etc.--require selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Classical approaches to submodular optimization require centralized access to the full dataset, which is impractical for truly large-scale problems. In this paper, we consider the problem of submodular function maximization in a distributed fashion. We develop a simple, two- stage protocol GREEDI, that is easily implemented using MapReduce style computations. We theoretically analyze our approach, and show that under certain natural conditions, performance close to the centralized approach can be achieved. We begin with monotone submodular maximization subject to a cardinality constraint, and then extend this approach to obtain approximation guarantees for (not necessarily monotone) submodular maximization subject to more general constraints including matroid or knapsack constraints. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including sparse Gaussian process inference and exemplar based clustering on tens of millions of examples using Hadoop.",
            "keywords": [
                "distributed computing",
                "submodular functions",
                "approximation algorithms",
                "greedy algorithms"
            ],
            "author": [
                "Baharan Mirzasoleiman",
                "Amin Karbasi",
                "Rik Sarkar",
                "Andreas Krause"
            ],
            "ref": "http://jmlr.org/papers/volume17/mirzasoleiman16a/mirzasoleiman16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the properties of variational approximations of Gibbs posteriors",
            "abstract": "The PAC-Bayesian approach is a powerful set of techniques to derive non-asymptotic risk bounds for random estimators. The corresponding optimal distribution of estimators, usually called the Gibbs posterior, is unfortunately often intractable. One may sample from it using Markov chain Monte Carlo, but this is usually too slow for big datasets. We consider instead variational approximations of the Gibbs posterior, which are fast to compute. We undertake a general study of the properties of such approximations. Our main finding is that such a variational approximation has often the same rate of convergence as the original PAC-Bayesian procedure it approximates. In addition, we show that, when the risk function is convex, a variational approximation can be obtained in polynomial time using a convex solver. We give finite sample oracle inequalities for the corresponding estimator. We specialize our results to several learning tasks (classification, ranking, matrix completion), discuss how to implement a variational approximation in each case, and illustrate the good properties of said approximation on real datasets.",
            "keywords": [],
            "author": [
                "Pierre Alquier",
                "James Ridgway",
                "Nicolas Chopin"
            ],
            "ref": "http://jmlr.org/papers/volume17/15-290/15-290.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Averaged Collapsed Variational Bayes Inference",
            "abstract": "This paper presents the Averaged CVB (ACVB) inference and offers convergence-guaranteed and practically useful fast Collapsed Variational Bayes (CVB) inferences. CVB inferences yield more precise inferences of Bayesian probabilistic models than Variational Bayes (VB) inferences. However, their convergence aspect is fairly unknown and has not been scrutinized. To make CVB more useful, we study their convergence behaviors in a empirical and practical approach. We develop a convergence- guaranteed algorithm for any CVB-based inference called ACVB, which enables automatic convergence detection and frees non- expert practitioners from the difficult and costly manual monitoring of inference processes. In experiments, ACVB inferences are comparable to or better than those of existing inference methods and deterministic, fast, and provide easier convergence detection. These features are especially convenient for practitioners who want precise Bayesian inference with assured convergence.",
            "keywords": [
                "nonparametric Bayes",
                "collapsed variational Bayes inference"
            ],
            "author": [
                "Katsuhiko Ishiguro",
                "Issei Sato",
                "Naonori Ueda"
            ],
            "ref": "http://jmlr.org/papers/volume18/14-249/14-249.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Influence Maximization for Multiple Products in Continuous-Time Diffusion Networks",
            "abstract": "A typical viral marketing model identifies influential users in a social network to maximize a single product adoption assuming unlimited user attention, campaign budgets, and time. In reality, multiple products need campaigns, users have limited attention, convincing users incurs costs, and advertisers have limited budgets and expect the adoptions to be maximized soon. Facing these user, monetary, and timing constraints, we formulate the problem as a submodular maximization task in a continuous-time diffusion model under the intersection of one matroid and multiple knapsack constraints. We propose a randomized algorithm estimating the user influence (Partial results in the paper on influence estimation have been published in a conference paper: Nan Du, Le Song, Manuel Gomez-Rodriguez, and Hongyuan Zha. Scalable influence estimation in continuous time diffusion networks. In Advances in Neural Information Processing Systems 26, 2013.) in a network ( nodes,  edges) to an accuracy of  with  randomizations and  computations. By exploiting the influence estimation algorithm as a subroutine, we develop an adaptive threshold greedy algorithm achieving an approximation factor  of the optimal when  out of the  knapsack constraints are active. Extensive experiments on networks of millions of nodes demonstrate that the proposed algorithms achieve the state-of- the-art in terms of effectiveness and scalability.",
            "keywords": [
                "Influence Maximization",
                "Influence Estimation",
                "Continuous-time Diffusion     Model",
                "Matroid"
            ],
            "author": [
                "Nan Du",
                "Yingyu Liang",
                "Maria-Florina Balcan",
                "Manuel Gomez-Rodriguez",
                "Hongyuan Zha",
                "Le Song"
            ],
            "ref": "http://jmlr.org/papers/volume18/14-400/14-400.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local algorithms for interactive clustering",
            "abstract": "We study the design of interactive clustering algorithms. The user supervision that we consider is in the form of cluster split/merge requests; such feedback is easy for users to provide because it only requires a high-level understanding of the clusters. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable properties in many applications. Local changes are desirable because in practice edits of other parts of the clustering are considered churn - changes that are perceived as quality-neutral or quality-negative. We show that in this framework we can still design provably correct algorithms given that our data satisfies natural separability properties. We also show that our framework works well in practice.",
            "keywords": [],
            "author": [
                "Pranjal Awasthi",
                "Maria Florina Balcan",
                "Konstantin Voevodski"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-085/15-085.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Communication-efficient Sparse Regression",
            "abstract": "We devise a communication-efficient approach to distributed sparse regression in the high-dimensional setting. The key idea is to average debiased or desparsified lasso estimators. We show the approach converges at the same rate as the lasso as long as the dataset is not split across too many machines, and consistently estimates the support under weaker conditions than the lasso. On the computational side, we propose a new parallel and computationally-efficient algorithm to compute the approximate inverse covariance required in the debiasing approach, when the dataset is split across samples. We further extend the approach to generalized linear models.",
            "keywords": [
                "Distributed Sparse Regression",
                "Averaging",
                "Debiasing",
                "lasso"
            ],
            "author": [
                "Jason D. Lee",
                "Qiang Liu",
                "Yuekai Sun",
                "Jonathan E. Taylor"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-002/16-002.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improving Variational Methods via Pairwise Linear Response Identities",
            "abstract": "Inference methods are often formulated as variational approximations: these approxima- tions allow easy evaluation of statistics by marginalization or linear response, but these estimates can be inconsistent. We show that by introducing constraints on covariance, one can ensure consistency of linear response with the variational parameters, and in so doing inference of marginal probability distributions is improved. For the Bethe approximation and its generalizations, improvements are achieved with simple choices of the constraints. The approximations are presented as variational frameworks; iterative procedures related to message passing are provided for finding the minima.",
            "keywords": [
                "variational inference",
                "graphical models",
                "message passing algorithms",
                "statisti-     cal physics"
            ],
            "author": [
                "Jack Raymond",
                "Federico Ricci-Tersenghi"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-070/16-070.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks",
            "abstract": "Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low- dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs.",
            "keywords": [
                "short-term memory",
                "recurrent neural networks",
                "sparse signal recovery",
                "low-rank re-     covery",
                "restricted isometry propertyc 2017 Adam Charles",
                "Dong Yin"
            ],
            "author": [
                "Adam S. Charles",
                "Dong Yin",
                "Christopher J. Rozell"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-270/16-270.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Persistence Images: A Stable Vector Representation of Persistent Homology",
            "abstract": "Many data sets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a data set. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finite- dimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the linked twist map) and a partial differential equation (the anisotropic Kuramoto-Sivashinsky equation) provide a novel application of the discriminatory power of PIs.",
            "keywords": [],
            "author": [
                "Henry Adams",
                "Tegan Emerson",
                "Michael Kirby",
                "Rachel Neville",
                "Chris Peterson",
                "Patrick Shipman",
                "Sofya Chepushtanova",
                "Eric Hanson",
                "Francis Motta",
                "Lori Ziegelmeier"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-337/16-337.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral Clustering Based on Local PCA",
            "abstract": "We propose a spectral clustering method based on local principal components analysis (PCA). After performing local PCA in selected neighborhoods, the algorithm builds a nearest neighbor graph weighted according to a discrepancy between the principal subspaces in the neighborhoods, and then applies spectral clustering. As opposed to standard spectral methods based solely on pairwise distances between points, our algorithm is able to resolve intersections. We establish theoretical guarantees for simpler variants within a prototypical mathematical framework for multi-manifold clustering, and evaluate our algorithm on various simulated data sets.",
            "keywords": [
                "multi-manifold clustering",
                "spectral clustering",
                "local principal component     analysis"
            ],
            "author": [
                "Ery Arias-Castro",
                "Gilad Lerman",
                "Teng Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume18/14-318/14-318.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Perturbed Proximal Gradient Algorithms",
            "abstract": "We study a version of the proximal gradient algorithm for which the gradient is intractable and is approximated by Monte Carlo methods (and in particular Markov Chain Monte Carlo). We derive conditions on the step size and the Monte Carlo batch size under which convergence is guaranteed: both increasing batch size and constant batch size are considered. We also derive non- asymptotic bounds for an averaged version. Our results cover both the cases of biased and unbiased Monte Carlo approximation. To support our findings, we discuss the inference of a sparse generalized linear model with random effect and the problem of learning the edge structure and parameters of sparse undirected graphical models.",
            "keywords": [],
            "author": [
                "Yves F. Atchadé",
                "Gersende Fort",
                "Eric Moulines"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-038/15-038.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Differential Privacy for Bayesian Inference through Posterior Sampling",
            "abstract": "Differential privacy formalises privacy-preserving mechanisms that provide access to a database. Can Bayesian inference be used directly to provide private access to data? The answer is yes: under certain conditions on the prior, sampling from the posterior distribution can lead to a desired level of privacy and utility. For a uniform treatment, we define differential privacy over arbitrary data set metrics, outcome spaces and distribution families. This allows us to also deal with non-i.i.d or non-tabular data sets. We then prove bounds on the sensitivity of the posterior to the data, which delivers a measure of robustness. We also show how to use posterior sampling to provide differentially private responses to queries, within a decision-theoretic framework. Finally, we provide bounds on the utility of answers to queries and on the ability of an adversary to distinguish between data sets. The latter are complemented by a novel use of Le Cam's method to obtain lower bounds on distinguishability. Our results hold for arbitrary metrics, including those for the common definition of differential privacy. For specific choices of the metric, we give a number of examples satisfying our assumptions.",
            "keywords": [],
            "author": [
                "Christos Dimitrakakis",
                "Blaine Nelson",
                "Zuhe Zhang",
                "Aikaterini Mitrokotsa",
                "Benjamin I. P. Rubinstein"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-257/15-257.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Refinery: An Open Source Topic Modeling Web Platform",
            "abstract": "We introduce Refinery, an open source platform for exploring large text document collections with topic models. Refinery is a standalone web application driven by a graphical interface, so it is usable by those without machine learning or programming expertise. Users can interactively organize articles by topic and also refine this organization with phrase-level analysis. Under the hood, we train Bayesian nonparametric topic models that can adapt model complexity to the provided data with scalable learning algorithms.  The project website  contains Python code and further documentation.",
            "keywords": [
                "topic models",
                "visualization"
            ],
            "author": [
                "Daeil Kim",
                "Benjamin F. Swanson",
                "Michael C. Hughes",
                "Erik B. Sudderth"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-441/15-441.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Using Conceptors to Manage Neural Long-Term Memories for Temporal Patterns",
            "abstract": "Biological brains can learn, recognize, organize, and re- generate large repertoires of temporal patterns. Here I propose a mechanism of neurodynamical pattern learning and representation, called conceptors, which offers an integrated account of a number of such phenomena and functionalities. It becomes possible to store a large number of temporal patterns in a single recurrent neural network. In the recall process, stored patterns can be morphed and focussed. Parametric families of patterns can be learnt from a very small number of examples. Stored temporal patterns can be content- addressed in ways that are analog to recalling static patterns in Hopfield networks.",
            "keywords": [
                "Recurrent neural network",
                "temporal pattern learning",
                "neural long-term mem-     ory"
            ],
            "author": [
                "Herbert Jaeger"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-449/15-449.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unified Formulation and Fast Accelerated Proximal Gradient Method for Classification",
            "abstract": "Binary classification is the problem of predicting the class a given sample belongs to. To achieve a good prediction performance, it is important to find a suitable model for a given dataset. However, it is often time consuming and impractical for practitioners to try various classification models because each model employs a different formulation and algorithm. The difficulty can be mitigated if we have a unified formulation and an efficient universal algorithmic framework for various classification models to expedite the comparison of performance of different models for a given dataset. In this paper, we present a unified formulation of various classification models (including -SVM, -SVM, -SVM, MM-FDA, MM-MPM, logistic regression, distance weighted discrimination) and develop a general optimization algorithm based on an accelerated proximal gradient (APG) method for the formulation. We design various techniques such as backtracking line search and adaptive restarting strategy in order to speed up the practical convergence of our method. We also give a theoretical convergence guarantee for the proposed fast APG algorithm. Numerical experiments show that our algorithm is stable and highly competitive to specialized algorithms designed for specific models (e.g., sequential minimal optimization (SMO) for SVM).",
            "keywords": [
                "restarted accelerated proximal gradient method",
                "binary classification",
                "mini-     mum norm problem",
                "vector projection computation"
            ],
            "author": [
                "Naoki Ito",
                "Akiko Takeda",
                "Kim-Chuan Toh"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-274/16-274.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning",
            "abstract": "imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the- art methods can be categorized into 4 groups: (i) under- sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods. The proposed toolbox depends only on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn  and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. Source code, binaries, and documentation can be downloaded from  github.com/scikit-learn-contrib/imbalanced-learn.",
            "keywords": [
                "Imbalanced Dataset",
                "Over-Sampling",
                "Under-Sampling",
                "Ensemble Learning",
                "Machine Learning"
            ],
            "author": [
                "Guillaume  Lemaître",
                "Fernando Nogueira",
                "Christos K. Aridas"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-365/16-365.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles",
            "abstract": "",
            "keywords": [],
            "author": [
                "Yann Ollivier",
                "Ludovic Arnold",
                "Anne Auger",
                "Nikolaus Hansen"
            ],
            "ref": "http://jmlr.org/papers/volume18/14-467/14-467.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Memory Efficient Kernel Approximation",
            "abstract": "Scaling kernel machines to massive data sets is a major challenge due to storage and computation issues in handling large kernel matrices, that are usually dense. Recently, many papers have suggested tackling this problem by using a low-rank approximation of the kernel matrix. In this paper, we first make the observation that the structure of shift-invariant kernels changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter. Based on this observation, we propose a new kernel approximation framework -- Memory Efficient Kernel Approximation (MEKA), which considers both low-rank and clustering structure of the kernel matrix. We show that the resulting algorithm outperforms state-of-the-art low-rank kernel approximation methods in terms of speed, approximation error, and memory usage. As an example, on the covtype dataset with half a million samples, MEKA takes around 70 seconds and uses less than 80 MB memory on a single machine to achieve 10% relative approximation error, while standard NystrÃ¶m approximation is about 6 times slower and uses more than 400MB memory to achieve similar approximation. We also present extensive experiments on applying MEKA to speed up kernel ridge regression.",
            "keywords": [
                "kernel approximation"
            ],
            "author": [
                "Si Si",
                "Cho-Jui Hsieh",
                "Inderjit S. Dhillon"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-025/15-025.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions",
            "abstract": "We show that kernel-based quadrature rules for computing integrals can be seen as a special case of random feature expansions for positive definite kernels, for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent and identically distributed samples from a specific non-uniform distribution, while the lower bound if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in - and -norm that match known results for special cases. Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipshitz-continuous losses.",
            "keywords": [
                "Quadrature",
                "positive-definite kernels"
            ],
            "author": [
                "Francis Bach"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-178/15-178.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analyzing Tensor Power Method Dynamics in Overcomplete Regime",
            "abstract": "We present a novel analysis of the dynamics of tensor power iterations in the overcomplete regime where the tensor CP rank is larger than the input dimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in general. We consider the case where the tensor components are randomly drawn, and show that the simple power iteration recovers the components with bounded error under mild initialization conditions. We apply our analysis to unsupervised learning of latent variable models, such as multi-view mixture models and spherical Gaussian mixtures. Given the third order moment tensor, we learn the parameters using tensor power iterations. We prove it can correctly learn the model parameters when the number of hidden components  is much larger than the data dimension , up to . We initialize the power iterations with data samples and prove its success under mild conditions on the signal-to-noise ratio of the samples. Our analysis significantly expands the class of latent variable models where spectral methods are applicable. Our analysis also deals with noise in the input tensor leading to sample complexity result in the application to learning latent variable models.",
            "keywords": [
                "tensor decomposition",
                "tensor power iteration",
                "overcomplete representation",
                "unsupervised learning",
                "latent variable modelsc 2017 Animashree Anandkumar",
                "Rong Ge"
            ],
            "author": [
                "Animashree An",
                "kumar",
                "Rong Ge",
                "Majid Janzamin"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-486/15-486.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "JSAT: Java Statistical Analysis Tool, a Library for Machine Learning",
            "abstract": "Java Statistical Analysis Tool (JSAT) is a Machine Learning library written in pure Java. It works to fill a void in the Java ecosystem for a general purpose library that is relatively high performance and flexible, which is not adequately fulfilled by Weka (Hall et al., 2009) and Java-ML (Abeel et al., 2009). Almost all of the algorithms are independently implemented using an Object- Oriented framework. JSAT is made available under the GNU GPL license here:  github.com/EdwardRaff/JSAT.",
            "keywords": [
                "java",
                "machine learning",
                "open source",
                "java library"
            ],
            "author": [
                "Edward Raff"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-131/16-131.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Identifying a Minimal Class of Models for High--dimensional Data",
            "abstract": "Model selection consistency in the high--dimensional regression setting can be achieved only if strong assumptions are fulfilled. We therefore suggest to pursue a different goal, which we call a minimal class of models. The minimal class of models includes models that are similar in their prediction accuracy but not necessarily in their elements. We suggest a random search algorithm to reveal candidate models. The algorithm implements simulated annealing while using a score for each predictor that we suggest to derive using a combination of the lasso and the elastic net. The utility of using a minimal class of models is demonstrated in the analysis of two data sets.",
            "keywords": [],
            "author": [
                "Daniel Nevo",
                "Ya'acov Ritov"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-172/16-172.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA",
            "abstract": "WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA's learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method. Our new package is tightly integrated with WEKA, making it just as accessible to end users as any other learning algorithm.",
            "keywords": [
                "Hyperparameter Optimization",
                "Model Selection"
            ],
            "author": [
                "Lars Kotthoff",
                "Chris Thornton",
                "Holger H. Hoos",
                "Frank Hutter",
                "Kevin Leyton-Brown"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-261/16-261.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "POMDPs.jl: A Framework for Sequential Decision Making under Uncertainty",
            "abstract": "POMDPs.jl is an open-source framework for solving Markov decision processes (MDPs) and partially observable MDPs (POMDPs). POMDPs.jl allows users to specify sequential decision making problems with minimal effort without sacrificing the expressive nature of POMDPs, making this framework viable for both educational and research purposes. It is written in the Julia language to allow flexible prototyping and large-scale computation that leverages the high-performance nature of the language. The associated JuliaPOMDP community also provides a number of state-of-the-art MDP and POMDP solvers and a rich library of support tools to help with implementing new solvers and evaluating the solution results. The most recent version of POMDPs.jl, the related packages, and documentation can be found at github.com/ JuliaPOMDP/POMDPs.jl.",
            "keywords": [
                "POMDP",
                "MDP",
                "sequential decision making",
                "Julia"
            ],
            "author": [
                "Maxim Egorov",
                "Zachary N. Sunberg",
                "Edward Balaban",
                "Tim A. Wheeler",
                "Jayesh K. Gupta",
                "Mykel J. Kochenderfer"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-300/16-300.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized P{\\'o}lya Urn for Time-Varying Pitman-Yor Processes",
            "abstract": "This article introduces a class of first-order stationary time- varying Pitman-Yor processes. Subsuming our construction of time-varying Dirichlet processes presented in (Caron et al., 2007), these models can be used for time-dynamic density estimation and clustering. Our intuitive and simple construction relies on a generalized PÃ³lya urn scheme. Significantly, this construction yields marginal distributions at each time point that can be explicitly characterized and easily controlled. Inference is performed using Markov chain Monte Carlo and sequential Monte Carlo methods. We demonstrate our models and algorithms on epidemiological and video tracking data.",
            "keywords": [
                "Bayesian nonparametrics",
                "clustering",
                "mixture models",
                "sequential Monte       Carlo",
                "particle Markov chain Monte Carlo"
            ],
            "author": [
                "François Caron",
                "Willie Neiswanger",
                "Frank Wood",
                "Arnaud Doucet",
                "Manuel Davy"
            ],
            "ref": "http://jmlr.org/papers/volume18/10-231/10-231.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Particle Gibbs Split-Merge Sampling for Bayesian Inference in Mixture Models",
            "abstract": "This paper presents an original Markov chain Monte Carlo method to sample from the posterior distribution of conjugate mixture models. This algorithm relies on a flexible split-merge procedure built using the particle Gibbs sampler introduced in Andrieu et al. (2009, 2010). The resulting so-called Particle Gibbs Split-Merge sampler does not require the computation of a complex acceptance ratio and can be implemented using existing sequential Monte Carlo libraries. We investigate its performance experimentally on synthetic problems as well as on geolocation data. Our results show that for a given computational budget, the Particle Gibbs Split-Merge sampler empirically outperforms existing split merge methods. The code and instructions allowing to reproduce the experiments is available at github.com/aroth85/pgsm.",
            "keywords": [],
            "author": [
                "Alexandre Bouchard-Côté",
                "Arnaud Doucet",
                "Andrew Roth"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-397/15-397.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Certifiably Optimal Low Rank Factor Analysis",
            "abstract": "Factor Analysis (FA) is a technique of fundamental importance that is widely used in classical and modern multivariate statistics, psychometrics, and econometrics. In this paper, we revisit the classical rank-constrained FA problem which seeks to approximate an observed covariance matrix () by the sum of a Positive Semidefinite (PSD) low-rank component () and a diagonal matrix () (with nonnegative entries) subject to  being PSD. We propose a flexible family of rank-constrained, nonlinear Semidefinite Optimization based formulations for this task. We introduce a reformulation of the problem as a smooth optimization problem with convex, compact constraints and propose a unified algorithmic framework, utilizing state of the art techniques in nonlinear optimization to obtain high-quality feasible solutions for our proposed formulation. At the same time, by using a variety of techniques from discrete and global optimization, we show that these solutions are certifiably optimal in many cases, even for problems with thousands of variables. Our techniques are general and make no assumption on the underlying problem data. The estimator proposed herein aids statistical interpretability and provides computational scalability and significantly improved accuracy when compared to current, publicly available popular methods for rank-constrained FA. We demonstrate the effectiveness of our proposal on an array of synthetic and real-life datasets. To our knowledge, this is the first paper that demonstrates how a previously intractable rank-constrained optimization problem can be solved to provable optimality by coupling developments in convex analysis and in global and discrete optimization.",
            "keywords": [
                "factor analysis",
                "rank minimization",
                "semidefinite optimization",
                "first order     methods",
                "nonlinear optimization",
                "global optimization"
            ],
            "author": [
                "Dimitris Bertsimas",
                "Martin S. Copenhaver",
                "Rahul Mazumder"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-613/15-613.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Group Sparse Optimization via lp,q Regularization",
            "abstract": "In this paper, we investigate a group sparse optimization problem via  regularization in three aspects: theory, algorithm and application. In the theoretical aspect, by introducing a notion of group restricted eigenvalue condition, we establish an oracle property and a global recovery bound of order  for any point in a level set of the  regularization problem, and by virtue of modern variational analysis techniques, we also provide a local analysis of recovery bound of order  for a path of local minima. In the algorithmic aspect, we apply the well-known proximal gradient method to solve the  regularization problems, either by analytically solving some specific  regularization subproblems, or by using the Newton method to solve general  regularization subproblems. In particular, we establish a local linear convergence rate of the proximal gradient method for solving the  regularization problem under some mild conditions and by first proving a second-order growth condition. As a consequence, the local linear convergence rate of proximal gradient method for solving the usual  regularization problem () is obtained. Finally in the aspect of application, we present some numerical results on both the simulated data and the real data in gene transcriptional regulation.",
            "keywords": [],
            "author": [
                "Yaohua Hu",
                "Chong Li",
                "Kaiwen Meng",
                "Jing Qin",
                "Xiaoqi Yang"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-651/15-651.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Preference-based Teaching",
            "abstract": "We introduce a new model of teaching named preference-based teaching and a corresponding complexity parameter---the preference-based teaching dimension (PBTD)---representing the worst-case number of examples needed to teach any concept in a given concept class. Although the PBTD coincides with the well- known recursive teaching dimension (RTD) on finite classes, it is radically different on infinite ones: the RTD becomes infinite already for trivial infinite classes (such as half- intervals) whereas the PBTD evaluates to reasonably small values for a wide collection of infinite classes including classes consisting of so-called closed sets w.r.t. a given closure operator, including various classes related to linear sets over  (whose RTD had been studied quite recently) and including the class of Euclidean half-spaces. On top of presenting these concrete results, we provide the reader with a theoretical framework (of a combinatorial flavor) which helps to derive bounds on the PBTD.",
            "keywords": [
                "teaching dimension",
                "preference relation",
                "recursive teaching dimension",
                "learning half-     spaces"
            ],
            "author": [
                "Ziyuan Gao",
                "Christoph Ries",
                "Hans U. Simon",
                "S",
                "ra Zilles"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-460/16-460.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Bayesian Passive-Aggressive Learning",
            "abstract": "We present online Bayesian Passive-Aggressive (BayesPA) learning, a generic online learning framework for hierarchical Bayesian models with max-margin posterior regularization. We show that BayesPA subsumes the standard online Passive- Aggressive (PA) learning and extends naturally to incorporate latent variables for both parametric and nonparametric Bayesian inference, therefore providing great flexibility for explorative analysis. As an important example, we apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric BayesPA topic models to infer the unknown number of topics in an online manner. Experimental results on 20newsgroups and a large Wikipedia multi-label dataset (with 1.1 millions of training documents and 0.9 million of unique terms in the vocabulary) show that our approaches significantly improve time efficiency while achieving comparable accuracy with the corresponding batch algorithms.",
            "keywords": [],
            "author": [
                "Tianlin Shi",
                "Jun Zhu"
            ],
            "ref": "http://jmlr.org/papers/volume18/14-188/14-188.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotic Analysis of Objectives Based on Fisher Information in Active Learning",
            "abstract": "Obtaining labels can be costly and time-consuming. Active learning allows a learning algorithm to intelligently query samples to be labeled for a more efficient learning. Fisher information ratio (FIR) has been used as an objective for selecting queries. However, little is known about the theory behind the use of FIR for active learning. There is a gap between the underlying theory and the motivation of its usage in practice. In this paper, we attempt to fill this gap and provide a rigorous framework for analyzing existing FIR-based active learning methods. In particular, we show that FIR can be asymptotically viewed as an upper bound of the expected variance of the log-likelihood ratio. Additionally, our analysis suggests a unifying framework that not only enables us to make theoretical comparisons among the existing querying methods based on FIR, but also allows us to give insight into the development of new active learning approaches based on this objective.",
            "keywords": [
                "classification active learning",
                "Fisher information ratio",
                "asymptotic log-loss"
            ],
            "author": [
                "Jamshid Sourati",
                "Murat Akcakaya",
                "Todd K. Leen",
                "Deniz Erdogmus",
                "Jennifer G. Dy"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-104/15-104.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Spectral Algorithm for Inference in Hidden semi-Markov Models",
            "abstract": "Hidden semi-Markov models (HSMMs) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models (HMMs). In this paper, we introduce a novel spectral algorithm to perform inference in HSMMs. Unlike expectation maximization (EM), our approach correctly estimates the probability of given observation sequence based on a set of training sequences. Our approach is based on estimating moments from the sample, whose number of dimensions depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few matrix inversions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the advantage of the algorithm over EM in terms of speed and accuracy, especially for large data sets.",
            "keywords": [
                "Graphical models",
                "hidden semi-Markov model",
                "spectral algorithm",
                "tensor analysis"
            ],
            "author": [
                "Igor Melnyk",
                "Arindam Banerjee"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-468/15-468.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simplifying Probabilistic Expressions in Causal Inference",
            "abstract": "Obtaining a non-parametric expression for an interventional distribution is one of the most fundamental tasks in causal inference. Such an expression can be obtained for an identifiable causal effect by an algorithm or by manual application of do-calculus. Often we are left with a complicated expression which can lead to biased or inefficient estimates when missing data or measurement errors are involved. We present an automatic simplification algorithm that seeks to eliminate symbolically unnecessary variables from these expressions by taking advantage of the structure of the underlying graphical model. Our method is applicable to all causal effect formulas and is readily available in the R package causaleffect.",
            "keywords": [
                "simplification",
                "probabilistic expression",
                "causal inference",
                "graphical model"
            ],
            "author": [
                "Santtu Tikka",
                "Juha Karvanen"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-166/16-166.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nearly optimal classification for semimetrics",
            "abstract": "We initiate the rigorous study of classification in semimetric spaces, which are point sets with a distance function that is non-negative and symmetric, but need not satisfy the triangle inequality. We define the density dimension dens and discover that it plays a central role in the statistical and algorithmic feasibility of learning in semimetric spaces. We compute this quantity for several widely used semimetrics and present nearly optimal sample compression algorithms, which are then used to obtain generalization guarantees, including fast rates. Our claim of near-optimality holds in both computational and statistical senses. When the sample has radius  and margin , we show that it can be compressed down to roughly  points, and further that finding a significantly better compression is algorithmically intractable unless P=NP. This compression implies generalization via standard Occam-type arguments, to which we provide a nearly matching lower bound.",
            "keywords": [
                "semimetric",
                "classification",
                "compression"
            ],
            "author": [
                "Lee-Ad Gottlieb",
                "Aryeh Kontorovich",
                "Pinhas Nisnevitch"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-217/16-217.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bridging Supervised Learning and Test-Based Co-optimization",
            "abstract": "This paper takes a close look at the important commonalities and subtle differences between the well-established field of supervised learning and the much younger one of co-optimization. It explains the relationships between the problems, algorithms and views on cost and performance of the two fields, all throughout providing a two-way dictionary for the respective terminologies used to describe these concepts. The intent is to facilitate advancement of both fields through transfer and cross-pollination of ideas, techniques and results. As a proof of concept, a theoretical study is presented on the connection between existence / lack of free lunch in the two fields, showcasing a few ideas for improving computational complexity of certain supervised learning approaches.",
            "keywords": [
                "supervised learning",
                "active learning",
                "co-optimization",
                "free lunch"
            ],
            "author": [
                "Elena Popovici"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-223/16-223.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GFA: Exploratory Analysis of Multiple Data Sources with Group Factor Analysis",
            "abstract": "The R package GFA provides a full pipeline for factor analysis of multiple data sources that are represented as matrices with co-occurring samples. It allows learning dependencies between subsets of the data sources, decomposed into latent factors. The package also implements sparse priors for the factorization, providing interpretable biclusters of the multi-source data.",
            "keywords": [
                "Bayesian latent variable modelling",
                "biclustering",
                "data integration",
                "factor     analysis"
            ],
            "author": [
                "Eemeli Leppäaho",
                "Muhammad Ammad-ud-din",
                "Samuel Kaski"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-509/16-509.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GPflow: A Gaussian Process Library using TensorFlow",
            "abstract": "GPflow is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. The distinguishing features of GPflow are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware.",
            "keywords": [],
            "author": [
                "Alexander G. de G. Matthews",
                "Mark van der Wilk",
                "Tom Nickson",
                "Keisuke Fujii",
                "Alexis Boukouvalas",
                "Pablo Le{\\'o}n-Villagr{\\'a}",
                "Zoubin Ghahramani",
                "James Hensman"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-537/16-537.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Local Dependence In Ordered Data",
            "abstract": "In many applications, data come with a natural ordering. This ordering can often induce local dependence among nearby variables. However, in complex data, the width of this dependence may vary, making simple assumptions such as a constant neighborhood size unrealistic. We propose a framework for learning this local dependence based on estimating the inverse of the Cholesky factor of the covariance matrix. Penalized maximum likelihood estimation of this matrix yields a simple regression interpretation for local dependence in which variables are predicted by their neighbors. Our proposed method involves solving a convex, penalized Gaussian likelihood problem with a hierarchical group lasso penalty. The problem decomposes into independent subproblems which can be solved efficiently in parallel using first-order methods. Our method yields a sparse, symmetric, positive definite estimator of the precision matrix, encoding a Gaussian graphical model. We derive theoretical results not found in existing methods attaining this structure. In particular, our conditions for signed support recovery and estimation consistency rates in multiple norms are as mild as those in a regression problem. Empirical results show our method performing favorably compared to existing methods. We apply our method to genomic data to flexibly model linkage disequilibrium. Our method is also applied to improve the performance of discriminant analysis in sound recording classification.",
            "keywords": [
                "Local dependence",
                "Gaussian graphical models",
                "precision matrices",
                "Cholesky     factor"
            ],
            "author": [
                "Guo Yu",
                "Jacob Bien"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-198/16-198.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Learning of Dynamic Multilayer Networks",
            "abstract": "A plethora of networks is being collected in a growing number of fields, including disease transmission, international relations, social interactions, and others. As data streams continue to grow, the complexity associated with these highly multidimensional connectivity data presents novel challenges. In this paper, we focus on the time-varying interconnections among a set of actors in multiple contexts, called layers. Current literature lacks flexible statistical models for dynamic multilayer networks, which can enhance quality in inference and prediction by efficiently borrowing information within each network, across time, and between layers. Motivated by this gap, we develop a Bayesian nonparametric model leveraging latent space representations. Our formulation characterizes the edge probabilities as a function of shared and layer-specific actors positions in a latent space, with these positions changing in time via Gaussian processes. This representation facilitates dimensionality reduction and incorporates different sources of information in the observed data. In addition, we obtain tractable procedures for posterior computation, inference, and prediction. We provide theoretical results on the flexibility of our model. Our methods are tested on simulations and infection studies monitoring dynamic face-to-face contacts among individuals in multiple days, where we perform better than current methods in inference and prediction.",
            "keywords": [
                "Dynamic multilayer network",
                "edge prediction",
                "face-to-face contact network",
                "Gaussian process"
            ],
            "author": [
                "Daniele Durante",
                "Nabanita Mukherjee",
                "Rebecca C. Steorts"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-391/16-391.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Time-Accuracy Tradeoffs in Kernel Prediction: Controlling Prediction Quality",
            "abstract": "",
            "keywords": [],
            "author": [
                "Samory Kpotufe",
                "Nakul Verma"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-538/16-538.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotic behavior of Support Vector Machine for spiked population model",
            "abstract": "For spiked population model, we investigate the large dimension  and large sample size  asymptotic behavior of the Support Vector Machine (SVM) classification method in the limit of  at fixed . We focus on the generalization performance by analytically evaluating the angle between the normal direction vectors of SVM separating hyperplane and corresponding Bayes optimal separating hyperplane. This is an analogous result to the one shown in Paul (2007) and Nadler (2008) for the angle between the sample eigenvector and the population eigenvector in random matrix theorem. We provide not just bound, but sharp prediction of the asymptotic behavior of SVM that can be determined by a set of nonlinear equations. Based on the analytical results, we propose a new method of selecting tuning parameter which significantly reduces the computational cost. A surprising finding is that SVM achieves its best performance at small value of the tuning parameter under spiked population model. These results are confirmed to be correct by comparing with those of numerical simulations on finite-size systems. We also apply our formulas to an actual dataset of breast cancer and find agreement between analytical derivations and numerical computations based on cross validation.",
            "keywords": [
                "Asymptotic behavior",
                "Spiked population model"
            ],
            "author": [
                "Hanwen Huang"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-564/16-564.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Semi-supervised Learning with Kernel Ridge Regression",
            "abstract": "This paper provides error analysis for distributed semi- supervised learning with kernel ridge regression (DSKRR) based on a divide-and-conquer strategy. DSKRR applies kernel ridge regression (KRR) to data subsets that are distributively stored on multiple servers to produce individual output functions, and then takes a weighted average of the individual output functions as a final estimator. Using a novel error decomposition which divides the generalization error of DSKRR into the approximation error, sample error and distributed error, we find that the sample error and distributed error reflect the power and limitation of DSKRR, compared with KRR processing the whole data. Thus a small distributed error provides a large range of the number of data subsets to guarantee a small generalization error. Our results show that unlabeled data play important roles in reducing the distributed error and enlarging the number of data subsets in DSKRR. Our analysis also applies to the case when the regression function is out of the reproducing kernel Hilbert space. Numerical experiments including toy simulations and a music-prediction task are employed to demonstrate our theoretical statements and show the power of unlabeled data in distributed learning.",
            "keywords": [
                "learning theory",
                "distributed learning",
                "kernel ridge regression",
                "semi-supervised     learning",
                "unlabeled data"
            ],
            "author": [
                "Xiangyu Chang",
                "Shao-Bo Lin",
                "Ding-Xuan Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-601/16-601.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Markov chain Monte Carlo methods for tall data",
            "abstract": "Markov chain Monte Carlo methods are often deemed too computationally intensive to be of any practical use for big data applications, and in particular for inference on datasets containing a large number  of individual data points, also known as tall datasets. In scenarios where data are assumed independent, various approaches to scale up the Metropolis- Hastings algorithm in a Bayesian inference context have been recently proposed in machine learning and computational statistics. These approaches can be grouped into two categories: divide-and-conquer approaches and, subsampling-based algorithms. The aims of this article are as follows. First, we present a comprehensive review of the existing literature, commenting on the underlying assumptions and theoretical guarantees of each method. Second, by leveraging our understanding of these limitations, we propose an original subsampling-based approach relying on a control variate method which samples under regularity conditions from a distribution provably close to the posterior distribution of interest, yet can require less than  data point likelihood evaluations at each iteration for certain statistical models in favourable scenarios. Finally, we emphasize that we have only been able so far to propose subsampling-based methods which display good performance in scenarios where the Bernstein-von Mises approximation of the target posterior distribution is excellent. It remains an open challenge to develop such methods in scenarios where the Bernstein-von Mises approximation is poor.",
            "keywords": [],
            "author": [
                "Rémi Bardenet",
                "Arnaud Doucet",
                "Chris Holmes"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-205/15-205.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers",
            "abstract": "There is a large literature explaining why AdaBoost is a successful classifier. The literature on AdaBoost focuses on classifier margins and boosting's interpretation as the optimization of an exponential likelihood function. These existing explanations, however, have been pointed out to be incomplete. A random forest is another popular ensemble method for which there is substantially less explanation in the literature. We introduce a novel perspective on AdaBoost and random forests that proposes that the two algorithms work for similar reasons. While both classifiers achieve similar predictive accuracy, random forests cannot be conceived as a direct optimization procedure. Rather, random forests is a self- averaging, interpolating algorithm which creates what we denote as a spiked-smooth classifier, and we view AdaBoost in the same light. We conjecture that both AdaBoost and random forests succeed because of this mechanism. We provide a number of examples to support this explanation. In the process, we question the conventional wisdom that suggests that boosting algorithms for classification require regularization or early stopping and should be limited to low complexity classes of learners, such as decision stumps. We conclude that boosting should be used like random forests: with large decision trees, without regularization or early stopping.",
            "keywords": [
                "AdaBoost",
                "random forests",
                "tree-ensembles",
                "overfitting"
            ],
            "author": [
                "Abraham J. Wyner",
                "Matthew Olson",
                "Justin Bleich",
                "David Mease"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-240/15-240.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Clustering from General Pairwise Observations with Applications to Time-varying Graphs",
            "abstract": "We present a general framework for graph clustering and bi- clustering where we are given a general observation (called a label) between each pair of nodes. This framework allows a rich encoding of various types of pairwise interactions between nodes. We propose a new tractable and robust approach to this problem based on convex optimization and maximum likelihood estimators. We analyze our algorithms under a general statistical model extending the planted partition and stochastic block models. Both sufficient and necessary conditions are provided for successful recovery of the underlying clusters. Our theoretical results subsume many existing graph clustering results for a wide range of settings, including planted partition, weighted clustering, submatrix localization and partially observed graphs. Furthermore, our results are applicable to novel settings including time-varying graphs, providing new insights to solutions of these problems. We provide empirical results on both synthetic and real data that corroborate with our theoretical findings.",
            "keywords": [
                "graph clustering",
                "convex optimization",
                "low-rank matrix",
                "information divergence",
                "time-     varying graphs",
                "pairwise observation"
            ],
            "author": [
                "Shiau Hong Lim",
                "Yudong Chen",
                "Huan Xu"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-659/15-659.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling Techniques",
            "abstract": "In a series of recent works, we have generalised the consistency results in the stochastic block model literature to the case of uniform and non-uniform hypergraphs. The present paper continues the same line of study, where we focus on partitioning weighted uniform hypergraphs---a problem often encountered in computer vision. This work is motivated by two issues that arise when a hypergraph partitioning approach is used to tackle computer vision problems: (i) The uniform hypergraphs constructed for higher-order learning contain all edges, but most have negligible weights. Thus, the adjacency tensor is nearly sparse, and yet, not binary. (ii) A more serious concern is that standard partitioning algorithms need to compute all edge weights, which is computationally expensive for hypergraphs. This is usually resolved in practice by merging the clustering algorithm with a tensor sampling strategy---an approach that is yet to be analysed rigorously. We build on our earlier work on partitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati, ICML, 2015), and address the aforementioned issues by proposing provable and efficient partitioning algorithms. Our analysis justifies the empirical success of practical sampling techniques. We also complement our theoretical findings by elaborate empirical comparison of various hypergraph partitioning schemes.",
            "keywords": [
                "Hypergraph partitioning",
                "planted model",
                "spectral method",
                "tensors",
                "sampling"
            ],
            "author": [
                "Debarghya Ghoshdastidar",
                "Ambedkar Dukkipati"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-100/16-100.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with Two-Point Feedback",
            "abstract": "We consider the closely related problems of bandit convex optimization with two-point feedback, and zero-order stochastic convex optimization with two function evaluations per round. We provide a simple algorithm and analysis which is optimal for convex Lipschitz functions. This improves on Duchi et al. (2015), which only provides an optimal result for smooth functions; Moreover, the algorithm and analysis are simpler, and readily extend to non-Euclidean problems. The algorithm is based on a small but surprisingly powerful modification of the gradient estimator.",
            "keywords": [
                "zero-order optimization",
                "bandit optimization",
                "stochastic optimization"
            ],
            "author": [
                "Ohad Shamir"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-632/16-632.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Perishability of Data: Dynamic Pricing under Varying-Coefficient Models",
            "abstract": "",
            "keywords": [
                "Dynamic Pricing",
                "Revenue Management",
                "Varying-Coefficient Models",
                "Regret",
                "Stochastic Gradient Descent"
            ],
            "author": [
                "Adel Javanmard"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-061/17-061.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Two New Approaches to Compressed Sensing Exhibiting Both Robust Sparse Recovery and the Grouping Effect",
            "abstract": "",
            "keywords": [
                "Sparse regression",
                "compressed sensing",
                "LASSO",
                "Sparse Group LASSO",
                "Elastic     Netc 2017 Mehmet Eren Ahsen"
            ],
            "author": [
                "Mehmet Eren Ahsen",
                "Niharika Challapalli",
                "Mathukumalli Vidyasagar"
            ],
            "ref": "http://jmlr.org/papers/volume18/14-453/14-453.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Consistency of Ordinal Regression Methods",
            "abstract": "Many of the ordinal regression models that have been proposed in the literature can be seen as methods that minimize a convex surrogate of the zero-one, absolute, or squared loss functions. A key property that allows to study the statistical implications of such approximations is that of Fisher consistency. Fisher consistency is a desirable property for surrogate loss functions and implies that in the population setting, i.e., if the probability distribution that generates the data were available, then optimization of the surrogate would yield the best possible model. In this paper we will characterize the Fisher consistency of a rich family of surrogate loss functions used in the context of ordinal regression, including support vector ordinal regression, ORBoosting and least absolute deviation. We will see that, for a family of surrogate loss functions that subsumes support vector ordinal regression and ORBoosting, consistency can be fully characterized by the derivative of a real-valued function at zero, as happens for convex margin-based surrogates in binary classification. We also derive excess risk bounds for a surrogate of the absolute error that generalize existing risk bounds for binary classification. Finally, our analysis suggests a novel surrogate of the squared error loss. We compare this novel surrogate with competing approaches on 9 different datasets. Our method shows to be highly competitive in practice, outperforming the least squares loss on 7 out of 9 datasets.",
            "keywords": [
                "Fisher consistency",
                "ordinal regression",
                "calibration",
                "surrogate loss"
            ],
            "author": [
                "Fabian Pedregosa",
                "Francis Bach",
                "Alexandre Gramfort"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-495/15-495.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Inference with Unnormalized Discrete Models and Localized Homogeneous Divergences",
            "abstract": "In this paper, we focus on parameters estimation of probabilistic models in discrete space. A naive calculation of the normalization constant of the probabilistic model on discrete space is often infeasible and statistical inference based on such probabilistic models has difficulty. In this paper, we propose a novel estimator for probabilistic models on discrete space, which is derived from an empirically localized homogeneous divergence. The idea of the empirical localization makes it possible to ignore an unobserved domain on sample space, and the homogeneous divergence is a discrepancy measure between two positive measures and has a weak coincidence axiom. The proposed estimator can be constructed without calculating the normalization constant and is asymptotically consistent and Fisher efficient. We investigate statistical properties of the proposed estimator and reveal a relationship between the empirically localized homogeneous divergence and a mixture of the -divergence. The -divergence is a non- homogeneous discrepancy measure that is frequently discussed in the context of information geometry. Using the relationship, we also propose an asymptotically consistent estimator of the normalization constant. Experiments showed that the proposed estimator comparably performs to the maximum likelihood estimator but with drastically lower computational cost.",
            "keywords": [
                "unnormalized model",
                "homogeneous divergence",
                "empirical localization"
            ],
            "author": [
                "Takashi Takenouchi",
                "Takafumi Kanamori"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-596/15-596.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Density Estimation in Infinite Dimensional Exponential Families",
            "abstract": "In this paper, we consider an infinite dimensional exponential family  of probability densities, which are parametrized by functions in a reproducing kernel Hilbert space , and show it to be quite rich in the sense that a broad class of densities on  can be approximated arbitrarily well in Kullback-Leibler (KL) divergence by elements in . Motivated by this approximation property, the paper addresses the question of estimating an unknown density  through an element in . Standard techniques like maximum likelihood estimation (MLE) or pseudo MLE (based on the method of sieves), which are based on minimizing the KL divergence between  and , do not yield practically useful estimators because of their inability to efficiently handle the log-partition function. We propose an estimator  based on minimizing the Fisher divergence,  between  and , which involves solving a simple finite-dimensional linear system. When , we show that the proposed estimator is consistent, and provide a convergence rate of  in Fisher divergence under the smoothness assumption that  for some , where  is a certain Hilbert-Schmidt operator on  and  denotes the image of . We also investigate the misspecified case of  and show that  as , and provide a rate for this convergence under a similar smoothness condition as above. Through numerical simulations we demonstrate that the proposed estimator outperforms the non- parametric kernel density estimator, and that the advantage of the proposed estimator grows as  increases.",
            "keywords": [],
            "author": [
                "Bharath Sriperumbudur",
                "Kenji Fukumizu",
                "Arthur Gretton",
                "Aapo Hyv\\\"{a}rinen",
                "Revant Kumar"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-011/16-011.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lens Depth Function and k-Relative Neighborhood Graph: Versatile Tools for Ordinal Data Analysis",
            "abstract": "In recent years it has become popular to study machine learning problems in a setting of ordinal distance information rather than numerical distance measurements. By ordinal distance information we refer to binary answers to distance comparisons such as . For many problems in machine learning and statistics it is unclear how to solve them in such a scenario. Up to now, the main approach is to explicitly construct an ordinal embedding of the data points in the Euclidean space, an approach that has a number of drawbacks. In this paper, we propose algorithms for the problems of medoid estimation, outlier identification, classification, and clustering when given only ordinal data. They are based on estimating the lens depth function and the -relative neighborhood graph on a data set. Our algorithms are simple, are much faster than an ordinal embedding approach and avoid some of its drawbacks, and can easily be parallelized.",
            "keywords": [
                "ordinal data",
                "ordinal distance information",
                "comparison-based algorithms",
                "lens depth function",
                "k-relative neighborhood graph",
                "ordinal embedding"
            ],
            "author": [
                "Matthäus Kleindessner",
                "Ulrike von Luxburg"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-061/16-061.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Joint Label Inference in Networks",
            "abstract": "We consider the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Our primary example, and the focus of this paper, is the joint inference of label types such as hometown, current city, and employers for people connected by a social network; by predicting these user profile fields, the network can provide a better experience to its users. Existing approaches such as Label Propagation (Zhu et al., 2003) fail to consider interactions between the label types. Our proposed method, called EDGEEXPLAIN explicitly models these interactions, while still allowing scalable inference under a distributed message- passing architecture. On a large subset of the Facebook social network, collected in a previous study (Chakrabarti et al., 2014), EDGEEXPLAIN outperforms label propagation for several label types, with lifts of up to  for recall@1 and  for recall@3.",
            "keywords": [
                "label inference",
                "graphs",
                "social networks",
                "variational methods"
            ],
            "author": [
                "Deepayan Chakrabarti",
                "Stanislav Funiak",
                "Jonathan Chang",
                "Sofus A. Macskassy"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-214/16-214.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Achieving Optimal Misclassification Proportion in Stochastic Block Models",
            "abstract": "Community detection is a fundamental statistical problem in network data analysis.  In this paper, we present a polynomial time two-stage method that provably achieves optimal statistical performance in misclassification proportion for stochastic block model under weak regularity conditions. Our two-stage procedure consists of a refinement stage motivated by penalized local maximum likelihood estimation. This stage can take a wide range of weakly consistent community detection procedures as its initializer, to which it applies and outputs a community assignment that achieves optimal misclassification proportion with high probability. The theoretical property is confirmed by simulated examples.",
            "keywords": [
                "Clustering",
                "Community detection",
                "Minimax rates",
                "Network analysis"
            ],
            "author": [
                "Chao Gao",
                "Zongming Ma",
                "Anderson Y. Zhang",
                "Harrison H. Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-245/16-245.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Propagation of Low-Rate Measurement Error to Subgraph Counts in Large Networks",
            "abstract": "Our work in this paper is inspired by a statistical observation that is both elementary and broadly relevant to network analysis in practice---that the uncertainty in approximating some true graph  by some estimated graph  manifests as errors in our knowledge of the presence/absence of edges between vertex pairs, which must necessarily propagate to any estimates of network summaries  we seek. Motivated by the common practice of using plug-in estimates  as proxies for , our focus is on the problem of characterizing the distribution of the discrepancy , in the case where  is a subgraph count. Specifically, we study the fundamental case where the statistic of interest is , the number of edges in . Our primary contribution in this paper is to show that in the empirically relevant setting of large graphs with low-rate measurement errors, the distribution of  is well-characterized by a Skellam distribution, when the errors are independent or weakly dependent. Under an assumption of independent errors, we are able to further show conditions under which this characterization is strictly better than that of an appropriate normal distribution. These results derive from our formulation of a general result, quantifying the accuracy with which the difference of two sums of dependent Bernoulli random variables may be approximated by the difference of two independent Poisson random variables, i.e., by a Skellam distribution. This general result is developed through the use of Stein's method, and may be of some general interest. We finish with a discussion of possible extension of our work to subgraph counts  of higher order.",
            "keywords": [
                "Limit distribution",
                "network analysis",
                "Skellam distribution"
            ],
            "author": [
                "Prakash Balach",
                "ran",
                "Eric D. Kolaczyk",
                "Weston D. Viles"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-497/16-497.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dense Distributions from Sparse Samples: Improved Gibbs Sampling Parameter Estimators for LDA",
            "abstract": "We introduce a novel approach for estimating Latent Dirichlet Allocation (LDA) parameters from collapsed Gibbs samples (CGS), by leveraging the full conditional distributions over the latent variable assignments to efficiently average over multiple samples, for little more computational cost than drawing a single additional collapsed Gibbs sample. Our approach can be understood as adapting the soft clustering methodology of Collapsed Variational Bayes (CVB0) to CGS parameter estimation, in order to get the best of both techniques. Our estimators can straightforwardly be applied to the output of any existing implementation of CGS, including modern accelerated variants. We perform extensive empirical comparisons of our estimators with those of standard collapsed inference algorithms on real-world data for both unsupervised LDA and Prior-LDA, a supervised variant of LDA for multi-label classification. Our results show a consistent advantage of our approach over traditional CGS under all experimental conditions, and over CVB0 inference in the majority of conditions. More broadly, our results highlight the importance of averaging over multiple samples in LDA parameter estimation, and the use of efficient computational techniques to do so.",
            "keywords": [
                "Latent Dirichlet Allocation",
                "topic models",
                "unsupervised learning",
                "multi-label     classification",
                "text mining",
                "collapsed Gibbs sampling",
                "CVB0",
                "Bayesian inferencec 2017 Papanikolaou",
                "Foulds"
            ],
            "author": [
                "Yannis Papanikolaou",
                "James R. Foulds",
                "Timothy N. Rubin",
                "Grigorios Tsoumakas"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-526/16-526.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fundamental Conditions for Low-CP-Rank Tensor Completion",
            "abstract": "We consider the problem of low canonical polyadic (CP) rank tensor completion. A completion is a tensor whose entries agree with the observed entries and its rank matches the given CP rank. We analyze the manifold structure corresponding to the tensors with the given rank and define a set of polynomials based on the sampling pattern and CP decomposition. Then, we show that finite completability of the sampled tensor is equivalent to having a certain number of algebraically independent polynomials among the defined polynomials. Our proposed approach results in characterizing the maximum number of algebraically independent polynomials in terms of a simple geometric structure of the sampling pattern, and therefore we obtain the deterministic necessary and sufficient condition on the sampling pattern for finite completability of the sampled tensor. Moreover, assuming that the entries of the tensor are sampled independently with probability  and using the mentioned deterministic analysis, we propose a combinatorial method to derive a lower bound on the sampling probability , or equivalently, the number of sampled entries that guarantees finite completability with high probability. We also show that the existing result for the matrix completion problem can be used to obtain a loose lower bound on the sampling probability . In addition, we obtain deterministic and probabilistic conditions for unique completability. It is seen that the number of samples required for finite or unique completability obtained by the proposed analysis on the CP manifold is orders-of- magnitude lower than that is obtained by the existing analysis on the Grassmannian manifold.",
            "keywords": [
                "Low-rank tensor completion"
            ],
            "author": [
                "Morteza Ashraphijuo",
                "Xiaodong Wang"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-189/17-189.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parallel Symmetric Class Expression Learning",
            "abstract": "In machine learning, one often encounters data sets where a general pattern is violated by a relatively small number of exceptions (for example, a rule that says that all birds can fly is violated by examples such as penguins). This complicates the concept learning process and may lead to the rejection of some simple and expressive rules that cover many cases. In this paper we present an approach to this problem in description logic learning by computing partial descriptions (which are not necessarily entirely complete) of both positive and negative examples and combining them. Our Symmetric Parallel Class Expression Learning approach enables the generation of general rules with exception patterns included. We demonstrate that this algorithm provides significantly better results (in terms of metrics such as accuracy, search space covered, and learning time) than standard approaches on some typical data sets. Further, the approach has the added benefit that it can be parallelised relatively simply, leading to much faster exploration of the search tree on modern computers.",
            "keywords": [
                "description logic learning",
                "parallel",
                "symmetric"
            ],
            "author": [
                "An C. Tran",
                "Jens Dietrich",
                "Hans W. Guesgen",
                "Stephen Marsl",
                ""
            ],
            "ref": "http://jmlr.org/papers/volume18/14-317/14-317.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Partial Policies to Speedup MDP Tree Search via Reduction to I.I.D. Learning",
            "abstract": "A popular approach for online decision-making in large MDPs is time-bounded tree search. The effectiveness of tree search, however, is largely influenced by the action branching factor, which limits the search depth given a time bound. An obvious way to reduce action branching is to consider only a subset of potentially good actions at each state as specified by a provided partial policy. In this work, we consider offline learning of such partial policies with the goal of speeding up search without significantly reducing decision-making quality. Our first contribution consists of reducing the learning problem to set learning. We give a reduction-style analysis of three such algorithms, each making different assumptions, which relates the set learning objectives to the sub-optimality of search using the learned partial policies. Our second contribution is to describe concrete implementations of the algorithms within the popular framework of Monte-Carlo tree search. Finally, the third contribution is to evaluate the learning algorithms on two challenging MDPs with large action branching factors. The results show that the learned partial policies can significantly improve the anytime performance of Monte-Carlo tree search.",
            "keywords": [
                "online sequential decision-making",
                "partial policy",
                "partial policy learning",
                "imitation     learning",
                "Monte-Carlo tree search"
            ],
            "author": [
                "Jervis Pinto",
                "Alan Fern"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-251/15-251.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hierarchically Compositional Kernels for Scalable Nonparametric Learning",
            "abstract": "We propose a novel class of kernels to alleviate the high computational cost of large-scale nonparametric learning with kernel methods. The proposed kernel is defined based on a hierarchical partitioning of the underlying data domain, where the NystrÃ¶m method (a globally low-rank approximation) is married with a locally lossless approximation in a hierarchical fashion. The kernel maintains (strict) positive-definiteness. The corresponding kernel matrix admits a recursively off- diagonal low-rank structure, which allows for fast linear algebra computations. Suppressing the factor of data dimension, the memory and arithmetic complexities for training a regression or a classifier are reduced from  and  to  and , respectively, where  is the number of training examples and  is the rank on each level of the hierarchy. Although other randomized approximate kernels entail a similar complexity, empirical results show that the proposed kernel achieves a matching performance with a smaller . We demonstrate comprehensive experiments to show the effective use of the proposed kernel on data sizes up to the order of millions.",
            "keywords": [
                "Hierarchical kernels"
            ],
            "author": [
                "Jie Chen",
                "Haim Avron",
                "Vikas Sindhwani"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-376/15-376.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sharp Oracle Inequalities for Square Root Regularization",
            "abstract": "We study a set of regularization methods for high-dimensional linear regression models. These penalized estimators have the square root of the residual sum of squared errors as loss function, and any weakly decomposable norm as penalty function. This fit measure is chosen because of its property that the estimator does not depend on the unknown standard deviation of the noise. On the other hand, a generalized weakly decomposable norm penalty is very useful in being able to deal with different underlying sparsity structures. We can choose a different sparsity inducing norm depending on how we want to interpret the unknown parameter vector . Structured sparsity norms, as defined in Micchelli et al. (2010), are special cases of weakly decomposable norms, therefore we also include the square root LASSO (Belloni et al., 2011), the group square root LASSO (Bunea et al., 2014) and a new method called the square root SLOPE (in a similar fashion to the SLOPE from Bogdan et al. 2015). For this collection of estimators our results provide sharp oracle inequalities with the Karush-Kuhn-Tucker conditions. We discuss some examples of estimators. Based on a simulation we illustrate some advantages of the square root SLOPE.",
            "keywords": [
                "Square root LASSO",
                "structured sparsity",
                "Karush-Kuhn-Tucker",
                "sharp oracale     inequality"
            ],
            "author": [
                "Benjamin Stucky",
                "Sara van de Geer"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-482/15-482.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Particle Approximations",
            "abstract": "Approximate inference in high-dimensional, discrete probabilistic models is a central problem in computational statistics and machine learning. This paper describes discrete particle variational inference (DPVI), a new approach that combines key strengths of Monte Carlo, variational and search- based techniques. DPVI is based on a novel family of particle- based variational approximations that can be fit using simple, fast, deterministic search techniques. Like Monte Carlo, DPVI can handle multiple modes, and yields exact results in a well- defined limit. Like unstructured mean-field, DPVI is based on optimizing a lower bound on the partition function; when this quantity is not of intrinsic interest, it facilitates convergence assessment and debugging. Like both Monte Carlo and combinatorial search, DPVI can take advantage of factorization, sequential structure, and custom search operators. This paper defines DPVI particle-based approximation family and partition function lower bounds, along with the sequential DPVI and local DPVI algorithm templates for optimizing them. DPVI is illustrated and evaluated via experiments on lattice Markov Random Fields, nonparametric Bayesian mixtures and block-models, and parametric as well as non-parametric hidden Markov models. Results include applications to real-world spike-sorting and relational modeling problems, and show that DPVI can offer appealing time/accuracy trade-offs as compared to multiple alternatives.",
            "keywords": [
                "Bayesian inference",
                "variational methods",
                "Dirichlet process mixture model",
                "Ising model",
                "hidden Markov model"
            ],
            "author": [
                "Ardavan Saeedi",
                "Tejas D. Kulkarni",
                "Vikash K. Mansinghka",
                "Samuel J. Gershman"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-615/15-615.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bayesian Framework for Learning Rule Sets for Interpretable Classification",
            "abstract": "We present a machine learning algorithm for building classifiers that are comprised of a small number of short rules. These are restricted disjunctive normal form models. An example of a classifier of this form is as follows: If  satisfies (condition  AND condition ) OR (condition ) OR , then . Models of this form have the advantage of being interpretable to human experts since they produce a set of rules that concisely describe a specific class. We present two probabilistic models with prior parameters that the user can set to encourage the model to have a desired size and shape, to conform with a domain-specific definition of interpretability. We provide a scalable MAP inference approach and develop theoretical bounds to reduce computation by iteratively pruning the search space. We apply our method (Bayesian Rule Sets -- BRS) to characterize and predict user behavior with respect to in-vehicle context-aware personalized recommender systems. Our method has a major advantage over classical associative classification methods and decision trees in that it does not greedily grow the model.",
            "keywords": [
                "disjunctive normal form",
                "statistical learning",
                "data mining",
                "association rules",
                "inter-     pretable classifier"
            ],
            "author": [
                "Tong Wang",
                "Cynthia Rudin",
                "Finale Doshi-Velez",
                "Yimin Liu",
                "Erica Klampfl",
                "Perry MacNeille"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-003/16-003.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Robust-Equitable Measure for Feature Ranking and Selection",
            "abstract": "In many applications, not all the features used to represent data samples are important. Often only a few features are relevant for the prediction task. The choice of dependence measures often affect the final result of many feature selection methods. To select features that have complex nonlinear relationships with the response variable, the dependence measure should be equitable, a concept proposed by Reshef et al. (2011); that is, the dependence measure treats linear and nonlinear relationships equally. Recently, Kinney and Atwal (2014) gave a mathematical definition of self- equitability. In this paper, we introduce a new concept of robust-equitability and identify a robust- equitable copula dependence measure, the robust copula dependence (RCD) measure. RCD is based on the -distance of the copula density from uniform and we show that it is equitable under both equitability definitions. We also prove theoretically that RCD is much easier to estimate than mutual information. Because of these theoretical properties, the RCD measure has the following advantages compared to existing dependence measures: it is robust to different relationship forms and robust to unequal sample sizes of different features. Experiments on both synthetic and real-world data sets confirm the theoretical analysis, and illustrate the advantage of using the dependence measure RCD for feature selection.",
            "keywords": [
                "dependence measure",
                "feature selection",
                "copula",
                "equitability"
            ],
            "author": [
                "A. Adam Ding",
                "Jennifer G. Dy",
                "Yi Li",
                "Yale Chang"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-079/16-079.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiscale Strategies for Computing Optimal Transport",
            "abstract": "This paper presents a multiscale approach to efficiently compute approximate optimal transport plans between point sets. It is particularly well-suited for point sets that are in high- dimensions, but are close to being intrinsically low- dimensional. The approach is based on an adaptive multiscale decomposition of the point sets. The multiscale decomposition yields a sequence of optimal transport problems, that are solved in a top-to-bottom fashion from the coarsest to the finest scale. We provide numerical evidence that this multiscale approach scales approximately linearly, in time and memory, in the number of nodes, instead of quadratically or worse for a direct solution. Empirically, the multiscale approach results in less than one percent relative error in the objective function. Furthermore, the multiscale plans constructed are of interest by themselves as they may be used to introduce novel features and notions of distances between point sets. An analysis of sets of brain MRI based on optimal transport distances illustrates the effectiveness of the proposed method on a real world data set. The application demonstrates that multiscale optimal transport distances have the potential to improve on state-of-the-art metrics currently used in computational anatomy.",
            "keywords": [],
            "author": [
                "Samuel Gerber",
                "Mauro Maggioni"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-108/16-108.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-parametric Policy Search with Limited Information Loss",
            "abstract": "Learning complex control policies from non-linear and redundant sensory input is an important challenge for reinforcement learning algorithms. Non-parametric methods that approximate values functions or transition models can address this problem, by adapting to the complexity of the data set. Yet, many current non-parametric approaches rely on unstable greedy maximization of approximate value functions, which might lead to poor convergence or oscillations in the policy update. A more robust policy update can be obtained by limiting the information loss between successive state-action distributions. In this paper, we develop a policy search algorithm with policy updates that are both robust and non-parametric. Our method can learn non- parametric control policies for infinite horizon continuous Markov decision processes with non-linear and redundant sensory representations. We investigate how we can use approximations of the kernel function to reduce the time requirements of the demanding non-parametric computations. In our experiments, we show the strong performance of the proposed method, and how it can be approximated efficiently. Finally, we show that our algorithm can learn a real-robot under-powered swing-up task directly from image data.",
            "keywords": [
                "Reinforcement Learning",
                "Kernel Methods",
                "Policy Search"
            ],
            "author": [
                "Herke van Hoof",
                "Gerhard Neumann",
                "Jan Peters"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-142/16-142.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tests of Mutual or Serial Independence of Random Vectors with Applications",
            "abstract": "The problem of testing mutual independence between many random vectors is addressed. The closely related problem of testing serial independence of a multivariate stationary sequence is also considered. The MÃ¶bius transformation of characteristic functions is used to characterize independence. A generalization to  vectors of distance covariance and Hilbert-Schmidt independence criterion () tests with the translation invariant kernel of a stable probability distribution is proposed. Both test statistics can be expressed in a simple form as a sum over all elements of a componentwise product of  doubly-centered matrices. It is shown that an  statistic with sufficiently small scale parameters is equivalent to a distance covariance statistic. Consistency and weak convergence of both types of statistics are established. Approximation of -values is made by randomization tests without recomputing interpoint distances for each randomized sample. The dependogram is adapted to the proposed tests for the graphical identification of sources of dependencies. Empirical rejection rates obtained through extensive simulations confirm both the applicability of the testing procedures in small samples and the high level of competitiveness in terms of power. Applications to meteorological and financial data provide some interesting interpretations of dependencies revealed by dependograms.",
            "keywords": [
                "Distance covariance",
                "Hilbert-Schmidt independence criterion"
            ],
            "author": [
                "Martin Bilodeau",
                "Aurélien Guetsop Nangue"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-184/16-184.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Recovering PCA and Sparse PCA via Hybrid-(l1,l2) Sparse Sampling of Data Elements",
            "abstract": "This paper addresses how well we can recover a data matrix when only given a few of its elements. We present a randomized algorithm that element-wise sparsifies the data, retaining only a few of its entries. Our new algorithm independently samples the data using probabilities that depend on both squares ( sampling) and absolute values ( sampling) of the entries. We prove that this hybrid algorithm () achieves a near-PCA reconstruction of the data, and () recovers sparse principal components of the data, from a sketch formed by a sublinear sample size. Hybrid-() inherits the -ability to sample the important elements, as well as the regularization properties of  sampling, and maintains strictly better quality than either  or  on their own. Extensive experimental results on synthetic, image, text, biological, and financial data show that not only are we able to recover PCA and sparse PCA from incomplete data, but we can speed up such computations significantly using our sparse sketch .",
            "keywords": [
                "element-wise sampling",
                "sparse representation",
                "pca",
                "sparse pca"
            ],
            "author": [
                "Abhisek Kundu",
                "Petros Drineas",
                "Malik Magdon-Ismail"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-258/16-258.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Time for a Change: a Tutorial for Comparing Multiple Classifiers Through Bayesian Analysis",
            "abstract": "The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better---more sound and useful--- alternatives for it.",
            "keywords": [
                "comparing classifiers",
                "null hypothesis significance testing",
                "pitfalls of p-values",
                "Bayesian hypothesis tests",
                "Bayesian correlated t-test",
                "Bayesian hierarchical correlated t-     test"
            ],
            "author": [
                "Alessio Benavoli",
                "Giorgio Corani",
                "Janez Demšar",
                "Marco Zaffalon"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-305/16-305.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Relational Reinforcement Learning for Planning with Exogenous Effects",
            "abstract": "Probabilistic planners have improved recently to the point that they can solve difficult tasks with complex and expressive models. In contrast, learners cannot tackle yet the expressive models that planners do, which forces complex models to be mostly handcrafted. We propose a new learning approach that can learn relational probabilistic models with both action effects and exogenous effects. The proposed learning approach combines a multi-valued variant of inductive logic programming for the generation of candidate models, with an optimization method to select the best set of planning operators to model a problem. We also show how to combine this learner with reinforcement learning algorithms to solve complete problems. Finally, experimental validation is provided that shows improvements over previous work in both simulation and a robotic task. The robotic task involves a dynamic scenario with several agents where a manipulator robot has to clear the tableware on a table. We show that the exogenous effects learned by our approach allowed the robot to clear the table in a more efficient way.",
            "keywords": [
                "Learning Models for Planning",
                "Model-Based RL",
                "Probabilistic Planning",
                "Active Learning"
            ],
            "author": [
                "David Mart\\'{i}nez",
                "Guillem Aleny\\`{a}",
                "Tony Ribeiro",
                "Katsumi Inoue",
                "Carme Torras"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-326/16-326.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Tensor Regression",
            "abstract": "We propose a Bayesian approach to regression with a scalar response on vector and tensor covariates. Vectorization of the tensor prior to analysis fails to exploit the structure, often leading to poor estimation and predictive performance. We introduce a novel class of multiway shrinkage priors for tensor coefficients in the regression setting and present posterior consistency results under mild conditions. A computationally efficient Markov chain Monte Carlo algorithm is developed for posterior computation. Simulation studies illustrate substantial gains over existing tensor regression methods in terms of estimation and parameter inference. Our approach is further illustrated in a neuroimaging application.",
            "keywords": [
                "Multiway Shrinkage Prior"
            ],
            "author": [
                "Rajarshi Guhaniyogi",
                "Shaan Qamar",
                "David B. Dunson"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-362/16-362.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Discriminative Clustering with Sparse Regularizers",
            "abstract": "Clustering high-dimensional data often requires some form of dimensionality reduction, where clustered variables are separated from noise-looking variables. We cast this problem as finding a low-dimensional projection of the data which is well-clustered. This yields a one-dimensional projection in the simplest situation with two clusters, and extends naturally to a multi-label scenario for more than two clusters. In this paper, (a) we first show that this joint clustering and dimension reduction formulation is equivalent to previously proposed discriminative clustering frameworks, thus leading to convex relaxations of the problem; (b) we propose a novel sparse extension, which is still cast as a convex relaxation and allows estimation in higher dimensions; (c) we propose a natural extension for the multi-label scenario; (d) we provide a new theoretical analysis of the performance of these formulations with a simple probabilistic model, leading to scalings over the form  for the affine invariant case and  for the sparse case, where  is the number of examples and  the ambient dimension; and finally, (e) we propose an efficient iterative algorithm with running-time complexity proportional to , improving on earlier algorithms for discriminative clustering with the square loss, which had quadratic complexity in the number of examples.",
            "keywords": [],
            "author": [
                "Nicolas Flammarion",
                "Balamurugan Palaniappan",
                "Francis Bach"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-429/16-429.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Making Decision Trees Feasible in Ultrahigh Feature and Label Dimensions",
            "abstract": "Due to the non-linear but highly interpretable representations, decision tree (DT) models have significantly attracted a lot of attention of researchers. However, it is difficult to understand and interpret DT models in ultrahigh dimensions and DT models usually suffer from the curse of dimensionality and achieve degenerated performance when there are many noisy features. To address these issues, this paper first presents a novel data- dependent generalization error bound for the perceptron decision tree (PDT), which provides the theoretical justification to learn a sparse linear hyperplane in each decision node and to prune the tree. Following our analysis, we introduce the notion of budget-aware classifier (BAC) with a budget constraint on the weight coefficients, and propose a supervised budgeted tree (SBT) algorithm to achieve non-linear prediction performance. To avoid generating an unstable and complicated decision tree and improve the generalization of the SBT, we present a pruning strategy by learning classifiers to minimize cross-validation errors on each BAC. To deal with ultrahigh label dimensions, based on three important phenomena of real-world data sets from a variety of application domains, we develop a sparse coding tree framework for multi-label annotation problems and provide the theoretical analysis. Extensive empirical studies verify that 1) SBT is easy to understand and interpret in ultrahigh dimensions and is more resilient to noisy features. 2) Compared with state-of-the-art algorithms, our proposed sparse coding tree framework is more efficient, yet accurate in ultrahigh label and feature dimensions.",
            "keywords": [
                "Classification",
                "Ultrahigh Feature Dimensions",
                "Ultrahigh Label Dimensions"
            ],
            "author": [
                "Weiwei Liu",
                "Ivor W. Tsang"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-466/16-466.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Scalable Deep Kernels with Recurrent Structure",
            "abstract": "Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure, and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP- LSTM are uniquely valuable.",
            "keywords": [],
            "author": [
                "Maruan Al-Shedivat",
                "Andrew Gordon Wilson",
                "Yunus Saatchi",
                "Zhiting Hu",
                "Eric P. Xing"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-498/16-498.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convolutional Neural Networks Analyzed via Convolutional Sparse Coding",
            "abstract": "Convolutional neural networks (CNN) have led to many state-of- the-art results spanning through various fields. However, a clear and profound theoretical understanding of the forward pass, the core algorithm of CNN, is still lacking. In parallel, within the wide field of sparse approximation, Convolutional Sparse Coding (CSC) has gained increasing attention in recent years. A theoretical study of this model was recently conducted, establishing it as a reliable and stable alternative to the commonly practiced patch-based processing. Herein, we propose a novel multi-layer model, ML-CSC, in which signals are assumed to emerge from a cascade of CSC layers. This is shown to be tightly connected to CNN, so much so that the forward pass of the CNN is in fact the thresholding pursuit serving the ML-CSC model. This connection brings a fresh view to CNN, as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network, and their stable estimation, all guaranteed under simple local sparsity conditions. Lastly, identifying the weaknesses in the above pursuit scheme, we propose an alternative to the forward pass, which is connected to deconvolutional and recurrent networks, and also has better theoretical guarantees.",
            "keywords": [
                "Deep Learning",
                "Convolutional Neural Networks",
                "Forward Pass",
                "Sparse Rep-     resentation",
                "Convolutional Sparse Coding",
                "Thresholding Algorithm"
            ],
            "author": [
                "Vardan Papyan",
                "Yaniv Romano",
                "Michael Elad"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-505/16-505.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization",
            "abstract": "We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate (SPDC) method, which alternates between maximizing over a randomly chosen dual variable and minimizing over the primal variables. An extrapolation step on the primal variables is performed to obtain accelerated convergence rate. We also develop a mini-batch version of the SPDC method which facilitates parallel computing, and an extension with weighted sampling probabilities on the dual variables, which has a better complexity than uniform sampling on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods.",
            "keywords": [
                "empirical risk minimization",
                "randomized algorithms",
                "convex-concave saddle     point problems",
                "primal-dual algorithms"
            ],
            "author": [
                "Yuchen Zhang",
                "Lin Xiao"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-568/16-568.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Angle-based Multicategory Distance-weighted SVM",
            "abstract": "Classification is an important supervised learning technique with numerous applications. We develop an angle-based multicategory distance-weighted support vector machine (MDWSVM) classification method that is motivated from the binary distance-weighted support vector machine (DWSVM) classification method. The new method has the merits of both support vector machine (SVM) and distance-weighted discrimination (DWD) but also alleviates both the data piling issue of SVM and the imbalanced data issue of DWD. Theoretical and numerical studies demonstrate the advantages of MDWSVM method over existing angle-based methods.",
            "keywords": [
                "Discriminant analysis",
                "Imbalanced data",
                "High dimension",
                "Support vector     machine"
            ],
            "author": [
                "Hui Sun",
                "Bruce A. Craig",
                "Lingsong Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-003/17-003.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Estimation of Kernel Mean Embeddings",
            "abstract": "In this paper, we study the minimax estimation of the Bochner integral \\[ \\mu_k(P) := \\int_\\mathcal{X} k(\\cdot,x)\\, dP(x), \\] also called the kernel mean embedding, based on random samples drawn i.i.d. from , where  is a positive definite kernel.  Various estimators (including the empirical estimator),  of  are studied in the literature wherein all of them satisfy  with  being the reproducing kernel Hilbert space induced by .  The main contribution of the paper is in showing that the above mentioned rate of  is minimax in  and -norms over the class of discrete measures and the class of measures that has an infinitely differentiable density, with  being a continuous translation- invariant kernel on . The interesting aspect of this result is that the minimax rate is independent of the smoothness of the kernel and the density of  (if it exists).",
            "keywords": [
                "Bochner integral"
            ],
            "author": [
                "Ilya Tolstikhin",
                "Bharath K. Sriperumbudur",
                "Krikamol Mu",
                "et"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-032/17-032.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Impact of Random Models on Clustering Similarity",
            "abstract": "Clustering is a central approach for unsupervised learning. After clustering is applied, the most fundamental analysis is to quantitatively compare clusterings. Such comparisons are crucial for the evaluation of clustering methods as well as other tasks such as consensus clustering. It is often argued that, in order to establish a baseline, clustering similarity should be assessed in the context of a random ensemble of clusterings. The prevailing assumption for the random clustering ensemble is the permutation model in which the number and sizes of clusters are fixed. However, this assumption does not necessarily hold in practice; for example, multiple runs of K-means clustering reurns clusterings with a fixed number of clusters, while the cluster size distribution varies greatly. Here, we derive corrected variants of two clustering similarity measures (the Rand index and Mutual Information) in the context of two random clustering ensembles in which the number and sizes of clusters vary. In addition, we study the impact of one-sided comparisons in the scenario with a reference clustering. The consequences of different random models are illustrated using synthetic examples, handwriting recognition, and gene expression data. We demonstrate that the choice of random model can have a drastic impact on the ranking of similar clustering pairs, and the evaluation of a clustering method with respect to a random baseline; thus, the choice of random clustering model should be carefully justified.",
            "keywords": [
                "clustering comparison",
                "clustering evaluation",
                "adjustment for chance",
                "Rand     index"
            ],
            "author": [
                "Alexander J. Gates",
                "Yong-Yeol Ahn"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-039/17-039.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hierarchical Clustering via Spreading Metrics",
            "abstract": "We study the cost function for hierarchical clusterings introduced by (Dasgupta, 2016) where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in (Dasgupta, 2016) that a top-down algorithm based on the uniform Sparsest Cut problem returns a hierarchical clustering of cost at most  times the cost of the optimal hierarchical clustering, where  is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao- Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most  times the cost of the optimal solution. We improve this by giving an -approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an - approximate hierarchical clustering for a generalization of this cost function also studied in (Dasgupta, 2016). Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with constant factor inapproximability results for this problem: 1) no polynomial size LP or SDP can achieve a constant factor approximation for this problem and 2) no polynomial time algorithm can achieve a constant factor approximation under the Small Set Expansion hypothesis.",
            "keywords": [
                "Hierarchical clustering",
                "clustering",
                "convex optimization",
                "linear programming"
            ],
            "author": [
                "Aurko Roy",
                "Sebastian Pokutta"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-081/17-081.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The MADP Toolbox: An Open Source Library for Planning and Learning in (Multi-)Agent Systems",
            "abstract": "This article describes the Multiagent Decision Process (MADP) Toolbox, a software library to support planning and learning for intelligent agents and multiagent systems in uncertain environments. Key features are that it supports partially observable environments and stochastic transition models; has unified support for single- and multiagent systems; provides a large number of models for decision-theoretic decision making, including one-shot and sequential decision making under various assumptions of observability and cooperation, such as Dec-POMDPs and POSGs; provides tools and parsers to quickly prototype new problems; provides an extensive range of planning and learning algorithms for single- and multiagent systems; is released under the GNU GPL v3 license; and is written in C++ and designed to be extensible via the object-oriented paradigm.",
            "keywords": [
                "software",
                "decision-theoretic planning",
                "reinforcement learning"
            ],
            "author": [
                "Frans A. Oliehoek",
                "Matthijs T. J. Spaan",
                "Bas Terwijn",
                "Philipp Robbel",
                "Jo\\~{a}o V. Messias"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-156/17-156.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A survey of Algorithms and Analysis for Adaptive Online Learning",
            "abstract": "We present tools for the analysis of Follow-The-Regularized- Leader (FTRL), Dual Averaging, and Mirror Descent algorithms when the regularizer (equivalently, prox-function or learning rate schedule) is chosen adaptively based on the data. Adaptivity can be used to prove regret bounds that hold on every round, and also allows for data-dependent regret bounds as in AdaGrad-style algorithms (e.g., Online Gradient Descent with adaptive per-coordinate learning rates). We present results from a large number of prior works in a unified manner, using a modular and tight analysis that isolates the key arguments in easily re-usable lemmas. This approach strengthens previously known FTRL analysis techniques to produce bounds as tight as those achieved by potential functions or primal-dual analysis. Further, we prove a general and exact equivalence between adaptive Mirror Descent algorithms and a corresponding FTRL update, which allows us to analyze Mirror Descent algorithms in the same framework. The key to bridging the gap between Dural Averaging and Mirror Descent algorithms lies in an analysis of the FTRL-Proximal algorithm family. Our regret bounds are proved in the most general form, holding for arbitrary norms and non- smooth regularizers with time-varying weight.",
            "keywords": [
                "online learning",
                "online convex optimization",
                "regret analysis",
                "adaptive     algorithms",
                "follow-the-regularized-leader",
                "mirror descent"
            ],
            "author": [
                "H. Brendan McMahan"
            ],
            "ref": "http://jmlr.org/papers/volume18/14-428/14-428.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A distributed block coordinate descent method for training l1 regularized linear classifiers",
            "abstract": "Distributed training of  regularized classifiers has received great attention recently. Most existing methods approach this problem by taking steps obtained from approximating the objective by a quadratic approximation that is decoupled at the individual variable level. These methods are designed for multicore systems where communication costs are low. They are inefficient on systems such as Hadoop running on a cluster of commodity machines where communication costs are substantial. In this paper we design a distributed algorithm for  regularization that is much better suited for such systems than existing algorithms. A careful cost analysis is used to support these points and motivate our method. The main idea of our algorithm is to do block optimization of many variables on the actual objective function within each computing node; this increases the computational cost per step that is matched with the communication cost, and decreases the number of outer iterations, thus yielding a faster overall method. Distributed Gauss-Seidel and Gauss-Southwell greedy schemes are used for choosing variables to update in each step. We establish global convergence theory for our algorithm, including Q-linear rate of convergence. Experiments on two benchmark problems show our method to be much faster than existing methods.",
            "keywords": [
                "Distributed learning"
            ],
            "author": [
                "Dhruv Mahajan",
                "S. Sathiya Keerthi",
                "S. Sundararajan"
            ],
            "ref": "http://jmlr.org/papers/volume18/14-484/14-484.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Learning with Regularized Least Squares",
            "abstract": "We study distributed learning with the least squares regularization scheme in a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach, the algorithm partitions a data set into disjoint data subsets, applies the least squares regularization scheme to each data subset to produce an output function, and then takes an average of the individual output functions as a final global estimator or predictor. We show with error bounds and learning rates in expectation in both the -metric and RKHS-metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine. Our derived learning rates in expectation are optimal and stated in a general setting without any eigenfunction assumption. The analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach. Even for the classical least squares regularization scheme in the RKHS associated with a general kernel, we give the best learning rate in expectation in the literature.",
            "keywords": [
                "Distributed learning",
                "divide-and-conquer",
                "error analysis",
                "integral operator"
            ],
            "author": [
                "Shao-Bo Lin",
                "Xin Guo",
                "Ding-Xuan Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-586/15-586.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Identifying Unreliable and Adversarial Workers in Crowdsourced Labeling Tasks",
            "abstract": "We study the problem of identifying unreliable and adversarial workers in crowdsourcing systems where workers (or users) provide labels for tasks (or items). Most existing studies assume that worker responses follow specific probabilistic models; however, recent evidence shows the presence of workers adopting non-random or even malicious strategies. To account for such workers, we suppose that workers comprise a mixture of honest and adversarial workers. Honest workers may be reliable or unreliable, and they provide labels according to an unknown but explicit probabilistic model. Adversaries adopt labeling strategies different from those of honest workers, whether probabilistic or not. We propose two reputation algorithms to identify unreliable honest workers and adversarial workers from only their responses. Our algorithms assume that honest workers are in the majority, and they classify workers with outlier label patterns as adversaries. Theoretically, we show that our algorithms successfully identify unreliable honest workers, workers adopting deterministic strategies, and worst- case sophisticated adversaries who can adopt arbitrary labeling strategies to degrade the accuracy of the inferred task labels. Empirically, we show that filtering out outliers using our algorithms can significantly improve the accuracy of several state-of-the-art label aggregation algorithms in real-world crowdsourcing datasets.",
            "keywords": [
                "crowdsourcing",
                "reputation",
                "adversary"
            ],
            "author": [
                "Srikanth Jagabathula",
                "Lakshminarayanan Subramanian",
                "Ashwin Venkataraman"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-650/15-650.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Easy-to-hard Learning Paradigm for Multiple Classes and Multiple Labels",
            "abstract": "Many applications, such as human action recognition and object detection, can be formulated as a multiclass classification problem. One-vs-rest (OVR) is one of the most widely used approaches for multiclass classification due to its simplicity and excellent performance. However, many confusing classes in such applications will degrade its results. For example, hand clap and boxing are two confusing actions. Hand clap is easily misclassified as boxing, and vice versa. Therefore, precisely classifying confusing classes remains a challenging task. To obtain better performance for multiclass classifications that have confusing classes, we first develop a classifier chain model for multiclass classification (CCMC) to transfer class information between classifiers. Then, based on an analysis of our proposed model, we propose an easy- to-hard learning paradigm for multiclass classification to automatically identify easy and hard classes and then use the predictions from simpler classes to help solve harder classes. Similar to CCMC, the classifier chain (CC) model is also proposed by Read et al. (2009) to capture the label dependency for multi-label classification. However, CC does not consider the order of difficulty of the labels and achieves degenerated performance when there are many confusing labels. Therefore, it is non- trivial to learn the appropriate label order for CC. Motivated by our analysis for CCMC, we also propose the easy-to-hard learning paradigm for multi-label classification to automatically identify easy and hard labels, and then use the predictions from simpler labels to help solve harder labels. We also demonstrate that our proposed strategy can be successfully applied to a wide range of applications, such as ordinal classification and relationship prediction. Extensive empirical studies validate our analysis and the effectiveness of our proposed easy-to-hard learning strategies.",
            "keywords": [],
            "author": [
                "Weiwei Liu",
                "Ivor W. Tsang",
                "Klaus-Robert M\\\"{u}ller"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-212/16-212.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fisher Consistency for Prior Probability Shift",
            "abstract": "We introduce Fisher consistency in the sense of unbiasedness as a desirable property for estimators of class prior probabilities. Lack of Fisher consistency could be used as a criterion to dismiss estimators that are unlikely to deliver precise estimates in test data sets under prior probability and more general data set shift. The usefulness of this unbiasedness concept is demonstrated with three examples of classifiers used for quantification: Adjusted Count, EM-algorithm and CDE- Iterate. We find that Adjusted Count and EM-algorithm are Fisher consistent. A counter-example shows that CDE-Iterate is not Fisher consistent and, therefore, cannot be trusted to deliver reliable estimates of class probabilities.",
            "keywords": [
                "Classification",
                "quantification",
                "class distribution estimation",
                "Fisher consis-     tency"
            ],
            "author": [
                "Dirk Tasche"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-048/17-048.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "openXBOW -- Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit",
            "abstract": "We introduce openXBOW, an open-source toolkit for the generation of bag-of-words (BoW) representations from multimodal input. In the BoW principle, word histograms were first used as features in document classification, but the idea was and can easily be adapted to, e.g., acoustic or visual descriptors, introducing a prior step of vector quantisation. The openXBOW toolkit supports arbitrary numeric input features and text input and concatenates computed sub-bags to a final bag. It provides a variety of extensions and options. To our knowledge, openXBOW is the first publicly available toolkit for the generation of crossmodal bags-of-words. The capabilities of the tool have been exemplified in different scenarios: sentiment analysis in tweets, classification of snore sounds, and time-dependent emotion recognition based on acoustic, linguistic, and visual information, where improved results over other feature representations were observed.",
            "keywords": [
                "bag-of-words",
                "multimodal signal processing",
                "histogram feature representa-     tions"
            ],
            "author": [
                "Maximilian Schmitt",
                "Björn Schuller"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-113/17-113.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rank Determination for Low-Rank Data Completion",
            "abstract": "Recently, fundamental conditions on the sampling patterns have been obtained for finite completability of low-rank matrices or tensors given the corresponding ranks. In this paper, we consider the scenario where the rank is not given and we aim to approximate the unknown rank based on the location of sampled entries and some given completion. We consider a number of data models, including single-view matrix, multi-view matrix, CP tensor, tensor-train tensor and Tucker tensor. For each of these data models, we provide an upper bound on the rank when an arbitrary low-rank completion is given. We characterize these bounds both deterministically, i.e., with probability one given that the sampling pattern satisfies certain combinatorial properties, and probabilistically, i.e., with high probability given that the sampling probability is above some threshold. Moreover, for both single-view matrix and CP tensor, we are able to show that the obtained upper bound is exactly equal to the unknown rank if the lowest-rank completion is given. Furthermore, we provide numerical experiments for the case of single-view matrix, where we use nuclear norm minimization to find a low-rank completion of the sampled data and we observe that in most of the cases the proposed upper bound on the rank is equal to the true rank.",
            "keywords": [
                "Low-rank data completion",
                "rank estimation",
                "tensor",
                "matrix",
                "manifold",
                "Tucker rank",
                "tensor-train rank",
                "CP rank"
            ],
            "author": [
                "Morteza Ashraphijuo",
                "Xiaodong Wang",
                "Vaneet Aggarwal"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-375/17-375.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Network Learning via Topological Order",
            "abstract": "We propose a mixed integer programming (MIP) model and iterative algorithms based on topological orders to solve optimization problems with acyclic constraints on a directed graph. The proposed MIP model has a significantly lower number of constraints compared to popular MIP models based on cycle elimination constraints and triangular inequalities. The proposed iterative algorithms use gradient descent and iterative reordering approaches, respectively, for searching topological orders. A computational experiment is presented for the Gaussian Bayesian network learning problem, an optimization problem minimizing the sum of squared errors of regression models with L1 penalty over a feature network with application of gene network inference in bioinformatics.",
            "keywords": [
                "Bayesian networks",
                "topological orders",
                "Gaussian Bayesian network"
            ],
            "author": [
                "Young Woong Park",
                "Diego Klabjan"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-033/17-033.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stability of Controllers for Gaussian Process Dynamics",
            "abstract": "Learning control has become an appealing alternative to the derivation of control laws based on classic control theory. However, a major shortcoming of learning control is the lack of performance guarantees which prevents its application in many real-world scenarios. As a step towards widespread deployment of learning control, we provide stability analysis tools for controllers acting on dynamics represented by Gaussian processes (GPs). We consider differentiable Markovian control policies and system dynamics given as (i) the mean of a GP, and (ii) the full GP distribution. For both cases, we analyze finite and infinite time horizons. Furthermore, we study the effect of disturbances on the stability results. Empirical evaluations on simulated benchmark problems support our theoretical results.",
            "keywords": [
                "Stability",
                "Reinforcement Learning",
                "Control"
            ],
            "author": [
                "Julia Vinogradska",
                "Bastian Bischoff",
                "Duy Nguyen-Tuong",
                "Jan Peters"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-590/16-590.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression",
            "abstract": "We consider the optimization of a quadratic objective function whose gradients are only accessible through a stochastic oracle that returns the gradient at any given point plus a zero-mean finite variance random error. We present the first algorithm that achieves jointly the optimal prediction error rates for least-squares regression, both in terms of forgetting the initial conditions in , and in terms of dependence on the noise and dimension  of the problem, as . Our new algorithm is based on averaged accelerated regularized gradient descent, and may also be analyzed through finer assumptions on initial conditions and the Hessian matrix, leading to dimension- free quantities that may still be small in some distances while the âoptimalâ terms above are large. In order to characterize the tightness of these new bounds, we consider an application to non-parametric regression and use the known lower bounds on the statistical performance (without computational limits), which happen to match our bounds obtained from a single pass on the data and thus show optimality of our algorithm in a wide variety of particular trade-offs between bias and variance.",
            "keywords": [
                "convex optimization",
                "least-squares regression",
                "stochastic gradient",
                "accelerated     gradient"
            ],
            "author": [
                "Aymeric Dieuleveut",
                "Nicolas Flammarion",
                "Francis Bach"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-335/16-335.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Confidence Sets with Expected Sizes for Multiclass Classification",
            "abstract": "Multiclass classification problems such as image annotation can involve a large number of classes. In this context, confusion between classes can occur, and single label classification may be misleading. We provide in the present paper a general device that, given an unlabeled dataset and a score function defined as the minimizer of some empirical and convex risk, outputs a set of class labels, instead of a single one. Interestingly, this procedure does not require that the unlabeled dataset explores the whole classes. Even more, the method is calibrated to control the expected size of the output set while minimizing the classification risk. We show the statistical optimality of the procedure and establish rates of convergence under the Tsybakov margin condition. It turns out that these rates are linear on the number of labels. We apply our methodology to convex aggregation of confidence sets based on the -fold cross validation principle also known as the superlearning principle (van der Laan et al., 2007). We illustrate the numerical performance of the procedure on real data and demonstrate in particular that with moderate expected size, w.r.t. the number of labels, the procedure provides significant improvement of the classification risk.",
            "keywords": [
                "Multiclass Classification",
                "Confidence Sets",
                "Empirical Risk Minimization",
                "Convex Loss"
            ],
            "author": [
                "Christophe Denis",
                "Mohamed Hebiri"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-596/16-596.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Learning to Rank with Top-k Feedback",
            "abstract": "We consider two settings of online learning to rank where feedback is restricted to top ranked items. The problem is cast as an online game between a learner and sequence of users, over  rounds. In both settings, the learners objective is to present ranked list of items to the users. The learner's performance is judged on the entire ranked list and true relevances of the items. However, the learner receives highly restricted feedback at end of each round, in form of relevances of only the top  ranked items, where . The first setting is non-contextual, where the list of items to be ranked is fixed. The second setting is contextual, where lists of items vary, in form of traditional query-document lists. No stochastic assumption is made on the generation process of relevances of items and contexts. We provide efficient ranking strategies for both the settings. The strategies achieve  regret, where regret is based on popular ranking measures in first setting and ranking surrogates in second setting. We also provide impossibility results for certain ranking measures and a certain class of surrogates, when feedback is restricted to the top ranked item, i.e. . We empirically demonstrate the performance of our algorithms on simulated and real world data sets.",
            "keywords": [
                "Learning to Rank",
                "Online Learning",
                "Partial Monitoring",
                "Online Bandits"
            ],
            "author": [
                "Sougata Chaudhuri",
                "Ambuj Tewari"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-285/16-285.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation",
            "abstract": "Gaussian processes (GPs) are flexible distributions over functions that enable high-level assumptions about unknown functions to be encoded in a parsimonious, flexible and general way. Although elegant, the application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. In this paper we develop a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. Unlike much of the previous venerable work in this area, the new framework is built on standard methods for approximate inference (variational free- energy, EP and Power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way all of the approximation is performed at `inference time' rather than at `modelling time', resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression and classification tasks.",
            "keywords": [
                "Gaussian process",
                "expectation propagation",
                "variational inference"
            ],
            "author": [
                "Thang D. Bui",
                "Josiah Yan",
                "Richard E. Turner"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-603/16-603.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Accelerating Stochastic Composition Optimization",
            "abstract": "We consider the stochastic nested composition optimization problem where the objective is a composition of two expected- value functions. We propose a new stochastic first-order method, namely the accelerated stochastic compositional proximal gradient (ASC-PG) method. This algorithm updates the solution based on noisy gradient queries using a two-timescale iteration. The ASC-PG is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty. We show that the ASC-PG exhibits faster convergence than the best known algorithms, and that it achieves the optimal sample-error complexity in several important special cases. We demonstrate the application of ASC-PG to reinforcement learning and conduct numerical experiments.",
            "keywords": [
                "Large-scale optimization",
                "stochastic gradient",
                "composition optimization"
            ],
            "author": [
                "Mengdi Wang",
                "Ji Liu",
                "Ethan X. Fang"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-504/16-504.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Bayesian Learning with Stochastic Natural Gradient Expectation Propagation and the Posterior Server",
            "abstract": "This paper makes two contributions to Bayesian machine learning algorithms. Firstly, we propose stochastic natural gradient expectation propagation (SNEP), a novel alternative to expectation propagation (EP), a popular variational inference algorithm. SNEP is a black box variational algorithm, in that it does not require any simplifying assumptions on the distribution of interest, beyond the existence of some Monte Carlo sampler for estimating the moments of the EP tilted distributions. Further, as opposed to EP which has no guarantee of convergence, SNEP can be shown to be convergent, even when using Monte Carlo moment estimates. Secondly, we propose a novel architecture for distributed Bayesian learning which we call the posterior server. The posterior server allows scalable and robust Bayesian learning in cases where a data set is stored in a distributed manner across a cluster, with each compute node containing a disjoint subset of data. An independent Monte Carlo sampler is run on each compute node, with direct access only to the local data subset, but which targets an approximation to the global posterior distribution given all data across the whole cluster. This is achieved by using a distributed asynchronous implementation of SNEP to pass messages across the cluster. We demonstrate SNEP and the posterior server on distributed Bayesian learning of logistic regression and neural networks.",
            "keywords": [
                "Distributed Learning",
                "Large Scale Learning",
                "Deep Learning",
                "Bayesian Learn-     ing",
                "Variational Inference",
                "Expectation Propagation",
                "Stochastic Approximation",
                "Natural     Gradient",
                "Markov chain Monte Carlo",
                "Parameter Server"
            ],
            "author": [
                "Leonard Hasenclever",
                "Stefan Webb",
                "Thibaut Lienart",
                "Sebastian Vollmer",
                "Balaji Lakshminarayanan",
                "Charles Blundell",
                "Yee Whye Teh"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-478/16-478.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Dictionary for Least Squares Representation",
            "abstract": "Dictionaries are collections of vectors used for the representation of a class of vectors in Euclidean spaces. Recent research on optimal dictionaries is focused on constructing dictionaries that offer sparse representations, i.e., -optimal representations. Here we consider the problem of finding optimal dictionaries with which representations of a given class of vectors is optimal in an -sense: optimality of representation is defined as attaining the minimal average -norm of the coefficients used to represent the vectors in the given class. With the help of recent results on rank-1 decompositions of symmetric positive semidefinite matrices, we provide an explicit description of -optimal dictionaries as well as their algorithmic constructions in polynomial time.",
            "keywords": [],
            "author": [
                "Mohammed Rayyan Sheriff",
                "Debasish Chatterjee"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-046/16-046.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Computational Limits of A Distributed Algorithm for Smoothing Spline",
            "abstract": "In this paper, we explore statistical versus computational trade-off to address a basic question in the application of a distributed algorithm: what is the minimal computational cost in obtaining statistical optimality? In smoothing spline setup, we observe a phase transition phenomenon for the number of deployed machines that ends up being a simple proxy for computing cost. Specifically, a sharp upper bound for the number of machines is established: when the number is below this bound, statistical optimality (in terms of nonparametric estimation or testing) is achievable; otherwise, statistical optimality becomes impossible. These sharp bounds partly capture intrinsic computational limits of the distributed algorithm considered in this paper, and turn out to be fully determined by the smoothness of the regression function. We name the asymptotic analysis on such split-and-aggregation estimation/inference as splitotic theory. As a side remark, we argue that sample splitting may be viewed as an alternative form of regularization, playing a similar role as smoothing parameter.",
            "keywords": [
                "divide-and-conquer",
                "computational limits",
                "smoothing spline"
            ],
            "author": [
                "Zuofeng Shang",
                "Guang Cheng"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-289/16-289.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hinge-Loss Markov Random Fields and Probabilistic Soft Logic",
            "abstract": "A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hinge-loss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL- MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible.",
            "keywords": [
                "Probabilistic graphical models",
                "statistical relational learning"
            ],
            "author": [
                "Stephen H. Bach",
                "Matthias Broecheler",
                "Bert Huang",
                "Lise Getoor"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-631/15-631.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximation Vector Machines for Large-scale Online Learning",
            "abstract": "One of the most challenging problems in kernel online learning is to bound the model size and to promote model sparsity. Sparse models not only improve computation and memory usage, but also enhance the generalization capacity -- a principle that concurs with the law of parsimony. However, inappropriate sparsity modeling may also significantly degrade the performance. In this paper, we propose Approximation Vector Machine (AVM), a model that can simultaneously encourage sparsity and safeguard its risk in compromising the performance. In an online setting context, when an incoming instance arrives, we approximate this instance by one of its neighbors whose distance to it is less than a predefined threshold. Our key intuition is that since the newly seen instance is expressed by its nearby neighbor the optimal performance can be analytically formulated and maintained. We develop theoretical foundations to support this intuition and further establish an analysis for the common loss functions including Hinge, smooth Hinge, and Logistic (i.e., for the classification task) and , , and -insensitive (i.e., for the regression task) to characterize the gap between the approximation and optimal solutions. This gap crucially depends on two key factors including the frequency of approximation (i.e., how frequent the approximation operation takes place) and the predefined threshold. We conducted extensive experiments for classification and regression tasks in batch and online modes using several benchmark datasets. The quantitative results show that our proposed AVM obtained comparable predictive performances with current state-of-the-art methods while simultaneously achieving significant computational speed-up due to the ability of the proposed AVM in maintaining the model size.",
            "keywords": [
                "kernel",
                "online learning",
                "large-scale machine learning",
                "sparsity",
                "big data",
                "core     set",
                "stochastic gradient descent"
            ],
            "author": [
                "Trung Le",
                "Tu Dinh Nguyen",
                "Vu Nguyen",
                "Dinh Phung"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-191/16-191.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Sampling from Time-Varying Log-Concave Distributions",
            "abstract": "We propose a computationally efficient random walk on a convex body which rapidly mixes with respect to a fixed log-concave distribution and closely tracks a time-varying log-concave distribution. We develop general theoretical guarantees on the required number of steps; this number can be calculated on the fly according to the distance from and the shape of the next distribution. We then illustrate the technique on several examples. Within the context of exponential families, the proposed method produces samples from a posterior distribution which is updated as data arrive in a streaming fashion. The sampling technique can be used to track time-varying truncated distributions, as well as to obtain samples from a changing mixture model, fitted in a streaming fashion to data. In the setting of linear optimization, the proposed method has oracle complexity with best known dependence on the dimension for certain geometries. In the context of online learning and repeated games, the algorithm is an efficient method for implementing no-regret mixture forecasting strategies. Remarkably, in some of these examples, only one step of the random walk is needed to track the next distribution.",
            "keywords": [],
            "author": [
                "Hariharan Narayanan",
                "Alexer Rakhlin"
            ],
            "ref": "http://jmlr.org/papers/volume18/14-223/14-223.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Document Neural Autoregressive Distribution Estimation",
            "abstract": "We present an approach based on feed-forward neural networks for learning the distribution over textual documents. This approach is inspired by the Neural Autoregressive Distribution Estimator (NADE) model which has been shown to be a good estimator of the distribution over discrete-valued high-dimensional vectors. In this paper, we present how NADE can successfully be adapted to textual data, retaining the property that sampling or computing the probability of an observation can be done exactly and efficiently. The approach can also be used to learn deep representations of documents that are competitive to those learned by alternative topic modeling approaches. Finally, we describe how the approach can be combined with a regular neural network N-gram model and substantially improve its performance, by making its learned representation sensitive to the larger, document-level context.",
            "keywords": [
                "Neural networks",
                "Deep learning",
                "Topic models",
                "Language models"
            ],
            "author": [
                "Stanislas Lauly",
                "Yin Zheng",
                "Alex",
                "re Allauzen",
                "Hugo Larochelle"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-017/16-017.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Target Curricula via Selection of Minimum Feature Sets: a Case Study in Boolean Networks",
            "abstract": "",
            "keywords": [
                "Multi-Label Classification",
                "Target Curriculum",
                "Boolean Networks"
            ],
            "author": [
                "Shannon Fenn",
                "Pablo Moscato"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-007/17-007.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A General Distributed Dual Coordinate Optimization Framework for Regularized Loss Minimization",
            "abstract": "In modern large-scale machine learning applications, the training data are often partitioned and stored on multiple machines. It is customary to employ the data parallelism approach, where the aggregated training loss is minimized without moving data across machines. In this paper, we introduce a novel distributed dual formulation for regularized loss minimization problems that can directly handle data parallelism in the distributed setting. This formulation allows us to systematically derive dual coordinate optimization procedures, which we refer to as Distributed Alternating Dual Maximization (DADM). The framework extends earlier studies described in  (Boyd et al., 2011; Ma et al., 2017; Jaggi et al., 2014; Yang, 2013)  and has rigorous theoretical analyses. Moreover, with the help of the new formulation, we develop the accelerated version of DADM (Acc-DADM) by generalizing the acceleration technique from  (Shalev-Shwartz and Zhang, 2014) to the distributed setting. We also provide theoretical results for the proposed accelerated version, and the new result improves previous ones  (Yang, 2013; Ma et al., 2017) whose iteration complexities grow linearly on the condition number. Our empirical studies validate our theory and show that our accelerated approach significantly improves the previous state- of-the-art distributed dual coordinate optimization algorithms.",
            "keywords": [],
            "author": [
                "Shun Zheng",
                "Jialei Wang",
                "Fen Xia",
                "Wei Xu",
                "Tong Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-463/16-463.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Second-Order Stochastic Optimization for Machine Learning in Linear Time",
            "abstract": "First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per- iteration cost of gradient based methods, and in certain settings improve upon the overall running time over popular first-order methods. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data.",
            "keywords": [
                "second-order optimization",
                "convex optimization"
            ],
            "author": [
                "Naman Agarwal",
                "Brian Bullins",
                "Elad Hazan"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-491/16-491.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Theory of Distributed Regression with Bias Corrected Regularization Kernel Network",
            "abstract": "Distributed learning is an effective way to analyze big data. In distributed regression, a typical approach is to divide the big data into multiple blocks, apply a base regression algorithm on each of them, and then simply average the output functions learnt from these blocks. Since the average process will decrease the variance, not the bias, bias correction is expected to improve the learning performance if the base regression algorithm is a biased one. Regularization kernel network is an effective and widely used method for nonlinear regression analysis. In this paper we will investigate a bias corrected version of regularization kernel network. We derive the error bounds when it is applied to a single data set and when it is applied as a base algorithm in distributed regression. We show that, under certain appropriate conditions, the optimal learning rates can be reached in both situations.",
            "keywords": [
                "Distributed learning",
                "kernel method",
                "regularization",
                "bias correction"
            ],
            "author": [
                "Zheng-Chu Guo",
                "Lei Shi",
                "Qiang Wu"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-423/17-423.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Probabilistic Line Searches for Stochastic Optimization",
            "abstract": "In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user- controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.",
            "keywords": [
                "stochastic optimization",
                "learning rates",
                "line searches",
                "Gaussian processes"
            ],
            "author": [
                "Maren Mahsereci",
                "Philipp Hennig"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-049/17-049.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Instrumental Variables with Structural and Non-Gaussianity Assumptions",
            "abstract": "Learning a causal effect from observational data requires strong assumptions. One possible method is to use instrumental variables, which are typically justified by background knowledge. It is possible, under further assumptions, to discover whether a variable is structurally instrumental to a target causal effect . However, the few existing approaches are lacking on how general these assumptions can be, and how to express possible equivalence classes of solutions. We present instrumental variable discovery methods that systematically characterize which set of causal effects can and cannot be discovered under local graphical criteria that define instrumental variables, without reconstructing full causal graphs. We also introduce the first methods to exploit non-Gaussianity assumptions, highlighting identifiability problems and solutions. Due to the difficulty of estimating such models from finite data, we investigate how to strengthen assumptions in order to make the statistical problem more manageable.",
            "keywords": [
                "causality",
                "causal discovery"
            ],
            "author": [
                "Ricardo Silva",
                "Shohei Shimizu"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-014/17-014.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classification of Time Sequences using Graphs of Temporal Constraints",
            "abstract": "We introduce two algorithms that learn to classify Symbolic and Scalar Time Sequences (SSTS); an extension of multivariate time series. An SSTS is a set of \\emph{events} and a set of scalars. An event is defined by a symbol and a time-stamp. A scalar is defined by a symbol and a function mapping a number for each possible time stamp of the data. The proposed algorithms rely on temporal patterns called Graph of Temporal Constraints (GTC). A GTC is a directed graph in which vertices express occurrences of specific events, and edges express temporal constraints between occurrences of pairs of events. Additionally, each vertex of a GTC can be augmented with numeric constraints on scalar values. We allow GTCs to be cyclic and/or disconnected. The first of the introduced algorithms extracts sets of co-dependent GTCs to be used in a voting mechanism. The second algorithm builds decision forest like representations where each node is a GTC. In both algorithms, extraction of GTCs and model building are interleaved. Both algorithms are closely related to each other and they exhibit complementary properties including complexity, performance, and interpretability. The main novelties of this work reside in direct building of the model and efficient learning of GTC structures. We explain the proposed algorithms and evaluate their performance against a diverse collection of 59 benchmark data sets. In these experiments, our algorithms come across as highly competitive and in most cases closely match or outperform state-of-the-art alternatives in terms of the computational speed while dominating in terms of the accuracy of classification of time sequences.",
            "keywords": [
                "classification",
                "temporal data",
                "sequential data",
                "graphical constraint models",
                "decision forests",
                "symbolic and scalar time sequences"
            ],
            "author": [
                "Mathieu Guillame-Bert",
                "Artur Dubrawski"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-403/15-403.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Stochastic Variance Reduced Gradient Methods by Sampling Extra Data with Replacement",
            "abstract": "We study the round complexity of minimizing the average of convex functions under a new setting of distributed optimization where each machine can receive two subsets of functions. The first subset is from a random partition and the second subset is randomly sampled with replacement. Under this setting, we define a broad class of distributed algorithms whose local computation can utilize both subsets and design a distributed stochastic variance reduced gradient method belonging to in this class. When the condition number of the problem is small, our method achieves the optimal parallel runtime, amount of communication and rounds of communication among all distributed first-order methods up to constant factors. When the condition number is relatively large, a lower bound is provided for the number of rounds of communication needed by any algorithm in this class. Then, we present an accelerated version of our method whose the rounds of communication matches the lower bound up to logarithmic terms, which establishes that this accelerated algorithm has the lowest round complexity among all algorithms in our class under this new setting.",
            "keywords": [
                "distributed optimization",
                "communication complexity",
                "first-order method",
                "lower bound"
            ],
            "author": [
                "Jason D. Lee",
                "Qihang Lin",
                "Tengyu Ma",
                "Tianbao Yang"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-640/16-640.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Partial Least Squares for Stationary Data",
            "abstract": "We consider the kernel partial least squares algorithm for non- parametric regression with stationary dependent data. Probabilistic convergence rates of the kernel partial least squares estimator to the true regression function are established under a source and an effective dimensionality condition. It is shown both theoretically and in simulations that long range dependence results in slower convergence rates. A protein dynamics example shows high predictive power of kernel partial least squares.",
            "keywords": [
                "effective dimensionality",
                "long range dependence",
                "nonparametric regression",
                "source condition"
            ],
            "author": [
                "Marco Singer",
                "Tatyana Krivobokova",
                "Axel Munk"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-306/17-306.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust and Scalable Bayes via a Median of Subset Posterior Measures",
            "abstract": "We propose a novel approach to Bayesian analysis that is provably robust to outliers in the data and often has computational advantages over standard methods. Our technique is based on splitting the data into non-overlapping subgroups, evaluating the posterior distribution given each independent subgroup, and then combining the resulting measures. The main novelty of our approach is the proposed aggregation step, which is based on the evaluation of a median in the space of probability measures equipped with a suitable collection of distances that can be quickly and efficiently evaluated in practice. We present both theoretical and numerical evidence illustrating the improvements achieved by our method.",
            "keywords": [
                "Big data",
                "geometric median",
                "distributed computing",
                "parallel MCMC"
            ],
            "author": [
                "Stanislav Minsker",
                "Sanvesh Srivastava",
                "Lizhen Lin",
                "David B. Dunson"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-655/16-655.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling",
            "abstract": "We study parameter inference in large-scale latent variable models. We first propose a unified treatment of online inference for latent variable models from a non-canonical exponential family, and draw explicit links between several previously proposed frequentist or Bayesian methods. We then propose a novel inference method for the frequentist estimation of parameters, that adapts MCMC methods to online inference of latent variable models with the proper use of local Gibbs sampling. Then, for latent Dirichlet allocation,we provide an extensive set of experiments and comparisons with existing work, where our new approach outperforms all previously proposed methods. In particular, using Gibbs sampling for latent variable inference is superior to variational inference in terms of test log-likelihoods. Moreover, Bayesian inference through variational methods perform poorly, sometimes leading to worse fits with latent variables of higher dimensionality.",
            "keywords": [
                "Latent Variables Models",
                "Online Learning",
                "Gibbs Sampling",
                "Topic Modelling"
            ],
            "author": [
                "Christophe Dupuy",
                "Francis Bach"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-374/16-374.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Poisson Random Fields for Dynamic Feature Models",
            "abstract": "We present the Wright-Fisher Indian buffet process (WF- IBP), a probabilistic model for time-dependent data assumed to have been generated by an unknown number of latent features. This model is suitable as a prior in Bayesian nonparametric feature allocation models in which the features underlying the observed data exhibit a dependency structure over time. More specifically, we establish a new framework for generating dependent Indian buffet processes, where the Poisson random field model from population genetics is used as a way of constructing dependent beta processes. Inference in the model is complex, and we describe a sophisticated Markov Chain Monte Carlo algorithm for exact posterior simulation. We apply our construction to develop a nonparametric focused topic model for collections of time-stamped text documents and test it on the full corpus of NIPS papers published from 1987 to 2015.",
            "keywords": [
                "Bayesian nonparametrics",
                "Indian buffet process",
                "topic model",
                "Markov chain     Monte Carlo"
            ],
            "author": [
                "Valerio Perrone",
                "Paul A. Jenkins",
                "Dario Spanò",
                "Yee Whye Teh"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-541/16-541.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gap Safe Screening Rules for Sparsity Enforcing Penalties",
            "abstract": "In high dimensional regression settings, sparsity enforcing penalties have proved useful to regularize the data-fitting term. A recently introduced technique called screening rules propose to ignore some variables in the optimization leveraging the expected sparsity of the solutions and consequently leading to faster solvers. When the procedure is guaranteed not to discard variables wrongly the rules are said to be safe. In this work, we propose a unifying framework for generalized linear models regularized with standard sparsity enforcing penalties such as  or  norms. Our technique allows to discard safely more variables than previously considered safe rules, particularly for low regularization parameters. Our proposed Gap Safe rules (so called because they rely on duality gap computation) can cope with any iterative solver but are particularly well suited to (block) coordinate descent methods. Applied to many standard learning tasks, Lasso, Sparse Group Lasso, multi-task Lasso, binary and multinomial logistic regression, etc., we report significant speed-ups compared to previously proposed safe rules on all tested data sets.",
            "keywords": [
                "Convex optimization",
                "screening rules",
                "Lasso",
                "multi-task Lasso",
                "sparse logistic     regression"
            ],
            "author": [
                "Eugene Ndiaye",
                "Olivier Fercoq",
                "Alex",
                "re Gramfort",
                "Joseph Salmon"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-577/16-577.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Knowledge Graph Completion via Complex Tensor Factorization",
            "abstract": "In statistical relational learning, knowledge graph completion deals with automatically understanding the structure of large knowledge graphs---labeled directed graphs---and predicting missing relationships---labeled edges. State-of-the-art embedding models propose different trade-offs between modeling expressiveness, and time and space complexity. We reconcile both expressiveness and complexity through the use of complex-valued embeddings and explore the link between such complex-valued embeddings and unitary diagonalization. We corroborate our approach theoretically and show that all real square matrices---thus all possible relation/adjacency matrices---are the real part of some unitarily diagonalizable matrix. This results opens the door to a lot of other applications of square matrices factorization. Our approach based on complex embeddings is arguably simple, as it only involves a Hermitian dot product, the complex counterpart of the standard dot product between real vectors, whereas other methods resort to more and more complicated composition functions to increase their expressiveness. The proposed complex embeddings are scalable to large data sets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.",
            "keywords": [
                "complex embeddings",
                "tensor factorization",
                "knowledge graph",
                "matrix comple-     tion"
            ],
            "author": [
                "Théo Trouillon",
                "Christopher R. Dance",
                "Éric Gaussier",
                "Johannes Welbl",
                "Sebastian Riedel",
                "Guillaume Bouchard"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-563/16-563.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stabilized Sparse Online Learning for Sparse Data",
            "abstract": "Stochastic gradient descent (SGD) is commonly used for optimization in large-scale machine learning problems. Lanford et al. (2009) introduce a sparse online learning method to induce sparsity via truncated gradient. With high- dimensional sparse data, however, this method suffers from slow convergence and high variance due to heterogeneity in feature sparsity. To mitigate this issue, we introduce a stabilized truncated stochastic gradient descent algorithm. We employ a soft- thresholding scheme on the weight vector where the imposed shrinkage is adaptive to the amount of information available in each feature. The variability in the resulted sparse weight vector is further controlled by stability selection integrated with the informative truncation. To facilitate better convergence, we adopt an annealing strategy on the truncation rate, which leads to a balanced trade-off between exploration and exploitation in learning a sparse weight vector. Numerical experiments show that our algorithm compares favorably with the original truncated gradient SGD in terms of prediction accuracy, achieving both better sparsity and stability.",
            "keywords": [
                "sparse online learning",
                "sparse features",
                "truncated gradient",
                "stability selection"
            ],
            "author": [
                "Yuting Ma",
                "Tian Zheng"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-190/16-190.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active-set Methods for Submodular Minimization Problems",
            "abstract": "We consider the submodular function minimization (SFM) and the quadratic minimization problems regularized by the Lovasz extension of the submodular function. These optimization problems are intimately related; for example, min-cut problems and total variation denoising problems, where the cut function is submodular and its Lovasz extension is given by the associated total variation. When a quadratic loss is regularized by the total variation of a cut function, it thus becomes a total variation denoising problem and we use the same terminology in this paper for general submodular functions. We propose a new active-set algorithm for total variation denoising with the assumption of an oracle that solves the corresponding SFM problem. This can be seen as local descent algorithm over ordered partitions with explicit convergence guarantees. It is more flexible than the existing algorithms with the ability for warm-restarts using the solution of a closely related problem. Further, we also consider the case when a submodular function can be decomposed into the sum of two submodular functions  and  and assume SFM oracles for these two functions. We propose a new active-set algorithm for total variation denoising (and hence SFM by thresholding the solution at zero). This algorithm also performs local descent over ordered partitions and its ability to warm start considerably improves the performance of the algorithm. In the experiments, we compare the performance of the proposed algorithms with state-of-the-art algorithms, showing that it reduces the calls to SFM oracles.",
            "keywords": [
                "discrete optimization",
                "submodular function minimization",
                "convex optimization",
                "cut     functions"
            ],
            "author": [
                "K. S. Sesh Kumar",
                "Francis Bach"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-101/17-101.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bayesian Mixed-Effects Model to Learn Trajectories of Changes from Repeated Manifold-Valued Observations",
            "abstract": "We propose a generic Bayesian mixed-effects model to estimate the temporal progression of a biological phenomenon from observations obtained at multiple time points for a group of individuals. The progression is modeled by continuous trajectories in the space of measurements. Individual trajectories of progression result from spatiotemporal transformations of an average trajectory. These transformations allow for the quantification of changes in direction and pace at which the trajectories are followed. The framework of Riemannian geometry allows the model to be used with any kind of measurements with smooth constraints. A stochastic version of the Expectation-Maximization algorithm is used to produce maximum a posteriori estimates of the parameters. We evaluated our method using a series of neuropsychological test scores from patients with mild cognitive impairments, later diagnosed with Alzheimer's disease, and simulated evolutions of symmetric positive definite matrices. The data-driven model of impairment of cognitive functions illustrated the variability in the ordering and timing of the decline of these functions in the population. We showed that the estimated spatiotemporal transformations effectively put into correspondence significant events in the progression of individuals.",
            "keywords": [
                "longitudinal model",
                "spatiotemporal analysis",
                "Riemannian geometry"
            ],
            "author": [
                "Jean-Baptiste Schiratti",
                "Stéphanie Allassonnière",
                "Olivier Colliot",
                "Stanley Durrleman"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-197/17-197.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
            "abstract": "Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also show how to tune SGD with momentum for approximate sampling. (4) We analyze stochastic-gradient MCMC algorithms. For Stochastic- Gradient Langevin Dynamics and Stochastic-Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.",
            "keywords": [
                "approximate Bayesian inference",
                "variational inference",
                "stochastic optimization",
                "stochas-     tic gradient MCMC"
            ],
            "author": [
                "Stephan M",
                "t",
                "Matthew D. Hoffman",
                "David M. Blei"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-214/17-214.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "STORE: Sparse Tensor Response Regression and Neuroimaging Analysis",
            "abstract": "Motivated by applications in neuroimaging analysis, we propose a new regression model, Sparse TensOr REsponse regression (STORE), with a tensor response and a vector predictor. STORE embeds two key sparse structures: element-wise sparsity and low-rankness. It can handle both a non-symmetric and a symmetric tensor response, and thus is applicable to both structural and functional neuroimaging data. We formulate the parameter estimation as a non-convex optimization problem, and develop an efficient alternating updating algorithm. We establish a non- asymptotic estimation error bound for the actual estimator obtained from the proposed algorithm. This error bound reveals an interesting interaction between the computational efficiency and the statistical rate of convergence. When the distribution of the error tensor is Gaussian, we further obtain a fast estimation error rate which allows the tensor dimension to grow exponentially with the sample size. We illustrate the efficacy of our model through intensive simulations and an analysis of the Autism spectrum disorder neuroimaging data.",
            "keywords": [
                "Functional connectivity analysis",
                "High-dimensional statistical learning",
                "Mag-     netic resonance imaging",
                "Non-asymptotic error bound"
            ],
            "author": [
                "Will Wei Sun",
                "Lexin Li"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-203/17-203.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Survey of Preference-Based Reinforcement Learning Methods",
            "abstract": "Reinforcement learning (RL) techniques optimize the accumulated long-term reward of a suitably chosen reward function. However, designing such a reward function often requires a lot of task- specific prior knowledge. The designer needs to consider different objectives that do not only influence the learned behavior but also the learning progress. To alleviate these issues, preference-based reinforcement learning algorithms (PbRL) have been proposed that can directly learn from an expert's preferences instead of a hand-designed numeric reward. PbRL has gained traction in recent years due to its ability to resolve the reward shaping problem, its ability to learn from non numeric rewards and the possibility to reduce the dependence on expert knowledge. We provide a unified framework for PbRL that describes the task formally and points out the different design principles that affect the evaluation task for the human as well as the computational complexity. The design principles include the type of feedback that is assumed, the representation that is learned to capture the preferences, the optimization problem that has to be solved as well as how the exploration/exploitation problem is tackled. Furthermore, we point out shortcomings of current algorithms, propose open research questions and briefly survey practical tasks that have been solved using PbRL.",
            "keywords": [
                "Reinforcement Learning",
                "Preference Learning",
                "Qualitative Feedback",
                "Markov     Decision Process",
                "Policy Search",
                "Temporal Difference Learning"
            ],
            "author": [
                "Christian Wirth",
                "Riad Akrour",
                "Gerhard Neumann",
                "Johannes Fürnkranz"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-634/16-634.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized SURE for optimal shrinkage of singular values in low-rank matrix denoising",
            "abstract": "We consider the problem of estimating a low-rank signal matrix from noisy measurements under the assumption that the distribution of the data matrix belongs to an exponential family. In this setting, we derive generalized Stein's unbiased risk estimation (SURE) formulas that hold for any spectral estimators which shrink or threshold the singular values of the data matrix. This leads to new data-driven spectral estimators, whose optimality is discussed using tools from random matrix theory and through numerical experiments. Under the spiked population model and in the asymptotic setting where the dimensions of the data matrix are let going to infinity, some theoretical properties of our approach are compared to recent results on asymptotically optimal shrinking rules for Gaussian noise. It also leads to new procedures for singular values shrinkage in finite-dimensional matrix denoising for Gamma- distributed and Poisson-distributed measurements.",
            "keywords": [
                "Matrix denoising",
                "singular value decomposition",
                "low-rank model",
                "Gaussian     spiked population model",
                "spectral estimator"
            ],
            "author": [
                "Jérémie Bigot",
                "Charles Deledalle",
                "Delphine Féral"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-266/16-266.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dimension Estimation Using Random Connection Models",
            "abstract": "Information about intrinsic dimension is crucial to perform dimensionality reduction, compress information, design efficient algorithms, and do statistical adaptation. In this paper we propose an estimator for the intrinsic dimension of a data set. The estimator is based on binary neighbourhood information about the observations in the form of two adjacency matrices, and does not require any explicit distance information. The underlying graph is modelled according to a subset of a specific random connection model, sometimes referred to as the Poisson blob model. Computationally the estimator scales like , and we specify its asymptotic distribution and rate of convergence. A simulation study on both real and simulated data shows that our approach compares favourably with some competing methods from the literature, including approaches that rely on distance information.",
            "keywords": [
                "adaptation",
                "dimensionality reduction",
                "intrinsic dimension",
                "random connec-     tion model"
            ],
            "author": [
                "Paulo Serra",
                "Michel M",
                "jes"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-232/16-232.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Inference for Spatio-temporal Spike-and-Slab Priors",
            "abstract": "In this work, we address the problem of solving a series of underdetermined linear inverse problemblems subject to a sparsity constraint. We generalize the spike-and-slab prior distribution to encode a priori correlation of the support of the solution in both space and time by imposing a transformed Gaussian process on the spike-and-slab probabilities. An expectation propagation (EP) algorithm for posterior inference under the proposed model is derived. For large scale problems, the standard EP algorithm can be prohibitively slow. We therefore introduce three different approximation schemes to reduce the computational complexity. Finally, we demonstrate the proposed model using numerical experiments based on both synthetic and real data sets.",
            "keywords": [
                "Linear inverse problems",
                "bayesian inference",
                "expectation propagation",
                "sparsity-     promoting priors"
            ],
            "author": [
                "Michael Riis Andersen",
                "Aki Vehtari",
                "Ole Winther",
                "Lars Kai Hansen"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-464/15-464.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Nonconvex Approach for Phase Retrieval: Reshaped Wirtinger Flow and Incremental Algorithms",
            "abstract": "We study the problem of solving a quadratic system of equations, i.e., recovering a vector signal  from its magnitude measurements . We develop a gradient descent algorithm (referred to as RWF for reshaped Wirtinger flow) by minimizing the quadratic loss of the magnitude measurements. Comparing with Wirtinger flow (WF) (Candes et al., 2015), the loss function of RWF is nonconvex and nonsmooth, but better resembles the least-squares loss when the phase information is also available. We show that for random Gaussian measurements, RWF enjoys linear convergence to the true signal as long as the number of measurements is . This improves the sample complexity of WF (), and achieves the same sample complexity as truncated Wirtinger flow (TWF) (Chen and Candes, 2015), but without any sophisticated truncation in the gradient loop. Furthermore, RWF costs less computationally than WF, and runs faster numerically than both WF and TWF. We further develop an incremental (stochastic) version of RWF (IRWF) and connect it with the randomized Kaczmarz method for phase retrieval. We demonstrate that IRWF outperforms existing incremental as well as batch algorithms with experiments.",
            "keywords": [
                "gradient descent",
                "phase retrieval",
                "nonconvex optimization",
                "regularity condition"
            ],
            "author": [
                "Huishuai Zhang",
                "Yingbin Liang",
                "Yuejie Chi"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-572/16-572.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency, Breakdown Robustness, and Algorithms for Robust Improper Maximum Likelihood Clustering",
            "abstract": "The robust improper maximum likelihood estimator (RIMLE) is a new method for robust multivariate clustering finding approximately Gaussian clusters. It maximizes a pseudo- likelihood defined by adding a component with improper constant density for accommodating outliers to a Gaussian mixture. A special case of the RIMLE is MLE for multivariate finite Gaussian mixture models. In this paper we treat existence, consistency, and breakdown theory for the RIMLE comprehensively. RIMLE's existence is proved under non-smooth covariance matrix constraints. It is shown that these can be implemented via a computationally feasible Expectation-Conditional Maximization algorithm.",
            "keywords": [
                "Robustness",
                "Improper density",
                "Mixture models",
                "Model-based clustering",
                "Maximum likelihood"
            ],
            "author": [
                "Pietro Coretto",
                "Christian Hennig"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-382/16-382.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Computationally Tractable Selection of Experiments in Measurement-Constrained Regression Models",
            "abstract": "We derive computationally tractable methods to select a small subset of experiment settings from a large pool of given design points. The primary focus is on linear regression models, while the technique extends to generalized linear models and Delta's method (estimating functions of linear regression models) as well. The algorithms are based on a continuous relaxation of an otherwise intractable combinatorial optimization problem, with sampling or greedy procedures as post-processing steps. Formal approximation guarantees are established for both algorithms, and numerical results on both synthetic and real-world data confirm the effectiveness of the proposed methods.",
            "keywords": [
                "optimal selection of experiments",
                "A-optimality",
                "computationally tractable     methods"
            ],
            "author": [
                "Yining Wang",
                "Adams Wei Yu",
                "Aarti Singh"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-175/17-175.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Conditional Gradient for Sparse Estimation",
            "abstract": "Sparsity is an important modeling tool that expands the applicability of convex formulations for data analysis, however it also creates significant challenges for efficient algorithm design. In this paper we investigate the generalized conditional gradient (GCG) algorithm for solving sparse optimization problems--- demonstrating that, with some enhancements, it can provide a more efficient alternative to current state of the art approaches. After studying the convergence properties of GCG for general convex composite problems, we develop efficient methods for evaluating polar operators, a subroutine that is required in each GCG iteration. In particular, we show how the polar operator can be efficiently evaluated in learning low-rank matrices, instantiated with detailed examples on matrix completion and dictionary learning. A further improvement is achieved by interleaving GCG with fixed-rank local subspace optimization. A series of experiments on matrix completion, multi-class classification, and multi-view dictionary learning shows that the proposed method can significantly reduce the training cost of current alternatives.",
            "keywords": [
                "generalized conditional gradient",
                "frank-wolfe",
                "dictionary learning",
                "matrix     completion",
                "multi-view learning"
            ],
            "author": [
                "Yaoliang Yu",
                "Xinhua Zhang",
                "Dale Schuurmans"
            ],
            "ref": "http://jmlr.org/papers/volume18/14-348/14-348.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Following the Leader and Fast Rates in Online Linear Prediction: Curved Constraint Sets and Other Regularities",
            "abstract": "Follow the leader (FTL) is a simple online learning algorithm that is known to perform well when the loss functions are convex and positively curved. In this paper we ask whether there are other settings when FTL achieves low regret. In particular, we study the fundamental problem of linear prediction over a convex, compact domain with non-empty interior. Amongst other results, we prove that the curvature of the boundary of the domain can act as if the losses were curved: In this case, we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero, FTL enjoys logarithmic regret, while for polytope domains and stochastic data it enjoys finite expected regret. The former result is also extended to strongly convex domains by establishing an equivalence between the strong convexity of sets and the minimum curvature of their boundary, which may be of independent interest. Building on a previously known meta-algorithm, we also get an algorithm that simultaneously enjoys the worst-case guarantees and the smaller regret of FTL when the data is `easy'. Finally, we show that such guarantees are achievable directly (e.g., by the follow the regularized leader algorithm or by a shrinkage-based variant of FTL) when the constraint set is an ellipsoid.",
            "keywords": [
                "online linear optimization",
                "follow the leader",
                "logarithmic regret",
                "strongly convex     decision set"
            ],
            "author": [
                "Ruitong Huang",
                "Tor Lattimore",
                "András György",
                "Csaba Szepesvári"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-079/17-079.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Matrix Completion with Noisy Entries and Outliers",
            "abstract": "This paper considers the problem of matrix completion when the observed entries are noisy and contain outliers. It begins with introducing a new optimization criterion for which the recovered matrix is defined as its solution. This criterion uses the celebrated Huber function from the robust statistics literature to downweigh the effects of outliers. A practical algorithm is developed to solve the optimization involved. This algorithm is fast, straightforward to implement, and monotonic convergent. Furthermore, the proposed methodology is theoretically shown to be stable in a well defined sense. Its promising empirical performance is demonstrated via a sequence of simulation experiments, including image inpainting.",
            "keywords": [
                "ES-Algorithm",
                "Huber function",
                "robust methods",
                "Soft-Impute"
            ],
            "author": [
                "Raymond K. W. Wong",
                "Thomas C. M. Lee"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-076/15-076.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Faithfulness of Probability Distributions and Graphs",
            "abstract": "A main question in graphical models and causal inference is whether, given a probability distribution  (which is usually an underlying distribution of data), there is a graph (or graphs) to which  is faithful. The main goal of this paper is to provide a theoretical answer to this problem. We work with general independence models, which contain probabilistic independence models as a special case. We exploit a generalization of ordering, called preordering, of the nodes of (mixed) graphs. This allows us to provide sufficient conditions for a given independence model to be Markov to a graph with the minimum possible number of edges, and more importantly, necessary and sufficient conditions for a given probability distribution to be faithful to a graph. We present our results for the general case of mixed graphs, but specialize the definitions and results to the better-known subclasses of undirected (concentration) and bidirected (covariance) graphs as well as directed acyclic graphs.",
            "keywords": [
                "causal discovery",
                "compositional graphoid",
                "directed acyclic graph",
                "faithfulness",
                "graphical model selection",
                "independence model",
                "Markov property",
                "mixed graph"
            ],
            "author": [
                "Kayvan Sadeghi"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-275/17-275.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Community Extraction in Multilayer Networks with Heterogeneous Community Structure",
            "abstract": "Multilayer networks are a useful way to capture and model multiple, binary or weighted relationships among a fixed group of objects. While community detection has proven to be a useful exploratory technique for the analysis of single-layer networks, the development of community detection methods for multilayer networks is still in its infancy. We propose and investigate a procedure, called Multilayer Extraction, that identifies densely connected vertex-layer sets in multilayer networks. Multilayer Extraction makes use of a significance based score that quantifies the connectivity of an observed vertex-layer set through comparison with a fixed degree random graph model. Multilayer Extraction directly handles networks with heterogeneous layers where community structure may be different from layer to layer. The procedure can capture overlapping communities, as well as background vertex-layer pairs that do not belong to any community. We establish consistency of the vertex-layer set optimizer of our proposed multilayer score under the multilayer stochastic block model. We investigate the performance of Multilayer Extraction on three applications and a test bed of simulations. Our theoretical and numerical evaluations suggest that Multilayer Extraction is an effective exploratory tool for analyzing complex multilayer networks. Publicly available code is available at github.com/jdwilson4/Multila yerExtraction.",
            "keywords": [
                "community detection",
                "clustering",
                "multiplex networks",
                "score based methods"
            ],
            "author": [
                "James D. Wilson",
                "John Palowitch",
                "Shankar Bhamidi",
                "Andrew B. Nobel"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-645/16-645.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Fourier Features for Gaussian Processes",
            "abstract": "This work brings together two powerful concepts in Gaussian processes: the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for Matérn kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the data set, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non- conjugate likelihoods, our MCMC scheme reduces the cost of computation from  (for a sparse Gaussian process) to  per iteration, where  is the number of data and  is the number of features.",
            "keywords": [
                "Gaussian processes",
                "Fourier features"
            ],
            "author": [
                "James Hensman",
                "Nicolas Durrande",
                "Arno Solin"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-579/16-579.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "HyperTools: a Python Toolbox for Gaining Geometric Insights into High-Dimensional Data",
            "abstract": "Dimensionality reduction algorithms have played a foundational role in facilitating the deep understanding of complex high- dimensional data. One particularly useful application of dimensionality reduction techniques is in data visualization. Low-dimensional visualizations can help practitioners understand where machine learning algorithms might leverage the geometric properties of a dataset to improve performance. Another challenge is to generalize insights across datasets [e.g. data from multiple modalities describing the same system (Haxby et al., 2011), artwork or photographs of similar content in different styles (Zhu et al., 2017), etc.]. Several recently developed techniques(e.g. Haxby et al., 2011; Chen et al., 2015) use the procrustean transformation (Schonemann, 1966) to align the geometries of two or more spaces so that data with different axes may be plotted in a common space. We propose that each of these techniques (dimensionality reduction, alignment, and visualization) applied in sequence should be cast as a single conceptual hyperplot operation for gaining geometric insights into high-dimensional data. Our Python toolbox enables this operation in a single (highly flexible) function call.",
            "keywords": [
                "visualization",
                "high-dimensional",
                "dimensionality reduction",
                "procrustes"
            ],
            "author": [
                "Andrew C. Heusser",
                "Kirsten Ziman",
                "Lucy L. W. Owen",
                "Jeremy R. Manning"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-434/17-434.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Automatic Differentiation in Machine Learning: a Survey",
            "abstract": "Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply âautodiffâ, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names âdynamic computational graphsâ and âdifferentiable programmingâ. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms âautodiffâ, âautomatic differentiationâ, and âsymbolic differentiationâ as these are encountered more and more in machine learning settings.",
            "keywords": [
                "Backpropagation"
            ],
            "author": [
                "Atilim Gunes Baydin",
                "Barak A. Pearlmutter",
                "Alexey Andreyevich Radul",
                "Jeffrey Mark Siskind"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-468/17-468.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Normal Bandits of Unknown Means and Variances",
            "abstract": "Consider the problem of sampling sequentially from a finite number of  populations, specified by random variables ,  and ; where  denotes the outcome from population  the  time it is sampled. It is assumed that for each fixed ,  is a sequence of i.i.d. normal random variables, with unknown mean  and unknown variance . The objective is to have a policy  for deciding from which of the  populations to sample from at any time  so as to maximize the expected sum of outcomes of  total samples or equivalently to minimize the regret due to lack on information of the parameters  and . In this paper, we present a simple inflated sample mean (ISM) index policy that is asymptotically optimal in the sense of Theorem 4 below. This resolves a standing open problem from \\cite{bkmab96}. Additionally, finite horizon regret bounds are given.",
            "keywords": [
                "Inflated Sample Means",
                "UCB policies",
                "Multi-armed Bandits"
            ],
            "author": [
                "Wesley Cowan",
                "Junya Honda",
                "Michael N. Katehakis"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-154/15-154.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Probabilistic preference learning with the Mallows rank model",
            "abstract": "Ranking and comparing items is crucial for collecting information about preferences in many areas, from marketing to politics. The Mallows rank model is among the most successful approaches to analyse rank data, but its computational complexity has limited its use to a particular form based on Kendall distance. We develop new computationally tractable methods for Bayesian inference in Mallows models that work with any right-invariant distance. Our method performs inference on the consensus ranking of the items, also when based on partial rankings, such as top- items or pairwise comparisons. We prove that items that none of the assessors has ranked do not influence the maximum a posteriori consensus ranking, and can therefore be ignored. When assessors are many or heterogeneous, we propose a mixture model for clustering them in homogeneous subgroups, with cluster-specific consensus rankings. We develop approximate stochastic algorithms that allow a fully probabilistic analysis, leading to coherent quantifications of uncertainties. We make probabilistic predictions on the class membership of assessors based on their ranking of just some items, and predict missing individual preferences, as needed in recommendation systems. We test our approach using several experimental and benchmark datasets.",
            "keywords": [
                "Incomplete rankings",
                "Pairwise comparisons",
                "Preference learning with uncer-     tainty",
                "Recommendation systems",
                "Markov Chain Monte Carloc 2018 Valeria Vitelli",
                "Øystein Sørensen",
                "Marta Crispino"
            ],
            "author": [
                "Valeria Vitelli",
                "Øystein Sørensen",
                "Marta Crispino",
                "Arnoldo Frigessi",
                "Elja Arjas"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-481/15-481.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gradient Estimation with Simultaneous Perturbation and Compressive Sensing",
            "abstract": "We propose a scheme for finding a \"good\" estimator for the gradient of a function on a high-dimensional space with few function evaluations, for applications where function evaluations are expensive and the function under consideration is not sensitive in all coordinates locally, making its gradient almost sparse. Exploiting the latter aspect, our method combines ideas from Spall's Simultaneous Perturbation Stochastic Approximation with compressive sensing. We theoretically justify its computational advantages and illustrate them empirically by numerical experiments. In particular, applications to estimating gradient outer product matrix as well as standard optimization problems are illustrated via simulations.",
            "keywords": [],
            "author": [
                "Vivek S. Borkar",
                "Vikranth R. Dwaracherla",
                "Neeraja Sahasrabudhe"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-592/15-592.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Principled Selection of Hyperparameters in the Latent Dirichlet Allocation Model",
            "abstract": "Latent Dirichlet Allocation (LDA) is a well known topic model that is often used to make inference regarding the properties of collections of text documents. LDA is a hierarchical Bayesian model, and involves a prior distribution on a set of latent topic variables. The prior is indexed by certain hyperparameters, and even though these have a large impact on inference, they are usually chosen either in an ad-hoc manner, or by applying an algorithm whose theoretical basis has not been firmly established. We present a method, based on a combination of Markov chain Monte Carlo and importance sampling, for estimating the maximum likelihood estimate of the hyperparameters. The method may be viewed as a computational scheme for implementation of an empirical Bayes analysis. It comes with theoretical guarantees, and a key feature of our approach is that we provide theoretically-valid error margins for our estimates. Experiments on both synthetic and real data show good performance of our methodology.",
            "keywords": [
                "Empirical Bayes inference",
                "latent Dirichlet allocation",
                "Markov chain Monte Carlo",
                "model selection"
            ],
            "author": [
                "Clint P. George",
                "Hani Doss"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-595/15-595.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Deep Learning the Ising Model Near Criticality",
            "abstract": "It is well established that neural networks with deep architectures perform better than shallow networks for many tasks in machine learning. In statistical physics, while there has been recent interest in representing physical data with generative modelling, the focus has been on shallow neural networks. A natural question to ask is whether deep neural networks hold any advantage over shallow networks in representing such data. We investigate this question by using unsupervised, generative graphical models to learn the probability distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep belief networks, and deep restricted Boltzmann networks are trained on thermal spin configurations from this system, and compared to the shallow architecture of the restricted Boltzmann machine. We benchmark the models, focussing on the accuracy of generating energetic observables near the phase transition, where these quantities are most difficult to approximate. Interestingly, after training the generative networks, we observe that the accuracy essentially depends only on the number of neurons in the first hidden layer of the network, and not on other model details such as network depth or model type. This is evidence that shallow networks are more efficient than deep networks at representing physical probability distributions associated with Ising systems near criticality.",
            "keywords": [
                "deep learning",
                "restricted Boltzmann machine",
                "deep belief network"
            ],
            "author": [
                "Alan Morningstar",
                "Roger G. Melko"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-527/17-527.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "pomegranate: Fast and Flexible Probabilistic Modeling in Python",
            "abstract": "We present pomegranate, an open source machine learning package for probabilistic modeling in Python. Probabilistic modeling encompasses a wide range of methods that explicitly describe uncertainty using probability distributions. Three widely used probabilistic models implemented in pomegranate are general mixture models, hidden Markov models, and Bayesian networks. A primary focus of pomegranate is to abstract away the complexities of training models from their definition. This allows users to focus on specifying the correct model for their application instead of being limited by their understanding of the underlying algorithms. An aspect of this focus involves the collection of additive sufficient statistics from data sets as a strategy for training models. This approach trivially enables many useful learning strategies, such as out-of-core learning, minibatch learning, and semi-supervised learning, without requiring the user to consider how to partition data or modify the algorithms to handle these tasks themselves. pomegranate is written in Cython to speed up calculations and releases the global interpreter lock to allow for built-in multithreaded parallelism, making it competitive with---or outperform---other implementations of similar algorithms. This paper presents an overview of the design choices in pomegranate, and how they have enabled complex features to be supported by simple code. The code is available at \\url{https://github.com/jmschrei/pomegranate}",
            "keywords": [
                "probabilistic modeling",
                "Python",
                "Cython",
                "machine learning"
            ],
            "author": [
                "Jacob Schreiber"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-636/17-636.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria",
            "abstract": "In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account risk, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile risk-constrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application.",
            "keywords": [
                "Markov Decision Process",
                "Reinforcement Learning",
                "Conditional Value-at-Risk",
                "Chance-     Constrained Optimization",
                "Policy Gradient Algorithms"
            ],
            "author": [
                "Yinlam Chow",
                "Mohammad Ghavamzadeh",
                "Lucas Janson",
                "Marco Pavone"
            ],
            "ref": "http://jmlr.org/papers/volume18/15-636/15-636.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Identifiability of $\\ell_1$-minimization Dictionary Learning: a Sufficient and Almost Necessary Condition",
            "abstract": "We study the theoretical properties of learning a dictionary from  signals  for  via -minimization. We assume that 's are  random linear combinations of the  columns from a complete (i.e., square and invertible) reference dictionary . Here, the random linear coefficients are generated from either the -sparse Gaussian model or the Bernoulli-Gaussian model. First, for the population case, we establish a sufficient and almost necessary condition for the reference dictionary  to be locally identifiable, i.e., a strict local minimum of the expected -norm objective function. Our condition covers both sparse and dense cases of the random linear coefficients and significantly improves the sufficient condition by Gribonval and Schnass (2010). In addition, we show that for a complete -coherent reference dictionary, i.e., a dictionary with absolute pairwise column inner-product at most , local identifiability holds even when the random linear coefficient vector has up to  nonzero entries. Moreover, our local identifiability results also translate to the finite sample case with high probability provided that the number of signals  scales as .",
            "keywords": [
                "dictionary learning"
            ],
            "author": [
                "Siqi Wu",
                "Bin Yu"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-119/16-119.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Behavior of Intrinsically High-Dimensional Spaces: Distances, Direct and Reverse Nearest Neighbors, and Hubness",
            "abstract": "Over the years, different characterizations of the curse of dimensionality have been provided, usually stating the conditions under which, in the limit of the infinite dimensionality, distances become indistinguishable. However, these characterizations almost never address the form of associated distributions in the finite, although high- dimensional, case. This work aims to contribute in this respect by investigating the distribution of distances, and of direct and reverse nearest neighbors, in intrinsically high-dimensional spaces. Indeed, we derive a closed form for the distribution of distances from a given point, for the expected distance from a given point to its th nearest neighbor, and for the expected size of the approximate set of neighbors of a given point in finite high-dimensional spaces. Additionally, the hubness problem is considered, which is related to the form of the function  representing the number of points that have a given point as one of their  nearest neighbors, which is also called the number of -occurrences. Despite the extensive use of this function, the precise characterization of its form is a longstanding problem. We derive a closed form for the number of -occurrences associated with a given point in finite high- dimensional spaces, together with the associated limiting probability distribution. By investigating the relationships with the hubness phenomenon emerging in network science, we find that the distribution of node (in-)degrees of some real-life, large-scale networks has connections with the distribution of -occurrences described herein.",
            "keywords": [
                "high-dimensional data",
                "distance concentration",
                "distribution of distances",
                "nearest neighbors",
                "reverse nearest neighbors"
            ],
            "author": [
                "Fabrizio Angiulli"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-151/17-151.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence of Unregularized Online Learning Algorithms",
            "abstract": "In this paper we study the convergence of online gradient descent algorithms in reproducing kernel Hilbert spaces (RKHSs) without regularization. We establish a sufficient condition and a necessary condition for the convergence of excess generalization errors in expectation. A sufficient condition for the almost sure convergence is also given. With high probability, we provide explicit convergence rates of the excess generalization errors for both averaged iterates and the last iterate, which in turn also imply convergence rates with probability one. To our best knowledge, this is the first high- probability convergence rate for the last iterate of online gradient descent algorithms in the general convex setting. Without any boundedness assumptions on iterates, our results are derived by a novel use of two measures of the algorithm's one- step progress, respectively by generalization errors and by distances in RKHSs, where the variances of the involved martingales are cancelled out by the descent property of the algorithm.",
            "keywords": [
                "Learning theory",
                "Online learning",
                "Convergence analysis"
            ],
            "author": [
                "Yunwen Lei",
                "Lei Shi",
                "Zheng-Chu Guo"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-457/17-457.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Analysis of Distributed Inference with Vector-Valued Gaussian Belief Propagation",
            "abstract": "This paper considers inference over distributed linear Gaussian models using factor graphs and Gaussian belief propagation (BP). The distributed inference algorithm involves only local computation of the information matrix and of the mean vector, and message passing between neighbors. Under broad conditions, it is shown that the message information matrix converges to a unique positive definite limit matrix for arbitrary positive semidefinite initialization, and it approaches an arbitrarily small neighborhood of this limit matrix at an exponential rate. A necessary and sufficient convergence condition for the belief mean vector to converge to the optimal centralized estimator is provided under the assumption that the message information matrix is initialized as a positive semidefinite matrix. Further, it is shown that Gaussian BP always converges when the underlying factor graph is given by the union of a forest and a single loop. The proposed convergence condition in the setup of distributed linear Gaussian models is shown to be strictly weaker than other existing convergence conditions and requirements, including the Gaussian Markov random field based walk-summability condition, and applicable to a large class of scenarios.",
            "keywords": [
                "Graphical Model",
                "Large-Scale Networks",
                "Linear Gaussian Model",
                "Markov     Random Field"
            ],
            "author": [
                "Jian Du",
                "Shaodan Ma",
                "Yik-Chung Wu",
                "Soummya Kar",
                "José M. F. Moura"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-556/16-556.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "auDeep: Unsupervised Learning of Representations from Audio with Deep Recurrent Neural Networks",
            "abstract": "auDeep is a Python toolkit for deep unsupervised representation learning from acoustic data. It is based on a recurrent sequence to sequence autoencoder approach which can learn representations of time series data by taking into account their temporal dynamics. We provide an extensive command line interface in addition to a Python API for users and developers, both of which are comprehensively documented and publicly available at https://github.com/auDeep/auDeep. Experimental results indicate that auDeep features are competitive with state-of-the art audio classification.",
            "keywords": [
                "deep feature learning",
                "sequence to sequence learning",
                "recurrent neural net-     works",
                "autoencoders"
            ],
            "author": [
                "Michael Freitag",
                "Shahin Amiriparian",
                "Sergey Pugachevskiy",
                "Nicholas Cummins",
                "Bj\\\"{o}rn Schuller"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-406/17-406.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Stability of Feature Selection Algorithms",
            "abstract": "Feature Selection is central to modern data science, from exploratory data analysis to predictive model-building. The âstabilityâ of a feature selection algorithm refers to the robustness of its feature preferences, with respect to data sampling and to its stochastic nature. An algorithm is `unstable' if a small change in data leads to large changes in the chosen feature subset. Whilst the idea is simple, quantifying this has proven more challenging---we note numerous proposals in the literature, each with different motivation and justification. We present a rigorous statistical treatment for this issue. In particular, with this work we consolidate the literature and provide (1) a deeper understanding of existing work based on a small set of properties, and (2) a clearly justified statistical approach with several novel benefits. This approach serves to identify a stability measure obeying all desirable properties, and (for the first time in the literature) allowing confidence intervals and hypothesis tests on the stability, enabling rigorous experimental comparison of feature selection algorithms.",
            "keywords": [
                "stability"
            ],
            "author": [
                "Sarah Nogueira",
                "Konstantinos Sechidis",
                "Gavin Brown"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-514/17-514.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Likelihood Estimation for Mixtures of Spherical Gaussians is NP-hard",
            "abstract": "This paper presents NP-hardness and hardness of approximation results for maximum likelihood estimation of mixtures of spherical Gaussians.",
            "keywords": [
                "Mixtures of Gaussians",
                "maximum likelihood"
            ],
            "author": [
                "Christopher Tosh",
                "Sanjoy Dasgupta"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-657/16-657.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The DFS Fused Lasso: Linear-Time Denoising over General Graphs",
            "abstract": "The fused lasso, also known as (anisotropic) total variation denoising, is widely used for piecewise constant signal estimation with respect to a given undirected graph. The fused lasso estimate is highly nontrivial to compute when the underlying graph is large and has an arbitrary structure. But for a special graph structure, namely, the chain graph, the fused lasso---or simply, 1d fused lasso---can be computed in linear time. In this paper, we revisit a result recently established in the online classification literature (Herbster et al., 2009; Cesa-Bianchi et al., 2013) and show that it has important implications for signal denoising on graphs. The result can be translated to our setting as follows. Given a general graph, if we run the standard depth-first search (DFS) traversal algorithm, then the total variation of any signal over the chain graph induced by DFS is no more than twice its total variation over the original graph. This result leads to several interesting theoretical and computational conclusions. Letting  and  denote the number of edges and nodes, respectively, of the graph in consideration, it implies that for an underlying signal with total variation  over the graph, the fused lasso (properly tuned) achieves a mean squared error rate of . Moreover, precisely the same mean squared error rate is achieved by running the 1d fused lasso on the DFS-induced chain graph. Importantly, the latter estimator is simple and computationally cheap, requiring  operations to construct the DFS-induced chain and  operations to compute the 1d fused lasso solution over this chain. Further, for trees that have bounded maximum degree, the error rate of  cannot be improved, in the sense that it is the minimax rate for signals that have total variation  over the tree. Finally, several related results also hold---for example, the analogous result holds for a roughness measure defined by the  norm of differences across edges in place of the total variation metric.",
            "keywords": [],
            "author": [
                "Oscar Hernan Madrid Padilla",
                "James Sharpnack",
                "James G. Scott",
                "Ryan J. Tibshirani"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-532/16-532.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Community Detection and Stochastic Block Models: Recent Developments",
            "abstract": "The stochastic block model (SBM) is a random graph model with planted clusters. It is widely employed as a canonical model to study clustering and community detection, and provides generally a fertile ground to study the statistical and computational tradeoffs that arise in network and data sciences. This note surveys the recent developments that establish the fundamental limits for community detection in the SBM, both with respect to information-theoretic and computational thresholds, and for various recovery requirements such as exact, partial and weak recovery (a.k.a., detection). The main results discussed are the phase transitions for exact recovery at the Chernoff-Hellinger threshold, the phase transition for weak recovery at the Kesten- Stigum threshold, the optimal distortion-SNR tradeoff for partial recovery, the learning of the SBM parameters and the gap between information-theoretic and computational thresholds. The note also covers some of the algorithms developed in the quest of achieving the limits, in particular two-round algorithms via graph-splitting, semi-definite programming, linearized belief propagation, classical and nonbacktracking spectral methods. A few open problems are also discussed.",
            "keywords": [
                "Community detection",
                "clustering",
                "stochastic block models",
                "random graphs",
                "unsupervised learning",
                "spectral algorithms",
                "computational gaps"
            ],
            "author": [
                "Emmanuel Abbe"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-480/16-480.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On $b$-bit Min-wise Hashing for Large-scale Regression and Classification with Sparse Data",
            "abstract": "Large-scale regression problems where both the number of variables, , and the number of observations, , may be large and in the order of millions or more, are becoming increasingly more common. Typically the data are sparse: only a fraction of a percent of the entries in the design matrix are non-zero. Nevertheless, often the only computationally feasible approach is to perform dimension reduction to obtain a new design matrix with far fewer columns and then work with this compressed data. -bit min-wise hashing (Li and KÃ¶nig, 2011; Li et al., 2011) is a promising dimension reduction scheme for sparse matrices which produces a set of random features such that regression on the resulting design matrix approximates a kernel regression with the resemblance kernel. In this work, we derive bounds on the prediction error of such regressions. For both linear and logistic models, we show that the average prediction error vanishes asymptotically as long as , where  is the average number of non-zero entries in each row of the design matrix and  is the coefficient of the linear predictor. We also show that ordinary least squares or ridge regression applied to the reduced data can in fact allow us fit more flexible models. We obtain non-asymptotic prediction error bounds for interaction models and for models where an unknown row normalisation must be applied in order for the signal to be linear in the predictors.",
            "keywords": [
                "large-scale data",
                "min-wise hashing",
                "resemblance kernel",
                "ridge regression"
            ],
            "author": [
                "Rajen D. Shah",
                "Nicolai Meinshausen"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-587/16-587.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity",
            "abstract": "The use of convex regularizers allows for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, a popular subclass of -based nonconvex sparsity-inducing and low-rank regularizers is considered. This includes nonconvex variants of lasso, sparse group lasso, tree- structured lasso, nuclear norm and total variation regularizers. We propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex one, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the proximal algorithm, Frank-Wolfe algorithm, alternating direction method of multipliers and stochastic gradient descent). This is further extended to consider cases where the convexified regularizer does not have a closed-form proximal step, and when the loss function is nonconvex nonsmooth. Extensive experiments on a variety of machine learning application scenarios show that optimizing the transformed problem is much faster than running the state-of-the-art on the original problem.",
            "keywords": [
                "Nonconvex optimization",
                "Nonconvex regularization",
                "Proximal algorithm",
                "Frank-     Wolfe algorithm"
            ],
            "author": [
                "Quanming Yao",
                "James T. Kwok"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-078/17-078.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Mode-Seeking Clustering and Density Ridge Estimation via Direct Estimation of Density-Derivative-Ratios",
            "abstract": "Modes and ridges of the probability density function behind observed data are useful geometric features. Mode-seeking clustering assigns cluster labels by associating data samples with the nearest modes, and estimation of density ridges enables us to find lower-dimensional structures hidden in data. A key technical challenge both in mode-seeking clustering and density ridge estimation is accurate estimation of the ratios of the first- and second-order density derivatives to the density. A naive approach takes a three-step approach of first estimating the data density, then computing its derivatives, and finally taking their ratios. However, this three-step approach can be unreliable because a good density estimator does not necessarily mean a good density derivative estimator, and division by the estimated density could significantly magnify the estimation error. To cope with these problems, we propose a novel estimator for the density-derivative-ratios. The proposed estimator does not involve density estimation, but rather directly approximates the ratios of density derivatives of any order. Moreover, we establish a convergence rate of the proposed estimator. Based on the proposed estimator, novel methods both for mode-seeking clustering and density ridge estimation are developed, and the respective convergence rates to the mode and ridge of the underlying density are also established. Finally, we experimentally demonstrate that the developed methods significantly outperform existing methods, particularly for relatively high-dimensional data.",
            "keywords": [],
            "author": [
                "Hiroaki Sasaki",
                "Takafumi Kanamori",
                "Aapo Hyvärinen",
                "Gang Niu",
                "Masashi Sugiyama"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-380/17-380.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "To Tune or Not to Tune the Number of Trees in Random Forest",
            "abstract": "The number of trees  in the random forest (RF) algorithm for supervised learning has to be set by the user. It is unclear whether  should simply be set to the largest computationally manageable value or whether a smaller  may be sufficient or in some cases even better. While the principle underlying bagging is that more trees are better, in practice the classification error rate sometimes reaches a minimum before increasing again for increasing number of trees. The goal of this paper is four-fold: (i) providing theoretical results showing that the expected error rate may be a non-monotonous function of the number of trees and explaining under which circumstances this happens; (ii) providing theoretical results showing that such non-monotonous patterns cannot be observed for other performance measures such as the Brier score and the logarithmic loss (for classification) and the mean squared error (for regression); (iii) illustrating the extent of the problem through an application to a large number (n = 306) of datasets from the public database OpenML; (iv) finally arguing in favor of setting  to a computationally feasible large number as long as classical error measures based on average loss are considered.",
            "keywords": [
                "Random forest",
                "number of trees",
                "bagging",
                "out-of-bag"
            ],
            "author": [
                "Philipp Probst",
                "Anne-Laure Boulesteix"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-269/17-269.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Divide-and-Conquer for Debiased $l_1$-norm Support Vector Machine in Ultra-high Dimensions",
            "abstract": "-norm support vector machine (SVM) generally has competitive performance compared to standard -norm support vector machine in classification problems, with the advantage of automatically selecting relevant features. We propose a divide-and-conquer approach in the large sample size and high-dimensional setting by splitting the data set across multiple machines, and then averaging the debiased estimators. Extension of existing theoretical studies to SVM is challenging in estimation of the inverse Hessian matrix that requires approximating the Dirac delta function via smoothing. We show that under appropriate conditions the aggregated estimator can obtain the same convergence rate as the central estimator utilizing all observations.",
            "keywords": [
                "classification",
                "debiased estimator",
                "distributed estimator",
                "divide and conquer"
            ],
            "author": [
                "Heng Lian",
                "Zengyan Fan"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-343/17-343.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Beyond the Hazard Rate: More Perturbation Algorithms for Adversarial Multi-armed Bandits",
            "abstract": "Recent work on follow the perturbed leader (FTPL) algorithms for the adversarial multi-armed bandit problem has highlighted the role of the hazard rate of the distribution generating the perturbations. Assuming that the hazard rate is bounded, it is possible to provide regret analyses for a variety of FTPL algorithms for the multi-armed bandit problem. This paper pushes the inquiry into regret bounds for FTPL algorithms beyond the bounded hazard rate condition. There are good reasons to do so: natural distributions such as the uniform and Gaussian violate the condition. We give regret bounds for both bounded support and unbounded support distributions without assuming the hazard rate condition. We also disprove a conjecture that the Gaussian distribution cannot lead to a low-regret algorithm. In fact, it turns out that it leads to near optimal regret, up to logarithmic factors. A key ingredient in our approach is the introduction of a new notion called the generalized hazard rate.",
            "keywords": [
                "online learning",
                "regret",
                "multi-armed bandits",
                "follow the perturbed leader"
            ],
            "author": [
                "Zifan Li",
                "Ambuj Tewari"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-364/17-364.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Faster Convergence of Cyclic Block Coordinate Descent-type Methods for Strongly Convex Minimization",
            "abstract": "The cyclic block coordinate descent-type (CBCD-type) methods, which perform iterative updates for a few coordinates (a block) simultaneously throughout the procedure, have shown remarkable computational performance for solving strongly convex minimization problems. Typical applications include many popular statistical machine learning methods such as elastic-net regression, ridge penalized logistic regression, and sparse additive regression. Existing optimization literature has shown that for strongly convex minimization, the CBCD-type methods attain iteration complexity of , where  is a pre-specified accuracy of the objective value, and  is the number of blocks. However, such iteration complexity explicitly depends on , and therefore is at least  times worse than the complexity  of gradient descent (GD) methods. To bridge this theoretical gap, we propose an improved convergence analysis for the CBCD-type methods. In particular, we first show that for a family of quadratic minimization problems, the iteration complexity  of the CBCD-type methods matches that of the GD methods in term of dependency on , up to a  factor. Thus our complexity bounds are sharper than the existing bounds by at least a factor of . We also provide a lower bound to confirm that our improved complexity bounds are tight (up to a  factor), under the assumption that the largest and smallest eigenvalues of the Hessian matrix do not scale with . Finally, we generalize our analysis to other strongly convex minimization problems beyond quadratic ones.",
            "keywords": [],
            "author": [
                "Xingguo Li",
                "Tuo Zhao",
                "Raman Arora",
                "Han Liu",
                "Mingyi Hong"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-157/17-157.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization",
            "abstract": "Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Ã¸uralg , for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Ã¸uralg with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Ã¸uralg can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.",
            "keywords": [
                "hyperparameter optimization",
                "model selection",
                "infinite-armed bandits",
                "online     optimization"
            ],
            "author": [
                "Lisha Li",
                "Kevin Jamieson",
                "Giulia DeSalvo",
                "Afshin Rostamizadeh",
                "Ameet Talwalkar"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-558/16-558.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Submatrix localization via message passing",
            "abstract": "The principal submatrix localization problem deals with recovering a  principal submatrix of elevated mean  in a large  symmetric matrix subject to additive standard Gaussian noise, or more generally, mean zero, variance one, subgaussian noise. This problem serves as a prototypical example for community detection, in which the community corresponds to the support of the submatrix. The main result of this paper is that in the regime , the support of the submatrix can be weakly recovered (with  misclassification errors on average) by an optimized message passing algorithm if , the signal-to-noise ratio, exceeds . This extends a result by Deshpande and Montanari previously obtained for  and  In addition, the algorithm can be combined with a voting procedure to achieve the information-theoretic limit of exact recovery with sharp constants for all . The total running time of the algorithm is . Another version of the submatrix localization problem, known as noisy biclustering, aims to recover a  submatrix of elevated mean  in a large  Gaussian matrix. The optimized message passing algorithm and its analysis are adapted to the bicluster problem assuming  and  A sharp information-theoretic condition for the weak recovery of both clusters is also identified.",
            "keywords": [
                "Submatrix localization",
                "biclustering",
                "message passing",
                "spectral algorithms     computational complexity"
            ],
            "author": [
                "Bruce Hajek",
                "Yihong Wu",
                "Jiaming Xu"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-297/17-297.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations",
            "abstract": "We introduce a method to train Quantized Neural Networks (QNNs) --- neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At train-time the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves  top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online.",
            "keywords": [],
            "author": [
                "Itay Hubara",
                "Matthieu Courbariaux",
                "Daniel Soudry",
                "Ran El-Yaniv",
                "Yoshua Bengio"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-456/16-456.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Significance-based community detection in weighted networks",
            "abstract": "Community detection is the process of grouping strongly connected nodes in a network. Many community detection methods for un-weighted networks have a theoretical basis in a null model. Communities discovered by these methods therefore have interpretations in terms of statistical significance. In this paper, we introduce a null for weighted networks called the continuous configuration model. First, we propose a community extraction algorithm for weighted networks which incorporates iterative hypothesis testing under the null. We prove a central limit theorem for edge-weight sums and asymptotic consistency of the algorithm under a weighted stochastic block model. We then incorporate the algorithm in a community detection method called CCME. To benchmark the method, we provide a simulation framework involving the null to plant âbackground\" nodes in weighted networks with communities. We show that the empirical performance of CCME on these simulations is competitive with existing methods, particularly when overlapping communities and background nodes are present. To further validate the method, we present two real-world networks with potential background nodes and analyze them with CCME, yielding results that reveal macro- features of the corresponding systems.",
            "keywords": [],
            "author": [
                "John Palowitch",
                "Shankar Bhamidi",
                "Andrew B. Nobel"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-377/17-377.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Method for Persistence Diagrams via Kernel Embedding and Weight Factor",
            "abstract": "Topological data analysis (TDA) is an emerging mathematical concept for characterizing shapes in complicated data. In TDA, persistence diagrams are widely recognized as a useful descriptor of data, distinguishing robust and noisy topological properties. This paper introduces a kernel method for persistence diagrams to develop a statistical framework in TDA. The proposed kernel is stable under perturbation of data, enables one to explicitly control the effect of persistence by a weight function, and allows an efficient and accurate approximate computation. The method is applied into practical data on granular systems, oxide glasses and proteins, showing advantages of our method compared to other relevant methods for persistence diagrams.",
            "keywords": [
                "topological data analysis",
                "persistence diagrams",
                "kernel method",
                "kernel em-     bedding"
            ],
            "author": [
                "Genki Kusano",
                "Kenji Fukumizu",
                "Yasuaki Hiraoka"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-317/17-317.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pycobra: A Python Toolbox for Ensemble Learning and Visualisation",
            "abstract": "We introduce pycobra, a Python library devoted to ensemble learning (regression and classification) and visualisation. Its main assets are the implementation of several ensemble learning algorithms, a flexible and generic interface to compare and blend any existing machine learning algorithm available in Python libraries (as long as a predict method is given), and visualisation tools such as Voronoi tessellations. pycobra is fully scikit-learn compatible and is released under the MIT open-source license. pycobra can be downloaded from the Python Package Index (PyPi) and Machine Learning Open Source Software (MLOSS). The current version (along with Jupyter notebooks, extensive documentation, and continuous integration tests) is available at https://github.com/bhargavvader/pycobra and official documentation website is https://modal.lille.inria.fr/pycobra.",
            "keywords": [
                "ensemble methods",
                "machine learning",
                "Voronoi tesselation",
                "Python"
            ],
            "author": [
                "Benjamin Guedj",
                "Bhargav Srinivasa Desikan"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-228/17-228.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "KELP: a Kernel-based Learning Platform",
            "abstract": "KELP is a Java framework that enables fast and easy implementation of kernel functions over discrete data, such as strings, trees or graphs and their combination with standard vectorial kernels. Additionally, it provides several kernel- based algorithms, e.g., online and batch kernel machines for classification, regression and clustering, and a Java environment for easy implementation of new algorithms. KELP is a versatile toolkit, very appealing both to experts and practitioners of machine learning and Java language programming, who can find extensive documentation, tutorials and examples of increasing complexity on the accompanying website. Interestingly, KELP can be also used without any knowledge of Java programming through command line tools and JSON/XML interfaces enabling the declaration and instantiation of articulated learning models using simple templates. Finally, the extensive use of modularity and interfaces in KELP enables developers to easily extend it with their own kernels and algorithms.",
            "keywords": [
                "Kernel Machines",
                "Structured Data and Kernels"
            ],
            "author": [
                "Simone Filice",
                "Giuseppe Castellucci",
                "Giovanni Da San Martino",
                "Aless",
                "ro Moschitti",
                "Danilo Croce",
                "Roberto Basili"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-087/16-087.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Uncovering Causality from Multivariate Hawkes Integrated Cumulants",
            "abstract": "We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each node of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. As a consequence, it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the process. A theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we show, on numerical experiments, that our approach is indeed very robust with respect to the shape of the kernels and gives appealing results on the MemeTracker database and on financial order book data.",
            "keywords": [
                "Hawkes Process",
                "Causality Inference",
                "Cumulants"
            ],
            "author": [
                "Massil Achab",
                "Emmanuel Bacry",
                "Stéphane Gaïffas",
                "Iacopo Mastromatteo",
                "Jean-François Muzy"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-284/17-284.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research",
            "abstract": "This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature.",
            "keywords": [
                "crowdsourcing",
                "data generation",
                "model evaluation",
                "hybrid intelligence",
                "be-     havioral experiments",
                "incentives"
            ],
            "author": [
                "Jennifer Wortman Vaughan"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-234/17-234.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Enhancing Identification of Causal Effects by Pruning",
            "abstract": "Causal models communicate our assumptions about causes and effects in real-world phenomena. Often the interest lies in the identification of the effect of an action which means deriving an expression from the observed probability distribution for the interventional distribution resulting from the action. In many cases an identifiability algorithm may return a complicated expression that contains variables that are in fact unnecessary. In practice this can lead to additional computational burden and increased bias or inefficiency of estimates when dealing with measurement error or missing data. We present graphical criteria to detect variables which are redundant in identifying causal effects. We also provide an improved version of a well-known identifiability algorithm that implements these criteria.",
            "keywords": [
                "causal inference",
                "identifiability",
                "causal model",
                "pruning"
            ],
            "author": [
                "Santtu Tikka",
                "Juha Karvanen"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-563/17-563.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "From Predictive Methods to Missing Data Imputation: An Optimization Approach",
            "abstract": "Missing data is a common problem in real-world settings and for this reason has attracted significant attention in the statistical literature. We propose a flexible framework based on formal optimization to impute missing data with mixed continuous and categorical variables. This framework can readily incorporate various predictive models including -nearest neighbors, support vector machines, and decision tree based methods, and can be adapted for multiple imputation. We derive fast first-order methods that obtain high quality solutions in seconds following a general imputation algorithm opt.impute presented in this paper. We demonstrate that our proposed method improves out-of-sample accuracy in large-scale computational experiments across a sample of 84 data sets taken from the UCI Machine Learning Repository. In all scenarios of missing at random mechanisms and various missing percentages, opt.impute produces the best overall imputation in most data sets benchmarked against five other methods: mean impute, -nearest neighbors, iterative knn, Bayesian PCA, and predictive-mean matching, with an average reduction in mean absolute error of 8.3 against the best cross-validated benchmark method. Moreover, opt.impute leads to improved out-of-sample performance of learning algorithms trained using the imputed data, demonstrated by computational experiments on 10 downstream tasks. For models trained using opt.impute single imputations with 50 data missing, the average out-of- sample  is 0.339 in the regression tasks and the average out-of-sample accuracy is 86.1 in the classification tasks, compared to 0.315 and 84.4 for the best cross-validated benchmark method. In the multiple imputation setting, downstream models trained using opt.impute obtain a statistically significant improvement over models trained using multivariate imputation by chained equations (mice) in 8/10 missing data scenarios considered.",
            "keywords": [
                "missing data imputation",
                "K-NN",
                "SVM"
            ],
            "author": [
                "Dimitris Bertsimas",
                "Colin Pawlowski",
                "Ying Daisy Zhuo"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-073/17-073.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Saturating Splines and Feature Selection",
            "abstract": "We extend the adaptive regression spline model by incorporating saturation, the natural requirement that a function extend as a constant outside a certain range. We fit saturating splines to data via a convex optimization problem over a space of measures, which we solve using an efficient algorithm based on the conditional gradient method. Unlike many existing approaches, our algorithm solves the original infinite- dimensional (for splines of degree at least two) optimization problem without pre-specified knot locations. We then adapt our algorithm to fit generalized additive models with saturating splines as coordinate functions and show that the saturation requirement allows our model to simultaneously perform feature selection and nonlinear function fitting. Finally, we briefly sketch how the method can be extended to higher order splines and to different requirements on the extension outside the data range.",
            "keywords": [
                "Convex optimization",
                "feature selection",
                "splines",
                "lasso"
            ],
            "author": [
                "Nicholas Boyd",
                "Trevor Hastie",
                "Stephen Boyd",
                "Benjamin Recht",
                "Michael I. Jordan"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-178/17-178.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization",
            "abstract": "A popular approach for solving stochastic optimization problems is the stochastic gradient descent (SGD) method. Although the SGD iteration is computationally cheap and its practical performance may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate choice of parameters. To avoid some of the drawbacks of SGD, stochastic proximal point (SPP) algorithms have been recently considered. We introduce a new variant of the SPP method for solving stochastic convex problems subject to (in)finite intersection of constraints satisfying a linear regularity condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex Lipschitz continuous objective functions, we prove nonasymptotic convergence rates in terms of the expected value function gap of order , where  is the iteration counter. We also derive better nonasymptotic convergence rates in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order . Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real problems.",
            "keywords": [
                "Stochastic convex optimization",
                "intersection of convex constraints",
                "stochastic     proximal point",
                "nonasymptotic convergence analysis"
            ],
            "author": [
                "Andrei Patrascu",
                "Ion Necoara"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-347/17-347.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exact Learning of Lightweight Description Logic Ontologies",
            "abstract": "We study the problem of learning description logic (DL) ontologies in Angluin et al.'s framework of exact learning via queries. We admit membership queries (âis a given subsumption entailed by the target ontology?â) and equivalence queries (âis a given ontology equivalent to the target ontology?â). We present three main results: (1) ontologies formulated in (two relevant versions of) the description logic DL-Lite can be learned with polynomially many queries of polynomial size; (2) this is not the case for ontologies formulated in the description logic , even when only acyclic ontologies are admitted; and (3) ontologies formulated in a fragment of  related to the web ontology language OWL 2 RL can be learned in polynomial time. We also show that neither membership nor equivalence queries alone are sufficient in cases (1) and (3).",
            "keywords": [
                "Exact Learning",
                "Description Logic"
            ],
            "author": [
                "Boris Konev",
                "Carsten Lutz",
                "Ana Ozaki",
                "Frank Wolter"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-256/16-256.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Concordance-assisted Learning for Optimal Treatment Decision",
            "abstract": "To find optimal decision rule, Fan et al. (2016) proposed an innovative concordance-assisted learning algorithm which is based on maximum rank correlation estimator. It makes better use of the available information through pairwise comparison. However the objective function is discontinuous and computationally hard to optimize. In this paper, we consider a convex surrogate loss function to solve this problem. In addition, our algorithm ensures sparsity of decision rule and renders easy interpretation. We derive the  error bound of the estimated coefficients under ultra-high dimension. Simulation results of various settings and application to STAR*D both illustrate that the proposed method can still estimate optimal treatment regime successfully when the number of covariates is large.",
            "keywords": [
                "concordance-assisted learning",
                "optimal treatment regime",
                "L1 norm",
                "support     vector machine"
            ],
            "author": [
                "Shuhan Liang",
                "Wenbin Lu",
                "Rui Song",
                "Lan Wang"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-159/17-159.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Post-Regularization Inference for Time-Varying Nonparanormal Graphical Models",
            "abstract": "We propose a novel class of time-varying nonparanormal graphical models, which allows us to model high dimensional heavy-tailed systems and the evolution of their latent network structures. Under this model we develop statistical tests for presence of edges both locally at a fixed index value and globally over a range of values. The tests are developed for a high-dimensional regime, are robust to model selection mistakes and do not require commonly assumed minimum signal strength. The testing procedures are based on a high dimensional, debiasing-free moment estimator, which uses a novel kernel smoothed Kendall's tau correlation matrix as an input statistic. The estimator consistently estimates the latent inverse Pearson correlation matrix uniformly in both the index variable and kernel bandwidth. Its rate of convergence is shown to be minimax optimal. Our method is supported by thorough numerical simulations and an application to a neural imaging data set.",
            "keywords": [
                "graphical model selection",
                "nonparanormal graph",
                "time-varying network anal-     ysis",
                "hypothesis test"
            ],
            "author": [
                "Junwei Lu",
                "Mladen Kolar",
                "Han Liu"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-145/17-145.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression",
            "abstract": "To model categorical response variables given their covariates, we propose a permuted and augmented stick-breaking (paSB) construction that one-to-one maps the observed categories to randomly permuted latent sticks. This new construction transforms multinomial regression into regression analysis of stick-specific binary random variables that are mutually independent given their covariate-dependent stick success probabilities, which are parameterized by the regression coefficients of their corresponding categories. The paSB construction allows transforming an arbitrary cross-entropy-loss binary classifier into a Bayesian multinomial one. Specifically, we parameterize the negative logarithms of the stick failure probabilities with a family of covariate-dependent softplus functions to construct nonparametric Bayesian multinomial softplus regression, and transform Bayesian support vector machine (SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression models are not only capable of providing probability estimates, quantifying uncertainty, increasing robustness, and producing nonlinear classification decision boundaries, but also amenable to posterior simulation. Example results demonstrate their attractive properties and performance.",
            "keywords": [
                "Discrete choice models",
                "logistic regression",
                "nonlinear classification",
                "softplus     regression"
            ],
            "author": [
                "Quan Zhang",
                "Mingyuan Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-409/17-409.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Steering Social Activity: A Stochastic Optimal Control Point Of View",
            "abstract": "User engagement in online social networking depends critically on the level of social activity in the corresponding platform---the number of online actions, such as posts, shares or replies, taken by their users. Can we design data-driven algorithms to increase social activity? At a user level, such algorithms may increase activity by helping users decide when to take an action to be more likely to be noticed by their peers. At a network level, they may increase activity by incentivizing a few influential users to take more actions, which in turn will trigger additional actions by other users. In this paper, we model social activity using the framework of marked temporal point processes, derive an alternate representation of these processes using stochastic differential equations (SDEs) with jumps and, exploiting this alternate representation, develop two efficient online algorithms with provable guarantees to steer social activity both at a user and at a network level. In doing so, we establish a previously unexplored connection between optimal control of jump SDEs and doubly stochastic marked temporal point processes, which is of independent interest. Finally, we experiment both with synthetic and real data gathered from Twitter and show that our algorithms consistently steer social activity more effectively than the state of the art.",
            "keywords": [
                "marked temporal point processes",
                "stochastic optimal control",
                "stochastic diffe-     rential equations with jumps",
                "social networks"
            ],
            "author": [
                "Ali Zarezade",
                "Abir De",
                "Utkarsh Upadhyay",
                "Hamid R. Rabiee",
                "Manuel Gomez-Rodriguez"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-416/17-416.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Search Problem in Mixture Models",
            "abstract": "We consider the task of learning the parameters of a  single component of a mixture model, for the case when we are given side information about that component; we call this the âsearch problem\" in mixture models. We would like to solve this with computational and sample complexity lower than solving the overall original problem, where one learns parameters of all components. Our main contributions are the development of a simple but general model for the notion of side information, and a corresponding simple matrix-based algorithm for solving the search problem in this general setting. We then specialize this model and algorithm to four common scenarios: Gaussian mixture models, LDA topic models, subspace clustering, and mixed linear regression. For each one of these we show that if (and only if) the side information is informative, we obtain parameter estimates with greater accuracy, and also improved computation complexity than existing moment based mixture model algorithms (e.g. tensor methods). We also illustrate several natural ways one can obtain such side information, for specific problem instances. Our experiments on real data sets (NY Times, Yelp, BSDS500) further demonstrate the practicality of our algorithms showing significant improvement in runtime and accuracy.",
            "keywords": [
                "mixture models",
                "search",
                "side information",
                "semi-supervised"
            ],
            "author": [
                "Avik Ray",
                "Joe Neeman",
                "Sujay Sanghavi",
                "Sanjay Shakkottai"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-483/16-483.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An $\\ell_{\\infty}$ Eigenvector Perturbation Bound and Its Application",
            "abstract": "In statistics and machine learning, we are interested in the eigenvectors (or singular vectors) of certain matrices (e.g. covariance matrices, data matrices, etc). However, those matrices are usually perturbed by noises or statistical errors, either from random sampling or structural patterns. The Davis- Kahan  theorem is often used to bound the difference between the eigenvectors of a matrix  and those of a perturbed matrix , in terms of  norm. In this paper, we prove that when  is a low-rank and incoherent matrix, the  norm perturbation bound of singular vectors (or eigenvectors in the symmetric case) is smaller by a factor of  or  for left and right vectors, where  and  are the matrix dimensions. The power of this new perturbation result is shown in robust covariance estimation, particularly when random variables have heavy tails. There, we propose new robust covariance estimators and establish their asymptotic properties using the newly developed perturbation bound. Our theoretical results are verified through extensive numerical experiments.",
            "keywords": [
                "Matrix perturbation theory",
                "Incoherence",
                "Low-rank matrices",
                "Sparsity"
            ],
            "author": [
                "Jianqing Fan",
                "Weichen Wang",
                "Yiqiao Zhong"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-140/16-140.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Tight Bound of Hard Thresholding",
            "abstract": "This paper is concerned with the hard thresholding operator which sets all but the  largest absolute elements of a vector to zero. We establish a tight bound to quantitatively characterize the deviation of the thresholded solution from a given signal. Our theoretical result is universal in the sense that it holds for all choices of parameters, and the underlying analysis depends only on fundamental arguments in mathematical optimization. We discuss the implications for two domains: Compressed Sensing. On account of the crucial estimate, we bridge the connection between the restricted isometry property (RIP) and the sparsity parameter for a vast volume of hard thresholding based algorithms, which renders an improvement on the RIP condition especially when the true sparsity is unknown. This suggests that in essence, many more kinds of sensing matrices or fewer measurements are admissible for the data acquisition procedure. Machine Learning. In terms of large-scale machine learning, a significant yet challenging problem is learning accurate sparse models in an efficient manner. In stark contrast to prior work that attempted the -relaxation for promoting sparsity, we present a novel stochastic algorithm which performs hard thresholding in each iteration, hence ensuring such parsimonious solutions. Equipped with the developed bound, we prove the {\\em global linear convergence} for a number of prevalent statistical models under mild assumptions, even though the problem turns out to be non-convex.",
            "keywords": [
                "sparsity",
                "hard thresholding",
                "compressed sensing"
            ],
            "author": [
                "Jie Shen",
                "Ping Li"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-299/16-299.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Exchangeable Graphs and Their Limits via Graphon Processes",
            "abstract": "In a recent paper, Caron and Fox suggest a probabilistic model for sparse graphs which are exchangeable when associating each vertex with a time parameter in . Here we show that by generalizing the classical definition of graphons as functions over probability spaces to functions over -finite measure spaces, we can model a large family of exchangeable graphs, including the Caron-Fox graphs and the traditional exchangeable dense graphs as special cases. Explicitly, modelling the underlying space of features by a -finite measure space  and the connection probabilities by an integrable function , we construct a random family  of growing graphs such that the vertices of  are given by a Poisson point process on  with intensity , with two points  of the point process connected with probability . We call such a random family a graphon process. We prove that a graphon process has convergent subgraph frequencies (with possibly infinite limits) and that, in the natural extension of the cut metric to our setting, the sequence converges to the generating graphon. We also show that the underlying graphon is identifiable only as an equivalence class over graphons with cut distance zero. More generally, we study metric convergence for arbitrary (not necessarily random) sequences of graphs, and show that a sequence of graphs has a convergent subsequence if and only if it has a subsequence satisfying a property we call uniform regularity of tails. Finally, we prove that every graphon is equivalent to a graphon on  equipped with Lebesgue measure.",
            "keywords": [
                "graphons",
                "graph convergence",
                "sparse graph convergence",
                "modelling of sparse     networks",
                "exchangeable graph modelsc 2018 Christian Borgs"
            ],
            "author": [
                "Christian Borgs",
                "Jennifer T. Chayes",
                "Henry Cohn",
                "Nina Holden"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-421/16-421.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Weighted SGD for $\\ell_p$ Regression with Randomized Preconditioning",
            "abstract": "In recent years, stochastic gradient descent (SGD) methods and randomized linear algebra (RLA) algorithms have been applied to many large-scale problems in machine learning and data analysis. SGD methods are easy to implement and applicable to a wide range of convex optimization problems. In contrast, RLA algorithms provide much stronger performance guarantees but are applicable to a narrower class of problems. We aim to bridge the gap between these two methods in solving constrained overdetermined linear regression problems---e.g.,  and  regression problems.",
            "keywords": [],
            "author": [
                "Jiyan Yang",
                "Yin-Lam Chow",
                "Christopher Ré",
                "Michael W. Mahoney"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-044/17-044.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gaussian Lower Bound for the Information Bottleneck Limit",
            "abstract": "The Information Bottleneck (IB) is a conceptual method for extracting the most compact, yet informative, representation of a set of variables, with respect to the target. It generalizes the notion of minimal sufficient statistics from classical parametric statistics to a broader information-theoretic sense. The IB curve defines the optimal trade-off between representation complexity and its predictive power. Specifically, it is achieved by minimizing the level of mutual information (MI) between the representation and the original variables, subject to a minimal level of MI between the representation and the target. This problem is shown to be in general NP hard. One important exception is the multivariate Gaussian case, for which the Gaussian IB (GIB) is known to obtain an analytical closed form solution, similar to Canonical Correlation Analysis (CCA). In this work we introduce a Gaussian lower bound to the IB curve; we find an embedding of the data which maximizes its âGaussian part\", on which we apply the GIB. This embedding provides an efficient (and practical) representation of any arbitrary data-set (in the IB sense), which in addition holds the favorable properties of a Gaussian distribution. Importantly, we show that the optimal Gaussian embedding is bounded from above by non-linear CCA. This allows a fundamental limit for our ability to Gaussianize arbitrary data- sets and solve complex problems by linear methods.",
            "keywords": [
                "Information Bottleneck",
                "Canonical Correlations",
                "ACE",
                "Gaussianization",
                "Mu-     tual Information Maximization"
            ],
            "author": [
                "Amichai Painsky",
                "Naftali Tishby"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-398/17-398.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "tick: a Python Library for Statistical Learning, with an emphasis on Hawkes Processes and Time-Dependent Models",
            "abstract": "This paper introduces tick, is a statistical learning library for Python 3, with a particular emphasis on time-dependent models, such as point processes, tools for generalized linear models and survival analysis. The core of the library provides model computational classes, solvers and proximal operators for regularization. It relies on a C++ implementation and state- of-the-art optimization algorithms to provide very fast computations in a single node multi-core setting. Source code and documentation can be downloaded from https://github.com/X-DataInitiative/tick.",
            "keywords": [],
            "author": [
                "Emmanuel Bacry",
                "Martin Bompaire",
                "Philip Deegan",
                "Stéphane Gaïffas",
                "Søren V. Poulsen"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-381/17-381.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SGDLibrary: A MATLAB library for stochastic optimization algorithms",
            "abstract": "We consider the problem of finding the minimizer of a function  of the finite-sum form . This problem has been studied intensively in recent years in the field of machine learning (ML). One promising approach for large-scale data is to use a stochastic optimization algorithm to solve the problem. SGDLibrary is a readable, flexible and extensible pure-MATLAB library of a collection of stochastic optimization algorithms. The purpose of the library is to provide researchers and implementers a comprehensive evaluation environment for the use of these algorithms on various ML problems.",
            "keywords": [
                "Stochastic optimization",
                "stochastic gradient",
                "finite-sum minimization prob-     lem"
            ],
            "author": [
                "Hiroyuki Kasai"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-632/17-632.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Reward Maximization Under Uncertainty: Leveraging Side-Observations on Networks",
            "abstract": "We study the stochastic multi-armed bandit (MAB) problem in the presence of side-observations across actions that occur as a result of an underlying network structure. In our model, a bipartite graph captures the relationship between actions and a common set of unknowns such that choosing an action reveals observations for the unknowns that it is connected to. This models a common scenario in online social networks where users respond to their friends' activity, thus providing side information about each other's preferences. Our contributions are as follows: 1) We derive an asymptotic lower bound (with respect to time) as a function of the bi-partite network structure on the regret of any uniformly good policy that achieves the maximum long-term average reward. 2) We propose two policies - a randomized policy; and a policy based on the well- known upper confidence bound (UCB) policies - both of which explore each action at a rate that is a function of its network position. We show, under mild assumptions, that these policies achieve the asymptotic lower bound on the regret up to a multiplicative factor, independent of the network structure. Finally, we use numerical examples on a real-world social network and a routing example network to demonstrate the benefits obtained by our policies over other existing policies.",
            "keywords": [
                "Multi-armed Bandits",
                "Side Observations",
                "Bipartite Graph"
            ],
            "author": [
                "Swapna Buccapatnam",
                "Fang Liu",
                "Atilla Eryilmaz",
                "Ness B. Shroff"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-340/16-340.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simultaneous Clustering and Estimation of Heterogeneous Graphical Models",
            "abstract": "We consider joint estimation of multiple graphical models arising from heterogeneous and high-dimensional observations. Unlike most previous approaches which assume that the cluster structure is given in advance, an appealing feature of our method is to learn cluster structure while estimating heterogeneous graphical models. This is achieved via a high dimensional version of Expectation Conditional Maximization (ECM) algorithm  (Meng and Rubin,  1993). A joint graphical lasso penalty is imposed on the conditional maximization step to extract both homogeneity and heterogeneity components across all clusters. Our algorithm is computationally efficient due to fast sparse learning routines and can be implemented without unsupervised learning knowledge. The superior performance of our method is demonstrated by extensive experiments and its application to a Glioblastoma cancer dataset reveals some new insights in understanding the Glioblastoma cancer. In theory, a non-asymptotic error bound is established for the output directly from our high dimensional ECM algorithm, and it consists of two quantities: statistical error (statistical accuracy) and optimization error (computational complexity). Such a result gives a theoretical guideline in terminating our ECM iterations.",
            "keywords": [
                "Clustering",
                "finite-sample analysis",
                "graphical models",
                "high-dimensional statis-     tics"
            ],
            "author": [
                "Botao Hao",
                "Will Wei Sun",
                "Yufeng Liu",
                "Guang Cheng"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-019/17-019.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging",
            "abstract": "We address the statistical and optimization impacts of the classical sketch and Hessian sketch used to approximately solve the Matrix Ridge Regression (MRR) problem. Prior research has quantified the effects of classical sketch on the strictly simpler least squares regression (LSR) problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR: namely, it recovers nearly optimal solutions. By contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the âmassâ in the responses and the optimal objective value. For both types of approximation, the regularization in the sketched MRR problem results in significantly different statistical properties from those of the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the bias and variance of sketched MRR; these bounds show that classical sketch significantly increases the variance, while Hessian sketch significantly increases the bias. Empirically, sketched MRR solutions can have risks that are higher by an order-of- magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases the gap between the risks of the true and sketched solutions to the MRR problem. Thus, in parallel or distributed settings, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the increased statistical risk incurred by sketching.",
            "keywords": [
                "Randomized Linear Algebra",
                "Matrix Sketching",
                "Ridge Regressionc 2018 Shusen Wang",
                "Alex Gittens"
            ],
            "author": [
                "Shusen Wang",
                "Alex Gittens",
                "Michael W. Mahoney"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-313/17-313.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Complete Graphical Characterization and Construction of Adjustment Sets in Markov Equivalence Classes of Ancestral Graphs",
            "abstract": "We present a graphical criterion for covariate adjustment that is sound and complete for four different classes of causal graphical models: directed acyclic graphs (DAGs), maximal ancestral graphs (MAGs), completed partially directed acyclic graphs (CPDAGs), and partial ancestral graphs (PAGs). Our criterion unifies covariate adjustment for a large set of graph classes. Moreover, we define an explicit set that satisfies our criterion, if there is any set that satisfies our criterion. We also give efficient algorithms for constructing all sets that fulfill our criterion, implemented in the R package dagitty. Finally, we discuss the relationship between our criterion and other criteria for adjustment, and we provide new soundness and completeness proofs for the adjustment criterion for DAGs.",
            "keywords": [
                "causal effects",
                "graphical models",
                "covariate adjustment",
                "latent variables"
            ],
            "author": [
                "Emilija Perkovi\\'c",
                "Johannes Textor",
                "Markus Kalisch",
                "Marloes H. Maathuis"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-319/16-319.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization",
            "abstract": "We show that the average stability notion introduced by Kearns and Ron (1999); Bousquet and Elisseeff (2002) is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses. In other words, when analyzing the stability rate of a given algorithm, we may assume the optimal preconditioning of the data. This implies that, at least from a statistical perspective, explicit regularization is not required in order to compensate for ill-conditioned data, which stands in contrast to a widely common approach that includes a regularization for analyzing the sample complexity of generalized linear models. Several important implications of our findings include: a) We demonstrate that the excess risk of empirical risk minimization (ERM) is controlled by the preconditioned stability rate. This immediately yields a relatively short and elegant proof for the fast rates attained by ERM in our context. b) We complement the recent bounds of Hardt et al. (2015) on the stability rate of the Stochastic Gradient Descent algorithm.",
            "keywords": [],
            "author": [
                "Alon Gonen",
                "Shai Shalev-Shwartz"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-503/16-503.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parallelizing Stochastic Gradient Descent for Least Squares Regression: Mini-batching, Averaging, and Model Misspecification",
            "abstract": "",
            "keywords": [
                "Stochastic Gradient Descent",
                "Stochastic Approximation",
                "Least Squares Regression",
                "Parallelization",
                "Mini Batch SGD",
                "Iterate Averaging",
                "Suffix Averaging",
                "Batchsize Doubling",
                "Model     Averaging",
                "Parameter Mixing",
                "Mis-specified models",
                "Heteroscedastic Noise",
                "Agnostic Learningc 2018 Prateek Jain"
            ],
            "author": [
                "Prateek Jain",
                "Sham M. Kakade",
                "Rahul Kidambi",
                "Praneeth Netrapalli",
                "Aaron Sidford"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-595/16-595.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Quadratic Variance Function (QVF) DAG Models via OverDispersion Scoring (ODS)",
            "abstract": "Learning DAG or Bayesian network models is an important problem in multi-variate causal inference. However, a number of challenges arises in learning large-scale DAG models including model identifiability and computational complexity since the space of directed graphs is huge. In this paper, we address these issues in a number of steps for a broad class of DAG models where the noise or variance is signal-dependent. Firstly we introduce a new class of identifiable DAG models, where each node has a distribution where the variance is a quadratic function of the mean (QVF DAG models). Our QVF DAG models include many interesting classes of distributions such as Poisson, Binomial, Geometric, Exponential, Gamma and many other distributions in which the noise variance depends on the mean. We prove that this class of QVF DAG models is identifiable, and introduce a new algorithm, the OverDispersion Scoring (ODS) algorithm, for learning large-scale QVF DAG models. Our algorithm is based on firstly learning the moralized or undirected graphical model representation of the DAG to reduce the DAG search-space, and then exploiting the quadratic variance property to learn the ordering. We show through theoretical results and simulations that our algorithm is statistically consistent in the high-dimensional  setting provided that the degree of the moralized graph is bounded and performs well compared to state-of-the-art DAG-learning algorithms. We also demonstrate through a real data example involving multi-variate count data, that our ODS algorithm is well-suited to estimating DAG models for count data in comparison to other methods used for discrete data.",
            "keywords": [
                "Bayesian Networks",
                "Directed Acyclic Graph",
                "Identifiability",
                "Multi-variate     Count Distribution"
            ],
            "author": [
                "Gunwoong Park",
                "Garvesh Raskutti"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-243/17-243.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Inference on Random Dot Product Graphs: a Survey",
            "abstract": "The random dot product graph (RDPG) is an independent-edge random graph that is analytically tractable and, simultaneously, either encompasses or can successfully approximate a wide range of random graphs, from relatively simple stochastic block models to complex latent position graphs. In this survey paper, we describe a comprehensive paradigm for statistical inference on random dot product graphs, a paradigm centered on spectral embeddings of adjacency and Laplacian matrices. We examine the graph-inferential analogues of several canonical tenets of classical Euclidean inference. In particular, we summarize a body of existing results on the consistency and asymptotic normality of the adjacency and Laplacian spectral embeddings, and the role these spectral embeddings can play in the construction of single- and multi-sample hypothesis tests for graph data. We investigate several real-world applications, including community detection and classification in large social networks and the determination of functional and biologically relevant network properties from an exploratory data analysis of the Drosophila connectome. We outline requisite background and current open problems in spectral graph inference.",
            "keywords": [],
            "author": [
                "Avanti Athreya",
                "Donniell E. Fishkind",
                "Minh Tang",
                "Carey E. Priebe",
                "Youngser Park",
                "Joshua T. Vogelstein",
                "Keith Levin",
                "Vince Lyzinski",
                "Yichen Qin",
                "Daniel L Sussman"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-448/17-448.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rate of Convergence of $k$-Nearest-Neighbor Classification Rule",
            "abstract": "A binary classification problem is considered. The excess error probability of the -nearest-neighbor classification rule according to the error probability of the Bayes decision is revisited by a decomposition of the excess error probability into approximation and estimation errors. Under a weak margin condition and under a modified Lipschitz condition or a local Lipschitz condition, tight upper bounds are presented such that one avoids the condition that the feature vector is bounded. The concept of modified Lipschitz condition is applied for discrete distributions, too. As a consequence of both concepts, we present the rate of convergence of  error for the corresponding nearest neighbor regression estimate.",
            "keywords": [
                "rate of convergence",
                "classification",
                "error probability"
            ],
            "author": [
                "Maik Döring",
                "László Györfi",
                "Harro Walk"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-755/17-755.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Theory of Learning with Corrupted Labels",
            "abstract": "",
            "keywords": [
                "Supervised Learning",
                "Generalized Supervision",
                "Decision Theory",
                "Minimax Bounds",
                "Data Processing"
            ],
            "author": [
                "Brendan van Rooyen",
                "Robert C. Williamson"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-315/16-315.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Interactive Algorithms: Pool, Stream and Precognitive Stream",
            "abstract": "We consider interactive algorithms in the pool-based setting, and in the stream-based setting. Interactive algorithms observe suggested elements (representing actions or queries), and interactively select some of them and receive responses. Pool- based algorithms can select elements at any order, while stream- based algorithms observe elements in sequence, and can only select elements immediately after observing them. We further consider an intermediate setting, which we term precognitive stream, in which the algorithm knows in advance the identity of all the elements in the sequence, but can select them only in the order of their appearance. For all settings, we assume that the suggested elements are generated independently from some source distribution, and ask what is the stream size required for emulating a pool algorithm with a given pool size, in the stream-based setting and in the precognitive stream setting. We provide algorithms and matching lower bounds for general pool algorithms, and for utility-based pool algorithms. We further derive nearly matching upper and lower bounds on the gap between the two settings for the special case of active learning for binary classification.",
            "keywords": [
                "interactive algorithms",
                "active learning",
                "pool-based"
            ],
            "author": [
                "Sivan Sabato",
                "Tom Hess"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-424/16-424.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "CoCoA: A General Framework for Communication-Efficient Distributed Optimization",
            "abstract": "The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for distributed computing environments, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly-convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly-convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the- art methods, as we illustrate with an extensive set of experiments on real distributed datasets.",
            "keywords": [
                "Convex optimization",
                "distributed systems",
                "large-scale machine learning",
                "par-     allel and distributed algorithmsc 2018 Virginia Smith",
                "Simone Forte",
                "Chenxin Ma",
                "Martin Takáč"
            ],
            "author": [
                "Virginia Smith",
                "Simone Forte",
                "Chenxin Ma",
                "Martin Takáč",
                "Michael I. Jordan",
                "Martin Jaggi"
            ],
            "ref": "http://jmlr.org/papers/volume18/16-512/16-512.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Concentration inequalities for empirical processes of linear time series",
            "abstract": "The paper considers suprema of empirical processes for linear time series indexed by functional classes. We derive an upper bound for the tail probability of the suprema under conditions on the size of the function class, the sample size, temporal dependence and the moment conditions of the underlying time series. Due to the dependence and heavy-tailness, our tail probability bound is substantially different from those classical exponential bounds obtained under the independence assumption in that it involves an extra polynomial decaying term. We allow both short- and long-range dependent processes. For empirical processes indexed by half intervals, our tail probability inequality is sharp up to a multiplicative constant.",
            "keywords": [
                "martingale decomposition",
                "tail probability",
                "heavy tail"
            ],
            "author": [
                "Likai Chen",
                "Wei Biao Wu"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-012/17-012.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Cluster Elastic Net for Multivariate Regression",
            "abstract": "We propose a method for simultaneously estimating regression coefficients and clustering response variables in a multivariate regression model, to increase prediction accuracy and give insights into the relationship between response variables. The estimates of the regression coefficients and clusters are found by using a penalized likelihood estimator, which includes a cluster fusion penalty, to shrink the difference in fitted values from responses in the same cluster, and an  penalty for simultaneous variable selection and estimation. We propose a two-step algorithm, that iterates between k-means clustering and solving the penalized likelihood function assuming the clusters are known, which has desirable parallel computational properties obtained by using the cluster fusion penalty. If the response variable clusters are known a priori then the algorithm reduces to just solving the penalized likelihood problem. Theoretical results are presented for the penalized least squares case, including asymptotic results allowing for . We extend our method to the setting where the responses are binomial variables. We propose a coordinate descent algorithm for the normal likelihood and a proximal gradient descent algorithm for the binomial likelihood, which can easily be extended to other generalized linear model (GLM) settings. Simulations and data examples from business operations and genomics are presented to show the merits of both the least squares and binomial methods.",
            "keywords": [
                "Multivariate Regression",
                "Clustering"
            ],
            "author": [
                "Bradley S. Price",
                "Ben Sherwood"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-445/17-445.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Characteristic and Universal Tensor Product Kernels",
            "abstract": "Maximum mean discrepancy (MMD), also called energy distance or N-distance in statistics and Hilbert-Schmidt independence criterion (HSIC), specifically distance covariance in statistics, are among the most popular and successful approaches to quantify the difference and independence of random variables, respectively. Thanks to their kernel-based foundations, MMD and HSIC are applicable on a wide variety of domains. Despite their tremendous success, quite little is known about when HSIC characterizes independence and when MMD with tensor product kernel can discriminate probability distributions. In this paper, we answer these questions by studying various notions of characteristic property of the tensor product kernel.",
            "keywords": [
                "tensor product kernel",
                "kernel mean embedding",
                "characteristic kernel",
                "I-     characteristic kernel",
                "universality",
                "maximum mean discrepancy"
            ],
            "author": [
                "Zoltán Szabó",
                "Bharath K. Sriperumbudur"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-492/17-492.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Certifiably Optimal Rule Lists for Categorical Data",
            "abstract": "We present the design and implementation of a custom discrete optimization technique for building rule lists over a categorical feature space. Our algorithm produces rule lists with optimal training performance, according to the regularized empirical risk, with a certificate of optimality. By leveraging algorithmic bounds, efficient data structures, and computational reuse, we achieve several orders of magnitude speedup in time and a massive reduction of memory consumption. We demonstrate that our approach produces optimal rule lists on practical problems in seconds. Our results indicate that it is possible to construct optimal sparse rule lists that are approximately as accurate as the COMPAS proprietary risk prediction tool on data from Broward County, Florida, but that are completely interpretable. This framework is a novel alternative to CART and other decision tree methods for interpretable modeling.",
            "keywords": [
                "rule lists",
                "decision trees",
                "optimization",
                "interpretable models"
            ],
            "author": [
                "Elaine Angelino",
                "Nicholas Larus-Stone",
                "Daniel Alabi",
                "Margo Seltzer",
                "Cynthia Rudin"
            ],
            "ref": "http://jmlr.org/papers/volume18/17-716/17-716.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Numerical Analysis near Singularities in RBF Networks",
            "abstract": "The existence of singularities often affects the learning dynamics in feedforward neural networks. In this paper, based on theoretical analysis results, we numerically analyze the learning dynamics of radial basis function (RBF) networks near singularities to understand to what extent singularities influence the learning dynamics. First, we show the explicit expression of the Fisher information matrix for RBF networks. Second, we demonstrate through numerical simulations that the singularities have a significant impact on the learning dynamics of RBF networks. Our results show that overlap singularities mainly have influence on the low dimensional RBF networks and elimination singularities have a more significant impact to the learning processes than overlap singularities in both low and high dimensional RBF networks, whereas the plateau phenomena are mainly caused by the elimination singularities. The results can also be the foundation to investigate the singular learning dynamics in deep feedforward neural networks.",
            "keywords": [
                "RBF networks",
                "Singularity",
                "Learning dynamics",
                "Numerical analysis",
                "Deep     learningc 2018 Weili Guo",
                "Haikun Wei",
                "Yew-Soon Ong",
                "Jaime Rubio Hervas",
                "Junsheng Zhao"
            ],
            "author": [
                "Weili Guo",
                "Haikun Wei (Corresponding author)",
                "Yew-Soon Ong",
                "Jaime Rubio Hervas",
                "Junsheng Zhao",
                "Hai Wang",
                "Kanjian Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume19/16-210/16-210.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximate Submodularity and its Applications: Subset Selection, Sparse Approximation and Dictionary Selection",
            "abstract": "We introduce the submodularity ratio as a measure of how “close” to submodular a set function  is. We show that when  has submodularity ratio , the greedy algorithm for maximizing  provides a -approximation. Furthermore, when  is bounded away from 0, the greedy algorithm for minimum submodular cover also provides essentially an  approximation for a universe of  elements. As a main application of this framework, we study the problem of selecting a subset of  random variables from a large set, in order to obtain the best linear prediction of another variable of interest. We analyze the performance of widely used greedy heuristics; in particular, by showing that the submodularity ratio is lower-bounded by the smallest -sparse eigenvalue of the covariance matrix, we obtain the strongest known approximation guarantees for the Forward Regression and Orthogonal Matching Pursuit algorithms. As a second application, we analyze greedy algorithms for the dictionary selection problem, and significantly improve the previously known guarantees. Our theoretical analysis is complemented by experiments on real-world and synthetic data sets; in particular, we focus on an analysis of how tight various spectral parameters and the submodularity ratio are in terms of predicting the performance of the greedy algorithms.",
            "keywords": [],
            "author": [
                "Abhimanyu Das",
                "David Kempe"
            ],
            "ref": "http://jmlr.org/papers/volume19/16-534/16-534.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal Data: Learning and Inference",
            "abstract": "Modeling continuous-time physiological processes that manifest a patient's evolving clinical states is a key step in approaching many problems in healthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model (HASMM): a versatile probabilistic model that is capable of capturing the modern electronic health record (EHR) data. Unlike existing models, the HASMM accommodates irregularly sampled, temporally correlated, and informatively censored physiological data, and can describe non-stationary clinical state transitions. Learning the HASMM parameters from the EHR data is achieved via a novel  forward-filtering backward-sampling Monte-Carlo EM algorithm that exploits the knowledge of the end-point clinical outcomes (informative censoring) in the EHR data, and implements the E-step by sequentially sampling the patients' clinical states in the reverse-time direction while conditioning on the future states. Real-time inferences are drawn via a forward-filtering algorithm that operates on a virtually constructed discrete-time embedded Markov chain that mirrors the patient's continuous-time state trajectory. We demonstrate the prognostic utility of the HASMM in a critical care prognosis setting using a real-world dataset for patients admitted to the Ronald Reagan UCLA Medical Center. In particular, we show that using HASMMs, a patient's clinical deterioration can be predicted 8-9 hours prior to intensive care unit admission, with a 22 AUC gain compared to the Rothman index, which is the state-of-the-art critical care risk scoring technology.",
            "keywords": [
                "Hidden Semi-Markov Models",
                "Medical Informatics"
            ],
            "author": [
                "Ahmed M. Alaa",
                "Mihaela van der Schaar"
            ],
            "ref": "http://jmlr.org/papers/volume19/16-656/16-656.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Can We Trust the Bootstrap in High-dimensions? The Case of Linear Models",
            "abstract": "We consider the performance of the bootstrap in high-dimensions for the setting of linear regression, where  but  is not close to zero. We consider ordinary least-squares as well as robust regression methods and adopt a minimalist performance requirement: can the bootstrap give us good confidence intervals for a single coordinate of  (where  is the true regression vector)? We show through a mix of numerical and theoretical work that the bootstrap is fraught with problems. Both of the most commonly used methods of bootstrapping for regression–residual bootstrap and pairs bootstrap–give very poor inference on  as the ratio  grows. We find that the residual bootstrap tend to give anti-conservative estimates (inflated Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of power) as the ratio  grows. We also show that the jackknife resampling technique for estimating the variance of  severely overestimates the variance in high dimensions. We contribute alternative procedures based on our theoretical results that result in dimensionality adaptive and robust bootstrap methods.",
            "keywords": [
                "Bootstrap",
                "high-dimensional inference",
                "random matrices"
            ],
            "author": [
                "Noureddine El Karoui",
                "Elizabeth Purdom"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-006/17-006.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "RSG: Beating Subgradient Method without Smoothness and Strong Convexity",
            "abstract": "In this paper, we study the efficiency of a Restarted SubGradient (RSG) method that periodically restarts the standard subgradient method (SG). We show that, when applied to a broad class of convex optimization problems, RSG method can find an -optimal solution with a lower complexity than the SG method. In particular, we first show that RSG can reduce the dependence of SG's iteration complexity on the distance between the initial solution and the optimal set to that between the -level set and the optimal set {multiplied by a logarithmic factor}. Moreover, we show the advantages of RSG over SG in solving a broad family of problems that satisfy a local error bound condition, and also demonstrate its advantages for three specific families of convex optimization problems with different power constants in the local error bound condition. (a) For the problems whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local quadratic growth property in the -sublevel set, RSG has an  iteration complexity. (c) For the problems that admit a local Kurdyka-{\\L}ojasiewicz property with a power constant of , RSG has an  iteration complexity. The novelty of our analysis lies at exploiting the lower bound of the first-order optimality residual at the -level set. It is this novelty that allows us to explore the local properties of functions (e.g., local quadratic growth property, local Kurdyka-\\L ojasiewicz property, more generally local error bound conditions) to develop the improved convergence of RSG. We also develop a practical variant of RSG enjoying faster convergence than the SG method, which can be run without knowing the involved parameters in the local error bound condition. We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression, classification and matrix completion.",
            "keywords": [
                "subgradient method",
                "improved convergence",
                "local error bound"
            ],
            "author": [
                "Tianbao Yang",
                "Qihang Lin"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-016/17-016.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Patchwork Kriging for Large-scale Gaussian Process Regression",
            "abstract": "This paper presents a new approach for Gaussian process (GP) regression for large datasets. The approach involves partitioning the regression input domain into multiple local regions with a different local GP model fitted in each region. Unlike existing local partitioned GP approaches, we introduce a technique for patching together the local GP models nearly seamlessly to ensure that the local GP models for two neighboring regions produce nearly the same response prediction and prediction error variance on the boundary between the two regions. This largely mitigates the well-known discontinuity problem that degrades the prediction accuracy of existing local partitioned GP methods over regional boundaries. Our main innovation is to represent the continuity conditions as additional pseudo-observations that the differences between neighboring GP responses are identically zero at an appropriately chosen set of boundary input locations. To predict the response at any input location, we simply augment the actual response observations with the pseudo-observations and apply standard GP prediction methods to the augmented data. In contrast to heuristic continuity adjustments, this has an advantage of working within a formal GP framework, so that the GP-based predictive uncertainty quantification remains valid. Our approach also inherits a sparse block-like structure for the sample covariance matrix, which results in computationally efficient closed-form expressions for the predictive mean and variance. In addition, we provide a new spatial partitioning scheme based on a recursive space partitioning along local principal component directions, which makes the proposed approach applicable for regression domains having more than two dimensions. Using three spatial datasets and three higher dimensional datasets, we investigate the numerical performance of the approach and compare it to several state-of-the-art approaches.",
            "keywords": [
                "Local Kriging",
                "Model Split and Merge",
                "Pseudo Observations"
            ],
            "author": [
                "Chiwoo Park",
                "Daniel Apley"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-042/17-042.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Bayes via Barycenter in Wasserstein Space",
            "abstract": "Divide-and-conquer based methods for Bayesian inference provide a general approach for tractable posterior inference when the sample size is large. These methods divide the data into smaller subsets, sample from the posterior distribution of parameters in parallel on all the subsets, and combine posterior samples from all the subsets to approximate the full data posterior distribution. The smaller size of any subset compared to the full data implies that posterior sampling on any subset is computationally more efficient than sampling from the true posterior distribution. Since the combination step takes negligible time relative to sampling, posterior computations can be scaled to massive data by dividing the full data into sufficiently large number of data subsets. One such approach relies on the geometry of posterior distributions estimated across different subsets and combines them through their barycenter in a Wasserstein space of probability measures. We provide theoretical guarantees on the accuracy of approximation that are valid in many applications. We show that the geometric method approximates the full data posterior distribution better than its competitors across diverse simulations and reproduces known results when applied to a movie ratings database.",
            "keywords": [],
            "author": [
                "Sanvesh Srivastava",
                "Cheng Li",
                "David B. Dunson"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-084/17-084.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Experience Selection in Deep Reinforcement Learning for Control",
            "abstract": "Experience replay is a technique that allows off-policy reinforcement-learning methods to reuse past experiences. The stability and speed of convergence of reinforcement learning, as well as the eventual performance of the learned policy, are strongly dependent on the experiences being replayed. Which experiences are replayed depends on two important choices. The first is which and how many experiences to retain in the experience replay buffer. The second choice is how to sample the experiences that are to be replayed from that buffer. We propose new methods for the combined problem of experience retention and experience sampling. We refer to the combination as experience selection. We focus our investigation specifically on the control of physical systems, such as robots, where exploration is costly. To determine which experiences to keep and which to replay, we investigate different proxies for their immediate and long-term utility. These proxies include age, temporal difference error and the strength of the applied exploration noise. Since no currently available method works in all situations, we propose guidelines for using prior knowledge about the characteristics of the control problem at hand to choose the appropriate experience replay strategy.",
            "keywords": [
                "reinforcement learning",
                "deep learning",
                "experience replay",
                "control"
            ],
            "author": [
                "Tim de Bruin",
                "Jens Kober",
                "Karl Tuyls",
                "Robert Babuška"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-131/17-131.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Constructive Approach to $L_0$ Penalized Regression",
            "abstract": "We propose a constructive approach to estimating sparse, high-dimensional linear regression models. The approach is a computational algorithm motivated from the KKT conditions for the -penalized least squares solutions. It generates a sequence of solutions iteratively, based on support detection using primal and dual information and root finding. We refer to the algorithm as SDAR for brevity. Under a sparse Riesz condition on the design matrix and certain other conditions, we show that with high probability, the  estimation error of the solution sequence decays exponentially to the minimax error bound in  iterations, where  is the number of important predictors and  is the relative magnitude of the nonzero target coefficients; and under a mutual coherence condition and certain other conditions, the  estimation error decays to the optimal error bound in  iterations. Moreover the SDAR solution recovers the oracle least squares estimator within a finite number of iterations with high probability if the sparsity level is known. Computational complexity analysis shows that the cost of SDAR is  per iteration. We also consider an adaptive version of SDAR for use in practical applications where the true sparsity level is unknown. Simulation studies demonstrate that SDAR outperforms Lasso, MCP and two greedy methods in accuracy and efficiency.",
            "keywords": [
                "Geometrical convergence",
                "KKT conditions",
                "nonasymptotic error bounds",
                "oracle property",
                "root finding"
            ],
            "author": [
                "Jian Huang",
                "Yuling Jiao",
                "Yanyan Liu",
                "Xiliang Lu"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-194/17-194.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Change-Point Computation for Large Graphical Models: A Scalable Algorithm for Gaussian Graphical Models with Change-Points",
            "abstract": "Graphical models with change-points are computationally challenging to fit, particularly in cases where the number of observation points and the number of nodes in the graph are large. Focusing on Gaussian graphical models, we introduce an approximate majorize-minimize (MM) algorithm that can be useful for computing change-points in large graphical models. The proposed algorithm is an order of magnitude faster than a brute force search. Under some regularity conditions on the data generating process, we show that with high probability, the algorithm converges to a value that is within statistical error of the true change-point. A fast implementation of the algorithm using Markov Chain Monte Carlo is also introduced. The performances of the proposed algorithms are evaluated on synthetic data sets and the algorithm is also used to analyze structural changes in the S{\\&}P 500 over the period 2000-2016.",
            "keywords": [
                "change-points",
                "Gaussian graphical models",
                "proximal gradient",
                "simulated an-     nealing"
            ],
            "author": [
                "Leland Bybee",
                "Yves Atchadé"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-218/17-218.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Robust Learning Approach for Regression Models Based on Distributionally Robust Optimization",
            "abstract": "We present a Distributionally Robust Optimization (DRO) approach to estimate a robustified regression plane in a linear regression setting, when the observed samples are potentially contaminated with adversarially corrupted outliers. Our approach mitigates the impact of outliers by hedging against a family of probability distributions on the observed data, some of which assign very low probabilities to the outliers. The set of distributions under consideration are close to the empirical distribution in the sense of the Wasserstein metric. We show that this DRO formulation can be relaxed to a convex optimization problem which encompasses a class of models. By selecting proper norm spaces for the Wasserstein metric, we are able to recover several commonly used regularized regression models. We provide new insights into the regularization term and give guidance on the selection of the regularization coefficient from the standpoint of a confidence region. We establish two types of performance guarantees for the solution to our formulation under mild conditions. One is related to its out-of-sample behavior (prediction bias), and the other concerns the discrepancy between the estimated and true regression planes (estimation bias). Extensive numerical results demonstrate the superiority of our approach to a host of regression models, in terms of the prediction and estimation accuracies. We also consider the application of our robust learning procedure to outlier detection, and show that our approach achieves a much higher AUC (Area Under the ROC Curve) than M-estimation (Huber, 1964, 1973).",
            "keywords": [
                "Robust Learning",
                "Distributionally Robust Optimization",
                "Wasserstein Metric",
                "Regularized Regression"
            ],
            "author": [
                "Ruidi Chen",
                "Ioannis Ch. Paschalidis"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-295/17-295.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Optimal Transport and the Rot Mover's Distance",
            "abstract": "This paper presents a unified framework for smooth convex regularization of discrete optimal transport problems. In this context, the regularized optimal transport turns out to be equivalent to a matrix nearness problem with respect to Bregman divergences. Our framework thus naturally generalizes a previously proposed regularization based on the Boltzmann-Shannon entropy related to the Kullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We call the regularized optimal transport distance the rot mover's distance in reference to the classical earth mover's distance. By exploiting alternate Bregman projections, we develop the alternate scaling algorithm and non-negative alternate scaling algorithm, to compute efficiently the regularized optimal plans depending on whether the domain of the regularizer lies within the non-negative orthant or not. We further enhance the separable case with a sparse extension to deal with high data dimensions. We also instantiate our framework and discuss the inherent specificities for well-known regularizers and statistical divergences in the machine learning and information geometry communities. Finally, we demonstrate the merits of our methods with experiments using synthetic data to illustrate the effect of different regularizers, penalties and dimensions, as well as real-world data for a pattern recognition application to audio scene classification.",
            "keywords": [
                "alternate projections",
                "convex analysis",
                "regularized optimal transport"
            ],
            "author": [
                "Arnaud Dessein",
                "Nicolas Papadakis",
                "Jean-Luc Rouas"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-361/17-361.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ELFI: Engine for Likelihood-Free Inference",
            "abstract": "Engine for Likelihood-Free Inference (ELFI) is a Python software library for performing likelihood-free inference (LFI). ELFI provides a convenient syntax for arranging components in LFI, such as priors, simulators, summaries or distances, to a network called ELFI graph. The components can be implemented in a wide variety of languages. The stand-alone ELFI graph can be used with any of the available inference methods without modifications. A central method implemented in ELFI is Bayesian Optimization for Likelihood-Free Inference (BOLFI), which has recently been shown to accelerate likelihood-free inference up to several orders of magnitude by surrogate-modelling the distance. ELFI also has an inbuilt support for output data storing for reuse and analysis, and supports parallelization of computation from multiple cores up to a cluster environment. ELFI is designed to be extensible and provides interfaces for widening its functionality. This makes the adding of new inference methods to ELFI straightforward and automatically compatible with the inbuilt features.",
            "keywords": [],
            "author": [
                "Jarno Lintusaari",
                "Henri Vuollekoski",
                "Antti Kangasrääsiö",
                "Kusti Skytén",
                "Marko Järvenpää",
                "Pekka Marttinen",
                "Michael U. Gutmann",
                "Aki Vehtari",
                "Jukka Corander",
                "Samuel Kaski"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-374/17-374.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Streaming kernel regression with provably adaptive mean, variance, and regularization",
            "abstract": "We consider the problem of streaming kernel regression, when the observations arrive sequentially and the goal is to recover the underlying mean function, assumed to belong to an RKHS. The variance of the noise is not assumed to be known. In this context, we tackle the problem of tuning the regularization parameter adaptively at each time step, while maintaining tight confidence bounds estimates on the value of the mean function at each point. To this end, we first generalize existing results for finite-dimensional linear regression with fixed regularization and known variance to the kernel setup with a regularization parameter allowed to be a measurable function of past observations. Then, using appropriate self-normalized inequalities we build upper and lower bound estimates for the variance, leading to Bernstein-like concentration bounds. The latter is used in order to define the adaptive regularization. The bounds resulting from our technique are valid uniformly over all observation points and all time steps, and are compared against the literature with numerical experiments. Finally, the potential of these tools is illustrated by an application to kernelized bandits, where we revisit the Kernel UCB and Kernel Thompson Sampling procedures, and show the benefits of the novel adaptive kernel tuning strategy.",
            "keywords": [
                "kernel",
                "regression",
                "online learning",
                "adaptive tuning"
            ],
            "author": [
                "Audrey Durand",
                "Odalric-Ambrym Maillard",
                "Joelle Pineau"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-404/17-404.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dual Principal Component Pursuit",
            "abstract": "We consider the problem of learning a linear subspace from data corrupted by outliers. Classical approaches are typically designed for the case in which the subspace dimension is small relative to the ambient dimension. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement; as such, it is particularly suitable for subspaces whose dimension is close to the ambient dimension (subspaces of high relative dimension). We pose the problem of computing normal vectors to the inlier subspace as a non-convex  minimization problem on the sphere, which we call Dual Principal Component Pursuit (DPCP) problem. We provide theoretical guarantees under which every global solution to DPCP is a vector in the orthogonal complement of the inlier subspace. Moreover, we relax the non-convex DPCP problem to a recursion of linear programs whose solutions are shown to converge in a finite number of steps to a vector orthogonal to the subspace. In particular, when the inlier subspace is a hyperplane, the solutions to the recursion of linear programs converge to the global minimum of the non-convex DPCP problem in a finite number of steps. We also propose algorithms based on alternating minimization and iteratively re-weighted least squares, which are suitable for dealing with large-scale data. Experiments on synthetic data show that the proposed methods are able to handle more outliers and higher relative dimensions than current state-of-the-art methods, while experiments in the context of the three-view geometry problem in computer vision suggest that the proposed methods can be a useful or even superior alternative to traditional RANSAC-based approaches for computer vision and other applications.",
            "keywords": [
                "Outliers",
                "Robust Principal Component Analysis",
                "High Relative Dimension"
            ],
            "author": [
                "Manolis C. Tsakiris",
                "René Vidal"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-436/17-436.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Proximal Gradient Algorithm for Partially Asynchronous Computer Clusters ",
            "abstract": "With ever growing data volume and model size, an error-tolerant, communication efficient, yet versatile distributed algorithm has become vital for the success of many large-scale machine learning applications. In this work we propose m-PAPG, an implementation of the flexible proximal gradient algorithm in model parallel systems equipped with the partially asynchronous communication protocol. The worker machines communicate asynchronously with a controlled staleness bound  and operate at different frequencies. We characterize various convergence properties of m-PAPG: 1) Under a general non-smooth and non-convex setting, we prove that every limit point of the sequence generated by m-PAPG is a critical point of the objective function; 2) Under an error bound condition of convex objective functions, we prove that the optimality gap decays linearly for every  steps; 3) Under the Kurdyka-{\\L}ojasiewicz inequality and a sufficient decrease assumption, we prove that the sequences generated by m-PAPG converge to the same critical point, provided that a proximal Lipschitz condition is satisfied.",
            "keywords": [
                "proximal gradient",
                "distributed system",
                "model parallel",
                "partially asynchronous"
            ],
            "author": [
                "Yi Zhou",
                "Yingbin Liang",
                "Yaoliang Yu",
                "Wei Dai",
                "Eric P. Xing"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-444/17-444.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Refining the Confidence Level for Optimistic Bandit Strategies",
            "abstract": "This paper introduces the first strategy for stochastic bandits with unit variance Gaussian noise that is simultaneously minimax optimal up to constant factors, asymptotically optimal, and never worse than the classical upper confidence bound strategy up to universal constant factors. Preliminary empirical evidence is also promising. Besides this, a conjecture on the optimal form of the regret is shown to be false and a finite-time lower bound on the regret of any strategy is presented that very nearly matches the finite-time upper bound of the newly proposed strategy.",
            "keywords": [],
            "author": [
                "Tor Lattimore"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-513/17-513.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ThunderSVM: A Fast SVM Library on GPUs and CPUs",
            "abstract": "Support Vector Machines (SVMs) are classic supervised learning models for classification, regression and distribution estimation. A survey conducted by Kaggle in 2017 shows that 26% of the data mining and machine learning practitioners are users of SVMs. However, SVM training and prediction are very expensive computationally for large and complex problems. This paper presents an efficient and open source SVM software toolkit called ThunderSVM which exploits the high-performance of Graphics Processing Units (GPUs) and multi-core CPUs. ThunderSVM supports all the functionalities–including classification (SVC), regression (SVR) and one-class SVMs–of LibSVM and uses identical command line options, such that existing LibSVM users can easily apply our toolkit. ThunderSVM can be used through multiple language interfaces including C/C++, Python, R and MATLAB. Our experimental results show that ThunderSVM is generally an order of magnitude faster than LibSVM while producing identical SVMs. In addition to the high efficiency, we design our convex optimization solver in a general way such that SVC, SVR, and one-class SVMs share the same solver for the ease of maintenance. Documentation, examples, and more about ThunderSVM are available at https://github.com/zeyiwen/thundersvm",
            "keywords": [
                "SVMs",
                "GPUs",
                "multi-core CPUs",
                "efficiency"
            ],
            "author": [
                "Zeyi Wen",
                "Jiashuai Shi",
                "Qinbin Li",
                "Bingsheng He",
                "Jian Chen"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-740/17-740.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Synthetic Control",
            "abstract": "We present a robust generalization of the synthetic control method for comparative case studies. Like the classical method cf. \\cite{abadie3}, we present an algorithm to estimate the unobservable counterfactual of a treatment unit. A distinguishing feature of our algorithm is that of de-noising the data matrix via singular value thresholding, which renders our approach robust in multiple facets: it automatically identifies a good subset of donors for the synthetic control, overcomes the challenges of missing data, and continues to work well in settings where covariate information may not be provided. We posit that the setting can be viewed as an instance of the Latent Variable Model and provide the first finite sample analysis (coupled with asymptotic results) for the estimation of the counterfactual. Our algorithm accurately imputes missing entries and filters corrupted observations in producing a consistent estimator of the underlying signal matrix, provided  for some ; here,  is the fraction of observed data and  is the time interval of interest. Under the same proportion of observations, we demonstrate that the mean-squared error in our counterfactual estimation scales as , where  is the variance of the inherent noise. Additionally, we introduce a Bayesian framework to quantify the estimation uncertainty. Our experiments, using both synthetic and real-world datasets, demonstrate that our robust generalization yields an improvement over the classical synthetic control method.",
            "keywords": [
                "Observation Studies",
                "Causal Inference"
            ],
            "author": [
                "Muhammad Amjad",
                "Devavrat Shah",
                "Dennis Shen"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-777/17-777.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Reverse Iterative Volume Sampling for Linear Regression",
            "abstract": "We study the following basic machine learning task: Given a fixed set of input points in  for a linear regression problem, we wish to predict a hidden response value for each of the points. We can only afford to attain the responses for a small subset of the points that are then used to construct linear predictions for all points in the dataset. The performance of the predictions is evaluated by the total square loss on all responses (the attained as well as the remaining hidden ones). We show that a good approximate solution to this least squares problem can be obtained from just dimension  many responses by using a joint sampling technique called volume sampling. Moreover, the least squares solution obtained for the volume sampled subproblem is an unbiased estimator of optimal solution based on all  responses. This unbiasedness is a desirable property that is not shared by other common subset selection techniques. Motivated by these basic properties, we develop a theoretical framework for studying volume sampling, resulting in a number of new matrix expectation equalities and statistical guarantees which are of importance not only to least squares regression but also to numerical linear algebra in general. Our methods also lead to a regularized variant of volume sampling, and we propose the first efficient algorithm for volume sampling which makes this technique a practical tool in the machine learning toolbox. Finally, we provide experimental evidence which confirms our theoretical findings.",
            "keywords": [
                "volume sampling",
                "linear regression",
                "row sampling",
                "active learning"
            ],
            "author": [
                "Michał Dereziński",
                "Manfred K. Warmuth"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-781/17-781.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems",
            "abstract": "A new class of non-homogeneous state-affine systems is introduced for use in reservoir computing. Sufficient conditions are identified that guarantee first, that the associated reservoir computers with linear readouts are causal, time-invariant, and satisfy the fading memory property and second, that a subset of this class is universal in the category of fading memory filters with stochastic almost surely uniformly bounded inputs. This means that any discrete-time filter that satisfies the fading memory property with random inputs of that type can be uniformly approximated by elements in the non-homogeneous state-affine family.",
            "keywords": [
                "reservoir computing",
                "universality",
                "state-affine systems",
                "SAS",
                "echo state networks",
                "ESN",
                "echo state affine systems",
                "machine learning",
                "fading memory property",
                "linear training"
            ],
            "author": [
                "Lyudmila Grigoryeva",
                "Juan-Pablo Ortega"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-020/18-020.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations",
            "abstract": "We put forth a deep learning approach for discovering nonlinear partial differential equations from scattered and potentially noisy observations in space and time. Specifically, we approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. The first network acts as a prior on the unknown solution and essentially enables us to avoid numerical differentiations which are inherently ill-conditioned and unstable. The second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal data-set. We test the effectiveness of our approach for several benchmark problems spanning a number of scientific domains and demonstrate how the proposed framework can help us accurately learn the underlying dynamics and forecast future states of the system. In particular, we study the Burgers', Korteweg-de Vries (KdV), Kuramoto-Sivashinsky, nonlinear Schr\\\"{o}dinger, and Navier-Stokes equations.",
            "keywords": [
                "Systems Identification",
                "Data-driven Scientific Discovery",
                "Physics Informed     Machine Learning",
                "Predictive Modeling",
                "Nonlinear Dynamics"
            ],
            "author": [
                "Maziar Raissi"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-046/18-046.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "OpenEnsembles: A Python Resource for Ensemble Clustering",
            "abstract": "In this paper we introduce OpenEnsembles, a Python toolkit for performing and analyzing ensemble clustering. Ensemble clustering is the process of creating many clustering solutions for a given dataset and utilizing the relationships observed across the ensemble to identify final solutions, which are more robust, stable or better than the individual solutions within the ensemble. The OpenEnsembles library provides a unified interface for applying transformations to data, clustering data, visualizing individual clustering solutions, visualizing and finishing the ensemble, and calculating validation metrics for a clustering solution for any given partitioning of the data. We have documented examples of using OpenEnsembles to create, analyze, and visualize a number of different types of ensemble approaches on toy and example datasets. OpenEnsembles is released under the GNU General Public License version 3, can be installed via Conda or the Python Package Index (pip), and is available at https://github.com/NaegleLab/OpenEnsembles.",
            "keywords": [
                "Unsupervised Learning",
                "Ensembles",
                "Clustering",
                "Ensemble Clustering"
            ],
            "author": [
                "Tom Ronan",
                "Shawn Anastasio",
                "Zhijie Qi",
                "Pedro Henrique S. Vieira Tavares",
                "Roman Sloutsky",
                "Kristen M. Naegle"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-100/18-100.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Importance Sampling for Minibatches",
            "abstract": "Minibatching is a very well studied and highly popular technique in supervised learning, used by practitioners due to its ability to accelerate training through better utilization of parallel processing power and reduction of stochastic variance. Another popular technique is importance sampling–a strategy for preferential sampling of more important examples also capable of accelerating the training process. However, despite considerable effort by the community in these areas, and due to the inherent technical difficulty of the problem, there is virtually no existing work combining the power of importance sampling with the strength of minibatching. In this paper we propose the first practical importance sampling for minibatches and give simple and rigorous complexity analysis of its performance. We illustrate on synthetic problems that for training data of certain properties, our sampling can lead to several orders of magnitude improvement in training time. We then test the new sampling on several popular data sets, and show that the improvement can reach an order of magnitude.",
            "keywords": [],
            "author": [
                "Dominik Csiba",
                "Peter Richtárik"
            ],
            "ref": "http://jmlr.org/papers/volume19/16-241/16-241.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Rank-Breaking: Computational and Statistical Tradeoffs",
            "abstract": "For massive and heterogeneous modern datasets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of rank aggregation, for the Plackett-Luce model, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data. Further, the proposed generalized rank-breaking algorithm involves set-wise comparisons as opposed to traditional pairwise comparisons. The maximum likelihood estimate of pairwise comparisons is computed efficiently using the celebrated minorization maximization algorithm (Hunter, 2004). To compute the pseudo-maximum likelihood estimate of the set-wise comparisons, we provide a generalization of the minorization maximization algorithm and give guarantees on its convergence.",
            "keywords": [
                "Rank aggregation",
                "Plackett-Luce model"
            ],
            "author": [
                "Ashish Khetan",
                "Sewoong Oh"
            ],
            "ref": "http://jmlr.org/papers/volume19/16-412/16-412.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gradient Descent Learns Linear Dynamical Systems",
            "abstract": "We prove that stochastic gradient descent efficiently converges to the global optimizer of the maximum likelihood objective of an unknown linear time-invariant dynamical system from a sequence of noisy observations generated by the system. Even though the objective function is non-convex, we provide polynomial running time and sample complexity bounds under strong but natural assumptions. Linear systems identification has been studied for many decades, yet, to the best of our knowledge, these are the first polynomial guarantees for the problem we consider.",
            "keywords": [],
            "author": [
                "Moritz Hardt",
                "Tengyu Ma",
                "Benjamin Recht"
            ],
            "ref": "http://jmlr.org/papers/volume19/16-465/16-465.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Direct Approach for Sparse Quadratic Discriminant Analysis",
            "abstract": "Quadratic discriminant analysis (QDA) is a standard tool for classification due to its simplicity and flexibility. Because the number of its parameters scales quadratically with the number of the variables, QDA is not practical, however, when the dimensionality is relatively large. To address this, we propose a novel procedure named DA-QDA for QDA in analyzing high-dimensional data. Formulated in a simple and coherent framework, DA-QDA aims to directly estimate the key quantities in the Bayes discriminant function including quadratic interactions and a linear index of the variables for classification. Under appropriate sparsity assumptions, we establish consistency results for estimating the interactions and the linear index, and further demonstrate that the misclassification rate of our procedure converges to the optimal Bayes risk, even when the dimensionality is exponentially high with respect to the sample size. An efficient algorithm based on the alternating direction method of multipliers (ADMM) is developed for finding interactions, which is much faster than its competitor in the literature. The promising performance of DA-QDA is illustrated via extensive simulation studies and the analysis of four real datasets.",
            "keywords": [
                "Bayes Risk",
                "Consistency",
                "High Dimensional Data",
                "Linear Discriminant Anal-     ysis",
                "Quadratic Discriminant Analysis"
            ],
            "author": [
                "Binyan Jiang",
                "Xiangyu Wang",
                "Chenlei Leng"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-285/17-285.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distribution-Specific Hardness of Learning Neural Networks",
            "abstract": "Although neural networks are routinely and successfully trained in practice using simple gradient-based methods, most existing theoretical results are negative, showing that learning such networks is difficult, in a worst-case sense over all data distributions. In this paper, we take a more nuanced view, and consider whether specific assumptions on the “niceness” of the input distribution, or “niceness” of the target function (e.g. in terms of smoothness, non-degeneracy, incoherence, random choice of parameters etc.), are sufficient to guarantee learnability using gradient-based methods. We provide evidence that neither class of assumptions alone is sufficient: On the one hand, for any member of a class of “nice” target functions, there are difficult input distributions. On the other hand, we identify a family of simple target functions, which are difficult to learn even if the input distribution is “nice”. To prove our results, we develop some tools which may be of independent interest, such as extending Fourier-based hardness techniques developed in the context of statistical queries (Blum et al., 1994), from the Boolean cube to Euclidean space and to more general classes of functions.",
            "keywords": [
                "neural networks",
                "computational hardness",
                "distributional assumptions"
            ],
            "author": [
                "Ohad Shamir"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-537/17-537.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Goodness-of-Fit Tests for Random Partitions via Symmetric Polynomials",
            "abstract": "We consider goodness-of-fit tests with i.i.d. samples generated from a categorical distribution . For a given , we test the null hypothesis whether  for some label permutation . The uncertainty of label permutation implies that the null hypothesis is composite instead of being singular. In this paper, we construct a testing procedure using statistics that are defined as indefinite integrals of some symmetric polynomials. This method is aimed directly at the invariance of the problem, and avoids the need of matching the unknown labels. The asymptotic distribution of the testing statistic is shown to be chi-squared, and its power is proved to be nearly optimal under a local alternative hypothesis. Various degenerate structures of the null hypothesis are carefully analyzed in the paper. A two-sample version of the test is also studied.",
            "keywords": [
                "hypothesis testing",
                "elementary symmetric polynomials",
                "Lagrange interpolat-     ing polynomials",
                "Vandermonde matrix"
            ],
            "author": [
                "Chao Gao"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-624/17-624.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Density Estimation for Dynamical Systems ",
            "abstract": "We study the density estimation problem with observations generated by certain dynamical systems that admit a unique underlying invariant Lebesgue density. Observations drawn from dynamical systems are not independent and moreover, usual mixing concepts may not be appropriate for measuring the dependence among these observations. By employing the -mixing concept to measure the dependence, we conduct statistical analysis on the consistency and convergence of the kernel density estimator. Our main results are as follows: First, we show that with properly chosen bandwidth, the kernel density estimator is universally consistent under -norm; Second, we establish convergence rates for the estimator with respect to several classes of dynamical systems under -norm. In the analysis, the density function  is only assumed to be H\\\"{o}lder continuous or pointwise H\\\"{o}lder controllable which is a weak assumption in the literature of nonparametric density estimation and also more realistic in the dynamical system context. Last but not least, we prove that the same convergence rates of the estimator under -norm and -norm can be achieved when the density function is H\\\"{o}lder continuous, compactly supported, and bounded. The bandwidth selection problem of the kernel density estimator for dynamical system is also discussed in our study via numerical simulations.",
            "keywords": [
                "Kernel density estimation",
                "dynamical system",
                "dependent observations",
                "C-     mixing process",
                "universal consistency",
                "convergence rates",
                "covering number",
                "learning theoryc 2018 Hanyuan Hang",
                "Ingo Steinwart"
            ],
            "author": [
                "Hanyuan Hang",
                "Ingo Steinwart",
                "Yunlong Feng",
                "Johan A.K. Suykens"
            ],
            "ref": "http://jmlr.org/papers/volume19/16-349/16-349.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Invariant Models for Causal Transfer Learning",
            "abstract": "Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.",
            "keywords": [
                "Transfer learning",
                "Multi-task learning",
                "Causality",
                "Domain adaptation"
            ],
            "author": [
                "Mateo Rojas-Carulla",
                "Bernhard Schölkopf",
                "Richard Turner",
                "Jonas Peters"
            ],
            "ref": "http://jmlr.org/papers/volume19/16-432/16-432.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The xyz algorithm for fast interaction search in high-dimensional data",
            "abstract": "When performing regression on a data set with  variables, it is often of interest to go beyond using main linear effects and include interactions as products between individual variables. For small-scale problems, these interactions can be computed explicitly but this leads to a computational complexity of at least  if done naively. This cost can be prohibitive if  is very large. We introduce a new randomised algorithm that is able to discover interactions with high probability and under mild conditions has a runtime that is subquadratic in . We show that strong interactions can be discovered in almost linear time, whilst finding weaker interactions requires  operations for  depending on their strength. The underlying idea is to transform interaction search into a closest pair problem which can be solved efficiently in subquadratic time. The algorithm is called  and is implemented in the language R. We demonstrate its efficiency for application to genome-wide association studies, where more than  interactions can be screened in under  seconds with a single-core  GHz CPU.",
            "keywords": [
                "interactions",
                "high-dimensional data",
                "regression",
                "computational tradeoffs"
            ],
            "author": [
                "Gian-Andrea Thanei",
                "Nicolai Meinshausen",
                "Rajen D. Shah"
            ],
            "ref": "http://jmlr.org/papers/volume19/16-515/16-515.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning",
            "abstract": "We show a Talagrand-type concentration inequality for Multi-Task Learning (MTL), with which we establish sharp excess risk bounds for MTL in terms of the Local Rademacher Complexity (LRC). We also give a new bound on the (LRC) for any norm regularized hypothesis classes, which applies not only to MTL, but also to the standard Single-Task Learning (STL) setting. By combining both results, one can easily derive fast-rate bounds on the excess risk for many prominent MTL methods, including–as we demonstrate–Schatten norm, group norm, and graph regularized MTL. The derived bounds reflect a relationship akin to a conservation law of asymptotic convergence rates. When compared to the rates obtained via a traditional, global Rademacher analysis, this very relationship allows for trading off slower rates with respect to the number of tasks for faster rates with respect to the number of available samples per task.",
            "keywords": [
                "Excess Risk Bounds",
                "Local Rademacher Complexity",
                "Multi-task Learningc 2018 Niloofar Yousefi",
                "Yunwen Lei",
                "Marius Kloft"
            ],
            "author": [
                "Niloofar Yousefi",
                "Yunwen Lei",
                "Marius Kloft",
                "Mansooreh Mollaghasemi",
                "Georgios C. Anagnostopoulos"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-144/17-144.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "State-by-state Minimax Adaptive Estimation for Nonparametric Hidden {M}arkov Models",
            "abstract": "In this paper, we introduce a new estimator for the emission densities of a nonparametric hidden Markov model. It is adaptive and minimax with respect to each state's regularity–as opposed to globally minimax estimators, which adapt to the worst regularity among the emission densities. Our method is based on Goldenshluger and Lepski's methodology. It is computationally efficient and only requires a family of preliminary estimators, without any restriction on the type of estimators considered. We present two such estimators that allow to reach minimax rates up to a logarithmic term: a spectral estimator and a least squares estimator. We show how to calibrate it in practice and assess its performance on simulations and on real data.",
            "keywords": [
                "hidden Markov model",
                "model selection",
                "nonparametric density estimation",
                "oracle inequality",
                "adaptive minimax estimation",
                "spectral method"
            ],
            "author": [
                "Luc Lehéricy"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-345/17-345.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning from Comparisons and Choices ",
            "abstract": "When tracking user-specific online activities, each user's preference is revealed in the form of choices and comparisons. For example, a user's purchase history is a record of her choices, i.e. which item was chosen among a subset of offerings. A user's preferences can be observed either explicitly as in movie ratings or implicitly as in viewing times of news articles. Given such individualized ordinal data in the form of comparisons and choices, we address the problem of collaboratively learning representations of the users and the items. The learned features can be used to predict a user's preference of an unseen item to be used in recommendation systems. This also allows one to compute similarities among users and items to be used for categorization and search. Motivated by the empirical successes of the MultiNomial Logit (MNL) model in marketing and transportation, and also more recent successes in word embedding and crowdsourced image embedding, we pose this problem as learning the MNL model parameters that best explain the data. We propose a convex relaxation for learning the MNL model, and show that it is minimax optimal up to a logarithmic factor by comparing its performance to a fundamental lower bound. This characterizes the minimax sample complexity of the problem, and proves that the proposed estimator cannot be improved upon other than by a logarithmic factor. Further, the analysis identifies how the accuracy depends on the topology of sampling via the spectrum of the sampling graph. This provides a guideline for designing surveys when one can choose which items are to be compared. This is accompanied by numerical simulations on synthetic and real data sets, confirming our theoretical predictions.",
            "keywords": [
                "Collaborative Ranking",
                "Nuclear Norm Minimization"
            ],
            "author": [
                "Sahand Negahban",
                "Sewoong Oh",
                "Kiran K. Thekumparampil",
                "Jiaming Xu"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-607/17-607.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models",
            "abstract": "Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an information-theoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing sparse outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds of unknown dimension obscured by gross corruptions.",
            "keywords": [
                "Variational Autoencoder",
                "Deep Generative Model"
            ],
            "author": [
                "Bin Dai",
                "Yu Wang",
                "John Aston",
                "Gang Hua",
                "David Wipf"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-704/17-704.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Efficient and Effective Generic Agglomerative Hierarchical Clustering Approach",
            "abstract": "We introduce an agglomerative hierarchical clustering (AHC) framework which is generic, efficient and effective. Our approach embeds a sub-family of Lance-Williams (LW) clusterings and relies on inner-products instead of squared Euclidean distances. We carry out a constrained bottom-up merging procedure on a sparsified normalized inner-product matrix. Our method is named SNK-AHC for Sparsified Normalized Kernel matrix based AHC. SNK-AHC is more scalable than the classic dissimilarity matrix based AHC. It can also produce better results when clusters have arbitrary shapes. Artificial and real-world benchmarks are used to exemplify these points. From a theoretical standpoint, SNK-AHC provides another interpretation of the classic techniques which relies on the concept of weighted penalized similarities. The differences between group average, Mcquitty, centroid, median and Ward, can be explained by their distinct averaging strategies for aggregating clusters inter-similarities and intra-similarities. Other features of SNK-AHC are examined. We provide sufficient conditions in order to have monotonic dendrograms, we elaborate a stored data matrix approach for centroid and median, we underline the diagonal translation invariance property of group average, Mcquitty and Ward and we show to what extent SNK-AHC can determine the number of clusters.",
            "keywords": [
                "Agglomerative hierarchical clustering",
                "Lance-Williams formula",
                "Kernel meth-     ods",
                "Scalability"
            ],
            "author": [
                "Julien Ah-Pine"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-117/18-117.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Markov Blanket and Markov Boundary of Multiple Variables",
            "abstract": "Markov blanket (Mb) and Markov boundary (MB) are two key concepts in Bayesian networks (BNs). In this paper, we study the problem of Mb and MB for multiple variables. First, we show that Mb possesses the additivity property under the local intersection assumption, that is, an Mb of multiple targets can be constructed by simply taking the union of Mbs of the individual targets and removing the targets themselves. MB is also proven to have additivity under the local intersection assumption. Second, we analyze the cases of violating additivity of Mb and MB and then put forward the notions of Markov blanket supplementary (MbS) and Markov boundary supplementary (MBS). The properties of MbS and MBS are studied in detail. Third, we build two MB discovery algorithms and prove their correctness under the local composition assumption. We also discuss the ways of practically doing conditional independence tests and analyze the complexities of the algorithms. Finally, we make a benchmarking study based on six synthetic BNs and then apply MB discovery to multi-class prediction based on a real data set. The experimental results reveal our algorithms have higher accuracies and lower complexities than existing algorithms.",
            "keywords": [
                "Markov blanket",
                "Markov boundary",
                "Markov blanket supplementary",
                "Markov boundary     supplementary"
            ],
            "author": [
                "Xu-Qing Liu",
                "Xin-Sheng Liu"
            ],
            "ref": "http://jmlr.org/papers/volume19/14-033/14-033.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Distribution Embeddings: Universal Kernels, Characteristic Kernels and Kernel Metrics on Distributions",
            "abstract": "Kernel mean embeddings have become a popular tool in machine learning. They map probability measures to functions in a reproducing kernel Hilbert space. The distance between two mapped measures defines a semi-distance over the probability measures known as the maximum mean discrepancy (MMD). Its properties depend on the underlying kernel and have been linked to three fundamental concepts of the kernel literature: universal, characteristic and strictly positive definite kernels. The contributions of this paper are three-fold. First, by slightly extending the usual definitions of universal, characteristic and strictly positive definite kernels, we show that these three concepts are essentially equivalent. Second, we give the first complete characterization of those kernels whose associated MMD-distance metrizes the weak convergence of probability measures. Third, we show that kernel mean embeddings can be extended from probability measures to generalized measures called Schwartz-distributions and analyze a few properties of these distribution embeddings.",
            "keywords": [
                "kernel mean embedding",
                "universal kernel",
                "characteristic kernel",
                "Schwartz-     distributions",
                "kernel metrics on distributions"
            ],
            "author": [
                "Carl-Johann Simon-Gabriel",
                "Bernhard Schölkopf"
            ],
            "ref": "http://jmlr.org/papers/volume19/16-291/16-291.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Forests, Decision Trees, and Categorical Predictors: The \"Absent Levels\" Problem",
            "abstract": "One advantage of decision tree based methods like random forests is their ability to natively handle categorical predictors without having to first transform them (e.g., by using feature engineering techniques). However, in this paper, we show how this capability can lead to an inherent “absent levels” problem for decision tree based methods that has never been thoroughly discussed, and whose consequences have never been carefully explored. This problem occurs whenever there is an indeterminacy over how to handle an observation that has reached a categorical split which was determined when the observation in question's level was absent during training. Although these incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's random forests FORTRAN code and the randomForest R package  (Liaw and Wiener, 2002) as motivating case studies, we examine how overlooking the absent levels problem can systematically bias a model. Furthermore, by using three real data examples, we illustrate how absent levels can dramatically alter a model's performance in practice, and we empirically demonstrate how some simple heuristics can be used to help mitigate the effects of the absent levels problem until a more robust theoretical solution is found.",
            "keywords": [
                "absent levels",
                "categorical predictors",
                "decision trees",
                "CART"
            ],
            "author": [
                "Timothy C. Au"
            ],
            "ref": "http://jmlr.org/papers/volume19/16-474/16-474.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Harmonic Mean Iteratively Reweighted Least Squares for Low-Rank Matrix Recovery",
            "abstract": "We propose a new iteratively reweighted least squares (IRLS) algorithm for the recovery of a matrix  of rank  from incomplete linear observations, solving a sequence of low complexity linear problems. The easily implementable algorithm, which we call harmonic mean iteratively reweighted least squares (HM-IRLS), optimizes a non-convex Schatten- quasi-norm penalization to promote low-rankness and carries three major strengths, in particular for the matrix completion setting. First, we observe a remarkable {global convergence behavior} of the algorithm's iterates to the low-rank matrix for relevant, interesting cases, for which any other state-of-the-art optimization approach fails the recovery. Secondly, HM-IRLS exhibits an empirical recovery probability close to  even for a number of measurements very close to the theoretical lower bound , i.e., already for significantly fewer linear observations than any other tractable approach in the literature. Thirdly, HM-IRLS exhibits a locally superlinear rate of convergence (of order ) if the linear observations fulfill a suitable null space property. While for the first two properties we have so far only strong empirical evidence, we prove the third property as our main theoretical result.",
            "keywords": [
                "Iteratively Reweighted Least Squares",
                "Low-Rank Matrix Recovery",
                "Matrix     Completion"
            ],
            "author": [
                "Christian Kümmerle",
                "Juliane Sigl"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-244/17-244.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Generalized Bellman Equations and Temporal-Difference Learning",
            "abstract": "We consider off-policy temporal-difference (TD) learning in discounted Markov decision processes, where the goal is to evaluate a policy in a model-free way by using observations of a state process generated without executing the policy. To curb the high variance issue in off-policy TD learning, we propose a new scheme of setting the -parameters of TD, based on generalized Bellman equations. Our scheme is to set  according to the eligibility trace iterates calculated in TD, thereby easily keeping these traces in a desired bounded range. Compared with prior work, this scheme is more direct and flexible, and allows much larger  values for off-policy TD learning with bounded traces. As to its soundness, using Markov chain theory, we prove the ergodicity of the joint state-trace process under nonrestrictive conditions, and we show that associated with our scheme is a generalized Bellman equation (for the policy to be evaluated) that depends on both the evolution of  and the unique invariant probability measure of the state-trace process. These results not only lead immediately to a characterization of the convergence behavior of least-squares based implementation of our scheme, but also prepare the ground for further analysis of gradient-based implementations.",
            "keywords": [
                "Markov decision process",
                "approximate policy evaluation",
                "generalized Bellman     equation",
                "reinforcement learning",
                "temporal-difference method",
                "Markov chain"
            ],
            "author": [
                "Huizhen Yu",
                "A. Rupam Mahmood",
                "Richard S. Sutton"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-283/17-283.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Emergence of Invariance and Disentanglement in Deep Representations ",
            "abstract": "Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.",
            "keywords": [],
            "author": [
                "Alessandro Achille",
                "Stefano Soatto"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-646/17-646.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Accelerating Cross-Validation in Multinomial Logistic Regression with $\\ell_1$-Regularization",
            "abstract": "We develop an approximate formula for evaluating a cross-validation estimator of predictive likelihood for multinomial logistic regression regularized by an -norm. This allows us to avoid repeated optimizations required for literally conducting cross-validation; hence, the computational time can be significantly reduced. The formula is derived through a perturbative approach employing the largeness of the data size and the model dimensionality. An extension to the elastic net regularization is also addressed. The usefulness of the approximate formula is demonstrated on simulated data and the ISOLET dataset from the UCI machine learning repository. MATLAB and python codes implementing the approximate formula are distributed in (Obuchi, 2017; Takahashi and Obuchi, 2017).",
            "keywords": [
                "classification",
                "multinomial logistic regression",
                "cross-validation",
                "linear pertur-     bation"
            ],
            "author": [
                "Tomoyuki Obuchi",
                "Yoshiyuki Kabashima"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-684/17-684.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Profile-Based Bandit with Unknown Profiles",
            "abstract": "Stochastic bandits have been widely studied since decades. A very large panel of settings have been introduced, some of them for the inclusion of some structure between actions. If actions are associated with feature vectors that underlie their usefulness, the discovery of a mapping parameter between such profiles and rewards can help the exploration process of the bandit strategies. This is the setting studied in this paper, but in our case the action profiles (constant feature vectors) are unknown beforehand. Instead, the agent is only given sample vectors, with mean centered on the true profiles, for a subset of actions at each step of the process. In this new bandit instance, policies have thus to deal with a doubled uncertainty, both on the profile estimators and the reward mapping parameters learned so far. We propose a new algorithm, called \\textit{SampLinUCB}, specifically designed for this case. Theoretical convergence guarantees are given for this strategy, according to various profile samples delivery scenarios. Finally, experiments are conducted on both artificial data and a task of focused data capture from online social networks. Obtained results demonstrate the relevance of the approach in various settings.",
            "keywords": [
                "Stochastic Linear Bandits",
                "Profile-based Exploration"
            ],
            "author": [
                "Sylvain Lamprier",
                "Thibault Gisselbrecht",
                "Patrick Gallinari"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-693/17-693.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "How Deep Are Deep Gaussian Processes?",
            "abstract": "Recent research has shown the potential utility of deep Gaussian processes. These deep structures are probability distributions, designed through hierarchical construction, which are conditionally Gaussian. In this paper, the current published body of work is placed in a common framework and, through recursion, several classes of deep Gaussian processes are defined. The resulting samples generated from a deep Gaussian process have a Markovian structure with respect to the depth parameter, and the effective depth of the resulting process is interpreted in terms of the ergodicity, or non-ergodicity, of the resulting Markov chain. For the classes of deep Gaussian processes introduced, we provide results concerning their ergodicity and hence their effective depth. We also demonstrate how these processes may be used for inference; in particular we show how a Metropolis-within-Gibbs construction across the levels of the hierarchy can be used to derive sampling tools which are robust to the level of resolution used to represent the functions on a computer. For illustration, we consider the effect of ergodicity in some simple numerical examples.",
            "keywords": [
                "deep learning",
                "deep Gaussian processes"
            ],
            "author": [
                "Matthew M. Dunlop",
                "Mark A. Girolami",
                "Andrew M. Stuart",
                "Aretha L. Teckentrup"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-015/18-015.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast MCMC Sampling Algorithms on Polytopes",
            "abstract": "We propose and analyze two new MCMC sampling algorithms, the Vaidya walk and the John walk, for generating samples from the uniform distribution over a polytope. Both random walks are sampling algorithms derived from interior point methods. The former is based on volumetric-logarithmic barrier introduced by Vaidya whereas the latter uses John's ellipsoids. We show that the Vaidya walk mixes in significantly fewer steps than the logarithmic-barrier based Dikin walk studied in past work. For a polytope in  defined by  linear constraints, we show that the mixing time from a warm start is bounded as , compared to the  mixing time bound for the Dikin walk. The cost of each step of the Vaidya walk is of the same order as the Dikin walk, and at most twice as large in terms of constant pre-factors. For the John walk, we prove an  bound on its mixing time and conjecture that an improved variant of it could achieve a mixing time of . Additionally, we propose variants of the Vaidya and John walks that mix in polynomial time from a deterministic starting point. The speed-up of the Vaidya walk over the Dikin walk are illustrated in numerical examples.",
            "keywords": [
                "MCMC methods",
                "interior point methods",
                "polytopes"
            ],
            "author": [
                "Yuansi Chen",
                "Raaz Dwivedi",
                "Martin J. Wainwright",
                "Bin Yu"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-158/18-158.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Modular Proximal Optimization for Multidimensional Total-Variation Regularization",
            "abstract": "We study TV regularization, a widely used technique for eliciting structured sparsity. In particular, we propose efficient algorithms for computing prox-operators for -norm TV. The most important among these is -norm TV, for whose prox-operator we present a new geometric analysis which unveils a hitherto unknown connection to taut-string methods. This connection turns out to be remarkably useful as it shows how our geometry guided implementation results in efficient weighted and unweighted 1D-TV solvers, surpassing state-of-the-art methods. Our 1D-TV solvers provide the backbone for building more complex (two or higher-dimensional) TV solvers within a modular proximal optimization approach. We review the literature for an array of methods exploiting this strategy, and illustrate the benefits of our modular design through extensive suite of experiments on (i) image denoising, (ii) image deconvolution, (iii) four variants of fused-lasso, and (iv) video denoising. To underscore our claims and permit easy reproducibility, we provide all the reviewed and our new TV solvers in an easy to use multi-threaded C++, Matlab and Python library.",
            "keywords": [
                "proximal optimization",
                "total variation",
                "regularized learning",
                "sparsity"
            ],
            "author": [
                "Alvaro Barbero",
                "Suvrit Sra"
            ],
            "ref": "http://jmlr.org/papers/volume19/13-538/13-538.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Semiparametric Exponential Family Graphical Models",
            "abstract": "We propose a new class of semiparametric exponential family graphical models for the analysis of high dimensional mixed data. Different from the existing mixed graphical models, we allow the nodewise conditional distributions to be semiparametric generalized linear models with unspecified base measure functions. Thus, one advantage of our method is that it is unnecessary to specify the type of each node and the method is more convenient to apply in practice. Under the proposed model, we consider both problems of parameter estimation and hypothesis testing in high dimensions. In particular, we propose a symmetric pairwise score test for the presence of a single edge in the graph. Compared to the existing methods for hypothesis tests, our approach takes into account of the symmetry of the parameters, such that the inferential results are invariant with respect to the different parametrizations of the same edge. Thorough numerical simulations and a real data example are provided to back up our theoretical results.",
            "keywords": [
                "Graphical Models",
                "Exponential Family"
            ],
            "author": [
                "Zhuoran Yang",
                "Yang Ning",
                "Han Liu"
            ],
            "ref": "http://jmlr.org/papers/volume19/15-493/15-493.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Theoretical Analysis of Cross-Validation for Estimating the Risk of the $k$-Nearest Neighbor Classifier",
            "abstract": "The present work aims at deriving theoretical guaranties on the behavior of some cross-validation procedures applied to the -nearest neighbors (NN) rule in the context of binary classification. Here we focus on the leave--out cross-validation (LO) used to assess the performance of the NN classifier. Remarkably this LO estimator can be efficiently computed in this context using closed-form formulas derived by Celisse and Mary-Huard (2011). We describe a general strategy to derive moment and exponential concentration inequalities for the LO estimator applied to the NN classifier. Such results are obtained first by exploiting the connection between the LO estimator and U-statistics, and second by making an intensive use of the generalized Efron-Stein inequality applied to the LO estimator. One other important contribution is made by deriving new quantifications of the discrepancy between the LO estimator and the classification error/risk of the NN classifier. The optimality of these bounds is discussed by means of several lower bounds as well as simulation experiments.",
            "keywords": [
                "Classification",
                "Cross-validation"
            ],
            "author": [
                "Alain Celisse",
                "Tristan Mary-Huard"
            ],
            "ref": "http://jmlr.org/papers/volume19/15-498/15-498.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": " Maximum Selection and Sorting with Adversarial Comparators",
            "abstract": "We study maximum selection and sorting of  numbers using imperfect pairwise comparators. The imperfect comparator returns the larger of the two inputs if the inputs are more than a given threshold apart and an adversarially-chosen input otherwise. We consider two adversarial models: a non-adaptive adversary that decides on the outcomes in advance and an adaptive adversary that decides on the outcome of each comparison depending on the previous comparisons and outcomes. Against the non-adaptive adversary, we derive a maximum-selection algorithm that uses at most  comparisons in expectation and a sorting algorithm that uses at most  comparisons in expectation. In the presence of the adaptive adversary, the proposed maximum-selection algorithm uses  comparisons to output a correct answer with probability at least , resolving an open problem in Ajtai et al. (2015). Our study is motivated by a density-estimation problem. Given samples from an unknown distribution, we would like to find a distribution among a known class of  candidate distributions that is close to the underlying distribution in  distance. Scheffe's algorithm, for example, in Devroye and Lugosi (2001) outputs a distribution at an  distance at most 9 times the minimum and runs in time . Using our algorithm, the runtime reduces to .",
            "keywords": [
                "noisy sorting",
                "adversarial comparators",
                "density estimation",
                "Scheffe estimatorc 2018 Jayadev Acharya",
                "Moein Falahatgar",
                "Ashkan Jafarpour"
            ],
            "author": [
                "Jayadev Acharya",
                "Moein Falahatgar",
                "Ashkan Jafarpour",
                "Alon Orlitsky",
                "Ananda Theertha Suresh"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-165/17-165.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A New and Flexible Approach to the Analysis of Paired Comparison Data",
            "abstract": "We consider the situation where  items are ranked by paired comparisons. It is usually assumed that the probability that item  is preferred over item  is  where  is a symmetric distribution function, which we refer to as the comparison function, and  and  are the merits or scores of the compared items. This modelling framework, which is ubiquitous in the paired comparison literature, strongly depends on the assumption that the comparison function  is known. In practice, however, this assumption is often unrealistic and may result in poor fit and erroneous inferences. This limitation has motivated us to relax the assumption that  is fully known and simultaneously estimate the merits of the objects and the underlying comparison function. Our formulation yields a flexible semi-definite programming problem that we use as a refinement step for estimating the paired comparison probability matrix. We provide a detailed sensitivity analysis and, as a result, we establish the consistency of the resulting estimators and provide bounds on the estimation and approximation errors. Some statistical properties of the resulting estimators as well as model selection criteria are investigated. Finally, using a large data-set of computer chess matches, we estimate the comparison function and find that the model used by the International Chess Federation does not seem to apply to computer chess.",
            "keywords": [
                "linear stochastic transitivity",
                "statistical ranking",
                "semi-definite programming",
                "model selection",
                "sensitivity analysis"
            ],
            "author": [
                "Ivo F. D. Oliveira",
                "Nir Ailon",
                "Ori Davidov"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-179/17-179.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simple Classification Using Binary Data",
            "abstract": "Binary, or one-bit, representations of data arise naturally in many applications, and are appealing in both hardware implementations and algorithm design. In this work, we study the problem of data classification from binary data obtained from the sign pattern of low-dimensional projections and propose a framework with low computation and resource costs. We illustrate the utility of the proposed approach through stylized and realistic numerical experiments, and provide a theoretical analysis for a simple case. We hope that our framework and analysis will serve as a foundation for studying similar types of approaches.",
            "keywords": [
                "binary measurements",
                "one-bit representations"
            ],
            "author": [
                "Deanna Needell",
                "Rayan Saab",
                "Tina Woolf"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-383/17-383.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hinge-Minimax Learner for the Ensemble of Hyperplanes",
            "abstract": "In this work we consider non-linear classifiers that comprise intersections of hyperplanes. We learn these classifiers by minimizing the “minimax” bound over the negative training examples and the hinge type loss of the positive training examples. These classifiers fit typical real-life datasets that consist of a small number of positive data points and a large number of negative data points. Such an approach is computationally appealing since the majority of training examples (belonging to the negative class) are represented by the statistics of their distribution, which is used in a single constraint on the empirical risk, as opposed to SVM, in which the number of variables is equal to the size of the training set. We first focus on intersection of  hyperplanes, for which we provide empirical risk bounds. We show that these bounds are dimensionally independent and decay as  for  samples. We then extend the K-hyperplane mixed risk to the latent mixed risk for training a union of  -hyperplane models, which can form an arbitrary complex, piecewise linear boundaries. We propose efficient algorithms for training the proposed models. Finally, we show how to combine hinge-minimax training with deep architectures and extend it to multi-class settings using transfer learning. The empirical evaluation of the proposed models shows their advantage over the existing methods in a small training labeled data regime.",
            "keywords": [
                "Minimiax",
                "Imbalanced Classification",
                "Intersection of K Hyperplanes"
            ],
            "author": [
                "Dolev Raviv",
                "Tamir Hazan",
                "Margarita Osadchy"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-402/17-402.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scaling up Data Augmentation MCMC via Calibration",
            "abstract": "There has been considerable interest in making Bayesian inference more scalable. In big data settings, most of the focus has been on reducing the computing time per iteration rather than reducing the number of iterations needed in Markov chain Monte Carlo (MCMC). This article considers data augmentation MCMC (DA-MCMC), a widely used technique. DA-MCMC samples tend to become highly autocorrelated in large samples, due to a mis-calibration problem in which conditional posterior distributions given augmented data are too concentrated. This makes it necessary to collect very long MCMC paths to obtain acceptably low MC error. To combat this inefficiency, we propose a family of calibrated data augmentation algorithms, which appropriately adjust the variance of conditional posterior distributions. A Metropolis-Hastings step is used to eliminate bias in the stationary distribution of the resulting sampler. Compared to existing alternatives, this approach can dramatically reduce MC error by reducing autocorrelation and increasing the effective number of DA-MCMC samples per unit of computing time. The approach is simple and applicable to a broad variety of existing data augmentation algorithms. We focus on three popular generalized linear models: probit, logistic and Poisson log-linear. Dramatic gains in computational efficiency are shown in applications.",
            "keywords": [
                "Bayesian Probit",
                "Biased subsampling",
                "Big n",
                "Data augmentation",
                "Log-linear     model",
                "Logistic regression",
                "Maximal correlation"
            ],
            "author": [
                "Leo L. Duan",
                "James E. Johndrow",
                "David B. Dunson"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-573/17-573.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Extrapolating Expected Accuracies for Large Multi-Class Problems",
            "abstract": "The difficulty of multi-class classification generally increases with the number of classes. Using data for a small set of the classes, can we predict how well the classifier scales as the number of classes increases? We propose a framework for studying this question, assuming that classes in both sets are sampled from the same population and that the classifier is based on independently learned scoring functions. Under this framework, we can express the classification accuracy on a set of  classes as the st moment of a discriminability function; the discriminability function itself does not depend on . We leverage this result to develop a non-parametric regression estimator for the discriminability function, which can extrapolate accuracy results to larger unobserved sets. We also formalize an alternative approach that extrapolates accuracy separately for each class, and identify tradeoffs between the two methods. We show that both methods can accurately predict classifier performance on label sets up to ten times the size of the original set, both in simulations as well as in realistic face recognition or character recognition tasks.",
            "keywords": [
                "Multi-class problems",
                "face recognition",
                "object recognition",
                "transfer learning"
            ],
            "author": [
                "Charles Zheng",
                "Rakesh Achanta",
                "Yuval Benjamini"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-701/17-701.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Inference via Low-Dimensional Couplings",
            "abstract": "We investigate the low-dimensional structure of deterministic transformations between random variables, i.e., transport maps between probability measures. In the context of statistics and machine learning, these transformations can be used to couple a tractable “reference” measure (e.g., a standard Gaussian) with a target measure of interest. Direct simulation from the desired measure can then be achieved by pushing forward reference samples through the map. Yet characterizing such a map---e.g., representing and evaluating it---grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of low-dimensional couplings, induced by transport maps that are sparse and/or decomposable. Our analysis not only facilitates the construction of transformations in high-dimensional settings, but also suggests new inference methodologies for continuous non-Gaussian graphical models. For instance, in the context of nonlinear state-space models, we describe new variational algorithms for filtering, smoothing, and sequential parameter inference. These algorithms can be understood as the natural generalization---to the non-Gaussian case---of the square-root Rauch--Tung--Striebel Gaussian smoother.",
            "keywords": [
                "transport map",
                "variational inference",
                "graphical models",
                "sparsity",
                "state-space     models"
            ],
            "author": [
                "Alessio Spantini",
                "Daniele Bigoni",
                "Youssef Marzouk"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-747/17-747.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Bayesian Inference of Sigmoidal Gaussian Cox Processes",
            "abstract": "We present an approximate Bayesian inference approach for estimating the intensity of a inhomogeneous Poisson process, where the intensity function is modelled using a Gaussian process (GP) prior via a sigmoid link function. Augmenting the model using a latent marked Poisson process and Polya--Gamma random variables we obtain a representation of the likelihood which is conjugate to the GP prior. We estimate the posterior using a variational free--form mean field optimisation together with the framework of sparse GPs. Furthermore, as alternative approximation we suggest a sparse Laplace's method for the posterior, for which an efficient expectation--maximisation algorithm is derived to find the posterior's mode. Both algorithms compare well against exact inference obtained by a Markov Chain Monte Carlo sampler and standard variational Gauss approach solving the same model, while being one order of magnitude faster. Furthermore, the performance and speed of our method is competitive with that of another recently proposed Poisson process model based on a quadratic link function, while not being limited to GPs with squared exponential kernels and rectangular domains.",
            "keywords": [],
            "author": [
                "Christian Donner",
                "Manfred Opper"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-759/17-759.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multivariate Bayesian Structural Time Series Model",
            "abstract": "This paper deals with inference and prediction for multiple correlated time series, where one also has the choice of using a candidate pool of contemporaneous predictors for each target series. Starting with a structural model for time series, we use Bayesian tools for model fitting, prediction and feature selection, thus extending some recent works along these lines for the univariate case. The Bayesian paradigm in this multivariate setting helps the model avoid overfitting, as well as captures correlations among multiple target time series with various state components. The model provides needed flexibility in selecting a different set of components and available predictors for each target series. The cyclical component in the model can handle large variations in the short term, which may be caused by external shocks. Extensive simulations were run to investigate properties such as estimation accuracy and performance in forecasting. This was followed by an empirical study with one-step-ahead prediction on the max log return of a portfolio of stocks that involve four leading financial institutions. Both the simulation studies and the extensive empirical study confirm that this multivariate model outperforms three other benchmark models, viz. a model that treats each target series as independent, the autoregressive integrated moving average model with regression (ARIMAX), and the multivariate ARIMAX (MARIMAX) model.",
            "keywords": [
                "Multivariate Time Series",
                "Feature Selection",
                "Bayesian Model Averaging",
                "Cyclical Component"
            ],
            "author": [
                "Jinwen Qiu",
                "S. Rao Jammalamadaka",
                "Ning Ning"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-009/18-009.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Inverse Reinforcement Learning via Nonparametric Spatio-Temporal Subgoal Modeling",
            "abstract": "Advances in the field of inverse reinforcement learning (IRL) have led to sophisticated inference frameworks that relax the original modeling assumption of observing an agent behavior that reflects only a single intention. Instead of learning a global behavioral model, recent IRL methods divide the demonstration data into parts, to account for the fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. In this work, we go one step further: using the intuitive concept of subgoals, we build upon the premise that even a single trajectory can be explained more efficiently locally within a certain context than globally, enabling a more compact representation of the observed behavior. Based on this assumption, we build an implicit intentional model of the agent's goals to forecast its behavior in unobserved situations. The result is an integrated Bayesian prediction framework that significantly outperforms existing IRL solutions and provides smooth policy estimates consistent with the expert's plan. Most notably, our framework naturally handles situations where the intentions of the agent change over time and classical IRL algorithms fail. In addition, due to its probabilistic nature, the model can be straightforwardly applied in active learning scenarios to guide the demonstration process of the expert.",
            "keywords": [
                "Learning from Demonstration",
                "Inverse Reinforcement Learning",
                "Bayesian     Nonparametric Modeling",
                "Subgoal Inference",
                "Graphical Models"
            ],
            "author": [
                "Adrian Šošić",
                "Elmar Rueckert",
                "Jan Peters",
                "Abdelhak M. Zoubir",
                "Heinz Koeppl"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-113/18-113.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Implicit Bias of Gradient Descent on Separable Data",
            "abstract": "We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.",
            "keywords": [
                "gradient descent",
                "implicit regularization",
                "generalization",
                "margin"
            ],
            "author": [
                "Daniel Soudry",
                "Elad Hoffer",
                "Mor Shpigel Nacson",
                "Suriya Gunasekar",
                "Nathan Srebro"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-188/18-188.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Quantum Sample Complexity of Learning Algorithms",
            "abstract": "In learning theory, the VC dimension of a concept class  is the most common way to measure its “richness.” A fundamental result says that the number of examples needed to learn an unknown target concept  under an unknown distribution , is tightly determined by the VC dimension  of the concept class . Specifically, in the PAC model  examples are necessary and sufficient for a learner to output, with probability , a hypothesis  that is -close to the target concept  (measured under ). In the related agnostic model, where the samples need not come from a , we know that  examples are necessary and sufficient to output an hypothesis  whose error is at most  worse than the error of the best concept in . Here we analyze quantum sample complexity, where each example is a coherent quantum state. This model was introduced by  Bshouty and Jackson (1999), who showed that quantum examples are more powerful than classical examples in some fixed-distribution settings. However,  Atıcı and Servedio (2005), improved by  Zhang (2010), showed that in the PAC setting (where the learner has to succeed for every distribution), quantum examples cannot be much more powerful: the required number of quantum examples is  Our main result is that quantum and classical sample complexity are in fact equal up to constant factors in both the PAC and agnostic models. We give two proof approaches. The first is a fairly simple information-theoretic argument that yields the above two classical bounds and yields the same bounds for quantum sample complexity up to a  factor. We then give a second approach that avoids the log-factor loss, based on analyzing the behavior of the “Pretty Good Measurement” on the quantum state-identification problems that correspond to learning. This shows classical and quantum sample complexity are equal up to constant factors for every concept class .",
            "keywords": [
                "Quantum computing",
                "PAC learning",
                "Sample complexity"
            ],
            "author": [
                "Srinivasan Arunachalam",
                "Ronald de Wolf"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-195/18-195.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scikit-Multiflow: A Multi-output Streaming Framework ",
            "abstract": "scikit-multiflow is a framework for learning from data streams and multi-output learning in Python. Conceived to serve as a platform to encourage the democratization of stream learning research, it provides multiple state-of-the-art learning methods, data generators and evaluators for different stream learning problems, including single-output, multi-output and multi-label. scikit-multiflow builds upon popular open source frameworks including scikit-learn, MOA and MEKA. Development follows the FOSS principles. Quality is enforced by complying with PEP8 guidelines, using continuous integration and functional testing.",
            "keywords": [
                "Machine Learning",
                "Stream Data",
                "Multi-output",
                "Drift Detection"
            ],
            "author": [
                "Jacob Montiel",
                "Jesse Read",
                "Albert Bifet",
                "Talel Abdessalem"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-251/18-251.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Bounds for Johnson-Lindenstrauss Transformations",
            "abstract": "In 1984, Johnson and Lindenstrauss proved that any finite set of data in a high-dimensional space can be projected to a lower-dimensional space while preserving the pairwise Euclidean distances between points up to a bounded relative error. If the desired dimension of the image is too small, however, Kane, Meka, and Nelson (2011) and Jayram and Woodruff (2013) proved that such a projection does not exist. In this paper, we provide a precise asymptotic threshold for the dimension of the image, above which, there exists a projection preserving the Euclidean distance, but, below which, there does not exist such a projection.",
            "keywords": [
                "Johnson-Lindenstrauss transformation",
                "Dimension reduction",
                "Phase transi-     tion"
            ],
            "author": [
                "Michael Burr",
                "Shuhong Gao",
                "Fiona Knoll"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-264/18-264.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An efficient distributed learning algorithm based on effective local functional approximations",
            "abstract": "Scalable machine learning over big data is an important problem that is receiving a lot of attention in recent years. On popular distributed environments such as Hadoop running on a cluster of commodity machines, communication costs are substantial and algorithms need to be designed suitably considering those costs. In this paper we give a novel approach to the distributed training of linear classifiers (involving smooth losses and  regularization) that is designed to reduce the total communication costs. At each iteration, the nodes minimize locally formed approximate objective functions; then the resulting minimizers are combined to form a descent direction to move. Our approach gives a lot of freedom in the formation of the approximate objective function as well as in the choice of methods to solve them. The method is shown to have  time convergence. The method can be viewed as an iterative parameter mixing method. A special instantiation yields a parallel stochastic gradient descent method with strong convergence. When communication times between nodes are large, our method is much faster than the Terascale method (Agarwal et al., 2011), which is a state of the art distributed solver based on the statistical query model (Chu et al., 2006) that computes function and gradient values in a distributed fashion. We also evaluate against other recent distributed methods and demonstrate superior performance of our method.",
            "keywords": [
                "Distributed learning",
                "Example partitioning",
                "L2 regularizationc 2018 Dhruv Mahajan",
                "Nikunj Agrawal"
            ],
            "author": [
                "Dhruv Mahajan",
                "Nikunj Agrawal",
                "S. Sathiya Keerthi",
                "Sundararajan Sellamanickam",
                "Leon Bottou"
            ],
            "ref": "http://jmlr.org/papers/volume19/15-020/15-020.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Using Side Information to Reliably Learn Low-Rank Matrices from Missing and Corrupted Observations",
            "abstract": "Learning a low-rank matrix from missing and corrupted observations is a fundamental problem in many machine learning applications. However, the role of side information in low-rank matrix learning has received little attention, and most current approaches are either ad-hoc or only applicable in certain restrictive cases. In this paper, we propose a general model that exploits side information to better learn low-rank matrices from missing and corrupted observations, and show that the proposed model can be further applied to several popular scenarios such as matrix completion and robust PCA. Furthermore, we study the effect of side information on sample complexity and show that by using our model, the efficiency for learning can be improved given sufficiently informative side information. This result thus provides theoretical insight into the usefulness of side information in our model. Finally, we conduct comprehensive experiments in three real-world applications---relationship prediction, semi-supervised clustering and noisy image classification, showing that our proposed model is able to properly exploit side information for more effective learning both in theory and practice.",
            "keywords": [
                "Side information",
                "low-rank matrix learning",
                "learning from missing and cor-     rupted observations",
                "matrix completion"
            ],
            "author": [
                "Kai-Yang Chiang",
                "Inderjit S. Dhillon",
                "Cho-Jui Hsieh"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-112/17-112.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Random Matrix Analysis and Improvement of Semi-Supervised Learning for Large Dimensional Data",
            "abstract": "This article provides an original understanding of the behavior of a class of graph-oriented semi-supervised learning algorithms in the limit of large and numerous data. It is demonstrated that the intuition at the root of these methods collapses in this limit and that, as a result, most of them become inconsistent. Corrective measures and a new data-driven parametrization scheme are proposed along with a theoretical analysis of the asymptotic performances of the resulting approach. A surprisingly close behavior between theoretical performances on Gaussian mixture models and on real data sets is also illustrated throughout the article, thereby suggesting the importance of the proposed analysis for dealing with practical data. As a result, significant performance gains are observed on practical data classification using the proposed parametrization.",
            "keywords": [
                "semi-supervised learning",
                "kernel methods",
                "random matrix theory"
            ],
            "author": [
                "Xiaoyi Mai"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-421/17-421.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust PCA by Manifold Optimization",
            "abstract": "Robust PCA is a widely used statistical procedure to recover an underlying low-rank matrix with grossly corrupted observations. This work considers the problem of robust PCA as a nonconvex optimization problem on the manifold of low-rank matrices and proposes two algorithms based on manifold optimization. It is shown that, with a properly designed initialization, the proposed algorithms are guaranteed to converge to the underlying low-rank matrix linearly. Compared with a previous work based on the factorization of low-rank matrices  Yi et al. (2016), the proposed algorithms reduce the dependence on the condition number of the underlying low-rank matrix theoretically. Simulations and real data examples confirm the competitive performance of our method.",
            "keywords": [
                "principal component analysis",
                "low-rank modeling"
            ],
            "author": [
                "Teng Zhang",
                "Yi Yang"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-473/17-473.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improved Asynchronous Parallel Optimization Analysis for Stochastic Incremental Methods",
            "abstract": "As data sets continue to increase in size and multi-core computer architectures are developed, asynchronous parallel optimization algorithms become more and more essential to the field of Machine Learning. Unfortunately, conducting the theoretical analysis asynchronous methods is difficult, notably due to the introduction of delay and inconsistency in inherently sequential algorithms. Handling these issues often requires resorting to simplifying but unrealistic assumptions. Through a novel perspective, we revisit and clarify a subtle but important technical issue present in a large fraction of the recent convergence rate proofs for asynchronous parallel optimization algorithms, and propose a simplification of the recently introduced “perturbed iterate” framework that resolves it. We demonstrate the usefulness of our new framework by analyzing three distinct asynchronous parallel incremental optimization algorithms: Hogwild (asynchronous SGD), KROMAGNON (asynchronous SVRG) and ASAGA, a novel asynchronous parallel version of the incremental gradient algorithm SAGA that enjoys fast linear convergence rates. We are able to both remove problematic assumptions and obtain better theoretical results. Notably, we prove that ASAGA and KROMAGNON can obtain a theoretical linear speedup on multi-core systems even without sparsity assumptions. We present results of an implementation on a 40-core architecture illustrating the practical speedups as well as the hardware overhead. Finally, we investigate the overlap constant, an ill-understood but central quantity for the theoretical analysis of asynchronous parallel algorithms. We find that it encompasses much more complexity than suggested in previous work, and often is order-of-magnitude bigger than traditionally thought.",
            "keywords": [
                "optimization",
                "machine learning",
                "large scale",
                "asynchronous parallel",
                "sparsityc 2018 Rémi Leblond"
            ],
            "author": [
                "Remi Leblond",
                "Fabian Pedregosa",
                "Simon Lacoste-Julien"
            ],
            "ref": "http://jmlr.org/papers/volume19/17-650/17-650.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Clustering is semidefinitely not that hard: Nonnegative SDP for manifold disentangling",
            "abstract": "In solving hard computational problems, semidefinite program (SDP) relaxations often play an important role because they come with a guarantee of optimality. Here, we focus on a popular semidefinite relaxation of K-means clustering which yields the same solution as the non-convex original formulation for well segregated datasets. We report an unexpected finding: when data contains (greater than zero-dimensional) manifolds, the SDP solution captures such geometrical structures. Unlike traditional manifold embedding techniques, our approach does not rely on manually defining a kernel but rather enforces locality via a nonnegativity constraint. We thus call our approach NOnnegative MAnifold Disentangling, or NOMAD. To build an intuitive understanding of its manifold learning capabilities, we develop a theoretical analysis of NOMAD on idealized datasets. While NOMAD is convex and the globally optimal solution can be found by generic SDP solvers with polynomial time complexity, they are too slow for modern datasets. To address this problem, we analyze a non-convex heuristic and present a new, convex and yet efficient, algorithm, based on the conditional gradient method. Our results render NOMAD a versatile, understandable, and powerful tool for manifold learning.",
            "keywords": [
                "K-means",
                "semidefinite programming",
                "manifolds"
            ],
            "author": [
                "Mariano Tepper",
                "Anirvan M. Sengupta",
                "Dmitri Chklovskii"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-088/18-088.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Seglearn: A Python Package for Learning Sequences and Time Series",
            "abstract": "seglearn is an open-source Python package for performing machine learning on time series or sequences. The implementation provides a flexible pipeline for tackling classification, regression, and forecasting problems with multivariate sequence and contextual data. Sequences and series may be learned directly with deep learning models or via feature representation with classical machine learning estimators. This package is compatible with scikit-learn and is listed under scikit-learn Related Projects. The package depends on numpy, scipy, and scikit-learn. seglearn is distributed under the BSD 3-Clause License. Documentation includes a detailed API description, user guide, and examples. Unit tests provide a high degree of code coverage. Source code and documentation can be downloaded from https://github.com/dmbee/seglearn.",
            "keywords": [
                "Machine-Learning",
                "Time-Series",
                "Sequences"
            ],
            "author": [
                "David M. Burns",
                "Cari M. Whyne"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-160/18-160.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DALEX: Explainers for Complex Predictive Models in R",
            "abstract": "Predictive modeling is invaded by elastic, yet complex methods such as neural networks or ensembles (model stacking, boosting or bagging). Such methods are usually described by a large number of parameters or hyper parameters - a price that one needs to pay for elasticity. The very number of parameters makes models hard to understand. This paper describes a consistent collection of explainers for predictive models, a.k.a. black boxes. Each explainer is a technique for exploration of a black box model. Presented approaches are model-agnostic, what means that they extract useful information from any predictive method irrespective of its internal structure. Each explainer is linked with a specific aspect of a model. Some are useful in decomposing predictions, some serve better in understanding performance, while others are useful in understanding importance and conditional responses of a particular variable. Every explainer presented here works for a single model or for a collection of models. In the latter case, models can be compared against each other. Such comparison helps to find strengths and weaknesses of different models and gives additional tools for model validation. Presented explainers are implemented in the DALEX package for R. They are based on a uniform standardized grammar of model exploration which may be easily extended.",
            "keywords": [
                "interpretable machine learning",
                "explainable artificial intelligence",
                "predictive     modelling"
            ],
            "author": [
                "Przemyslaw Biecek"
            ],
            "ref": "http://jmlr.org/papers/volume19/18-416/18-416.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptation Based on Generalized Discrepancy",
            "abstract": "We present a new algorithm for domain adaptation improving upon a discrepancy minimization algorithm, (DM), previously shown to outperform a number of algorithms for this problem. Unlike many previously proposed solutions for domain adaptation, our algorithm does not consist of a fixed reweighting of the losses over the training sample. Instead, the reweighting depends on the hypothesis sought. The algorithm is derived from a less conservative notion of discrepancy than the DM algorithm called generalized discrepancy. We present a detailed description of our algorithm and show that it can be formulated as a convex optimization problem. We also give a detailed theoretical analysis of its learning guarantees which helps us select its parameters. Finally, we report the results of experiments demonstrating that it improves upon discrepancy minimization.",
            "keywords": [
                "domain adaptation"
            ],
            "author": [
                "Corinna Cortes",
                "Mehryar Mohri",
                "Andrés Muñoz Medina"
            ],
            "ref": "http://jmlr.org/papers/volume20/15-192/15-192.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Transport Analysis of Infinitely Deep Neural Network",
            "abstract": "We investigated the feature map inside deep neural networks (DNNs) by tracking the transport map. We are interested in the role of depth---why do DNNs perform better than shallow models?---and the interpretation of DNNs---what do intermediate layers do? Despite the rapid development in their application, DNNs remain analytically unexplained because the hidden layers are nested and the parameters are not faithful. Inspired by the integral representation of shallow NNs, which is the continuum limit of the width, or the hidden unit number, we developed the flow representation and transport analysis of DNNs. The flow representation is the continuum limit of the depth, or the hidden layer number, and it is specified by an ordinary differential equation (ODE) with a vector field. We interpret an ordinary DNN as a transport map or an Euler broken line approximation of the flow. Technically speaking, a dynamical system is a natural model for the nested feature maps. In addition, it opens a new way to the coordinate-free treatment of DNNs by avoiding the redundant parametrization of DNNs. Following Wasserstein geometry, we analyze a flow in three aspects: dynamical system, continuity equation, and Wasserstein gradient flow. A key finding is that we specified a series of transport maps of the denoising autoencoder (DAE), which is a cornerstone for the development of deep learning. Starting from the shallow DAE, this paper develops three topics: the transport map of the deep DAE, the equivalence between the stacked DAE and the composition of DAEs, and the development of the double continuum limit or the integral representation of the flow representation. As partial answers to the research questions, we found that deeper DAEs converge faster and the extracted features are better; in addition, a deep Gaussian DAE transports mass to decrease the Shannon entropy of the data distribution. We expect that further investigations on these questions lead to the development of an interpretable and principled alternatives to DNNs.",
            "keywords": [
                "representation learning",
                "denoising autoencoder",
                "flow representation",
                "contin-     uum limit",
                "backward heat equation",
                "Wasserstein geometry"
            ],
            "author": [
                "Sho Sonoda",
                "Noboru Murata"
            ],
            "ref": "http://jmlr.org/papers/volume20/16-243/16-243.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parsimonious Online Learning with Kernels via Sparse Projections in Function Space",
            "abstract": "Despite their attractiveness, popular perception is that techniques for nonparametric function approximation do not scale to streaming data due to an intractable growth in the amount of storage they require. To solve this problem in a memory-affordable way, we propose an online technique based on functional stochastic gradient descent in tandem with supervised sparsification based on greedy function subspace projections. The method, called parsimonious online learning with kernels (POLK), provides a controllable tradeoff between its solution accuracy and the amount of memory it requires. We derive conditions under which the generated function sequence converges almost surely to the optimal function, and we establish that the memory requirement remains finite. We evaluate POLK for kernel multi-class logistic regression and kernel hinge-loss classification on three canonical data sets: a synthetic Gaussian mixture model, the MNIST hand-written digits, and the Brodatz texture database. On all three tasks, we observe a favorable trade-off of objective function evaluation, classification performance, and complexity of the nonparametric regressor extracted by the proposed method.",
            "keywords": [
                "kernel methods",
                "online learning",
                "stochastic optimization",
                "supervised learning",
                "orthogonal matching pursuit"
            ],
            "author": [
                "Alec Koppel",
                "Garrett Warnell",
                "Ethan Stump",
                "Alejandro Ribeiro"
            ],
            "ref": "http://jmlr.org/papers/volume20/16-585/16-585.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Rate of a Simulated Annealing Algorithm with Noisy Observations",
            "abstract": "In this paper we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem. More precisely, we address the problem of finding a global minimizer of a function with noisy evaluations. We provide a rate of convergence and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1. This work is completed with a set of numerical experimentations and assesses the practical performance both on benchmark test cases and on real world examples.",
            "keywords": [
                "Simulated Annealing",
                "Stochastic Optimization",
                "Markov Process",
                "Conver-     gence Rate"
            ],
            "author": [
                "Clément Bouttier",
                "Ioana Gavra"
            ],
            "ref": "http://jmlr.org/papers/volume20/16-588/16-588.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-Convex Projected Gradient Descent for Generalized Low-Rank Tensor Regression",
            "abstract": "In this paper, we consider the problem of learning high-dimensional tensor regression problems with low-rank structure. One of the core challenges associated with learning high-dimensional models is computation since the underlying optimization problems are often non-convex. While convex relaxations could lead to polynomial-time algorithms they are often slow in practice. On the other hand, limited theoretical guarantees exist for non-convex methods. In this paper we provide a general framework that provides theoretical guarantees for learning high-dimensional tensor regression models under different low-rank structural assumptions using the projected gradient descent algorithm applied to a potentially non-convex constraint set  in terms of its localized Gaussian width (due to Gaussian design). We juxtapose our theoretical results for non-convex projected gradient descent algorithms with previous results on regularized convex approaches. The two main differences between the convex and non-convex approach are: (i) from a computational perspective whether the non-convex projection operator is computable and whether the projection has desirable contraction properties and (ii) from a statistical error bound perspective, the non-convex approach has a superior rate for a number of examples. We provide three concrete examples of low-dimensional structure which address these issues and explain the pros and cons for the non-convex and convex approaches. We supplement our theoretical results with simulations which show that, under several common settings of generalized low rank tensor regression, the projected gradient descent approach is superior both in terms of statistical error and run-time provided the step-sizes of the projected descent algorithm are suitably chosen.",
            "keywords": [
                "tensors",
                "non-convex optimization",
                "high-dimensional regression"
            ],
            "author": [
                "Han Chen",
                "Garvesh Raskutti",
                "Ming Yuan"
            ],
            "ref": "http://jmlr.org/papers/volume20/16-607/16-607.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "scikit-multilearn: A Python library for Multi-Label Classification",
            "abstract": "The scikit-multilearn is a Python library for performing multi-label classification. It is compatible with the scikit-learn and scipy ecosystems and uses sparse matrices for all internal operations; provides native Python implementations of popular multi-label classification methods alongside a novel framework for label space partitioning and division and includes modern algorithm adaptation methods, network-based label space division approaches, which extracts label dependency information and multi-label embedding classifiers. The library provides Python wrapped access to the extensive multi-label method stack from Java libraries and makes it possible to extend deep learning single-label methods for multi-label tasks. The library allows multi-label stratification and data set management. The implementation is more efficient in problem transformation than other established libraries, has good test coverage and follows PEP8. Source code and documentation can be downloaded from http://scikit.ml and also via pip The project is BSD-licensed.",
            "keywords": [
                "Python",
                "multi-label classification",
                "label-space clustering",
                "multi-label embed-     ding"
            ],
            "author": [
                "Piotr Szymański",
                "Tomasz Kajdanowicz"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-100/17-100.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Approximations for Generalized Linear Problems",
            "abstract": "In stochastic optimization, the population risk is generally approximated by the empirical risk which is in turn minimized by an iterative algorithm. However, in the large-scale setting, empirical risk minimization may be computationally restrictive. In this paper, we design an efficient algorithm to approximate the population risk minimizer in generalized linear problems such as binary classification with surrogate losses and generalized linear regression models. We focus on large-scale problems where the iterative minimization of the empirical risk is computationally intractable, i.e., the number of observations  is much larger than the dimension of the parameter  (). We show that under random sub-Gaussian design, the true minimizer of the population risk is approximately proportional to the corresponding ordinary least squares (OLS) estimator. Using this relation, we design an algorithm that achieves the same accuracy as the empirical risk minimizer through iterations that attain up to a quadratic convergence rate, and that are computationally cheaper than any batch optimization algorithm by at least a factor of . We provide theoretical guarantees for our algorithm, and analyze the convergence behavior in terms of data dimensions. Finally, we demonstrate the performance of our algorithm on well-known classification and regression problems, through extensive numerical studies on large-scale datasets, and show that it achieves the highest performance compared to several other widely used optimization algorithms.",
            "keywords": [
                "Generalized Linear Problems",
                "Stochastic optimization",
                "Subsampling"
            ],
            "author": [
                "Murat Erdogdu",
                "Mohsen Bayati",
                "Lee H. Dicker"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-279/17-279.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Forward-Backward Selection with Early Dropping",
            "abstract": "Forward-backward selection is one of the most basic and commonly-used feature selection algorithms available. It is also general and conceptually applicable to many different types of data. In this paper, we propose a heuristic that significantly improves its running time, while preserving predictive performance. The idea is to temporarily discard the variables that are conditionally independent with the outcome given the selected variable set. Depending on how those variables are reconsidered and reintroduced, this heuristic gives rise to a family of algorithms with increasingly stronger theoretical guarantees. In distributions that can be faithfully represented by Bayesian networks or maximal ancestral graphs, members of this algorithmic family are able to correctly identify the Markov blanket in the sample limit. In experiments we show that the proposed heuristic increases computational efficiency by about 1-2 orders of magnitude, while selecting fewer or the same number of variables and retaining predictive performance. Furthermore, we show that the proposed algorithm and feature selection with LASSO perform similarly when restricted to select the same number of variables, making the proposed algorithm an attractive alternative for problems where no (efficient) algorithm for LASSO exists.",
            "keywords": [
                "Feature Selection",
                "Forward Selection",
                "Markov Blanket Discovery",
                "Bayesian     Networks"
            ],
            "author": [
                "Giorgos Borboudakis",
                "Ioannis Tsamardinos"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-334/17-334.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Pricing in High-dimensions",
            "abstract": "We study the pricing problem faced by a firm that sells a large number of products, described via a wide range of features, to customers that arrive over time. Customers independently make purchasing decisions according to a general choice model that includes products features and customers' characteristics, encoded as -dimensional numerical vectors, as well as the price offered. The parameters of the choice model are a priori unknown to the firm, but can be learned as the (binary-valued) sales data accrues over time. The firm's objective is to maximize its revenue. We benchmark the performance using the classic regret minimization framework where the regret is defined as the expected revenue loss against a clairvoyant policy that knows the parameters of the choice model in advance, and always offers the revenue-maximizing price. This setting is motivated in part by the prevalence of online marketplaces that allow for real-time pricing. We assume a structured choice model, parameters of which depend on  out of the  product features. Assuming that the market noise distribution is known, we propose a dynamic policy, called Regularized Maximum Likelihood Pricing (RMLP) that leverages the (sparsity) structure of the high-dimensional model and obtains a logarithmic regret in . More specifically, the regret of our algorithm is of . Furthermore, we show that no policy can obtain regret better than . {In addition, we propose a generalization of our policy to a setting that the market noise distribution is unknown but belongs to a parametrized family of distributions. This policy obtains regret of . We further show that no policy can obtain regret better than  in such environments.}",
            "keywords": [
                "Revenue Management",
                "Dynamic Pricing",
                "High-dimensional Regression",
                "Max-     imum Likelihood",
                "Regret"
            ],
            "author": [
                "Adel Javanmard",
                "Hamid Nazerzadeh"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-357/17-357.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graphical Lasso and Thresholding: Equivalence and Closed-form Solutions",
            "abstract": "Graphical Lasso (GL) is a popular method for learning the structure of an undirected graphical model, which is based on an  regularization technique. The objective of this paper is to compare the computationally-heavy GL technique with a numerically-cheap heuristic method that is based on simply thresholding the sample covariance matrix. To this end, two notions of sign-consistent and inverse-consistent matrices are developed, and then it is shown that the thresholding and GL methods are equivalent if: (i) the thresholded sample covariance matrix is both sign-consistent and inverse-consistent, and (ii) the gap between the largest thresholded and the smallest un-thresholded entries of the sample covariance matrix is not too small. By building upon this result, it is proved that the GL method---as a conic optimization problem---has an explicit closed-form solution if the thresholded sample covariance matrix has an acyclic structure. This result is then generalized to arbitrary sparse support graphs, where a formula is found to obtain an approximate solution of GL. Furthermore, it is shown that the approximation error of the derived explicit formula decreases exponentially fast with respect to the length of the minimum-length cycle of the sparsity graph. The developed results are demonstrated on synthetic data, functional MRI data, traffic flows for transportation networks, and massive randomly generated data sets. We show that the proposed method can obtain an accurate approximation of the GL for instances with the sizes as large as  (more than 3.2 billion variables) in less than 30 minutes on a standard laptop computer running MATLAB, while other state-of-the-art methods do not converge within 4 hours",
            "keywords": [
                "Graphical Lasso",
                "Graphical Model",
                "Sparse Graphs"
            ],
            "author": [
                "Salar Fattahi",
                "Somayeh Sojoudi"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-501/17-501.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Approach to One-Bit Compressed Sensing Based on Probably Approximately Correct Learning Theory ",
            "abstract": "In this paper, the problem of one-bit compressed sensing (OBCS) is formulated as a problem in probably approximately correct (PAC) learning. It is shown that the Vapnik-Chervonenkis (VC-) dimension of the set of half-spaces in  generated by -sparse vectors is bounded below by  and above by . By coupling this estimate with well-established results in PAC learning theory, we show that a consistent algorithm can recover a -sparse vector with  measurements, given only the signs of the measurement vector. This result holds for \\textit{all} probability measures on . The theory is also applicable to the case of noisy labels, where the signs of the measurements are flipped with some unknown probability.",
            "keywords": [],
            "author": [
                "Mehmet Eren Ahsen",
                "Mathukumalli Vidyasagar"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-504/17-504.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Kernel K-Means Clustering with Nystrom Approximation: Relative-Error Bounds",
            "abstract": "Kernel -means clustering can correctly identify and extract a far more varied collection of cluster structures than the linear -means clustering algorithm. However, kernel -means clustering is computationally expensive when the non-linear feature map is high-dimensional and there are many input points. Kernel approximation, e.g., the Nystrom method, has been applied in previous works to approximately solve kernel learning problems when both of the above conditions are present. This work analyzes the application of this paradigm to kernel -means clustering, and shows that applying the linear -means clustering algorithm to  features constructed using a so-called rank-restricted Nystrom approximation results in cluster assignments that satisfy a  approximation ratio in terms of the kernel -means cost function, relative to the guarantee provided by the same algorithm without the use of the Nystrom method. As part of the analysis, this work establishes a novel  relative-error trace norm guarantee for low-rank approximation using the rank-restricted Nystrom approximation. Empirical evaluations on the  million instance MNIST8M dataset demonstrate the scalability and usefulness of kernel -means clustering with Nystrom approximation. This work argues that spectral clustering using Nystrom approximation---a popular and computationally efficient, but theoretically unsound approach to non-linear clustering---should be replaced with the efficient and theoretically sound combination of kernel -means clustering with Nystrom approximation. The superior performance of the latter approach is empirically verified.",
            "keywords": [
                "kernel k-means clustering"
            ],
            "author": [
                "Shusen Wang",
                "Alex Gittens",
                "Michael W. Mahoney"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-517/17-517.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Train and Test Tightness of LP Relaxations in Structured Prediction",
            "abstract": "Structured prediction is used in areas including computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation for the striking observation that approximations based on linear programming (LP) relaxations are often tight (exact) on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that this training tightness generalizes to test data.",
            "keywords": [],
            "author": [
                "Ofer Meshi",
                "Ben London",
                "Adrian Weller",
                "David Sontag"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-535/17-535.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximations of the Restless Bandit Problem",
            "abstract": "The multi-armed restless bandit problem is studied in the case where the pay-off distributions are stationary -mixing. This version of the problem provides a more realistic model for most real-world applications, but cannot be optimally solved in practice, since it is known to be PSPACE-hard. The objective of this paper is to characterize a sub-class of the problem where good approximate solutions can be found using tractable approaches. Specifically, it is shown that under some conditions on the -mixing coefficients, a modified version of UCB can prove effective. The main challenge is that, unlike in the i.i.d. setting, the distributions of the sampled pay-offs may not have the same characteristics as those of the original bandit arms. In particular, the -mixing property does not necessarily carry over. This is overcome by carefully controlling the effect of a sampling policy on the pay-off distributions. Some of the proof techniques developed in this paper can be more generally used in the context of online sampling under dependence. Proposed algorithms are accompanied with corresponding regret analysis.",
            "keywords": [],
            "author": [
                "Steffen Grünewälder",
                "Azadeh Khaleghi"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-547/17-547.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Smooth neighborhood recommender systems",
            "abstract": "Recommender systems predict users' preferences over a large number of items by pooling similar information from other users and/or items in the presence of sparse observations. One major challenge is how to utilize user-item specific covariates and networks describing user-item interactions in a high-dimensional situation, for accurate personalized prediction. In this article, we propose a smooth neighborhood recommender in the framework of the latent factor models. A similarity kernel is utilized to borrow neighborhood information from continuous covariates over a user-item specific network, such as a user's social network, where the grouping information defined by discrete covariates is also integrated through the network. Consequently, user-item specific information is built into the recommender to battle the `cold-start” issue in the absence of observations in collaborative and content-based filtering. Moreover, we utilize a “divide-and-conquer” version of the alternating least squares algorithm to achieve scalable computation, and establish asymptotic results for the proposed method, demonstrating that it achieves superior prediction accuracy. Finally, we illustrate that the proposed method improves substantially over its competitors in simulated examples and real benchmark data--Last.fm music data.",
            "keywords": [
                "Blockwise coordinate decent",
                "Cold-start",
                "Kernel smoothing",
                "Neighborhood",
                "Personalized prediction",
                "Singular value decomposition"
            ],
            "author": [
                "Ben Dai",
                "Junhui Wang",
                "Xiaotong Shen",
                "Annie Qu"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-629/17-629.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Delay and Cooperation in Nonstochastic Bandits",
            "abstract": "We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than  hops to arrive, where  is a delay parameter. We introduce Exp3-Coop, a cooperative version of the Exp3 algorithm and prove that with  actions and  agents the average per-agent regret after  rounds is at most of order , where  is the independence number of the -th power of the communication graph . We then show that for any connected graph, for  the regret bound is , strictly better than the minimax regret  for noncooperating agents. More informed choices of  lead to bounds which are arbitrarily close to the full information minimax regret  when  is dense. When  has sparse components, we show that a variant of Exp3-Coop, allowing agents to choose their parameters according to their centrality in , strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.",
            "keywords": [
                "Multi-armed bandits",
                "distributed learning",
                "cooperative multi-agent systems",
                "regret minimization"
            ],
            "author": [
                "Nicolò Cesa-Bianchi",
                "Claudio Gentile",
                "Yishay Mansour"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-631/17-631.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiplicative local linear hazard estimation and best one-sided cross-validation",
            "abstract": "This paper develops detailed mathematical statistical theory of a new class of cross-validation techniques of local linear kernel hazards and their multiplicative bias corrections. The new class of cross-validation combines principles of local information and recent advances in indirect cross-validation. A few applications of cross-validating multiplicative kernel hazard estimation do exist in the literature. However, detailed mathematical statistical theory and small sample performance are introduced via this paper and further upgraded to our new class of best one-sided cross-validation. Best one-sided cross-validation turns out to have excellent performance in its practical illustrations, in its small sample performance and in its mathematical statistical theoretical performance.",
            "keywords": [],
            "author": [
                "Maria Luz Gámiz",
                "María Dolores  Martínez-Miranda",
                "Jens Perch Nielsen"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-663/17-663.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "spark-crowd: A Spark Package for Learning from Crowdsourced Big Data",
            "abstract": "As the data sets increase in size, the process of manually labeling data becomes unfeasible by small groups of experts. Thus, it is common to rely on crowdsourcing platforms which provide inexpensive, but noisy, labels. Although implementations of algorithms to tackle this problem exist, none of them focus on scalability, limiting the area of application to relatively small data sets. In this paper, we present spark-crowd, an Apache Spark package for learning from crowdsourced data with scalability in mind.",
            "keywords": [
                "crowdsourcing",
                "crowdsourced data",
                "learning from crowds",
                "big data",
                "multiple     noisy labeling"
            ],
            "author": [
                "Enrique G. Rodrigo",
                "Juan A. Aledo",
                "José A. Gámez"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-743/17-743.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Accelerated Alternating Projections for Robust Principal Component Analysis",
            "abstract": "We study robust PCA for the fully observed setting, which is about separating a low rank matrix  and a sparse matrix  from their sum . In this paper, a new algorithm, dubbed accelerated alternating projections, is introduced for robust PCA which significantly improves the computational efficiency of the existing alternating projections proposed in (Netrapalli et al., 2014) when updating the low rank factor. The acceleration is achieved by first projecting a matrix onto some low dimensional subspace before obtaining a new estimate of the low rank matrix via truncated SVD. Exact recovery guarantee has been established which shows linear convergence of the proposed algorithm. Empirical performance evaluations establish the advantage of our algorithm over other state-of-the-art algorithms for robust PCA.",
            "keywords": [
                "Robust PCA",
                "Alternating Projections",
                "Matrix Manifold",
                "Tangent Space"
            ],
            "author": [
                "HanQin Cai",
                "Jian-Feng Cai",
                "Ke Wei"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-022/18-022.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectrum Estimation from a Few Entries",
            "abstract": "Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering spectral properties of the underlying matrix from a sampling of its entries. In this paper, we address the problem of directly recovering the spectrum, which is the set of singular values, and also in sample-efficient approaches for recovering a spectral sum function, which is an aggregate sum of a fixed function applied to each of the singular values. Our approach is to first estimate the Schatten -norms of a matrix for a small set of values of , and then apply Chebyshev approximation when estimating a spectral sum function or apply moment matching in Wasserstein distance when estimating the singular values directly. The main technical challenge is in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures called network motifs in a graph and provide guarantees that match its empirical performance. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods, below the matrix completion threshold, above which matrix completion algorithms recover the underlying low-rank matrix exactly.",
            "keywords": [
                "spectrum estimation",
                "matrix completion"
            ],
            "author": [
                "Ashish Khetan",
                "Sewoong Oh"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-027/18-027.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics",
            "abstract": "Kernel-based methods exhibit well-documented performance in various nonlinear learning tasks. Most of them rely on a preselected kernel, whose prudent choice presumes task-specific prior information. Especially when the latter is not available, multi-kernel learning has gained popularity thanks to its flexibility in choosing kernels from a prescribed kernel dictionary. Leveraging the random feature approximation and its recent orthogonality-promoting variant, the present contribution develops a scalable multi-kernel learning scheme (termed Raker) to obtain the sought nonlinear learning function `on the fly,' first for static environments. To further boost performance in dynamic environments, an adaptive multi-kernel learning scheme (termed AdaRaker) is developed. AdaRaker accounts not only for data-driven learning of kernel combination, but also for the unknown dynamics. Performance is analyzed in terms of both static and dynamic regrets. AdaRaker is uniquely capable of tracking nonlinear learning functions in environments with unknown dynamics, and with with analytic performance guarantees Tests with synthetic and real datasets are carried out to showcase the effectiveness of the novel algorithms.",
            "keywords": [
                "Online learning",
                "reproducing kernel Hilbert space",
                "multi-kernel learning",
                "random features"
            ],
            "author": [
                "Yanning Shen",
                "Tianyi Chen",
                "Georgios B. Giannakis"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-030/18-030.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Determining the Number of Latent Factors in Statistical Multi-Relational Learning",
            "abstract": "Statistical relational learning is primarily concerned with learning and inferring relationships between entities in large-scale knowledge graphs.  Nickel et al. (2011) proposed a RESCAL tensor factorization model for statistical relational learning, which achieves better or at least comparable results on common benchmark data sets when compared to other state-of-the-art methods. Given a positive integer , RESCAL computes an -dimensional latent vector for each entity. The latent factors can be further used for solving relational learning tasks, such as collective classification, collective entity resolution and link-based clustering. The focus of this paper is to determine the number of latent factors in the RESCAL model. Due to the structure of the RESCAL model, its log-likelihood function is not concave. As a result, the corresponding maximum likelihood estimators (MLEs) may not be consistent. Nonetheless, we design a specific pseudometric, prove the consistency of the MLEs under this pseudometric and establish its rate of convergence. Based on these results, we propose a general class of information criteria and prove their model selection consistencies when the number of relations is either bounded or diverges at a proper rate of the number of entities. Simulations and real data examples show that our proposed information criteria have good finite sample properties.",
            "keywords": [],
            "author": [
                "Chengchun Shi",
                "Wenbin Lu",
                "Rui Song"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-037/18-037.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Joint PLDA for Simultaneous Modeling of Two Factors",
            "abstract": "Probabilistic linear discriminant analysis (PLDA) is a method used for biometric problems like speaker or face recognition that models the variability of the samples using two latent variables, one that depends on the class of the sample and another one that is assumed independent across samples and models the within-class variability. In this work, we propose a generalization of PLDA that enables joint modeling of two sample-dependent factors: the class of interest and a nuisance condition. The approach does not change the basic form of PLDA but rather modifies the training procedure to consider the dependency across samples of the latent variable that models within-class variability. While the identity of the nuisance condition is needed during training, it is not needed during testing since we propose a scoring procedure that marginalizes over the corresponding latent variable. We show results on a multilingual speaker-verification task, where the language spoken is considered a nuisance condition. The proposed joint PLDA approach leads to significant performance gains in this task for two different data sets, in particular when the training data contains mostly or only monolingual speakers.",
            "keywords": [
                "Probabilistic linear discriminant analysis",
                "speaker recognition",
                "factor analysis",
                "language variability"
            ],
            "author": [
                "Luciana Ferrer",
                "Mitchell McLaren"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-134/18-134.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations",
            "abstract": "The success of deep convolutional architectures is often attributed in part to their ability to learn multiscale and invariant representations of natural signals. However, a precise study of these properties and how they affect learning guarantees is still missing. In this paper, we consider deep convolutional representations of signals; we study their invariance to translations and to more general groups of transformations, their stability to the action of diffeomorphisms, and their ability to preserve signal information. This analysis is carried by introducing a multilayer kernel based on convolutional kernel networks and by studying the geometry induced by the kernel mapping. We then characterize the corresponding reproducing kernel Hilbert space (RKHS), showing that it contains a large class of convolutional neural networks with homogeneous activation functions. This analysis allows us to separate data representation from learning, and to provide a canonical measure of model complexity, the RKHS norm, which controls both stability and generalization of any learned model. In addition to models in the constructed RKHS, our stability analysis also applies to convolutional networks with generic activations such as rectified linear units, and we discuss its relationship with recent generalization bounds based on spectral norms.",
            "keywords": [
                "invariant representations",
                "deep learning",
                "stability"
            ],
            "author": [
                "Alberto Bietti",
                "Julien Mairal"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-190/18-190.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "TensorLy: Tensor Learning in Python",
            "abstract": "Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of traditional machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed TensorLy, a Python library that provides a high-level API for tensor methods and deep tensorized neural networks. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and to seamlessly integrate with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with several libraries such as NumPy or PyTorch to name but a few. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly",
            "keywords": [],
            "author": [
                "Jean Kossaifi",
                "Yannis Panagakis",
                "Anima Anandkumar",
                "Maja Pantic"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-277/18-277.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Monotone Learning with Rectified Wire Networks",
            "abstract": "We introduce a new neural network model, together with a tractable and monotone online learning algorithm. Our model describes feed-forward networks for classification, with one output node for each class. The only nonlinear operation is rectification using a ReLU function with a bias. However, there is a rectifier on every edge rather than at the nodes of the network. There are also weights, but these are positive, static, and associated with the nodes. Our rectified wire networks are able to represent arbitrary Boolean functions. Only the bias parameters, on the edges of the network, are learned. Another departure in our approach, from standard neural networks, is that the loss function is replaced by a constraint. This constraint is simply that the value of the output node associated with the correct class should be zero. Our model has the property that the exact norm-minimizing parameter update, required to correctly classify a training item, is the solution to a quadratic program that can be computed with a few passes through the network. We demonstrate a training algorithm using this update, called sequential deactivation (SDA), on MNIST and some synthetic datasets. Upon adopting a natural choice for the nodal weights, SDA has no hyperparameters other than those describing the network structure. Our experiments explore behavior with respect to network size and depth in a family of sparse expander networks.",
            "keywords": [
                "Neural Networks",
                "Online Training Algorithms",
                "Rectified Linear Unit    Some of the simplest modes of machine learning are monotone in character"
            ],
            "author": [
                "Veit Elser",
                "Dan Schmidt",
                "Jonathan Yedidia"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-281/18-281.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pyro: Deep Universal Probabilistic Programming",
            "abstract": "Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large data sets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.",
            "keywords": [
                "Probabilistic programming",
                "graphical models",
                "approximate Bayesian inference",
                "generative models",
                "deep learningc 2019 Eli Bingham"
            ],
            "author": [
                "Eli Bingham",
                "Jonathan P. Chen",
                "Martin Jankowiak",
                "Fritz Obermeyer",
                "Neeraj Pradhan",
                "Theofanis Karaletsos",
                "Rohit Singh",
                "Paul Szerlip",
                "Paul Horsfall",
                "Noah D. Goodman"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-403/18-403.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Iterated Learning in Dynamic Social Networks",
            "abstract": "A classic finding by (Kalish et al., 2007) shows that no language can be learned iteratively by rational agents in a self-sustained manner. In other words, if  teaches a foreign language to , who then teaches what she learned to , and so on, the language will quickly get lost and agents will wind up teaching their own common native language. If so, how can linguistic novelty ever be sustained? We address this apparent paradox by considering the case of iterated learning in a social network: we show that by varying the lengths of the learning sessions over time or by keeping the networks dynamic, it is possible for iterated learning to endure forever with arbitrarily small loss.",
            "keywords": [],
            "author": [
                "Bernard Chazelle",
                "Chu Wang"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-539/18-539.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exact Clustering of Weighted Graphs via Semidefinite Programming",
            "abstract": "As a model problem for clustering, we consider the densest -disjoint-clique problem of partitioning a weighted complete graph into  disjoint subgraphs such that the sum of the densities of these subgraphs is maximized. We establish that such subgraphs can be recovered from the solution of a particular semidefinite relaxation with high probability if the input graph is sampled from a distribution of clusterable graphs. Specifically, the semidefinite relaxation is exact if the graph consists of \\(k\\) large disjoint subgraphs, corresponding to clusters, with weight concentrated within these subgraphs, plus a moderate number of nodes not belonging to any cluster. Further, we establish that if noise is weakly obscuring these clusters, i.e, the between-cluster edges are assigned very small weights, then we can recover significantly smaller clusters. For example, we show that in approximately sparse graphs, where the between-cluster weights tend to zero as the size  of the graph tends to infinity, we can recover clusters of size polylogarithmic in  under certain conditions on the distribution of edge weights. Empirical evidence from numerical simulations is also provided to support these theoretical phase transitions to perfect recovery of the cluster structure.",
            "keywords": [
                "clustering",
                "densest subgraph",
                "stochastic block models",
                "semidefinite program-     ming"
            ],
            "author": [
                "Aleksis Pirinen",
                "Brendan Ames"
            ],
            "ref": "http://jmlr.org/papers/volume20/16-128/16-128.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernels for Sequentially Ordered Data",
            "abstract": "We present a novel framework for learning with sequential data of any kind, such as multivariate time series, strings, or sequences of graphs. The main result is a ”sequentialization” that transforms any kernel on a given domain into a kernel for sequences in that domain. This procedure preserves properties such as positive definiteness, the associated kernel feature map is an ordered variant of sample (cross-)moments, and this sequentialized kernel is consistent in the sense that it converges to a kernel for paths if sequences converge to paths (by discretization). Further, classical kernels for sequences arise as special cases of this method. We use dynamic programming and low-rank techniques for tensors to provide efficient algorithms to compute this sequentialized kernel.",
            "keywords": [
                "Sequential data",
                "kernels",
                "signature",
                "ordered moments"
            ],
            "author": [
                "Franz J. Kiraly",
                "Harald Oberhauser"
            ],
            "ref": "http://jmlr.org/papers/volume20/16-314/16-314.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "NetSDM: Semantic Data Mining with Network Analysis",
            "abstract": "Semantic data mining (SDM) is a form of relational data mining that uses annotated data together with complex semantic background knowledge to learn rules that can be easily interpreted. The drawback of SDM is a high computational complexity of existing SDM algorithms, resulting in long run times even when applied to relatively small data sets. This paper proposes an effective SDM approach, named NetSDM, which first transforms the available semantic background knowledge into a network format, followed by network analysis based node ranking and pruning to significantly reduce the size of the original background knowledge. The experimental evaluation of the NetSDM methodology on acute lymphoblastic leukemia and breast cancer data demonstrates that NetSDM achieves radical time efficiency improvements and that learned rules are comparable or better than the rules obtained by the original SDM algorithms.",
            "keywords": [
                "data mining",
                "semantic data mining",
                "ontologies",
                "subgroup discovery"
            ],
            "author": [
                "Jan Kralj",
                "Marko Robnik-Sikonja",
                "Nada Lavrac"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-066/17-066.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Relationship Between Agnostic Selective Classification, Active Learning and the Disagreement Coefficient",
            "abstract": "A selective classifier  comprises a classification function  and a binary selection function , which determines if the classifier abstains from prediction, or uses  to predict. The classifier is called pointwise-competitive if it classifies each point identically to the best classifier in hindsight (from the same class), whenever it does not abstain. The quality of such a classifier is quantified by its rejection mass, defined to be the probability mass of the points it rejects. A “fast” rejection rate is achieved if the rejection mass is bounded from above by  where  is the number of labeled examples used to train the classifier (and  hides logarithmic factors). Pointwise-competitive selective (PCS) classifiers are intimately related to disagreement-based active learning and it is known that in the realizable case, a fast rejection rate of a known PCS algorithm (called Consistent Selective Strategy) is equivalent to an exponential speedup of the well-known CAL active algorithm. We focus on the agnostic setting, for which there is a known algorithm called LESS that learns a PCS classifier and achieves a fast rejection rate (depending on Hannekeâs disagreement coefficient) under strong assumptions. We present an improved PCS learning algorithm called ILESS for which we show a fast rate (depending on Hanneke's disagreement coefficient) without any assumptions. Our rejection bound smoothly interpolates the realizable and agnostic settings. The main result of this paper is an equivalence between the following three entities: (i) the existence of a fast rejection rate for any PCS learning algorithm (such as ILESS); (ii) a poly-logarithmic bound for Hanneke's disagreement coefficient; and (iii) an exponential speedup for a new disagreement-based active learner called {\\ActiveiLESS}.",
            "keywords": [
                "active learning",
                "selective prediction",
                "disagreement coefficient",
                "selective sam-     pling",
                "selective classification",
                "reject option",
                "pointwise-competitive",
                "selective classification",
                "statistical learning theory",
                "PAC learning",
                "sample complexity"
            ],
            "author": [
                "Roei Gelbhart",
                "Ran El-Yaniv"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-147/17-147.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Matched Bipartite Block Model with Covariates",
            "abstract": "Community detection or clustering is a fundamental task in the analysis of network data. Many real networks have a bipartite structure which makes community detection challenging. In this paper, we consider a model which allows for matched communities in the bipartite setting, in addition to node covariates with information about the matching. We derive a simple fast algorithm for fitting the model based on variational inference ideas and show its effectiveness on both simulated and real data. A variation of the model to allow for degree-correction is also considered, in addition to a novel approach to fitting such degree-corrected models.",
            "keywords": [
                "bipartite networks",
                "community detection",
                "stochastic block model",
                "bipartite     matching"
            ],
            "author": [
                "Zahra S. Razaee",
                "Arash A. Amini",
                "Jingyi Jessica Li"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-153/17-153.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Policies for Observing Time Series and Related Restless Bandit Problems",
            "abstract": "The trade-off between the cost of acquiring and processing data, and uncertainty due to a lack of data is fundamental in machine learning. A basic instance of this trade-off is the problem of deciding when to make noisy and costly observations of a discrete-time Gaussian random walk, so as to minimise the posterior variance plus observation costs. We present the first proof that a simple policy, which observes when the posterior variance exceeds a threshold, is optimal for this problem. The proof generalises to a wide range of cost functions other than the posterior variance. It is based on a new verification theorem by Nino-Mora that guarantees threshold structure for Markov decision processes, and on the relation between binary sequences known as Christoffel words and the dynamics of discontinuous nonlinear maps, which frequently arise in physics, control and biology. This result implies that optimal policies for linear-quadratic-Gaussian control with costly observations have a threshold structure. It also implies that the restless bandit problem of observing multiple such time series, has a well-defined Whittle index policy. We discuss computation of that index, give closed-form formulae for it, and compare the performance of the associated index policy with heuristic policies.",
            "keywords": [
                "restless bandits",
                "Whittle index",
                "Christoffel words",
                "Sturmian words",
                "Kalman     filter"
            ],
            "author": [
                "Christopher R. Dance",
                "Tomi Silander"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-185/17-185.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A New Approach to Laplacian Solvers and Flow Problems",
            "abstract": "This paper investigates the behavior of the Min-Sum message passing scheme to solve systems of linear equations in the Laplacian matrices of graphs and to compute electric flows. Voltage and flow problems involve the minimization of quadratic functions and are fundamental primitives that arise in several domains. Algorithms that have been proposed are typically centralized and involve multiple graph-theoretic constructions or sampling mechanisms that make them difficult to implement and analyze. On the other hand, message passing routines are distributed, simple, and easy to implement. In this paper we establish a framework to analyze Min-Sum to solve voltage and flow problems. We characterize the error committed by the algorithm on general weighted graphs in terms of hitting times of random walks defined on the computation trees that support the operations of the algorithms with time. For -regular graphs with equal weights, we show that the convergence of the algorithms is controlled by the total variation distance between the distributions of non-backtracking random walks defined on the original graph that start from neighboring nodes. The framework that we introduce extends the analysis of Min-Sum to settings where the contraction arguments previously considered in the literature (based on the assumption of walk summability or scaled diagonal dominance) can not be used, possibly in the presence of constraints.",
            "keywords": [
                "Min-Sum",
                "message passing",
                "decentralized algorithms",
                "Laplacian solver"
            ],
            "author": [
                "Patrick Rebeschini",
                "Sekhar Tatikonda"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-286/17-286.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Well-Tempered Landscape for Non-convex Robust Subspace Recovery",
            "abstract": "We present a mathematical analysis of a non-convex energy landscape for robust subspace recovery. We prove that an underlying subspace is the only stationary point and local minimizer in a specified neighborhood under a deterministic condition on a dataset. If the deterministic condition is satisfied, we further show that a geodesic gradient descent method over the Grassmannian manifold can exactly recover the underlying subspace when the method is properly initialized. Proper initialization by principal component analysis is guaranteed with a simple deterministic condition. Under slightly stronger assumptions, the gradient descent method with a piecewise constant step-size scheme achieves linear convergence. The practicality of the deterministic condition is demonstrated on some statistical models of data, and the method achieves almost state-of-the-art recovery guarantees on the Haystack Model for different regimes of sample size and ambient dimension. In particular, when the ambient dimension is fixed and the sample size is large enough, we show that our gradient method can exactly recover the underlying subspace for any fixed fraction of outliers (less than 1).",
            "keywords": [
                "robust subspace recovery",
                "non-convex optimization",
                "dimension reduction"
            ],
            "author": [
                "Tyler Maunu",
                "Teng Zhang",
                "Gilad Lerman"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-324/17-324.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximation Hardness for A Class of Sparse Optimization Problems",
            "abstract": "In this paper, we consider three typical optimization problems with a convex loss function and a nonconvex sparse penalty or constraint. For the sparse penalized problem, we prove that finding an -optimal solution to an  problem is strongly NP-hard for any  such that . For two constrained versions of the sparse optimization problem, we show that it is intractable to approximately compute a solution path associated with increasing values of some tuning parameter. The hardness results apply to a broad class of loss functions and sparse penalties. They suggest that one cannot even approximately solve these three problems in polynomial time, unless P  NP.",
            "keywords": [
                "nonconvex optimization",
                "computational complexity",
                "variable selection",
                "NP-     hardness"
            ],
            "author": [
                "Yichen Chen",
                "Yinyu Ye",
                "Mengdi Wang"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-373/17-373.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bootstrap Method for Error Estimation in Randomized Matrix Multiplication",
            "abstract": "In recent years, randomized methods for numerical linear algebra have received growing interest as a general approach to large-scale problems. Typically, the essential ingredient of these methods is some form of randomized dimension reduction, which accelerates computations, but also creates random approximation error. In this way, the dimension reduction step encodes a tradeoff between cost and accuracy. However, the exact numerical relationship between cost and accuracy is typically unknown, and consequently, it may be difficult for the user to precisely know (1) how accurate a given solution is, or (2) how much computation is needed to achieve a given level of accuracy. In the current paper, we study randomized matrix multiplication (sketching) as a prototype setting for addressing these general problems. As a solution, we develop a bootstrap method for directly estimating the accuracy as a function of the reduced dimension (as opposed to deriving worst-case bounds on the accuracy in terms of the reduced dimension). From a computational standpoint, the proposed method does not substantially increase the cost of standard sketching methods, and this is made possible by an “extrapolation” technique. In addition, we provide both theoretical and empirical results to demonstrate the effectiveness of the proposed method.",
            "keywords": [
                "matrix sketching",
                "randomized matrix multiplication"
            ],
            "author": [
                "Miles E. Lopes",
                "Shusen Wang",
                "Michael W. Mahoney"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-451/17-451.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Decontamination of Mutual Contamination Models",
            "abstract": "Many machine learning problems can be characterized by \\emph{mutual contamination models}. In these problems, one observes several random samples from different convex combinations of a set of unknown base distributions and the goal is to infer these base distributions. This paper considers the general setting where the base distributions are defined on arbitrary probability spaces. We examine three popular machine learning problems that arise in this general setting: multiclass classification with label noise, demixing of mixed membership models, and classification with partial labels. In each case, we give sufficient conditions for identifiability and present algorithms for the infinite and finite sample settings, with associated performance guarantees.",
            "keywords": [
                "multiclass classification with label noise",
                "classification with partial labels",
                "mixed membership models",
                "topic modeling"
            ],
            "author": [
                "Julian Katz-Samuels",
                "Gilles Blanchard",
                "Clayton Scott"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-576/17-576.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Utilizing Second Order Information in Minibatch Stochastic Variance Reduced Proximal Iterations",
            "abstract": "We present a novel minibatch stochastic optimization method for empirical risk minimization of linear predictors. The method efficiently leverages both sub-sampled first-order and higher-order information, by incorporating variance-reduction and acceleration techniques. We prove improved iteration complexity over state-of-the-art methods under suitable conditions. In particular, the approach enjoys global fast convergence for quadratic convex objectives and local fast convergence for general convex objectives.  Experiments are provided to demonstrate the empirical advantage of the proposed method over existing approaches in the literature.",
            "keywords": [],
            "author": [
                "Jialei Wang",
                "Tong Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-594/17-594.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DSCOVR: Randomized Primal-Dual Block Coordinate Algorithms for Asynchronous Distributed Optimization",
            "abstract": "Machine learning with big data often involves large optimization models. For distributed optimization over a cluster of machines, frequent communication and synchronization of all model parameters (optimization variables) can be very costly. A promising solution is to use parameter servers to store different subsets of the model parameters, and update them asynchronously at different machines using local datasets. In this paper, we focus on distributed optimization of large linear models with convex loss functions, and propose a family of randomized primal-dual block coordinate algorithms that are especially suitable for asynchronous distributed implementation with parameter servers. In particular, we work with the saddle-point formulation of such problems which allows simultaneous data and model partitioning, and exploit its structure by doubly stochastic coordinate optimization with variance reduction (DSCOVR). Compared with other first-order distributed algorithms, we show that DSCOVR may require less amount of overall computation and communication, and less or no synchronization. We discuss the implementation details of the DSCOVR algorithms, and present numerical experiments on an industrial distributed computing system.",
            "keywords": [],
            "author": [
                "Lin Xiao",
                "Adams Wei Yu",
                "Qihang Lin",
                "Weizhu Chen"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-608/17-608.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Frequent Directions with Application in Online Learning",
            "abstract": "The frequent directions (FD) technique is a deterministic approach for online sketching that has many applications in machine learning. The conventional FD is a heuristic procedure that often outputs rank deficient matrices. To overcome the rank deficiency problem, we propose a new sketching strategy called robust frequent directions (RFD) by introducing a regularization term. RFD can be derived from an optimization problem. It updates the sketch matrix and the regularization term adaptively and jointly. RFD reduces the approximation error of FD without increasing the computational cost. We also apply RFD to online learning and propose an effective hyperparameter-free online Newton algorithm. We derive a regret bound for our online Newton algorithm based on RFD, which guarantees the robustness of the algorithm. The experimental studies demonstrate that the proposed method outperforms state-of-the-art second order online learning algorithms.",
            "keywords": [
                "Matrix approximation",
                "sketching",
                "frequent directions",
                "online convex optimization"
            ],
            "author": [
                "Luo Luo",
                "Cheng Chen",
                "Zhihua Zhang",
                "Wu-Jun Li",
                "Tong Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-773/17-773.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Boosted Kernel Ridge Regression: Optimal Learning Rates and Early Stopping",
            "abstract": "In this paper, we introduce a learning algorithm, boosted kernel ridge regression (BKRR), that combines -Boosting with the kernel ridge regression (KRR). We analyze the learning performance of this algorithm in the framework of learning theory. We show that BKRR provides a new bias-variance trade-off via tuning the number of boosting iterations, which is different from KRR via adjusting the regularization parameter. A (semi-)exponential bias-variance trade-off is derived for BKRR, exhibiting a stable relationship between the generalization error and the number of iterations. Furthermore, an adaptive stopping rule is proposed, with which BKRR achieves the optimal learning rate without saturation.",
            "keywords": [
                "learning theory",
                "kernel ridge regression",
                "boosting"
            ],
            "author": [
                "Shao-Bo Lin",
                "Yunwen Lei",
                "Ding-Xuan Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-063/18-063.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analysis of spectral clustering algorithms for community detection: the general bipartite setting",
            "abstract": "We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a kmeans type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly.",
            "keywords": [],
            "author": [
                "Zhixin Zhou",
                "Arash A.Amini"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-170/18-170.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient augmentation and relaxation learning for individualized treatment rules using observational data",
            "abstract": "Individualized treatment rules aim to identify if, when, which, and to whom treatment should be applied. A globally aging population, rising healthcare costs, and increased access to patient-level data have created an urgent need for high-quality estimators of individualized treatment rules that can be applied to observational data. A recent and promising line of research for estimating individualized treatment rules recasts the problem of estimating an optimal treatment rule as a weighted classification problem. We consider a class of estimators for optimal treatment rules that are analogous to convex large-margin classifiers. The proposed class applies to observational data and is doubly-robust in the sense that correct specification of either a propensity or outcome model leads to consistent estimation of the optimal individualized treatment rule. Using techniques from semiparametric efficiency theory, we derive rates of convergence for the proposed estimators and use these rates to characterize the bias-variance trade-off for estimating individualized treatment rules with classification-based methods. Simulation experiments informed by these results demonstrate that it is possible to construct new estimators within the proposed framework that significantly outperform existing ones. We illustrate the proposed methods using data from a labor training program and a study of inflammatory bowel syndrome.",
            "keywords": [],
            "author": [
                "Ying-Qi Zhao",
                "Eric B. Laber",
                "Yang Ning",
                "Sumona Saha",
                "Bruce E. Sands"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-191/18-191.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "No-Regret Bayesian Optimization with Unknown Hyperparameters",
            "abstract": "Bayesian optimization (BO) based on Gaussian process models is a powerful paradigm to optimize black-box functions that are expensive to evaluate. While several BO algorithms provably converge to the global optimum of the unknown function, they assume that the hyperparameters of the kernel are known in advance. This is not the case in practice and misspecification often causes these algorithms to converge to poor local optima. In this paper, we present the first BO algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters. During optimization we slowly adapt the hyperparameters of stationary kernels and thereby expand the associated function class over time, so that the BO algorithm considers more complex function candidates. Based on the theoretical insights, we propose several practical algorithms that achieve the empirical sample efficiency of BO with online hyperparameter estimation, but retain theoretical convergence guarantees. We evaluate our method on several benchmark problems.",
            "keywords": [
                "Bayesian optimization",
                "Unknown hyperparameters"
            ],
            "author": [
                "Felix Berkenkamp",
                "Angela P. Schoellig",
                "Andreas Krause"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-213/18-213.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Combination of Probabilistic Classifiers using Multivariate Normal Mixtures",
            "abstract": "Ensemble methods are a powerful tool, often outperforming individual prediction models. Existing Bayesian ensembles either do not model the correlations between sources, or they are only capable of combining non-probabilistic predictions. We propose a new model, which overcomes these disadvantages. Transforming the probabilistic predictions with the inverse additive logistic transformation allows us to model the correlations with multivariate normal mixtures. We derive an efficient Gibbs sampler for the proposed model and implement a regularization method to make it more robust. We compare our method to related work and the classical linear opinion pool. Empirical evaluation on several toy and real-world data sets, including a case study on air-pollution forecasting, shows that the method outperforms other methods, while being robust and easy to use.",
            "keywords": [
                "correlated classifiers",
                "ensemble",
                "probabilistic models",
                "additive logistic trans-     formation"
            ],
            "author": [
                "Gregor Pirš",
                "Erik Štrumbelj"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-241/18-241.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Thompson Sampling Guided Stochastic Searching on the Line for Deceptive Environments with Applications to Root-Finding Problems",
            "abstract": "The multi-armed bandit problem forms the foundation for solving a wide range of online stochastic optimization problems through a simple, yet effective mechanism. One simply casts the problem as a gambler who repeatedly pulls one out of N slot machine arms, eliciting random rewards. Learning of reward probabilities is then combined with reward maximization, by carefully balancing reward exploration against reward exploitation. In this paper, we address a particularly intriguing variant of the multi-armed bandit problem, referred to as the Stochastic Point Location (SPL) problem. The gambler is here only told whether the optimal arm (point) lies to the “left” or to the “right” of the arm pulled, with the feedback being erroneous with probability . This formulation thus targets optimization in continuous action spaces with both informative and deceptive feedback. To tackle this class of problems, we formulate a compact and scalable Bayesian representation of the solution space that simultaneously captures both the location of the optimal arm as well as the probability of receiving correct feedback. We further introduce the accompanying Thompson Sampling guided Stochastic Point Location (TS-SPL) scheme for balancing exploration against exploitation. By learning , TS-SPL also supports deceptive environments that are lying about the direction of the optimal arm. This, in turn, allows us to address the fundamental Stochastic Root Finding (SRF) problem. Empirical results demonstrate that our scheme deals with both deceptive and informative environments, significantly outperforming competing algorithms both for SRF and SPL.",
            "keywords": [
                "thompson sampling",
                "searching on the line",
                "probabilistic bisection search",
                "deceptive environment"
            ],
            "author": [
                "Sondre Glimsdal",
                "Ole-Christoffer Granmo"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-263/18-263.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tunability: Importance of Hyperparameters of Machine Learning Algorithms",
            "abstract": "Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, define data-based defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to choose adequate hyperparameter spaces for tuning.",
            "keywords": [
                "machine learning",
                "supervised learning",
                "classification",
                "hyperparameters",
                "tun-     ing"
            ],
            "author": [
                "Philipp Probst",
                "Anne-Laure Boulesteix",
                "Bernd Bischl"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-444/18-444.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Deep Reinforcement Learning for Swarm Systems ",
            "abstract": "Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, the observation vector for decentralized decision making is represented by a concatenation of the (local) information an agent gathers about other agents. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions, where we treat the agents as samples and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and neural networks trained end-to-end. We evaluate the representation on two well-known problems from the swarm literature in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents, facilitating the development of complex collective strategies.",
            "keywords": [
                "deep reinforcement learning",
                "swarm systems",
                "mean embeddings",
                "neural net-     works"
            ],
            "author": [
                "Maximilian Hüttenrauch",
                "Adrian Šošić",
                "Gerhard Neumann"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-476/18-476.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Neural Architecture Search: A Survey",
            "abstract": "Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated \\emph{neural architecture search} methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.",
            "keywords": [
                "Neural Architecture Search",
                "AutoML",
                "AutoDL",
                "Search Space Design",
                "Search     Strategy"
            ],
            "author": [
                "Thomas Elsken",
                "Jan Hendrik Metzen",
                "Frank Hutter"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-598/18-598.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices",
            "abstract": "Given a large matrix , we consider the problem of computing a sketch matrix  which is significantly smaller than but still well approximates . We consider the problems in the streaming model, where the algorithm can only make one pass over the input with limited working space, and we are interested in minimizing the covariance error  The popular Frequent Directions algorithm of \\cite{liberty2013simple} and its variants achieve optimal space-error tradeoffs. However, whether the running time can be improved remains an unanswered question. In this paper, we almost settle the question by proving that the time complexity of this problem is equivalent to that of matrix multiplication up to lower order terms. Specifically, we provide new space-optimal algorithms with faster running times and also show that the running times of our algorithms can be improved if and only if the state-of-the-art running time of matrix multiplication can be improved significantly.",
            "keywords": [
                "Matrix Sketching",
                "Frequent Directions"
            ],
            "author": [
                "Zengfeng Huang"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-875/18-875.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Common-directions Method for Regularized Empirical Risk Minimization",
            "abstract": "State-of-the-art first- and second-order optimization methods are able to achieve either fast global linear convergence rates or quadratic convergence, but not both of them. In this work, we propose an interpolation between first- and second-order methods for regularized empirical risk minimization that exploits the problem structure to efficiently combine multiple update directions. Our method attains both optimal global linear convergence rate for first-order methods, and local quadratic convergence. Experimental results show that our method outperforms state-of-the-art first- and second-order optimization methods in terms of the number of data accesses, while is competitive in training time.",
            "keywords": [],
            "author": [
                "Po-Wei Wang",
                "Ching-pei Lee",
                "Chih-Jen Lin"
            ],
            "ref": "http://jmlr.org/papers/volume20/16-309/16-309.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Approximation Methods for Speech Recognition",
            "abstract": "We study the performance of kernel methods on the acoustic modeling task for automatic speech recognition, and compare their performance to deep neural networks (DNNs). To scale the kernel methods to large data sets, we use the random Fourier feature method of Rahimi and Recht (2007). We propose two novel techniques for improving the performance of kernel acoustic models. First, we propose a simple but effective feature selection method which reduces the number of random features required to attain a fixed level of performance. Second, we present a number of metrics which correlate strongly with speech recognition performance when computed on the heldout set; we attain improved performance by using these metrics to decide when to stop training. Additionally, we show that the linear bottleneck method of Sainath et al. (2013a) improves the performance of our kernel models significantly, in addition to speeding up training and making the models more compact. Leveraging these three methods, the kernel methods attain token error rates between  better and  worse than fully-connected DNNs across four speech recognition data sets, including the TIMIT and Broadcast News benchmark tasks.",
            "keywords": [
                "kernel methods",
                "deep neural networks",
                "acoustic modeling",
                "automatic speech     recognition",
                "feature selection",
                "logistic regressionc 2019 Avner May",
                "Alireza Bagheri Garakani",
                "Zhiyun Lu",
                "Dong Guo",
                "Kuan Liu"
            ],
            "author": [
                "Avner May",
                "Alireza Bagheri Garakani",
                "Zhiyun Lu",
                "Dong Guo",
                "Kuan Liu",
                "Aurélien Bellet",
                "Linxi Fan",
                "Michael Collins",
                "Daniel Hsu",
                "Brian Kingsbury",
                "Michael Picheny",
                "Fei Sha"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-026/17-026.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Estimation of Derivatives Using Locally Weighted Least Absolute Deviation Regression",
            "abstract": "In nonparametric regression, the derivative estimation has attracted much attention in recent years due to its wide applications. In this paper, we propose a new method for the derivative estimation using the locally weighted least absolute deviation regression. Different from the local polynomial regression, the proposed method does not require a finite variance for the error term and so is robust to the presence of heavy-tailed errors. Meanwhile, it does not require a zero median or a positive density at zero for the error term in comparison with the local median regression. We further show that the proposed estimator with random difference is asymptotically equivalent to the (infinitely) composite quantile regression estimator. In other words, running one regression is equivalent to combining infinitely many quantile regressions. In addition, the proposed method is also extended to estimate the derivatives at the boundaries and to estimate higher-order derivatives. For the equidistant design, we derive theoretical results for the proposed estimators, including the asymptotic bias and variance, consistency, and asymptotic normality. Finally, we conduct simulation studies to demonstrate that the proposed method has better performance than the existing methods in the presence of outliers and heavy-tailed errors, and analyze the Chinese house price data for the past ten years to illustrate the usefulness of the proposed method.",
            "keywords": [
                "composite quantile regression",
                "differenced method",
                "LowLAD",
                "LowLSR",
                "out-     lier and heavy-tailed error",
                "robust nonparametric derivative estimationc 2019 WenWu Wang",
                "Ping Yu",
                "Lu Lin"
            ],
            "author": [
                "WenWu Wang",
                "Ping Yu",
                "Lu Lin",
                "Tiejun Tong"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-340/17-340.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Sup-norm Perturbation of HOSVD and Low Rank Tensor Denoising",
            "abstract": "The higher order singular value decomposition (HOSVD) of tensors is a generalization of matrix SVD. The perturbation analysis of HOSVD under random noise is more delicate than its matrix counterpart. Recently, polynomial time algorithms have been proposed where statistically optimal estimates of the singular subspaces and the low rank tensors are attainable in the Euclidean norm. In this article, we analyze the sup-norm perturbation bounds of HOSVD and introduce estimators of the singular subspaces with sharp deviation bounds in the sup-norm. We also investigate a low rank tensor denoising estimator and demonstrate its fast convergence rate with respect to the entry-wise errors. The sup-norm perturbation bounds reveal unconventional phase transitions for statistical learning applications such as the exact clustering in high dimensional Gaussian mixture model and the exact support recovery in sub-tensor localizations. In addition, the bounds established for HOSVD also elaborate the one-sided sup-norm perturbation bounds for the singular subspaces of unbalanced (or fat) matrices.",
            "keywords": [
                "HOSVD",
                "Entry-wise perturbation",
                "Gaussian noise"
            ],
            "author": [
                "Dong Xia",
                "Fan Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-397/17-397.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-scale Online Learning: Theory and Applications to Online Auctions and Pricing",
            "abstract": "We consider revenue maximization in online auction/pricing problems. A seller sells an identical item in each period to a new buyer, or a new set of buyers. For the online pricing problem, both when the arriving buyer bids or only responds to the posted price, we design algorithms whose regret bounds scale with the best fixed price in-hindsight, rather than the range of the values. Under the bidding model, we further show our algorithms achieve a revenue convergence rate that matches the offline sample complexity of the single-item single-buyer auction. We also show regret bounds that are scale free, and match the offline sample complexity, when comparing to a benchmark that requires a lower bound on the market share. We further expand our results beyond pricing to multi-buyer auctions, and obtain online learning algorithms for auctions, with convergence rates matching the known sample complexity upper bound of online single-item multi-buyer auctions. These results are obtained by generalizing the classical learning from experts and multi-armed bandit problems to their multi-scale versions. In this version, the reward of each action is in a different range, and the regret with respect to a given action scales with its own range, rather than the maximum range. We obtain almost optimal multi-scale regret bounds by introducing a new Online Mirror Descent (OMD) algorithm whose mirror map is the multi-scale version of the negative entropy function. We further generalize to the bandit setting by introducing the stochastic variant of this OMD algorithm.",
            "keywords": [
                "online learning",
                "multi-scale learning",
                "auction theory",
                "bandit information"
            ],
            "author": [
                "Sébastien Bubeck",
                "Nikhil R. Devanur",
                "Zhiyi Huang",
                "Rad Niazadeh"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-498/17-498.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks",
            "abstract": "We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting  be the number of weights and  be the number of layers, we prove that the VC-dimension is , and provide examples with VC-dimension . This improves both the previously known upper bounds and lower bounds. In terms of the number  of non-linear units, we prove a tight bound  on the VC-dimension. All of these bounds generalize to arbitrary piecewise linear activation functions, and also hold for the pseudodimensions of these function classes. Combined with previous results, this gives an intriguing range of dependencies of the VC-dimension on depth for networks with different non-linearities: there is no dependence for piecewise-constant, linear dependence for piecewise-linear, and no more than quadratic dependence for general piecewise-polynomial.",
            "keywords": [
                "VC-dimension",
                "pseudodimension",
                "neural networks",
                "ReLU activation function"
            ],
            "author": [
                "Peter L. Bartlett",
                "Nick Harvey",
                "Christopher Liaw",
                "Abbas Mehrabian"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-612/17-612.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Representer Theorem for Deep Kernel Learning",
            "abstract": "In this paper we provide a finite-sample and an infinite-sample representer theorem for the concatenation of (linear combinations of) kernel functions of reproducing kernel Hilbert spaces. These results serve as mathematical foundation for the analysis of machine learning algorithms based on compositions of functions. As a direct consequence in the finite-sample case, the corresponding infinite-dimensional minimization problems can be recast into (nonlinear) finite-dimensional minimization problems, which can be tackled with nonlinear optimization algorithms. Moreover, we show how concatenated machine learning problems can be reformulated as neural networks and how our representer theorem applies to a broad class of state-of-the-art deep learning methods.",
            "keywords": [
                "deep kernel learning",
                "representer theorem",
                "artificial neural networks",
                "multi-     layer kernel"
            ],
            "author": [
                "Bastian Bohn",
                "Michael Griebel",
                "Christian Rieger"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-621/17-621.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Learning for Cost-Sensitive Classification",
            "abstract": "We design an active learning algorithm for cost-sensitive multiclass classification: problems where different errors have different costs. Our algorithm, COAL, makes predictions by regressing to each label's cost and predicting the smallest. On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort. We empirically compare COAL to passive learning and several active learning baselines, showing significant improvements in labeling effort and test cost on real-world datasets.",
            "keywords": [
                "Active Learning",
                "Cost-sensitive Learning",
                "Structured Prediction",
                "Statistical     Learning Theory"
            ],
            "author": [
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "Tzu-Kuo Huang",
                "Hal Daumé III",
                "John Langford"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-681/17-681.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Proximal Distance Algorithms: Theory and Practice",
            "abstract": "Proximal distance algorithms combine the classical penalty method of constrained minimization with distance majorization. If  is the loss function, and  is the constraint set in a constrained minimization problem, then the proximal distance principle mandates minimizing the penalized loss  and following the solution  to its limit as  tends to . At each iteration the squared Euclidean distance  is majorized by the spherical quadratic , where  denotes the projection of the current iterate  onto . The minimum of the surrogate function  is given by the proximal map . The next iterate  automatically decreases the original penalized loss for fixed . Since many explicit projections and proximal maps are known, it is straightforward to derive and implement novel optimization algorithms in this setting. These algorithms can take hundreds if not thousands of iterations to converge, but the simple nature of each iteration makes proximal distance algorithms competitive with traditional algorithms. For convex problems, proximal distance algorithms reduce to proximal gradient algorithms and therefore enjoy well understood convergence properties. For nonconvex problems, one can attack convergence by invoking Zangwill's theorem. Our numerical examples demonstrate the utility of proximal distance algorithms in various high-dimensional settings, including a) linear programming, b) constrained least squares, c) projection to the closest kinship matrix, d) projection onto a second-order cone constraint, e) calculation of Horn's copositive matrix index, f) linear complementarity programming, and g) sparse principal components analysis. The proximal distance algorithm in each case is competitive or superior in speed to traditional methods such as the interior point method and the alternating direction method of multipliers (ADMM). Source code for the numerical examples can be found at https://github.com/klkeys/proxdist.",
            "keywords": [
                "constrained optimization",
                "EM algorithm",
                "majorization",
                "projection"
            ],
            "author": [
                "Kevin L. Keys",
                "Hua Zhou",
                "Kenneth Lange"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-687/17-687.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learnability of Solutions to Conjunctive Queries",
            "abstract": "The problem of learning the solution space of an unknown formula has been studied in multiple embodiments in computational learning theory. In this article, we study a family of such learning problems; this family contains, for each relational structure, the problem of learning the solution space of an unknown conjunctive query evaluated on the structure. A progression of results aimed to classify the learnability of each of the problems in this family, and thus far a culmination thereof was a positive learnability result generalizing all previous ones. This article completes the classification program towards which this progression of results strived, by presenting a negative learnability result that complements the mentioned positive learnability result. In addition, a further negative learnability result is exhibited, which indicates a dichotomy within the problems to which the first negative result applies. In order to obtain our negative results, we make use of universal-algebraic concepts.",
            "keywords": [
                "concept learning",
                "computational learning theory",
                "dichotomy theorems",
                "re-     ductions"
            ],
            "author": [
                "Hubie Chen",
                "Matthew Valeriote"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-734/17-734.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variance-based Regularization with Convex Objectives",
            "abstract": "We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error. Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certificates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance. We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems.",
            "keywords": [
                "variance regularization",
                "robust optimization"
            ],
            "author": [
                "John Duchi",
                "Hongseok Namkoong"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-750/17-750.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Consistent Vertex Nomination Schemes",
            "abstract": "Given a vertex of interest in a network , the vertex nomination problem seeks to find the corresponding vertex of interest (if it exists) in a second network . A vertex nomination scheme produces a list of the vertices in , ranked according to how likely they are judged to be the corresponding vertex of interest in . The vertex nomination problem and related information retrieval tasks have attracted much attention in the machine learning literature, with numerous applications to social and biological networks. However, the current framework has often been confined to a comparatively small class of network models, and the concept of statistically consistent vertex nomination schemes has been only shallowly explored. In this paper, we extend the vertex nomination problem to a very general statistical model of graphs. Further, drawing inspiration from the long-established classification framework in the pattern recognition literature, we provide definitions for the key notions of Bayes optimality and consistency in our extended vertex nomination framework, including a derivation of the Bayes optimal vertex nomination scheme. In addition, we prove that no universally consistent vertex nomination schemes exist. Illustrative examples are provided throughout.",
            "keywords": [
                "Vertex nomination",
                "graph inference"
            ],
            "author": [
                "Vince Lyzinski",
                "Keith Levin",
                "Carey E. Priebe"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-048/18-048.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Semi-Analytic Resampling in Lasso",
            "abstract": "An approximate method for conducting resampling in Lasso, the  penalized linear regression, in a semi-analytic manner is developed, whereby the average over the resampled datasets is directly computed without repeated numerical sampling, thus enabling an inference free of the statistical fluctuations due to sampling finiteness, as well as a significant reduction of computational time. The proposed method is based on a message passing type algorithm, and its fast convergence is guaranteed by the state evolution analysis, when covariates are provided as zero-mean independently and identically distributed Gaussian random variables. It is employed to implement bootstrapped Lasso (Bolasso) and stability selection, both of which are variable selection methods using resampling in conjunction with Lasso, and resolves their disadvantage regarding computational cost. To examine approximation accuracy and efficiency, numerical experiments were carried out using simulated datasets. Moreover, an application to a real-world dataset, the wine quality dataset, is presented. To process such real-world datasets, an objective criterion for determining the relevance of selected variables is also introduced by the addition of noise variables and resampling. MATLAB codes implementing the proposed method are distributed in (Obuchi, 2018).",
            "keywords": [
                "bootstrap method",
                "Lasso",
                "variable selection",
                "message passing algorithm"
            ],
            "author": [
                "Tomoyuki Obuchi",
                "Yoshiyuki Kabashima"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-109/18-109.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lazifying Conditional Gradient Algorithms",
            "abstract": "Conditional gradient algorithms (also often called Frank-Wolfe algorithms) are popular due to their simplicity of only requiring a linear optimization oracle and more recently they also gained significant traction for online learning. While simple in principle, in many cases the actual implementation of the linear optimization oracle is costly. We show a general method to lazify various conditional gradient algorithms, which in actual computations leads to several orders of magnitude of speedup in wall-clock time. This is achieved by using a faster separation oracle instead of a linear optimization oracle, relying only on few linear optimization oracle calls.",
            "keywords": [],
            "author": [
                "Gábor Braun",
                "Sebastian Pokutta",
                "Daniel Zink"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-114/18-114.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Redundancy Techniques for Straggler Mitigation in Distributed Optimization and Learning",
            "abstract": "Performance of distributed optimization and learning systems is bottlenecked by “straggler” nodes and slow communication links, which significantly delay computation. We propose a distributed optimization framework where the dataset is “encoded” to have an over-complete representation with built-in redundancy, and the straggling nodes in the system are dynamically treated as missing, or as “erasures” at every iteration, whose loss is compensated by the embedded redundancy. For quadratic loss functions, we show that under a simple encoding scheme, many optimization algorithms (gradient descent, L-BFGS, and proximal gradient) operating under data parallelism converge to an approximate solution even when stragglers are ignored. Furthermore, we show a similar result for a wider class of convex loss functions when operating under model parallelism. The applicable classes of objectives covers several popular learning problems such as linear regression, LASSO, support vector machine, collaborative filtering, and generalized linear models including logistic regression. These convergence results are deterministic, i.e., they establish sample path convergence for arbitrary sequences of delay patterns or distributions on the nodes, and are independent of the tail behavior of the delay distribution. We demonstrate that equiangular tight frames have desirable properties as encoding matrices, and propose efficient mechanisms for encoding large-scale data. We implement the proposed technique on Amazon EC2 clusters, and demonstrate its performance over several learning problems, including matrix factorization, LASSO, ridge regression and logistic regression, and compare the proposed method with uncoded, asynchronous, and data replication strategies.",
            "keywords": [],
            "author": [
                "Can Karakus",
                "Yifan Sun",
                "Suhas Diggavi",
                "Wotao Yin"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-148/18-148.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analysis of Langevin Monte Carlo via Convex Optimization",
            "abstract": "In this paper, we provide new insights on the Unadjusted Langevin Algorithm. We show that this method can be formulated as the first order optimization algorithm for an objective functional defined on the Wasserstein space of order . Using this interpretation and techniques borrowed from convex optimization, we give a non-asymptotic analysis of this method to sample from log-concave smooth target distribution on . Based on this interpretation, we propose two new methods for sampling from a non-smooth target distribution. These new algorithms are natural extensions of the Stochastic Gradient Langevin Dynamics (SGLD) algorithm, which is a popular extension of the Unadjusted Langevin Algorithm for largescale Bayesian inference. Using the optimization perspective, we provide non-asymptotic convergence analysis for the newly proposed methods.",
            "keywords": [
                "Unadjasted Langevin Algorithm",
                "convex optimization",
                "Bayesian inference",
                "gradient flow"
            ],
            "author": [
                "Alain Durmus",
                "Szymon Majewski",
                "Błażej Miasojedow"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-173/18-173.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Deep Optimal Stopping",
            "abstract": "In this paper we develop a deep learning method for optimal stopping problems which directly learns the optimal stopping rule from Monte Carlo samples. As such, it is broadly applicable in situations where the underlying randomness can efficiently be simulated. We test the approach on three problems: the pricing of a Bermudan max-call option, the pricing of a callable multi barrier reverse convertible and the problem of optimally stopping a fractional Brownian motion. In all three cases it produces very accurate results in high-dimensional situations with short computing times.",
            "keywords": [
                "optimal stopping",
                "deep learning",
                "Bermudan option",
                "callable multi barrier     reverse convertible"
            ],
            "author": [
                "Sebastian Becker",
                "Patrick Cheridito",
                "Arnulf Jentzen"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-232/18-232.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": " Fairness Constraints: A Flexible Approach for Fair Classification ",
            "abstract": "Algorithmic decision making is employed in an increasing number of real-world applicationstions to aid human decision making. While it has shown considerable promise in terms of improved decision accuracy, in some scenarios, its outcomes have been also shown to impose an unfair (dis)advantage on people from certain social groups (e.g., women, blacks). In this context, there is a need for computational techniques to limit unfairness in algorithmic decision making. In this work, we take a step forward to fulfill that need and introduce a flexible constraint-based framework to enable the design of fair margin-based classifiers. The main technical innovation of our framework is a general and intuitive measure of decision boundary unfairness, which serves as a tractable proxy to several of the most popular computational definitions of unfairness from the literature. Leveraging our measure, we can reduce the design of fair margin-based classifiers to adding tractable constraints on their decision boundaries. Experiments on multiple synthetic and real-world datasets show that our framework is able to successfully limit unfairness, often at a small cost in terms of accuracy.",
            "keywords": [
                "Supervised learning",
                "margin-based classifiers",
                "fairness",
                "discrimination"
            ],
            "author": [
                "Muhammad Bilal Zafar",
                "Isabel Valera",
                "Manuel Gomez-Rodriguez",
                "Krishna P. Gummadi"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-262/18-262.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Score Matching for Non-Negative Data",
            "abstract": "A common challenge in estimating parameters of probability density functions is the intractability of the normalizing constant. While in such cases maximum likelihood estimation may be implemented using numerical integration, the approach becomes computationally intensive. The score matching method of Hyvärinen (2005) avoids direct calculation of the normalizing constant and yields closed-form estimates for exponential families of continuous distributions over . Hyvärinen (2007) extended the approach to distributions supported on the non-negative orthant, . In this paper, we give a generalized form of score matching for non-negative data that improves estimation efficiency. As an example, we consider a general class of pairwise interaction models. Addressing an overlooked inexistence problem, we generalize the regularized score matching method of Lin et al. (2016) and improve its theoretical guarantees for non-negative Gaussian graphical models.",
            "keywords": [
                "exponential family",
                "graphical model",
                "positive data",
                "score matching"
            ],
            "author": [
                "Shiqing Yu",
                "Mathias Drton",
                "Ali Shojaie"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-278/18-278.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonuniformity of P-values Can Occur Early in Diverging Dimensions",
            "abstract": "Evaluating the joint significance of covariates is of fundamental importance in a wide range of applications. To this end, p-values are frequently employed and produced by algorithms that are powered by classical large-sample asymptotic theory. It is well known that the conventional p-values in Gaussian linear model are valid even when the dimensionality is a non-vanishing fraction of the sample size, but can break down when the design matrix becomes singular in higher dimensions or when the error distribution deviates from Gaussianity. A natural question is when the conventional p-values in generalized linear models become invalid in diverging dimensions. We establish that such a breakdown can occur early in nonlinear models. Our theoretical characterizations are confirmed by simulation studies.",
            "keywords": [
                "Nonuniformity",
                "p-value",
                "breakdown point",
                "generalized linear model",
                "high     dimensionality"
            ],
            "author": [
                "Yingying Fan",
                "Emre Demirkaya",
                "Jinchi Lv"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-314/18-314.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Prediction Risk for the Horseshoe Regression",
            "abstract": "We show that prediction performance for global-local shrinkage regression can overcome two major difficulties of global shrinkage regression: (i) the amount of relative shrinkage is monotone in the singular values of the design matrix and (ii) the shrinkage is determined by a single tuning parameter. Specifically, we show that the horseshoe regression, with heavy-tailed component-specific local shrinkage parameters, in conjunction with a global parameter providing shrinkage towards zero, alleviates both these difficulties and consequently, results in an improved risk for prediction. Numerical demonstrations of improved prediction over competing approaches in simulations and in a pharmacogenomics data set confirm our theoretical findings.",
            "keywords": [
                "Global-local Priors",
                "Principal Components",
                "Shrinkage Regression"
            ],
            "author": [
                "Anindya Bhadra",
                "Jyotishka Datta",
                "Yunfan Li",
                "Nicholas G. Polson",
                "Brandon Willard"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-321/18-321.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantification Under Prior Probability Shift: the Ratio Estimator and its Extensions",
            "abstract": "The quantification problem consists of determining the prevalence of a given label in a target population. However, one often has access to the labels in a sample from the training population but not in the target population. A common assumption in this situation is that of prior probability shift, that is, once the labels are known, the distribution of the features is the same in the training and target populations. In this paper, we derive a new lower bound for the risk of the quantification problem under the prior shift assumption. Complementing this lower bound, we present a new approximately minimax class of estimators, ratio estimators, which generalize several previous proposals in the literature. Using a weaker version of the prior shift assumption, which can be tested, we show that ratio estimators can be used to build confidence intervals for the quantification problem. We also extend the ratio estimator so that it can: (i) incorporate labels from the target population, when they are available and (ii) estimate how the prevalence of positive labels varies according to a function of certain covariates.",
            "keywords": [
                "quantification",
                "prior probability shift",
                "data set shift",
                "domain shift"
            ],
            "author": [
                "Afonso Fernandes Vaz",
                "Rafael Izbicki",
                "Rafael Bassi Stern"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-456/18-456.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning to Match via Inverse Optimal Transport",
            "abstract": "We propose a unified data-driven framework based on inverse optimal transport that can learn adaptive, nonlinear interaction cost function from noisy and incomplete empirical matching matrix and predict new matching in various matching contexts. We emphasize that the discrete optimal transport plays the role of a variational principle which gives rise to an optimization based framework for modeling the observed empirical matching data. Our formulation leads to a non-convex optimization problem which can be solved efficiently by an alternating optimization method. A key novel aspect of our formulation is the incorporation of marginal relaxation via regularized Wasserstein distance, significantly improving the robustness of the method in the face of noisy or missing empirical matching data. Our model falls into the category of prescriptive models, which not only predict potential future matching, but is also able to explain what leads to empirical matching and quantifies the impact of changes in matching factors. The proposed approach has wide applicability including predicting matching in online dating, labor market, college application and crowdsourcing. We back up our claims with numerical experiments on both synthetic data and real world data sets.",
            "keywords": [
                "Matching",
                "Inverse Problem",
                "Optimal Transport",
                "Robustification",
                "Variational     Inferencec 2019 Ruilin Li",
                "Xiaojing Ye"
            ],
            "author": [
                "Ruilin Li",
                "Xiaojing Ye",
                "Haomin Zhou",
                "Hongyuan Zha"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-700/18-700.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tight Lower Bounds on the VC-dimension of Geometric Set Systems",
            "abstract": "The VC-dimension of a set system is a way to capture its complexity and has been a key parameter studied extensively in machine learning and geometry communities. In this paper, we resolve two longstanding open problems on bounding the VC-dimension of two fundamental set systems: -fold unions/intersections of half-spaces and the simplices set system. Among other implications, it settles an open question in machine learning that was first studied in the foundational paper of Blumer et al. (1989) as well as by Eisenstat and Angluin (2007) and Johnson (2008).",
            "keywords": [
                "VC-dimension",
                "union of concepts",
                "intersection of concepts",
                "combinatorial problems"
            ],
            "author": [
                "Mónika Csikós",
                "Nabil H. Mustafa",
                "Andrey Kupavskii"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-719/18-719.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SMART: An Open Source Data Labeling Platform for Supervised Learning",
            "abstract": "SMART is an open source web application designed to help data scientists and research teams efficiently build labeled training data sets for supervised machine learning tasks. SMART provides users with an intuitive interface for creating labeled data sets, supports active learning to help reduce the required amount of labeled data, and incorporates inter-rater reliability statistics to provide insight into label quality. SMART is designed to be platform agnostic and easily deployable to meet the needs of as many different research teams as possible. The project website https://rtiinternational.github.io/SMART/ contains links to the code repository and extensive user documentation.",
            "keywords": [
                "software",
                "supervised learning",
                "data labeling",
                "active learning"
            ],
            "author": [
                "Rob Chew",
                "Michael Wenger",
                "Caroline Kery",
                "Jason Nance",
                "Keith Richards",
                "Emily Hadley",
                "Peter Baumgartner"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-859/18-859.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the optimality of the Hedge algorithm in the stochastic regime",
            "abstract": "In this paper, we study the behavior of the Hedge algorithm in the online stochastic setting. We prove that anytime Hedge with decreasing learning rate, which is one of the simplest algorithm for the problem of prediction with expert advice, is remarkably both worst-case optimal and adaptive to the easier stochastic and adversarial with a gap problems. This shows that, in spite of its small, non-adaptive learning rate, Hedge possesses the same optimal regret guarantee in the stochastic case as recently introduced adaptive algorithms. Moreover, our analysis exhibits qualitative differences with other versions of the Hedge algorithm, such as the fixed-horizon variant (with constant learning rate) and the one based on the so-called “doubling trick”, both of which fail to adapt to the easier stochastic setting. Finally, we determine the intrinsic limitations of anytime Hedge in the stochastic case, and discuss the improvements provided by more adaptive algorithms.",
            "keywords": [],
            "author": [
                "Jaouad Mourtada",
                "Stéphane Gaïffas"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-869/18-869.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Differentiable Game Mechanics",
            "abstract": "Deep learning is built on the foundational guarantee that gradient descent on an objective function converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, that exhibit multiple interacting losses. The behavior of gradient-based methods in games is not well understood -- and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new tools to understand and control the dynamics in -player differentiable games. The key result is to decompose the game Jacobian into two components. The first, symmetric component, is related to potential games, which reduce to gradient descent on an implicit function. The second, antisymmetric component, relates to Hamiltonian games, a new class of games that obey a conservation law akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in differentiable games. Basic experiments show SGA is competitive with recently proposed algorithms for finding stable fixed points in GANs -- while at the same time being applicable to, and having guarantees in, much more general cases.",
            "keywords": [
                "game theory",
                "generative adversarial networks",
                "deep learning",
                "classical mechan-     ics",
                "hamiltonian mechanics",
                "gradient descent"
            ],
            "author": [
                "Alistair Letcher",
                "David Balduzzi",
                "Sébastien Racanière",
                "James Martens",
                "Jakob Foerster",
                "Karl Tuyls",
                "Thore Graepel"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-008/19-008.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Space-Time Partitioning by Sampling and Pruning Spanning Trees",
            "abstract": "A typical problem in spatial data analysis is regionalization or spatially constrained clustering, which consists of aggregating small geographical areas into larger regions. A major challenge when partitioning a map is the huge number of possible partitions that compose the search space. This is compounded if we are partitioning spatio-temporal data rather than purely spatial data. We introduce a spatio-temporal product partition model that deals with the regionalization problem in a probabilistic way. Random spanning trees are used as a tool to tackle the problem of searching the space of possible partitions making feasible this exploration. Based on this framework, we propose an efficient Gibbs sampler algorithm to sample from the posterior distribution of the parameters, specially the random partition. The proposed Gibbs sampler scheme carries out a random walk on the space of the spanning trees and the partitions induced by deleting tree edges. In the purely spatial situation, we compare our proposed model with other state-of-art regionalization techniques to partition maps using simulated and real social and health data. To illustrate how the temporal component is handled by the algorithm and to show how the spatial clusters vary along the time we presented an application using human development index data. The analysis shows that our proposed model is better than state-of-art alternatives. Another appealing feature of the method is that the prior distribution for the partition is interpretable with a trivial coin flipping mechanism allowing its easy elicitation.",
            "keywords": [
                "Spatial Clustering",
                "Product Partition Models",
                "Random Spanning Trees"
            ],
            "author": [
                "Leonardo V. Teixeira",
                "Renato M. Assunção",
                "Rosangela H. Loschi"
            ],
            "ref": "http://jmlr.org/papers/volume20/16-615/16-615.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Streaming Principal Component Analysis From Incomplete Data ",
            "abstract": "Linear subspace models are pervasive in computational sciences and particularly used for large datasets which are often incomplete due to privacy issues or sampling constraints. Therefore, a critical problem is developing an efficient algorithm for detecting low-dimensional linear structure from incomplete data efficiently, in terms of both computational complexity and storage. In this paper we propose a streaming subspace estimation algorithm called Subspace Navigation via Interpolation from Partial Entries (SNIPE) that efficiently processes blocks of incomplete data to estimate the underlying subspace model. In every iteration, SNIPE finds the subspace that best fits the new data block but remains close to the previous estimate. We show that SNIPE is a streaming solver for the underlying nonconvex matrix completion problem, that it converges globally {to a stationary point of this program} regardless of initialization, and that the convergence is locally linear with high probability. We also find that SNIPE shows state-of-the-art performance in our numerical simulations.",
            "keywords": [
                "Principal component analysis",
                "Subspace identification",
                "Matrix completion",
                "Streaming algorithms",
                "Nonconvex optimization"
            ],
            "author": [
                "Armin Eftekhari",
                "Gregory Ongie",
                "Laura Balzano",
                "Michael B. Wakin"
            ],
            "ref": "http://jmlr.org/papers/volume20/16-627/16-627.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An asymptotic analysis of distributed nonparametric methods",
            "abstract": "We investigate and compare the fundamental performance of several distributed learning methods that have been proposed recently. We do this in the context of a distributed version of the classical signal-in-Gaussian-white-noise model, which serves as a benchmark model for studying performance in this setting. The results show how the design and tuning of a distributed method can have great impact on convergence rates and validity of uncertainty quantification. Moreover, we highlight the difficulty of designing nonparametric distributed procedures that automatically adapt to smoothness.",
            "keywords": [
                "distributed learning",
                "nonparametric models",
                "high-dimensional models",
                "Gaus-     sian processes"
            ],
            "author": [
                "Botond Szabó",
                "Harry van Zanten"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-666/17-666.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model Selection via the VC Dimension",
            "abstract": "We derive an objective function that can be optimized to give an estimator for the Vapnik-Chervonenkis dimension for use in model selection in regression problems. We verify our estimator is consistent. Then, we verify it performs well compared to seven other model selection techniques. We do this for a variety of types of data sets.",
            "keywords": [
                "Vapnik-Chervonenkis dimension",
                "model selection",
                "Bayesian information cri-     terion",
                "sparsity methods",
                "empirical risk minimization"
            ],
            "author": [
                "Merlin Mpoudeu",
                "Bertrand Clarke"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-669/17-669.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dependent relevance determination for smooth and structured sparse regression",
            "abstract": "In many problem settings, parameter vectors are not merely sparse but dependent in such a way that non-zero coefficients tend to cluster together. We refer to this form of dependency as “region sparsity.” Classical sparse regression methods, such as the lasso and automatic relevance determination (ARD), which model parameters as independent a priori, and therefore do not exploit such dependencies. Here we introduce a hierarchical model for smooth, region-sparse weight vectors and tensors in a linear regression setting. Our approach represents a hierarchical extension of the relevance determination framework, where we add a transformed Gaussian process to model the dependencies between the prior variances of regression weights. We combine this with a structured model of the prior variances of Fourier coefficients, which eliminates unnecessary high frequencies. The resulting prior encourages weights to be region-sparse in two different bases simultaneously. We develop Laplace approximation and Monte Carlo Markov Chain (MCMC) sampling to provide efficient inference for the posterior. Furthermore, a two-stage convex relaxation of the Laplace approximation approach is also provided to relax the inevitable non-convexity during the optimization. We finally show substantial improvements over comparable methods for both simulated and real datasets from brain imaging.",
            "keywords": [
                "Bayesian nonparametric",
                "Sparsity",
                "Structure learning",
                "Gaussian Process",
                "fMRIc 2019 Anqi Wu",
                "Oluwasanmi Koyejo"
            ],
            "author": [
                "Anqi Wu",
                "Oluwasanmi Koyejo",
                "Jonathan Pillow"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-757/17-757.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Particle-Based Variational Approach to Bayesian Non-negative Matrix Factorization",
            "abstract": "Bayesian Non-negative Matrix Factorization (BNMF) is a promising approach for understanding uncertainty and structure in matrix data. However, a large volume of applied work optimizes traditional non-Bayesian NMF objectives that fail to provide a principled understanding of the non-identifiability inherent in NMF---an issue ideally addressed by a Bayesian approach. Despite their suitability, current BNMF approaches have failed to gain popularity in an applied setting; they sacrifice flexibility in modeling for tractable computation, tend to get stuck in local modes, and can require many thousands of samples for meaningful uncertainty estimates. We address these issues through a particle-based variational approach to BNMF that only requires the joint likelihood to be differentiable for computational tractability, uses a novel transfer-based initialization technique to identify multiple modes in the posterior, and thus allows domain experts to inspect a small set of factorizations that faithfully represent the posterior. On several real datasets, we obtain better particle approximations to the BNMF posterior in less time than baselines and demonstrate the significant role that multimodality plays in NMF-related tasks.",
            "keywords": [
                "Bayesian",
                "Non-negative Matrix Factorization",
                "Stein discrepancy",
                "Non-identifiability"
            ],
            "author": [
                "Muhammad A Masood",
                "Finale Doshi-Velez"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-153/18-153.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Best Arm Identification for Contaminated Bandits",
            "abstract": "This paper studies active learning in the context of robust statistics. Specifically, we propose a variant of the Best Arm Identification problem for contaminated bandits, where each arm pull has probability epsilon of generating a sample from an arbitrary contamination distribution instead of the true underlying distribution. The goal is to identify the best (or approximately best) true distribution with high probability, with a secondary goal of providing guarantees on the quality of this distribution. The primary challenge of the contaminated bandit setting is that the true distributions are only partially identifiable, even with infinite samples. To address this, we develop tight, non-asymptotic sample complexity bounds for high-probability estimation of the first two robust moments (median and median absolute deviation) from contaminated samples. These concentration inequalities are the main technical contributions of the paper and may be of independent interest. Using these results, we adapt several classical Best Arm Identification algorithms to the contaminated bandit setting and derive sample complexity upper bounds for our problem. Finally, we provide matching information-theoretic lower bounds on the sample complexity (up to a small logarithmic factor).",
            "keywords": [
                "multi-armed bandits",
                "best arm identification",
                "robust statistics",
                "contamination    model"
            ],
            "author": [
                "Jason Altschuler",
                "Victor-Emmanuel Brunel",
                "Alan Malek"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-395/18-395.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "AffectiveTweets: a Weka Package for Analyzing Affect in Tweets",
            "abstract": "AffectiveTweets is a set of programs for analyzing emotion and sentiment of social media messages such as tweets. It is implemented as a package for the Weka machine learning workbench and provides methods for calculating state-of-the-art affect analysis features from tweets that can be fed into machine learning algorithms implemented in Weka. It also implements methods for building affective lexicons and distant supervision methods for training affective models from unlabeled tweets. The package was used by several teams in the shared tasks: EmoInt 2017 and Affect in Tweets SemEval 2018 Task 1.",
            "keywords": [
                "Twitter",
                "Sentiment Analysis",
                "Emotion Analysis",
                "Affective Computing",
                "Lexi-     con Induction"
            ],
            "author": [
                "Felipe Bravo-Marquez",
                "Eibe Frank",
                "Bernhard Pfahringer",
                "Saif M. Mohammad"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-450/18-450.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "iNNvestigate Neural Networks!",
            "abstract": "In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and predictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this shortcoming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library innvestigate addresses this by providing a common interface and out-of-the-box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of innvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.",
            "keywords": [],
            "author": [
                "Maximilian Alber",
                "Sebastian Lapuschkin",
                "Philipp Seegerer",
                "Miriam Hägele",
                "Kristof T. Schütt",
                "Grégoire Montavon",
                "Wojciech Samek",
                "Klaus-Robert Müller",
                "Sven Dähne",
                "Pieter-Jan Kindermans"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-540/18-540.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simultaneous Private Learning of Multiple Concepts",
            "abstract": "We investigate the direct-sum problem in the context of differentially private PAC learning: What is the sample complexity of solving  learning tasks simultaneously under differential privacy, and how does this cost compare to that of solving  learning tasks without privacy? In our setting, an individual example consists of a domain element  labeled by  unknown concepts . The goal of a multi-learner is to output  hypotheses  that generalize the input examples. Without concern for privacy, the sample complexity needed to simultaneously learn  concepts is essentially the same as needed for learning a single concept. Under differential privacy, the basic strategy of learning each hypothesis independently yields sample complexity that grows polynomially with . For some concept classes, we give multi-learners that require fewer samples than the basic strategy. Unfortunately, however, we also give lower bounds showing that even for very simple concept classes, the sample cost of private multi-learning must grow polynomially in .",
            "keywords": [
                "Differential privacy",
                "PAC learning",
                "Agnostic learning"
            ],
            "author": [
                "Mark Bun",
                "Kobbi Nissim",
                "Uri Stemmer"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-549/18-549.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-Dimensional Poisson Structural Equation Model Learning via $\\ell_1$-Regularized Regression",
            "abstract": "In this paper, we develop a new approach to learning high-dimensional Poisson structural equation models from only observational data without strong assumptions such as faithfulness and a sparse moralized graph. A key component of our method is to decouple the ordering estimation or parent search where the problems can be efficiently addressed using -regularized regression and the moments relation. We show that sample size  is sufficient for our polynomial time Moments Ratio Scoring (MRS) algorithm to recover the true directed graph, where  is the number of nodes and  is the maximum indegree. We verify through simulations that our algorithm is statistically consistent in the high-dimensional  setting, and performs well compared to state-of-the-art ODS, GES, and MMHC algorithms. We also demonstrate through multivariate real count data that our MRS algorithm is well-suited to estimating DAG models for multivariate count data in comparison to other methods used for discrete data.",
            "keywords": [
                "Bayesian Networks",
                "Directed Acyclic Graph",
                "Identifiability",
                "Structure Learn-     ing"
            ],
            "author": [
                "Gunwoong Park",
                "Sion Park"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-819/18-819.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PyOD: A Python Toolbox for Scalable Outlier Detection",
            "abstract": "PyOD is an open-source Python toolbox for performing scalable outlier detection on multivariate data. Uniquely, it provides access to a wide range of outlier detection algorithms, including established outlier ensembles and more recent neural network-based approaches, under a single, well-documented API designed for use by both practitioners and researchers. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, maintainability checks, interactive examples and parallelization are emphasized as core components in the toolbox's development. PyOD is compatible with both Python 2 and 3 and can be installed through Python Package Index (PyPI) or https://github.com/yzhao062/pyod.",
            "keywords": [
                "anomaly detection",
                "outlier detection",
                "outlier ensembles",
                "neural networks",
                "machine learning",
                "data mining"
            ],
            "author": [
                "Yue Zhao",
                "Zain Nasrullah",
                "Zheng Li"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-011/19-011.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Relative Error Bound Analysis for Nuclear Norm Regularized Matrix Completion",
            "abstract": "In this paper, we develop a relative error bound for nuclear norm regularized matrix completion, with the focus on the completion of full-rank matrices. Under the assumption that the top eigenspaces of the target matrix are incoherent, we derive a relative upper bound for recovering the best low-rank approximation of the unknown matrix. Although multiple works have been devoted to analyzing the recovery error of full-rank matrix completion, their error bounds are usually additive, making it impossible to obtain the perfect recovery case and more generally difficult to leverage the skewed distribution of eigenvalues. Our analysis is built upon the optimality condition of the regularized formulation and existing guarantees for low-rank matrix completion. To the best of our knowledge, this is the first relative bound that has been proved for the regularized formulation of matrix completion.",
            "keywords": [
                "matrix completion",
                "nuclear norm regularization",
                "least squares",
                "low-rank",
                "full-rank"
            ],
            "author": [
                "Lijun Zhang",
                "Tianbao Yang",
                "Rong Jin",
                "Zhi-Hua Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume20/15-504/15-504.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Geometric Multiscale Approximations for Intrinsically Low-dimensional Data",
            "abstract": "We consider the problem of efficiently approximating and encoding high-dimensional data sampled from a probability distribution  in , that is nearly supported on a -dimensional set  - for example supported on a -dimensional manifold. Geometric Multi-Resolution Analysis (GMRA) provides a robust and computationally efficient procedure to construct low-dimensional geometric approximations of  at varying resolutions. We introduce GMRA approximations that adapt to the unknown regularity of , by introducing a thresholding algorithm on the geometric wavelet coefficients. We show that these data-driven, empirical geometric approximations perform well, when the threshold is chosen as a suitable universal function of the number of samples , on a large class of measures , that are allowed to exhibit different regularity at different scales and locations, thereby efficiently encoding data from more complex measures than those supported on manifolds. These GMRA approximations are associated to a dictionary, together with a fast transform mapping data to -dimensional coefficients, and an inverse of such a map, all of which are data-driven. The algorithms for both the dictionary construction and the transforms have complexity  with the constant  exponential in . Our work therefore establishes Adaptive GMRA as a fast dictionary learning algorithm, with approximation guarantees, for intrinsically low-dimensional data. We include several numerical experiments on both synthetic and real data, confirming our theoretical results and demonstrating the effectiveness of Adaptive GMRA.",
            "keywords": [
                "Dictionary Learning",
                "Multi-Resolution Analysis",
                "Adaptive Approximation",
                "Manifold Learning"
            ],
            "author": [
                "Wenjing Liao",
                "Mauro Maggioni"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-252/17-252.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Change Surfaces for Expressive Multidimensional Changepoints and Counterfactual Prediction",
            "abstract": "Identifying changes in model parameters is fundamental in machine learning and statistics. However, standard changepoint models are limited in expressiveness, often addressing unidimensional problems and assuming instantaneous changes. We introduce change surfaces as a multidimensional and highly expressive generalization of changepoints. We provide a model-agnostic formalization of change surfaces, illustrating how they can provide variable, heterogeneous, and non-monotonic rates of change across multiple dimensions. Additionally, we show how change surfaces can be used for counterfactual prediction. As a concrete instantiation of the change surface framework, we develop Gaussian Process Change Surfaces (GPCS). We demonstrate counterfactual prediction with Bayesian posterior mean and credible sets, as well as massive scalability by introducing novel methods for additive non-separable kernels. Using two large spatio-temporal datasets we employ GPCS to discover and characterize complex changes that can provide scientific and policy relevant insights. Specifically, we analyze twentieth century measles incidence across the United States and discover previously unknown heterogeneous changes after the introduction of the measles vaccine. Additionally, we apply the model to requests for lead testing kits in New York City, discovering distinct spatial and demographic patterns.",
            "keywords": [
                "Change surface",
                "changepoint",
                "counterfactual",
                "Gaussian process",
                "scalable     inference",
                "kernel methodc 2019 William Herlands"
            ],
            "author": [
                "William Herlands",
                "Daniel B. Neill",
                "Hannes Nickisch",
                "Andrew Gordon Wilson"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-352/17-352.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hamiltonian Monte Carlo with Energy Conserving Subsampling",
            "abstract": "Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional posterior distributions with proposed parameter draws obtained by iterating on a discretized version of the Hamiltonian dynamics. The iterations make HMC computationally costly, especially in problems with large data sets, since it is necessary to compute posterior densities and their derivatives with respect to the parameters. Naively computing the Hamiltonian dynamics on a subset of the data causes HMC to lose its key ability to generate distant parameter proposals with high acceptance probability. The key insight in our article is that efficient subsampling HMC for the parameters is possible if both the dynamics and the acceptance probability are computed from the same data subsample in each complete HMC iteration. We show that this is possible to do in a principled way in a HMC-within-Gibbs framework where the subsample is updated using a pseudo marginal MH step and the parameters are then updated using an HMC step, based on the current subsample. We show that our subsampling methods are fast and compare favorably to two popular sampling algorithms that use gradient estimates from data subsampling. We also explore the current limitations of subsampling HMC algorithms by varying the quality of the variance reducing control variates used in the estimators of the posterior density and its gradients.",
            "keywords": [
                "Bayesian inference",
                "Big Data",
                "Markov chain Monte Carlo",
                "Estimated like-     lihood",
                "Stochastic gradient Hamiltonian Monte Carlo",
                "Stochastic Gradient Langevin Dy-     namicsc 2019 Khue-Dung Dang",
                "Matias Quiroz",
                "Robert Kohn",
                "Minh-Ngoc Tran"
            ],
            "author": [
                "Khue-Dung Dang",
                "Matias Quiroz",
                "Robert Kohn",
                "Minh-Ngoc Tran",
                "Mattias Villani"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-452/17-452.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-Convex Matrix Completion and Related Problems via Strong Duality",
            "abstract": "This work studies the strong duality of non-convex matrix factorization problems: we show that under certain dual conditions, these problems and the dual have the same optimum. This has been well understood for convex optimization, but little was known for non-convex problems. We propose a novel analytical framework and prove that under certain dual conditions, the optimal solution of the matrix factorization program is the same as that of its bi-dual and thus the global optimality of the non-convex program can be achieved by solving its bi-dual which is convex. These dual conditions are satisfied by a wide class of matrix factorization problems, although matrix factorization is hard to solve in full generality. This analytical framework may be of independent interest to non-convex optimization more broadly. We apply our framework to two prototypical matrix factorization problems: matrix completion and robust Principal Component Analysis. These are examples of efficiently recovering a hidden matrix given limited reliable observations. Our framework shows that exact recoverability and strong duality hold with nearly-optimal sample complexity for the two problems.",
            "keywords": [
                "strong duality",
                "non-convex optimization",
                "matrix factorization",
                "matrix completion",
                "robust principal component analysis"
            ],
            "author": [
                "Maria-Florina Balcan",
                "Yingyu Liang",
                "Zhao Song",
                "David P. Woodruff",
                "Hongyang Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-611/17-611.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularization via Mass Transportation",
            "abstract": "The goal of regression and classification methods in supervised learning is to minimize the empirical risk, that is, the expectation of some loss function quantifying the prediction error under the empirical distribution. When facing scarce training data, overfitting is typically mitigated by adding regularization terms to the objective that penalize hypothesis complexity. In this paper we introduce new regularization techniques using ideas from distributionally robust optimization, and we give new probabilistic interpretations to existing techniques. Specifically, we propose to minimize the worst-case expected loss, where the worst case is taken over the ball of all (continuous or discrete) distributions that have a bounded transportation distance from the (discrete) empirical distribution. By choosing the radius of this ball judiciously, we can guarantee that the worst-case expected loss provides an upper confidence bound on the loss on test data, thus offering new generalization bounds. We prove that the resulting regularized learning problems are tractable and can be tractably kernelized for many popular loss functions. The proposed approach to regluarization is also extended to neural networks. We validate our theoretical out-of-sample guarantees through simulated and empirical experiments.",
            "keywords": [
                "Distributionally robust optimization",
                "optimal transport",
                "Wasserstein distance",
                "robust optimization"
            ],
            "author": [
                "Soroosh Shafieezadeh-Abadeh",
                "Daniel Kuhn",
                "Peyman Mohajerin Esfahani"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-633/17-633.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Transport: Fast Probabilistic Approximation with Exact Solvers",
            "abstract": "We propose a simple subsampling scheme for fast randomized approximate computation of optimal transport distances on finite spaces. This scheme operates on a random subset of the full data and can use any exact algorithm as a black-box back-end, including state-of-the-art solvers and entropically penalized versions. It is based on averaging the exact distances between empirical measures generated from independent samples from the original measures and can easily be tuned towards higher accuracy or shorter computation times. To this end, we give non-asymptotic deviation bounds for its accuracy in the case of discrete optimal transport problems. In particular, we show that in many important instances, including images (2D-histograms), the approximation error is independent of the size of the full problem. We present numerical experiments that demonstrate that a very good approximation in typical applications can be obtained in a computation time that is several orders of magnitude smaller than what is required for exact computation of the full problem.",
            "keywords": [
                "computational vs statistical accuracy",
                "covering numbers",
                "empirical optimal trans-     port",
                "resampling",
                "risk bounds",
                "spanning tree",
                "Wasserstein distancec 2019 Max Sommerfeld"
            ],
            "author": [
                "Max Sommerfeld",
                "Jörn Schrieber",
                "Yoav Zemel",
                "Axel Munk"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-079/18-079.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Solving the OSCAR and SLOPE Models Using a Semismooth Newton-Based Augmented Lagrangian Method",
            "abstract": "The octagonal shrinkage and clustering algorithm for regression (OSCAR), equipped with the -norm and a pair-wise -norm regularizer, is a useful tool for feature selection and grouping in high-dimensional data analysis. The computational challenge posed by OSCAR, for high dimensional and/or large sample size data, has not yet been well resolved due to the non-smoothness and non-separability of the regularizer involved. In this paper, we successfully resolve this numerical challenge by proposing a sparse semismooth Newton-based augmented Lagrangian method to solve the more general SLOPE (the sorted L-one penalized estimation) model. By appropriately exploiting the inherent sparse and low-rank property of the generalized Jacobian of the semismooth Newton system in the augmented Lagrangian subproblem, we show how the computational complexity can be substantially reduced. Our algorithm offers a notable computational advantage in the high-dimensional statistical regression settings. Numerical experiments are conducted on real data sets, and the results demonstrate that our algorithm is far superior, in both speed and robustness, to the existing state-of-the-art algorithms based on first-order iterative schemes, including the widely used accelerated proximal gradient (APG) method and the alternating direction method of multipliers (ADMM).",
            "keywords": [
                "Linear Regression",
                "OSCAR",
                "Sparsity",
                "Augmented Lagrangian Method",
                "Semi-     smooth Newton methodc 2019 Ziyan Luo",
                "Defeng Sun"
            ],
            "author": [
                "Ziyan Luo",
                "Defeng Sun",
                "Kim-Chuan Toh",
                "Naihua Xiu"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-172/18-172.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": " Scalable Interpretable Multi-Response Regression via SEED",
            "abstract": "Sparse reduced-rank regression is an important tool for uncovering meaningful dependence structure between large numbers of predictors and responses in many big data applications such as genome-wide association studies and social media analysis. Despite the recent theoretical and algorithmic advances, scalable estimation of sparse reduced-rank regression remains largely unexplored. In this paper, we suggest a scalable procedure called sequential estimation with eigen-decomposition (SEED) which needs only a single top- sparse singular value decomposition from a generalized eigenvalue problem to find the optimal low-rank and sparse matrix estimate. Our suggested method is not only scalable but also performs simultaneous dimensionality reduction and variable selection. Under some mild regularity conditions, we show that SEED enjoys nice sampling properties including consistency in estimation, rank selection, prediction, and model selection. Moreover, SEED employs only basic matrix operations that can be efficiently parallelized in high performance computing devices. Numerical studies on synthetic and real data sets show that SEED outperforms the state-of-the-art approaches for large-scale matrix estimation problem.",
            "keywords": [
                "reduced-rank regression",
                "scalability",
                "high dimensionality",
                "greedy algorithm",
                "sparse eigenvector estimationc 2019 Zemin Zheng"
            ],
            "author": [
                "Zemin Zheng",
                "M. Taha Bahadori",
                "Yan Liu",
                "Jinchi Lv"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-200/18-200.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Likelihood for Gaussian Process Classification and Generalized Linear Mixed Models under Case-Control Sampling",
            "abstract": "Modern data sets in various domains often include units that were sampled non-randomly from the population and have a latent correlation structure. Here we investigate a common form of this setting, where every unit is associated with a latent variable, all latent variables are correlated, and the probability of sampling a unit depends on its response. Such settings often arise in case-control studies, where the sampled units are correlated due to spatial proximity, family relations, or other sources of relatedness. Maximum likelihood estimation in such settings is challenging from both a computational and statistical perspective, necessitating approximations that take the sampling scheme into account. We propose a family of approximate likelihood approaches which combine composite likelihood and expectation propagation. We demonstrate the efficacy of our solutions via extensive simulations. We utilize them to investigate the genetic architecture of several complex disorders collected in case-control genetic association studies, where hundreds of thousands of genetic variants are measured for every individual, and the underlying disease liabilities of individuals are correlated due to genetic similarity. Our work is the first to provide a tractable likelihood-based solution for case-control data with complex dependency structures.",
            "keywords": [
                "Gaussian Processes",
                "Expectation Propagation",
                "Composite Likelihood",
                "Selec-     tion Bias",
                "Linear Mixed Modelsc 2019 Omer Weissbrod",
                "Shachar Kaufman"
            ],
            "author": [
                "Omer Weissbrod",
                "Shachar Kaufman",
                "David Golan",
                "Saharon Rosset"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-298/18-298.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Unfaithful $K$-separable Gaussian Graphical Models",
            "abstract": "The global Markov property for Gaussian graphical models ensures graph separation implies conditional independence. Specifically if a node set  graph separates nodes  and  then  is conditionally independent of  given . The opposite direction need not be true, that is,  need not imply  is a node separator of  and . When it does, the relation  is called faithful. In this paper we provide a characterization of faithful relations and then provide an algorithm to test faithfulness based only on knowledge of other conditional relations of the form . We study two classes of separable Gaussian graphical models, namely, weakly -separable and strongly -separable Gaussian graphical models. Using the above test for faithfulness, we introduce algorithms to learn the topologies of weakly -separable and strongly -separable Gaussian graphical models with  sample complexity. For strongly -separable Gaussian graphical models, we additionally provide a method with error bounds for learning the off-diagonal precision matrix entries.",
            "keywords": [
                "Gaussian graphical model selection",
                "separable graphs",
                "high-dimensional sta-     tistical learning",
                "faithful conditional independence relations"
            ],
            "author": [
                "De Wen Soh",
                "Sekhar Tatikonda"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-329/18-329.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Representer Theorem for Deep Neural Networks",
            "abstract": "We propose to optimize the activation functions of a deep neural network by adding a corresponding functional regularization to the cost function. We justify the use of a second-order total-variation criterion. This allows us to derive a general representer theorem for deep neural networks that makes a direct connection with splines and sparsity. Specifically, we show that the optimal network configuration can be achieved with activation functions that are nonuniform linear splines with adaptive knots. The bottom line is that the action of each neuron is encoded by a spline whose parameters (including the number of knots) are optimized during the training procedure. The scheme results in a computational structure that is compatible with existing deep-ReLU, parametric ReLU, APL (adaptive piecewise-linear) and MaxOut architectures. It also suggests novel optimization challenges and makes an explicit link with  minimization and sparsity-promoting techniques.",
            "keywords": [
                "splines",
                "regularization",
                "sparsity",
                "learning",
                "deep neural networks"
            ],
            "author": [
                "Michael Unser"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-418/18-418.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Efficient Two Step Algorithm for High Dimensional Change Point Regression Models Without Grid Search",
            "abstract": "We propose a two step algorithm based on  regularization for the detection and estimation of parameters of a high dimensional change point regression model and provide the corresponding rates of convergence for the change point as well as the regression parameter estimates. Importantly, the computational cost of our estimator is only Lasso, where Lasso represents the computational burden of one Lasso optimization in a model of size . In comparison, existing grid search based approaches to this problem require a computational cost of at least  optimizations. Additionally, the proposed method is shown to be able to consistently detect the case of 'no change', i.e., where no finite change point exists in the model. We allow the true change point parameter  to possibly move to the boundaries of its parametric space, and the jump size  to possibly diverge as  increases. We then characterize the corresponding effects on the rates of convergence of the change point and regression estimates. In particular, we show that, while an increasing jump size may have a beneficial effect on the change point estimate, however the optimal rate of regression parameter estimates are preserved only upto a certain rate of the increasing jump size. This behavior in the rate of regression parameter estimates is unique to high dimensional change point regression models only. Simulations are performed to empirically evaluate performance of the proposed estimators. The methodology is applied to community level socio-economic data of the U.S., collected from the 1990 U.S. census and other sources.",
            "keywords": [
                "Change point regression",
                "High dimensional models"
            ],
            "author": [
                "Abhishek Kaul",
                "Venkata K. Jandhyala",
                "Stergios B. Fotopoulos"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-460/18-460.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Measuring the Effects of Data Parallelism on Neural Network Training",
            "abstract": "Recent hardware developments have dramatically increased the scale of data parallelism available for neural network training. Among the simplest ways to harness next-generation hardware is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured by the number of steps necessary to reach a goal out-of-sample error. We study how this relationship varies with the training algorithm, model, and data set, and find extremely large variation between workloads. Along the way, we show that disagreements in the literature on how batch size affects model quality can largely be explained by differences in metaparameter tuning and compute budgets at different batch sizes. We find no evidence that larger batch sizes degrade out-of-sample performance. Finally, we discuss the implications of our results on efforts to train neural networks much faster in the future. Our experimental data is publicly available as a database of 71,638,836 loss measurements taken over the course of training for 168,160 individual models across 35 workloads.",
            "keywords": [
                "neural networks",
                "stochastic gradient descent",
                "data parallelism",
                "batch size"
            ],
            "author": [
                "Christopher J. Shallue",
                "Jaehoon Lee",
                "Joseph Antognini",
                "Jascha Sohl-Dickstein",
                "Roy Frostig",
                "George E. Dahl"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-789/18-789.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Inference for Linear Support Vector Machine",
            "abstract": "The growing size of modern data brings many new challenges to existing statistical inference methodologies and theories, and calls for the development of distributed inferential approaches. This paper studies distributed inference for linear support vector machine (SVM) for the binary classification task. Despite a vast literature on SVM, much less is known about the inferential properties of SVM, especially in a distributed setting. In this paper, we propose a multi-round distributed linear-type (MDL) estimator for conducting inference for linear SVM. The proposed estimator is computationally efficient. In particular, it only requires an initial SVM estimator and then successively refines the estimator by solving simple weighted least squares problem. Theoretically, we establish the Bahadur representation of the estimator. Based on the representation, the asymptotic normality is further derived, which shows that the MDL estimator achieves the optimal statistical efficiency, i.e., the same efficiency as the classical linear SVM applying to the entire data set in a single machine setup. Moreover, our asymptotic result avoids the condition on the number of machines or data batches, which is commonly assumed in distributed estimation literature, and allows the case of diverging dimension. We provide simulation studies to demonstrate the performance of the proposed MDL estimator.",
            "keywords": [
                "Linear support vector machine",
                "distributed inference",
                "Bahadur representa-     tion"
            ],
            "author": [
                "Xiaozhou Wang",
                "Zhuoyi Yang",
                "Xi Chen",
                "Weidong Liu"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-801/18-801.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sharp Restricted Isometry Bounds for the Inexistence of Spurious Local Minima in Nonconvex Matrix Recovery",
            "abstract": "Nonconvex matrix recovery is known to contain no spurious local minima under a restricted isometry property (RIP) with a sufficiently small RIP constant . If  is too large, however, then counterexamples containing spurious local minima are known to exist. In this paper, we introduce a proof technique that is capable of establishing sharp thresholds on  to guarantee the inexistence of spurious local minima. Using the technique, we prove that in the case of a rank-1 ground truth, an RIP constant of  is both necessary and sufficient for exact recovery from any arbitrary initial point (such as a random point). We also prove a local recovery result: given an initial point  satisfying , any descent algorithm that converges to second-order optimality guarantees exact recovery.",
            "keywords": [
                "matrix factorization",
                "nonconvex optimization",
                "Restricted Isometry Property",
                "matrix sensing"
            ],
            "author": [
                "Richard Y. Zhang",
                "Somayeh Sojoudi",
                "Javad Lavaei"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-020/19-020.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Attribute Patterns in High-Dimensional Structured Latent Attribute Models",
            "abstract": "Structured latent attribute models (SLAMs) are a special family of discrete latent variable models widely used in social and biological sciences. This paper considers the problem of learning significant attribute patterns from a SLAM with potentially high-dimensional configurations of the latent attributes. We address the theoretical identifiability issue, propose a penalized likelihood method for the selection of the attribute patterns, and further establish the selection consistency in such an overfitted SLAM with a diverging number of latent patterns. The good performance of the proposed methodology is illustrated by simulation studies and two real datasets in educational assessments.",
            "keywords": [],
            "author": [
                "Yuqi Gu",
                "Gongjun Xu"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-197/19-197.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graph Reduction with Spectral and Cut Guarantees",
            "abstract": "Can one reduce the size of a graph without significantly altering its basic properties? The graph reduction problem is hereby approached from the perspective of restricted spectral approximation, a modification of the spectral similarity measure used for graph sparsification. This choice is motivated by the observation that restricted approximation carries strong spectral and cut guarantees, and that it implies approximation results for unsupervised learning problems relying on spectral embeddings. The article then focuses on coarsening - the most common type of graph reduction. Sufficient conditions are derived for a small graph to approximate a larger one in the sense of restricted approximation. These findings give rise to algorithms  that, compared to both standard and advanced graph reduction methods, find coarse graphs of improved quality, often by a large margin, without sacrificing speed.",
            "keywords": [
                "graph reduction and coarsening",
                "spectral methods"
            ],
            "author": [
                "Andreas Loukas"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-680/18-680.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generic Inference in Latent Gaussian Process Models",
            "abstract": "We develop an automated variational method for inference in models with Gaussian process (GP) priors and general likelihoods. The method supports multiple outputs and multiple latent functions and does not require detailed knowledge of the conditional likelihood, only needing its evaluation as a black-box function. Using a mixture of Gaussians as the variational distribution, we show that the evidence lower bound and its gradients can be estimated efficiently using samples from univariate Gaussian distributions. Furthermore, the method is scalable to large datasets which is achieved by using an augmented prior via the inducing-variable approach underpinning most sparse GP approximations, along with parallel computation and stochastic optimization. We evaluate our approach quantitatively and qualitatively with experiments on small datasets, medium-scale datasets and large datasets, showing its competitiveness under different likelihood models and sparsity levels. On the large-scale experiments involving prediction of airline delays and classification of handwritten digits, we show that our method is on par with the state-of-the-art hard-coded approaches for scalable GP regression and classification.",
            "keywords": [
                "Gaussian processes",
                "black-box likelihoods",
                "nonlinear likelihoods",
                "scalable     inference"
            ],
            "author": [
                "Edwin V. Bonilla",
                "Karl Krauth",
                "Amir Dezfouli"
            ],
            "ref": "http://jmlr.org/papers/volume20/16-437/16-437.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Binarsity: a penalization for one-hot encoded features in linear supervised learning",
            "abstract": "This paper deals with the problem of large-scale linear supervised learning in settings where a large number of continuous features are available. We propose to combine the well-known trick of one-hot encoding of continuous features with a new penalization called binarsity. In each group of binary features coming from the one-hot encoding of a single raw continuous feature, this penalization uses total-variation regularization together with an extra linear constraint. This induces two interesting properties on the model weights of the one-hot encoded features: they are piecewise constant, and are eventually block sparse. Non-asymptotic oracle inequalities for generalized linear models are proposed. Moreover, under a sparse additive model assumption, we prove that our procedure matches the state-of-the-art in this setting. Numerical experiments illustrate the good performances of our approach on several datasets. It is also noteworthy that our method has a numerical complexity comparable to standard  penalization.",
            "keywords": [
                "Supervised learning",
                "Features binarization",
                "Sparse additive modeling",
                "Total-variation",
                "Oracle inequalities"
            ],
            "author": [
                "Mokhtar Z. Alaya",
                "Simon Bussy",
                "Stéphane Gaïffas",
                "Agathe Guilloux"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-170/17-170.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ivanov-Regularised Least-Squares Estimators over Large RKHSs and Their Interpolation Spaces",
            "abstract": "We study kernel least-squares estimation under a norm constraint. This form of regularisation is known as Ivanov regularisation and it provides better control of the norm of the estimator than the well-established Tikhonov regularisation. Ivanov regularisation can be studied under minimal assumptions. In particular, we assume only that the RKHS is separable with a bounded and measurable kernel. We provide rates of convergence for the expected squared  error of our estimator under the weak assumption that the variance of the response variables is bounded and the unknown regression function lies in an interpolation space between  and the RKHS. We then obtain faster rates of convergence when the regression function is bounded by clipping the estimator. In fact, we attain the optimal rate of convergence. Furthermore, we provide a high-probability bound under the stronger assumption that the response variables have subgaussian errors and that the regression function lies in an interpolation space between  and the RKHS. Finally, we derive adaptive results for the settings in which the regression function is bounded.",
            "keywords": [
                "Interpolation Space",
                "Ivanov Regularisation",
                "Regression",
                "RKHS"
            ],
            "author": [
                "Stephen Page",
                "Steffen Grünewälder"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-672/17-672.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction",
            "abstract": "Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable features and identify the support vectors. It has achieved great successes in many real-world applications. However, for large-scale problems involving a huge number of samples and ultra-high dimensional features, solving sparse SVMs remains challenging. By noting that sparse SVMs induce sparsities in both feature and sample spaces, we propose a novel approach, which is based on accurate estimations of the primal and dual optima of sparse SVMs, to simultaneously identify the inactive features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identified inactive samples and features from the training phase, leading to substantial savings in the computational cost without sacrificing the accuracy. Moreover, we show that our method can be extended to multi-class sparse support vector machines. To the best of our knowledge, the proposed method is the first static feature and sample reduction method for sparse SVMs and multi-class sparse SVMs. Experiments on both synthetic and real data sets demonstrate that our approach significantly outperforms state-of-the-art methods and the speedup gained by our approach can be orders of magnitude.",
            "keywords": [
                "screening",
                "SVM",
                "dual problem",
                "optimization"
            ],
            "author": [
                "Bin Hong",
                "Weizhong Zhang",
                "Wei Liu",
                "Jieping Ye",
                "Deng Cai",
                "Xiaofei He",
                "Jie Wang"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-723/17-723.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximate Profile Maximum Likelihood",
            "abstract": "We propose an efficient algorithm for approximate computation of the profile maximum likelihood (PML), a variant of maximum likelihood maximizing the probability of observing a sufficient statistic rather than the empirical sample. The PML has appealing theoretical properties, but is difficult to compute exactly. Inspired by observations gleaned from exactly solvable cases, we look for an approximate PML solution, which, intuitively, clumps comparably frequent symbols into one symbol. This amounts to lower-bounding a certain matrix permanent by summing over a subgroup of the symmetric group rather than the whole group during the computation. We extensively experiment with the approximate solution, and the empirical performance of our approach is competitive and sometimes significantly better than state-of-the-art performances for various estimation problems.",
            "keywords": [
                "Profile maximum likelihood",
                "dynamic programming",
                "sufficient statistic",
                "par-     tition of multi-partite numbers"
            ],
            "author": [
                "Dmitri S. Pavlichin",
                "Jiantao Jiao",
                "Tsachy Weissman"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-075/18-075.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ADMMBO: Bayesian Optimization with Unknown Constraints using ADMM",
            "abstract": "There exist many problems in science and engineering that involve optimization of an unknown or partially unknown objective function. Recently, Bayesian Optimization (BO) has emerged as a powerful tool for solving optimization problems whose objective functions are only available as a black box and are expensive to evaluate. Many practical problems, however, involve optimization of an unknown objective function subject to unknown constraints. This is an important yet challenging problem for which, unlike optimizing an unknown function, existing methods face several limitations. In this paper, we present a novel constrained Bayesian optimization framework to optimize an unknown objective function subject to unknown constraints. We introduce an equivalent optimization by augmenting the objective function with constraints, introducing auxiliary variables for each constraint, and forcing the new variables to be equal to the main variable. Building on the Alternating Direction Method of Multipliers (ADMM) algorithm, we propose ADMM-Bayesian Optimization (ADMMBO) to solve the problem in an iterative fashion. Our framework leads to multiple unconstrained subproblems with unknown objective functions, which we then solve via BO. Our method resolves several challenges of state-of-the-art techniques: it can start from infeasible points, is insensitive to initialization, can efficiently handle `decoupled problems' and has a concrete stopping criterion. Extensive experiments on a number of challenging BO benchmark problems show that our proposed approach outperforms the state-of-the-art methods in terms of the speed of obtaining a feasible solution and convergence to the global optimum as well as minimizing the number of total evaluations of unknown objective and constraints functions.",
            "keywords": [],
            "author": [
                "Setareh Ariafar",
                "Jaume Coll-Font",
                "Dana Brooks",
                "Jennifer Dy"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-227/18-227.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Deep Exploration via Randomized Value Functions",
            "abstract": "We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.",
            "keywords": [
                "Reinforcement learning",
                "exploration",
                "value function"
            ],
            "author": [
                "Ian Osband",
                "Benjamin Van Roy",
                "Daniel J. Russo",
                "Zheng Wen"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-339/18-339.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ORCA: A Matlab/Octave Toolbox for Ordinal Regression",
            "abstract": "Ordinal regression, also named ordinal classification, studies classification problems where there exist a natural order between class labels. This structured order of the labels is crucial in all steps of the learning process in order to take full advantage of the data. ORCA (Ordinal Regression and Classification Algorithms) is a Matlab/Octave framework that implements and integrates different ordinal classification algorithms and specifically designed performance metrics. The framework simplifies the task of experimental comparison to a great extent, allowing the user to: (i) describe experiments by simple configuration files; (ii) automatically run different data partitions; (iii) parallelize the executions; (iv) generate a variety of performance reports and (v) include new algorithms by using its intuitive interface. Source code, binaries, documentation, descriptions and links to data sets and tutorials (including examples of educational purpose) are available at https://github.com/ayrna/orca.",
            "keywords": [
                "Ordinal regression",
                "ordinal classification",
                "Matlab",
                "Octave"
            ],
            "author": [
                "Javier Sánchez-Monedero",
                "Pedro A. Gutiérrez",
                "María Pérez-Ortiz"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-349/18-349.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Learning via Manifold Regularization",
            "abstract": "This paper frames causal structure estimation as a machine learning task. The idea is to treat indicators of causal relationships between variables as `labels' and to exploit available data on the variables of interest to provide features for the labelling task. Background scientific knowledge or any available interventional data provide labels on some causal relationships and the remainder are treated as unlabelled. To illustrate the key ideas, we develop a distance-based approach (based on bivariate histograms) within a manifold regularization framework. We present empirical results on three different biological data sets (including examples where causal effects can be verified by experimental intervention), that together demonstrate the efficacy and general nature of the approach as well as its simplicity from a user's point of view.",
            "keywords": [
                "causal learning",
                "manifold regularization",
                "semi-supervised learning",
                "interven-     tional data"
            ],
            "author": [
                "Steven M. Hill",
                "Chris J. Oates",
                "Duncan A. Blythe",
                "Sach Mukherjee"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-383/18-383.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Unsupervised Basis Function Adaptation for Reinforcement Learning",
            "abstract": "When using reinforcement learning (RL) algorithms it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on an agent's performance, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is currently interest among researchers in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures. One relatively unexplored method of adapting approximation architectures involves using feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. In this article we will: (a) informally discuss the potential advantages offered by such methods; (b) introduce a new algorithm based on such methods which adapts a state aggregation approximation architecture on-line and is designed for use in conjunction with SARSA; (c) provide theoretical results, in a policy evaluation setting, regarding this particular algorithm's complexity, convergence properties and potential to reduce VF error; and finally (d) test experimentally the extent to which this algorithm can improve performance given a number of different test problems. Taken together our results suggest that our algorithm (and potentially such methods more generally) can provide a versatile and computationally lightweight means of significantly boosting RL performance given suitable conditions which are commonly encountered in practice.",
            "keywords": [
                "reinforcement learning",
                "unsupervised learning",
                "basis function adaptation",
                "state aggregation"
            ],
            "author": [
                "Edward Barker",
                "Charl Ras"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-392/18-392.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Time-to-Event Prediction with Neural Networks and Cox Regression",
            "abstract": "New methods for time-to-event prediction are proposed by extending the Cox proportional hazards model with neural networks. Building on methodology from nested case-control studies, we propose a loss function that scales well to large data sets and enables fitting of both proportional and non-proportional extensions of the Cox model. Through simulation studies, the proposed loss function is verified to be a good approximation for the Cox partial log-likelihood. The proposed methodology is compared to existing methodologies on real-world data sets and is found to be highly competitive, typically yielding the best performance in terms of Brier score and binomial log-likelihood. A python package for the proposed methods is available at https://github.com/havakv/pycox.",
            "keywords": [
                "Cox regression",
                "customer churn",
                "neural networks",
                "non-proportional hazards"
            ],
            "author": [
                "Håvard Kvamme",
                "Ørnulf Borgan",
                "Ida Scheel"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-424/18-424.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Logical Explanations for Deep Relational Machines Using Relevance Information",
            "abstract": "Our interest in this paper is in the construction of symbolic explanations for predictions made by a deep neural network. We will focus attention on deep relational machines (DRMs: a term introduced in Lodhi (2013)). A DRM is a deep network in which the input layer consists of Boolean-valued functions (features) that are defined in terms of relations provided as domain, or background, knowledge. Our DRMs differ from those in Lodhi (2013), which uses an Inductive Logic Programming (ILP) engine to first select features (we use random selections from a space of features that satisfies some approximate constraints on logical relevance and non-redundancy). But why do the DRMs predict what they do? One way of answering this was provided in recent work Ribeiro et al. (2016), by constructing readable proxies for a black-box predictor. The proxies are intended only to model the predictions of the black-box in local regions of the instance-space. But readability alone may not be enough: to be understandable, the local models must use relevant concepts in an meaningful manner. We investigate the use of a Bayes-like approach to identify logical proxies for local predictions of a DRM. As a preliminary step, we show that DRM's with our randomised propositionalization method achieve predictive performance that is comparable to the best reports in the ILP literature. Our principal results on logical explanations show: (a) Models in first-order logic can approximate the DRM's prediction closely in a small local region; and (b) Expert-provided relevance information can play the role of a prior to distinguish between logical explanations that perform equivalently on prediction alone.",
            "keywords": [
                "Explainable AI",
                "Inductive Logic Programming"
            ],
            "author": [
                "Ashwin Srinivasan",
                "Lovekesh Vig",
                "Michael Bain"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-517/18-517.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Decoupling Sparsity and Smoothness in the Dirichlet Variational Autoencoder Topic Model",
            "abstract": "Recent work on variational autoencoders (VAEs) has enabled the development of generative topic models using neural networks. Topic models based on latent Dirichlet allocation (LDA) successfully use the Dirichlet distribution as a prior for the topic and word distributions to enforce sparseness. However, there is a trade-off between sparsity and smoothness in Dirichlet distributions. Sparsity is important for a low reconstruction error during training of the autoencoder, whereas smoothness enables generalization and leads to a better log-likelihood of the test data. Both of these properties are encoded in the Dirichlet parameter vector. By rewriting this parameter vector into a product of a sparse binary vector and a smoothness vector, we decouple the two properties, leading to a model that features both a competitive topic coherence and a high log-likelihood. Efficient training is enabled using rejection sampling variational inference for the reparameterization of the Dirichlet distribution. Our experiments show that our method is competitive with other recent VAE topic models.",
            "keywords": [
                "variational autoencoders",
                "topic models",
                "Dirichlet distribution",
                "reparameteri-     zation"
            ],
            "author": [
                "Sophie Burkhardt",
                "Stefan Kramer"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-569/18-569.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "More Efficient Estimation for Logistic Regression with Optimal Subsamples",
            "abstract": "In this paper, we propose improved estimation method for logistic regression based on subsamples taken according the optimal subsampling probabilities developed in Wang et al. (2018). Both asymptotic results and numerical results show that the new estimator has a higher estimation efficiency. We also develop a new algorithm based on Poisson subsampling, which does not require to approximate the optimal subsampling probabilities all at once. This is computationally advantageous when available random-access memory is not enough to hold the full data. Interestingly, asymptotic distributions also show that Poisson subsampling produces a more efficient estimator if the sampling ratio, the ratio of the subsample size to the full data sample size, does not converge to zero. We also obtain the unconditional asymptotic distribution for the estimator based on Poisson subsampling. Pilot estimators are required to calculate subsampling probabilities and to correct biases in un-weighted estimators; interestingly, even if pilot estimators are inconsistent, the proposed method still produce consistent and asymptotically normal estimators.",
            "keywords": [
                "Asymptotic Distribution",
                "Logistic Regression",
                "Massive Data",
                "Optimal Sub-     sampling"
            ],
            "author": [
                "HaiYing Wang"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-596/18-596.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spurious Valleys in One-hidden-layer Neural Network Optimization Landscapes",
            "abstract": "Neural networks provide a rich class of high-dimensional, non-convex optimization problems. Despite their non-convexity, gradient-descent methods often successfully optimize these models. This has motivated a recent spur in research attempting to characterize properties of their loss surface that may explain such success. In this paper, we address this phenomenon by studying a key topological property of the loss: the presence or absence of spurious valleys, defined as connected components of sub-level sets that do not include a global minimum. Focusing on a class of one-hidden-layer neural networks defined by smooth (but generally non-linear) activation functions, we identify a notion of intrinsic dimension and show that it provides necessary and sufficient conditions for the absence of spurious valleys. More concretely, finite intrinsic dimension guarantees that for sufficiently overparametrised models no spurious valleys exist, independently of the data distribution. Conversely, infinite intrinsic dimension implies that spurious valleys do exist for certain data distributions, independently of model overparametrisation. Besides these positive and negative results, we show that, although spurious valleys may exist in general, they are confined to low risk levels and avoided with high probability on overparametrised models.",
            "keywords": [],
            "author": [
                "Luca Venturi",
                "Afonso S. Bandeira",
                "Joan Bruna"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-674/18-674.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Variance-Reduced Cubic Regularization Methods ",
            "abstract": "We propose a stochastic variance-reduced cubic regularized Newton method (SVRC) for non-convex optimization. At the core of SVRC is a novel semi-stochastic gradient along with a semi-stochastic Hessian, which are specifically designed for cubic regularization method. For a nonconvex function with  component functions, we show that our algorithm is guaranteed to converge to an -approximate local minimum within \\footnote{Here  hides poly-logarithmic factors.} second-order oracle calls, which outperforms the state-of-the-art cubic regularization algorithms including subsampled cubic regularization. To further reduce the sample complexity of Hessian matrix computation in cubic regularization based methods, we also propose a sample efficient stochastic variance-reduced cubic regularization (Lite-SVRC) algorithm for finding the local minimum more efficiently. Lite-SVRC converges to an -approximate local minimum within  Hessian sample complexity, which is faster than all existing cubic regularization based methods. Numerical experiments with different nonconvex optimization problems conducted on real datasets validate our theoretical results for both SVRC and Lite-SVRC.",
            "keywords": [
                "Cubic Regularization",
                "Nonconvex Optimization",
                "Variance Reduction",
                "Hessian     Sample Complexity"
            ],
            "author": [
                "Dongruo Zhou",
                "Pan Xu",
                "Quanquan Gu"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-055/19-055.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gaussian Processes with Linear Operator Inequality Constraints",
            "abstract": "This paper presents an approach for constrained Gaussian Process (GP) regression where we assume that a set of linear transformations of the process are bounded. It is motivated by machine learning applications for high-consequence engineering systems, where this kind of information is often made available from phenomenological knowledge. We consider a GP  over functions on  taking values in , where the process  is still Gaussian when  is a linear operator. Our goal is to model  under the constraint that realizations of  are confined to a convex set of functions. In particular, we require that , given two functions  and  where  pointwise. This formulation provides a consistent way of encoding multiple linear constraints, such as shape-constraints based on e.g. boundedness, monotonicity or convexity. We adopt the approach of using a sufficiently dense set of virtual observation locations where the constraint is required to hold, and derive the exact posterior for a conjugate likelihood. The results needed for stable numerical implementation are derived, together with an efficient sampling scheme for estimating the posterior process.",
            "keywords": [
                "Gaussian processes",
                "Linear constraints",
                "Virtual observations",
                "Uncertainty     Quantification"
            ],
            "author": [
                "Christian Agrell"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-065/19-065.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiclass Boosting: Margins, Codewords, Losses, and Algorithms",
            "abstract": "The problem of multiclass boosting is considered. A new formulation is presented, combining multi-dimensional predictors, multi-dimensional real-valued codewords, and proper multiclass margin loss functions. This leads to a number of contributions, such as maximum capacity codeword sets, a family of proper and margin enforcing losses, denoted as  losses, and two new multiclass boosting algorithms. These are descent procedures on the functional space spanned by a set of weak learners. The first, CD-MCBoost, is a coordinate descent procedure that updates one predictor component at a time. The second, GD-MCBoost, a gradient descent procedure that updates all components jointly. Both MCBoost algorithms are defined with respect to a  loss and can reduce to classical boosting procedures (such as AdaBoost and LogitBoost) for binary problems. Beyond the algorithms themselves, the proposed formulation enables a unified treatment of many previous multiclass boosting algorithms. This is used to show that the latter implement different combinations of optimization strategy, codewords, weak learners, and loss function, highlighting some of their deficiencies. It is shown that no previous method matches the support of MCBoost for real codewords of maximum capacity, a proper margin-enforcing loss function, and any family of multidimensional predictors and weak learners. Experimental results confirm the superiority of MCBoost, showing that the two proposed MCBoost algorithms outperform comparable prior methods on a number of datasets.\\\\ \\\\ \\textbf{Keywords}: Boosting, Multiclass Boosting, Multiclass Classification, Margin Maximization, Loss Function.",
            "keywords": [
                "Boosting",
                "Multiclass Boosting",
                "Multiclass Classification",
                "Margin Maximization"
            ],
            "author": [
                "Mohammad Saberian",
                "Nuno Vasconcelos"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-137/17-137.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Maximum Entropy Estimation",
            "abstract": "We consider the problem of estimating a probability distribution that maximizes the entropy while satisfying a finite number of moment constraints, possibly corrupted by noise. Based on duality of convex programming, we present a novel approximation scheme using a smoothed fast gradient method that is equipped with explicit bounds on the approximation error. We further demonstrate how the presented scheme can be used for approximating the chemical master equation through the zero-information moment closure method, and for an approximate dynamic programming approach in the context of constrained Markov decision processes with uncountable state and action spaces.",
            "keywords": [
                "Entropy maximization",
                "convex optimization",
                "relative entropy minimization",
                "fast gradient method"
            ],
            "author": [
                "Tobias Sutter",
                "David Sutter",
                "Peyman Mohajerin Esfahani",
                "John Lygeros"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-486/17-486.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Decentralized Dictionary Learning Over Time-Varying Digraphs",
            "abstract": "This paper studies Dictionary Learning problems wherein the learning task is distributed over a multi-agent network, modeled as a time-varying directed graph. This formulation is relevant, for instance, in Big Data scenarios where massive amounts of data are collected/stored in different locations (e.g., sensors, clouds) and aggregating and/or processing all data in a fusion center might be inefficient or unfeasible, due to resource limitations, communication overheads or privacy issues. We develop a unified decentralized algorithmic framework for this class of nonconvex problems, which is proved to converge to stationary solutions at a sublinear rate. The new method hinges on Successive Convex Approximation techniques, coupled with a decentralized tracking mechanism aiming at locally estimating the gradient of the smooth part of the sum-utility. To the best of our knowledge, this is the first provably convergent decentralized algorithm for Dictionary Learning and, more generally, bi-convex problems over (time-varying) (di)graphs.",
            "keywords": [
                "Decentralized algorithms",
                "dictionary learning",
                "directed graph",
                "non-convex     optimization"
            ],
            "author": [
                "Amir Daneshmand",
                "Ying Sun",
                "Gesualdo Scutari",
                "Francisco Facchinei",
                "Brian M. Sadler"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-534/17-534.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric Bayesian Aggregation for Massive Data",
            "abstract": "We develop a set of scalable Bayesian inference procedures for a general class of nonparametric regression models. Specifically, nonparametric Bayesian inferences are separately performed on each subset randomly split from a massive dataset, and then the obtained local results are aggregated into global counterparts. This aggregation step is explicit without involving any additional computation cost. By a careful partition, we show that our aggregated inference results obtain an oracle rule in the sense that they are equivalent to those obtained directly from the entire data (which are computationally prohibitive). For example, an aggregated credible ball achieves desirable credibility level and also frequentist coverage while possessing the same radius as the oracle ball.",
            "keywords": [
                "Credible region",
                "divide-and-conquer",
                "Gaussian process prior",
                "linear functional"
            ],
            "author": [
                "Zuofeng Shang",
                "Botao Hao",
                "Guang Cheng"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-641/17-641.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Provably Accurate Double-Sparse Coding",
            "abstract": "Sparse coding is a crucial subroutine in algorithms for various signal processing, deep learning, and other machine learning applications. The central goal is to learn an overcomplete dictionary that can sparsely represent a given input dataset. However, a key challenge is that storage, transmission, and processing of the learned dictionary can be untenably high if the data dimension is high. In this paper, we consider the double-sparsity model introduced by Rubinstein et al. (2010b)  where the dictionary itself is the product of a fixed, known basis and a data-adaptive sparse component. First, we introduce a simple algorithm for double-sparse coding that can be amenable to efficient implementation via neural architectures. Second, we theoretically analyze its performance and demonstrate asymptotic sample complexity and running time benefits over existing (provable) approaches for sparse coding. To our knowledge, our work introduces the first computationally efficient algorithm for double-sparse coding that enjoys rigorous statistical guarantees. Finally, we corroborate our theory with several numerical experiments on simulated data, suggesting that our method may be useful for problem sizes encountered in practice.",
            "keywords": [
                "Sparse coding",
                "provable algorithms"
            ],
            "author": [
                "Thanh V. Nguyen",
                "Raymond K. W. Wong",
                "Chinmay Hegde"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-728/17-728.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model-free Nonconvex Matrix Completion: Local Minima Analysis and Applications in Memory-efficient Kernel PCA",
            "abstract": "This work studies low-rank approximation of a positive semidefinite matrix from partial entries via nonconvex optimization. We characterized how well local-minimum based low-rank factorization approximates a fixed positive semidefinite matrix without any assumptions on the rank-matching, the condition number or eigenspace incoherence parameter. Furthermore, under certain assumptions on rank-matching and well-boundedness of condition numbers and eigenspace incoherence parameters, a corollary of our main theorem improves the state-of-the-art sampling rate results for nonconvex matrix completion with no spurious local minima in Ge et al. (2016, 2017). In addition, we have investigated when the proposed nonconvex optimization results in accurate low-rank approximations even in presence of large condition numbers, large incoherence parameters, or rank mismatching. We also propose to apply the nonconvex optimization to memory-efficient kernel PCA. Compared to the well-known Nyström methods, numerical experiments indicate that the proposed nonconvex optimization approach yields more stable results in both low-rank approximation and clustering.",
            "keywords": [
                "low-rank approximation",
                "matrix completion",
                "nonconvex optimization",
                "model-     free analysis",
                "local minimum analysis"
            ],
            "author": [
                "Ji Chen",
                "Xiaodong Li"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-776/17-776.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimal Sample Subspace Learning: Theory and Algorithms",
            "abstract": "Subspace segmentation, or subspace learning, is a challenging and complicated task in machine learning. This paper builds a primary frame and solid theoretical bases for the minimal subspace segmentation (MSS) of finite samples. The existence and conditional uniqueness of MSS are discussed with conditions generally satisfied in applications. Utilizing weak prior information of MSS, the minimality inspection of segments is further simplified to the prior detection of partitions. The MSS problem is then modeled as a computable optimization problem via the self-expressiveness of samples. A closed form of the representation matrices is first given for the self-expressiveness, and the connection of diagonal blocks is addressed. The MSS model uses a rank restriction on the sum of segment ranks. Theoretically, it can retrieve the minimal sample subspaces that could be heavily intersected. The optimization problem is solved via a basic manifold conjugate gradient algorithm, alternative optimization and hybrid optimization, therein considering solutions to both the primal MSS problem and its pseudo-dual problem. The MSS model is further modified for handling noisy data and solved by an ADMM algorithm. The reported experiments show the strong ability of the MSS method to retrieve minimal sample subspaces that are heavily intersected.",
            "keywords": [
                "Subspace learning",
                "Clustering",
                "Rank restriction",
                "Sparse optimization"
            ],
            "author": [
                "Zhenyue Zhang",
                "Yuqing Xia"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-084/18-084.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence of Gaussian Belief Propagation Under General Pairwise Factorization: Connecting Gaussian MRF with Pairwise Linear Gaussian Model",
            "abstract": "Gaussian belief propagation (BP) is a low-complexity and distributed method for computing the marginal distributions of a high-dimensional joint Gaussian distribution. However, Gaussian BP is only guaranteed to converge in singly connected graphs and may fail to converge in loopy graphs. Therefore, convergence analysis is a core topic in Gaussian BP. Existing conditions for verifying the convergence of Gaussian BP are all tailored for one particular pairwise factorization of the distribution in Gaussian Markov random field (MRF) and may not be valid for another pairwise factorization. On the other hand, convergence conditions of Gaussian BP in pairwise linear Gaussian model are developed independently from those in Gaussian MRF, making the convergence results highly scattered with diverse settings. In this paper, the convergence condition of Gaussian BP is investigated under a general pairwise factorization, which includes Gaussian MRF and pairwise linear Gaussian model as special cases. Upon this, existing convergence conditions in Gaussian MRF are extended to any pairwise factorization. Moreover, the newly established link between Gaussian MRF and pairwise linear Gaussian model reveals an easily verifiable sufficient convergence condition in pairwise linear Gaussian model, which provides a unified criterion for assessing the convergence of Gaussian BP in multiple applications. Numerical examples are presented to corroborate the theoretical results of this paper.",
            "keywords": [
                "Gaussian belief propagation",
                "convergence analysis",
                "Gaussian Markov random     field",
                "pairwise linear Gaussian model"
            ],
            "author": [
                "Bin Li",
                "Yik-Chung Wu"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-133/18-133.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Optimization for Policy Search via Online-Offline Experimentation",
            "abstract": "Online field experiments are the gold-standard way of evaluating changes to real-world interactive machine learning systems. Yet our ability to explore complex, multi-dimensional policy spaces—such as those found in recommendation and ranking problems—is often constrained by the limited number of experiments that can be run simultaneously. To alleviate these constraints, we augment online experiments with an offline simulator and apply multi-task Bayesian optimization to tune live machine learning systems. We describe practical issues that arise in these types of applications, including biases that arise from using a simulator and assumptions for the multi-task kernel. We measure empirical learning curves which show substantial gains from including data from biased offline experiments, and show how these learning curves are consistent with theoretical results for multi-task Gaussian process generalization. We find that improved kernel inference is a significant driver of multi-task generalization. Finally, we show several examples of Bayesian optimization efficiently tuning a live machine learning system by combining offline and online experiments.",
            "keywords": [
                "Bayesian optimization",
                "multi-task Gaussian process",
                "policy search"
            ],
            "author": [
                "Benjamin Letham",
                "Eytan Bakshy"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-225/18-225.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Characterizing the Sample Complexity of Pure Private Learners",
            "abstract": "Kasiviswanathan et al. (FOCS 2008) defined private learning as a combination of PAC learning and differential privacy. Informally, a private learner is applied to a collection of labeled individual information and outputs a hypothesis while preserving the privacy of each individual. Kasiviswanathan et al. left open the question of characterizing the sample complexity of private learners. We give a combinatorial characterization of the sample size sufficient and necessary to learn a class of concepts under pure differential privacy. This characterization is analogous to the well known characterization of the sample complexity of non-private learning in terms of the VC dimension of the concept class. We introduce the notion of probabilistic representation of a concept class, and our new complexity measure  corresponds to the size of the smallest probabilistic representation of the concept class. We show that any private learning algorithm for a concept class  with sample complexity  implies , and that there exists a private learning algorithm with sample complexity . We further demonstrate that a similar characterization holds for the database size needed for computing a large class of optimization problems under pure differential privacy, and also for the well studied problem of private data release.",
            "keywords": [
                "Differential privacy",
                "PAC learning"
            ],
            "author": [
                "Amos Beimel",
                "Kobbi Nissim",
                "Uri Stemmer"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-269/18-269.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robustifying Independent Component Analysis by Adjusting for Group-Wise Stationary Noise",
            "abstract": "We introduce coroICA, confounding-robust independent component analysis, a novel ICA algorithm which decomposes linearly mixed multivariate observations into independent components that are corrupted (and rendered dependent) by hidden group-wise stationary confounding. It extends the ordinary ICA model in a theoretically sound and explicit way to incorporate group-wise (or environment-wise) confounding. We show that our proposed general noise model allows to perform ICA in settings where other noisy ICA procedures fail. Additionally, it can be used for applications with grouped data by adjusting for different stationary noise within each group. Our proposed noise model has a natural relation to causality and we explain how it can be applied in the context of causal inference. In addition to our theoretical framework, we provide an efficient estimation procedure and prove identifiability of the unmixing matrix under mild assumptions. Finally, we illustrate the performance and robustness of our method on simulated data, provide audible and visual examples, and demonstrate the applicability to real-world scenarios by experiments on publicly available Antarctic ice core data as well as two EEG data sets. We provide a scikit-learn compatible pip-installable Python package coroICA as well as R and Matlab implementations accompanied by a documentation at https://sweichwald.de/coroICA/",
            "keywords": [
                "blind source separation",
                "causal inference",
                "confounding noise",
                "group analysis",
                "heterogeneous data",
                "independent component analysis",
                "non-stationary signal"
            ],
            "author": [
                "Niklas Pfister",
                "Sebastian Weichwald",
                "Peter Bühlmann",
                "Bernhard Schölkopf"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-399/18-399.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Collective Matrix Completion",
            "abstract": "Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries. Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system. However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one. In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution. Then, we relax the assumption of exponential family distribution for the noise. In this setting, we do not assume any specific model for the observations. The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.",
            "keywords": [],
            "author": [
                "Mokhtar Z. Alaya",
                "Olga Klopp"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-483/18-483.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Asymptotic and Finite-Time Optimality of Bayesian Predictors",
            "abstract": "The problem is that of sequential probability forecasting for finite-valued time series. The data is generated by an unknown probability distribution over the space of all one-way infinite sequences. Two settings are considered: the realizable and the non-realizable one. Assume first that the probability measure generating the sequence belongs to a given set  (realizable case), but the latter is completely arbitrary (uncountably infinite, without any structure given). It is shown that the minimax asymptotic average loss---which may be positive---is always attainable, and it is attained by a Bayesian predictor whose prior is discrete and concentrated on . Moreover, the finite-time loss of the Bayesian predictor is also optimal up to an additive  term (where  is the time step). This upper bound is complemented by a lower bound that goes to infinity but may do so arbitrarily slow. Passing to the non-realizable setting, let the probability measure generating the data be arbitrary, and consider the given set  as a set of experts to compete with. The goal is to minimize the regret with respect to the experts. It is shown that in this setting it is possible that all Bayesian strategies are strictly suboptimal even asymptotically. In other words, a sublinear regret may be attainable but the regret of every Bayesian predictor is linear. A very general recommendation for choosing a model can be made based on these results: it is better to take a model large enough to make sure it includes the process that generates the data, even if it entails positive asymptotic average loss, for otherwise any combination of predictors in the model class may be useless.",
            "keywords": [
                "sequence prediction",
                "Bayesian prediction",
                "complete-class theorems"
            ],
            "author": [
                "Daniil Ryabko"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-512/18-512.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Optimized Risk Scores",
            "abstract": "Risk scores are simple classification models that let users make quick risk predictions by adding and subtracting a few small numbers. These models are widely used in medicine and criminal justice, but are difficult to learn from data because they need to be calibrated, sparse, use small integer coefficients, and obey application-specific constraints. In this paper, we introduce a machine learning method to learn risk scores. We formulate the risk score problem as a mixed integer nonlinear program, and present a cutting plane algorithm to recover its optimal solution. We improve our algorithm with specialized techniques that generate feasible solutions, narrow the optimality gap, and reduce data-related computation. Our algorithm can train risk scores in a way that scales linearly in the number of samples in a dataset, and that allows practitioners to address application-specific constraints without parameter tuning or post-processing. We benchmark the performance of different methods to learn risk scores on publicly available datasets, comparing risk scores produced by our method to risk scores built using methods that are used in practice. We also discuss the practical benefits of our method through a real-world application where we build a customized risk score for ICU seizure prediction in collaboration with the Massachusetts General Hospital.",
            "keywords": [],
            "author": [
                "Berk Ustun",
                "Cynthia Rudin"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-615/18-615.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric Estimation of Probability Density Functions of Random Persistence Diagrams",
            "abstract": "Topological data analysis refers to a broad set of techniques that are used to make inferences about the shape of data. A popular topological summary is the persistence diagram. Through the language of random sets, we describe a notion of global probability density function for persistence diagrams that fully characterizes their behavior and in part provides a noise likelihood model. Our approach encapsulates the number of topological features and considers the appearance or disappearance of those near the diagonal in a stable fashion. In particular, the structure of our kernel individually tracks long persistence features, while considering those near the diagonal as a collective unit. The choice to describe short persistence features as a group reduces computation time while simultaneously retaining accuracy. Indeed, we prove that the associated kernel density estimate converges to the true distribution as the number of persistence diagrams increases and the bandwidth shrinks accordingly. We also establish the convergence of the mean absolute deviation estimate, defined according to the bottleneck metric. Lastly, examples of kernel density estimation are presented for typical underlying datasets as well as for virtual electroencephalographic data related to cognition.",
            "keywords": [],
            "author": [
                "Vasileios Maroulas",
                "Joshua L Mike",
                "Christopher Oballe"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-618/18-618.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximation Algorithms for Stochastic Clustering",
            "abstract": "We consider stochastic settings for clustering, and develop provably-good approximation algorithms for a number of these notions. These algorithms yield better approximation ratios compared to the usual deterministic clustering setting. Additionally, they offer a number of advantages including clustering which is fairer and has better long-term behavior for each user. In particular, they ensure that every user is guaranteed to get good service (on average). We also complement some of these with impossibility results.",
            "keywords": [
                "clustering",
                "k-center",
                "k-median",
                "lottery"
            ],
            "author": [
                "David G. Harris",
                "Shi Li",
                "Thomas Pensyl",
                "Aravind Srinivasan",
                "Khoa Trinh"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-716/18-716.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Guarantees for a Class of Non-convex and Non-smooth Optimization Problems",
            "abstract": "We consider the problem of finding critical points of functions that are non-convex and non-smooth. Studying a fairly broad class of such problems, we analyze the behavior of three gradient-based methods (gradient descent, proximal update, and Frank-Wolfe update). For each of these methods, we establish rates of convergence for general problems, and also prove faster rates for continuous sub-analytic functions. We also show that our algorithms can escape strict saddle points for a class of non-smooth functions, thereby generalizing known results for smooth functions. Our analysis leads to a simplification of the popular CCCP algorithm, used for optimizing functions that can be written as a difference of two convex functions. Our simplified algorithm retains all the convergence properties of CCCP, along with a significantly lower cost per iteration. We illustrate our methods and theory via applications to the problems of best subset selection, robust estimation, mixture density estimation, and shape-from-shading reconstruction.",
            "keywords": [],
            "author": [
                "Koulik Khamaru",
                "Martin J. Wainwright"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-762/18-762.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantifying Uncertainty in Online Regression Forests",
            "abstract": "Accurately quantifying uncertainty in predictions is essential for the deployment of machine learning algorithms in critical applications where mistakes are costly. Most approaches to quantifying prediction uncertainty have focused on settings where the data is static, or bounded. In this paper, we investigate methods that quantify the prediction uncertainty in a streaming setting, where the data is potentially unbounded. We propose two meta-algorithms that produce prediction intervals for online regression forests of arbitrary tree models; one based on conformal prediction, and the other based on quantile regression. We show that the approaches are able to maintain specified error rates, with constant computational cost per example and bounded memory usage. We provide empirical evidence that the methods outperform the state-of-the-art in terms of maintaining error guarantees, while being an order of magnitude faster. We also investigate how the algorithms are able to recover from concept drift.",
            "keywords": [
                "Online learning",
                "Uncertainty",
                "Decision Trees"
            ],
            "author": [
                "Theodore Vasiloudis",
                "Gianmarco De Francisci Morales",
                "Henrik Boström"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-006/19-006.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SimpleDet: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition",
            "abstract": "Object detection and instance recognition play a central role in many AI applications like autonomous driving, video surveillance and medical image analysis. However, training object detection models on large scale datasets remains computationally expensive and time consuming. This paper presents an efficient and open source object detection framework called SimpleDet which enables the training of state-of-the-art detection models on consumer grade hardware at large scale. SimpleDet covers a wide range of models including both high-performance and high-speed ones. SimpleDet is well-optimized for both low precision training and distributed training and achieves 70% higher throughput for the Mask R-CNN detector compared with existing frameworks. Codes, examples and documents of SimpleDet can be found at https://github.com/tusimple/simpledet.",
            "keywords": [
                "Object Detection",
                "Instance Recognition",
                "Distributed Training"
            ],
            "author": [
                "Yuntao Chen",
                "Chenxia Han",
                "Yanghao Li",
                "Zehao Huang",
                "Yi Jiang",
                "Naiyan Wang",
                "Zhaoxiang Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-205/19-205.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GraSPy: Graph Statistics in Python",
            "abstract": "We introduce graspy, a Python library devoted to statistical inference, machine learning, and visualization of random graphs and graph populations. This package provides flexible and easy-to-use algorithms for analyzing and understanding graphs with a sklearn compliant API. graspy can be downloaded from Python Package Index (PyPi), and is released under the Apache 2.0 open-source license. The documentation and all releases are available at https://neurodata.io/graspy.",
            "keywords": [
                "Python",
                "graph analysis",
                "network analysis",
                "statistical inference"
            ],
            "author": [
                "Jaewon Chung",
                "Benjamin D. Pedigo",
                "Eric W. Bridgeford",
                "Bijan K. Varjavand",
                "Hayden S. Helm",
                "Joshua T. Vogelstein"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-490/19-490.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Convergence Rates for Convex Distributed Optimization in Networks",
            "abstract": "This work proposes a theoretical analysis of distributed optimization of convex functions using a network of computing units. We investigate this problem under two communication schemes (centralized and decentralized) and four classical regularity assumptions: Lipschitz continuity, strong convexity, smoothness, and a combination of strong convexity and smoothness. Under the decentralized communication scheme, we provide matching upper and lower bounds of complexity along with algorithms achieving this rate up to logarithmic constants. For non-smooth objective functions, while the dominant term of the error is in , the structure of the communication network only impacts a second-order term in , where  is time. In other words, the error due to limits in communication resources decreases at a fast rate even in the case of non-strongly convex objective functions. Such a convergence rate is achieved by the novel multi-step primal-dual (MSPD) algorithm. Under the centralized communication scheme, we show that the naive distribution of standard optimization algorithms is optimal for smooth objective functions, and provide a simple yet efficient algorithm called distributed randomized smoothing (DRS) based on a local smoothing of the objective function for non-smooth functions. We then show that DRS is within a  multiplicative factor of the optimal convergence rate, where  is the underlying dimension.",
            "keywords": [
                "distributed optimization",
                "convex optimization"
            ],
            "author": [
                "Kevin Scaman",
                "Francis Bach",
                "Sébastien Bubeck",
                "Yin Tat Lee",
                "Laurent Massoulié"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-543/19-543.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning by Unsupervised Nonlinear Diffusion",
            "abstract": "This paper proposes and analyzes a novel clustering algorithm, called learning by unsupervised nonlinear diffusion (LUND), that combines graph-based diffusion geometry with techniques based on density and mode estimation.  LUND is suitable for data generated from mixtures of distributions with densities that are both multimodal and supported near nonlinear sets.  A crucial aspect of this algorithm is the use of time of a data-adapted diffusion process, and associated diffusion distances, as a scale parameter that is different from the local spatial scale parameter used in many clustering algorithms. We prove estimates for the behavior of diffusion distances with respect to this time parameter under a flexible nonparametric data model, identifying a range of times in which the mesoscopic equilibria of the underlying process are revealed, corresponding to a gap between within-cluster and between-cluster diffusion distances. These structures may be missed by the top eigenvectors of the graph Laplacian, commonly used in spectral clustering. This analysis is leveraged to prove sufficient conditions guaranteeing the accuracy of LUND.  We implement LUND and confirm its theoretical properties on illustrative data sets, demonstrating its theoretical and empirical advantages over both spectral and density-based clustering.",
            "keywords": [
                "unsupervised learning",
                "clustering",
                "spectral graph theory",
                "manifold     learning"
            ],
            "author": [
                "Mauro Maggioni",
                "James M. Murphy"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-873/18-873.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Kernel Regression with Coefficient-based $\\ell_q-$regularization",
            "abstract": "In this paper, we consider the regularized kernel regression with . In form, the algorithm minimizes a least-square loss functional adding a coefficient-based penalty term over a linear span of features generated by a kernel function. We study the asymptotic behavior of the algorithm under the framework of learning theory. The contribution of this paper is two-fold. First, we derive a tight bound on the empirical covering numbers of the related function space involved in the error analysis. Based on this result, we obtain the convergence rates for the regularized kernel regression which is the best so far. Second, for the case , we show that the regularization parameter plays a role as a trade-off between sparsity and convergence rates. Under some mild conditions, the fraction of non-zero coefficients in a local minimizer of the algorithm will tend to  at a polynomial decay rate when the sample size  becomes large. As the concerned algorithm is non-convex, we also discuss how to generate a minimizing sequence iteratively, which can help us to search a local minimizer around any initial point.",
            "keywords": [
                "Learning Theory",
                "Kernel Regression"
            ],
            "author": [
                "Lei Shi",
                "Xiaolin Huang",
                "Yunlong Feng",
                "Johan A.K. Suykens"
            ],
            "ref": "http://jmlr.org/papers/volume20/13-124/13-124.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Kernel Multiple Change-point Algorithm via Model Selection",
            "abstract": "We consider a general formulation of the multiple change-point problem, in which the data is assumed to belong to a set equipped with a positive semidefinite kernel. We propose a model-selection penalty allowing to select the number of change points in Harchaoui and Cappé's kernel-based change-point detection method. The model-selection penalty generalizes non-asymptotic model-selection penalties for the change-in-mean problem with univariate data. We prove a non-asymptotic oracle inequality for the resulting kernel-based change-point detection method, whatever the unknown number of change points, thanks to a concentration result for Hilbert-space valued random variables which may be of independent interest. Experiments on synthetic and real data illustrate the proposed method, demonstrating its ability to detect subtle changes in the distribution of data.",
            "keywords": [
                "model selection",
                "kernel methods",
                "change-point detection"
            ],
            "author": [
                "Sylvain Arlot",
                "Alain Celisse",
                "Zaid Harchaoui"
            ],
            "ref": "http://jmlr.org/papers/volume20/16-155/16-155.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets",
            "abstract": "Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the  and  norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable to sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. We also develop a screening method---called DPC (decomposition of convex set)---for nonnegative Lasso. Experiments on both synthetic and real data sets show that TLFre and DPC improve the efficiency of SGL and nonnegative Lasso by several orders of magnitude.",
            "keywords": [
                "Sparse",
                "Sparse Group Lasso",
                "Screening"
            ],
            "author": [
                "Jie Wang",
                "Zhanqiu Zhang",
                "Jieping Ye"
            ],
            "ref": "http://jmlr.org/papers/volume20/16-383/16-383.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Reduced PC-Algorithm: Improved Causal Structure Learning in Large Random Networks",
            "abstract": "We consider the task of estimating a high-dimensional directed acyclic graph, given observations from a linear structural equation model with arbitrary noise distribution. By exploiting properties of common random graphs, we develop a new algorithm that requires conditioning only on small sets of variables. The proposed algorithm, which is essentially a modified version of the PC-Algorithm, offers significant gains in both computational complexity and estimation accuracy. In particular, it results in more efficient and accurate estimation in large networks containing hub nodes, which are common in biological systems. We prove the consistency of the proposed algorithm, and show that it also requires a less stringent faithfulness assumption than the PC-Algorithm. Simulations in low and high-dimensional settings are used to illustrate these findings. An application to gene expression data suggests that the proposed algorithm can identify a greater number of clinically relevant genes than current methods.",
            "keywords": [
                "causal discovery",
                "directed acyclic graphs",
                "faithfulness",
                "high dimensions"
            ],
            "author": [
                "Arjun Sondhi",
                "Ali Shojaie"
            ],
            "ref": "http://jmlr.org/papers/volume20/17-601/17-601.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Convergence of Gaussian Belief Propagation with Nodes of Arbitrary Size",
            "abstract": "This paper is concerned with a multivariate extension of Gaussian message passing applied to pairwise Markov graphs (MGs). Gaussian message passing applied to pairwise MGs is often labeled Gaussian belief propagation (GaBP) and can be used to approximate the marginal of each variable contained in the pairwise MG. We propose a multivariate extension of GaBP (we label this GaBP-m) that can be used to estimate higher-dimensional marginals. Beyond the ability to estimate higher-dimensional marginals, GaBP-m exhibits better convergence behavior than GaBP, and can also provide more accurate univariate marginals. The theoretical results of this paper are based on an extension of the computation tree analysis conducted on univariate nodes to the multivariate case. The main contribution of this paper is the development of a convergence condition for GaBP-m that moves beyond the walk-summability of the precision matrix. Based on this convergence condition, we derived an upper bound for the number of iterations required for convergence of the GaBP-m algorithm. An upper bound on the dissimilarity between the approximate and exact marginal covariance matrices was established. We argue that GaBP-m is robust towards a certain change in variables, a property not shared by iterative solvers of linear systems, such as the conjugate gradient (CG) and preconditioned conjugate gradient (PCG) methods. The advantages of using GaBP-m over GaBP are also illustrated empirically.",
            "keywords": [
                "belief propagation",
                "Gaussian distributions",
                "higher-dimensional marginals",
                "preconditioning"
            ],
            "author": [
                "Francois Kamper",
                "Sarel J. Steel",
                "Johan A. du Preez"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-040/18-040.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Unsupervised Evaluation and Weighted Aggregation of Ranked Classification Predictions",
            "abstract": "Ensemble methods that aggregate predictions from a set of diverse base learners consistently outperform individual classifiers. Many such popular strategies have been developed in a supervised setting, where the sample labels have been provided to the ensemble algorithm. However, with the rising interest in unsupervised algorithms for machine learning and growing amounts of uncurated data, the reliance on labeled data precludes the application of ensemble algorithms to many real world problems. To this end we develop a new theoretical framework for ensemble learning, the Strategy for Unsupervised Multiple Method Aggregation (SUMMA), that estimates the performances of base classifiers and uses these estimates to form an ensemble classifier. SUMMA also generates an ensemble ranking of samples based on the confidence score it assigns to each sample. We illustrate the performance of SUMMA using a synthetic example as well as two real world problems.",
            "keywords": [
                "Ensemble learning",
                "Ensemble classifier",
                "Unsupervised Learning",
                "AUC"
            ],
            "author": [
                "Mehmet Eren Ahsen",
                "Robert M Vogel",
                "Gustavo A Stolovitzky"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-094/18-094.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Canonical Correlation Analysis",
            "abstract": "We study the sample complexity of canonical correlation analysis (CCA), i.e., the number of samples needed to estimate the population canonical correlation and directions up to arbitrarily small error. With mild assumptions on the data distribution, we show that in order to achieve -suboptimality in a properly defined measure of alignment between the estimated canonical directions and the population solution, we can solve the empirical objective exactly with  samples, where  is the singular value gap of the whitened cross-covariance matrix and  is an upper bound of the condition number of auto-covariance matrices. Moreover, we can achieve the same learning accuracy by drawing the same level of samples and solving the empirical objective approximately with a stochastic optimization algorithm; this algorithm is based on the shift-and-invert power iterations and only needs to process the dataset for  passes. Finally, we show that, given an estimate of the canonical correlation, the streaming version of the shift-and-invert power iterations achieves the same learning accuracy with the same level of sample complexity, by processing the data only once.",
            "keywords": [
                "Canonical correlation analysis",
                "sample complexity",
                "shift-and-invert precon-     ditioning"
            ],
            "author": [
                "Chao Gao",
                "Dan Garber",
                "Nathan Srebro",
                "Jialei Wang",
                "Weiran Wang"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-095/18-095.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Embarrassingly Parallel Inference for Gaussian Processes",
            "abstract": "Training Gaussian process-based models typically involves an  computational bottleneck due to inverting the covariance matrix. Popular methods for overcoming this matrix inversion problem cannot adequately model all types of latent functions, and are often not parallelizable. However, judicious choice of model structure can ameliorate this problem. A mixture-of-experts model that uses a mixture of  Gaussian processes offers modeling flexibility and opportunities for scalable inference. Our embarrassingly parallel algorithm combines low-dimensional matrix inversions with importance sampling to yield a flexible, scalable mixture-of-experts model that offers comparable performance to Gaussian process regression at a much lower computational cost.",
            "keywords": [
                "Gaussian process",
                "parallel inference",
                "machine learning"
            ],
            "author": [
                "Michael Minyi Zhang",
                "Sinead A. Williamson"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-374/18-374.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DBSCAN: Optimal Rates For Density-Based Cluster Estimation",
            "abstract": "We study the problem of optimal estimation of the density cluster tree under various smoothness assumptions on the underlying density. Inspired by the seminal work of Chaudhuri et al. (2014), we formulate a new notion of clustering consistency which is better suited to smooth densities, and derive minimax rates for cluster tree estimation under Hölder smooth densities of arbitrary degree. We present a computationally efficient, rate optimal cluster tree estimator based on simple extensions of the popular DBSCAN algorithm of Ester et al. (1996). Our procedure relies on kernel density estimators and returns a sequence of nested random geometric graphs whose connected components form a hierarchy of clusters. The resulting optimal rates for cluster tree estimation depend on the degree of smoothness of the underlying density and, interestingly, match the minimax rates for density estimation under the sup-norm loss. Our results complement and extend the analysis of the DBSCAN algorithm in Sriperumbudur and Steinwart (2012). Finally, we consider level set estimation and cluster consistency for densities with jump discontinuities. We demonstrate that the DBSCAN algorithm attains the minimax rate in terms of the jump size and sample size in this setting as well.",
            "keywords": [],
            "author": [
                "Daren Wang",
                "Xinyang Lu",
                "Alessandro Rinaldo"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-470/18-470.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Shared Subspace Models for Multi-Group Covariance Estimation",
            "abstract": "We develop a model-based method for evaluating heterogeneity among several  covariance matrices in the large , small  setting. This is done by assuming a spiked covariance model for each group and sharing information about the space spanned by the group-level eigenvectors. We use an empirical Bayes method to identify a low-dimensional subspace which explains variation across all groups and use an MCMC algorithm to estimate the posterior uncertainty of eigenvectors and eigenvalues on this subspace. The implementation and utility of our model is illustrated with analyses of high-dimensional multivariate gene expression.",
            "keywords": [],
            "author": [
                "Alexander M. Franks",
                "Peter Hoff"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-484/18-484.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals",
            "abstract": "We show that many machine learning goals can be expressed as “rate constraints” on a model's predictions. We study the problem of training non-convex models subject to these rate constraints (or other non-convex or non-differentiable constraints). In the non-convex setting, the standard approach of Lagrange multipliers may fail. Furthermore, if the constraints are non-differentiable, then one cannot optimize the Lagrangian with gradient-based methods. To solve these issues, we introduce a new “proxy-Lagrangian” formulation. This leads to an algorithm that, assuming access to an optimization oracle, produces a stochastic classifier by playing a two-player non-zero-sum game solving for what we call a semi-coarse correlated equilibrium, which in turn corresponds to an approximately optimal and feasible solution to the constrained optimization problem. We then give a procedure that shrinks the randomized solution down to a mixture of at most  deterministic solutions, given  constraints. This culminates in a procedure that can solve non-convex constrained optimization problems with possibly non-differentiable and non-convex constraints, and enjoys theoretical guarantees. We provide extensive experimental results covering a broad range of policy goals, including various fairness metrics, accuracy, coverage, recall, and churn.",
            "keywords": [
                "constrained optimization",
                "non-convex",
                "fairness",
                "churn",
                "swap regret"
            ],
            "author": [
                "Andrew Cotter",
                "Heinrich Jiang",
                "Maya Gupta",
                "Serena Wang",
                "Taman Narayan",
                "Seungil You",
                "Karthik Sridharan"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-616/18-616.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Automatic Smoothing for Generalized Additive Models",
            "abstract": "Generalized additive models (GAMs) are regression models wherein parameters of probability distributions depend on input variables through a sum of smooth functions, whose degrees of smoothness are selected by  regularization. Such models have become the de-facto standard nonlinear regression models when interpretability and flexibility are required, but reliable and fast methods for automatic smoothing in large data sets are still lacking. We develop a general methodology for automatically learning the optimal degree of  regularization for GAMs using an empirical Bayes approach. The smooth functions are penalized by hyper-parameters that are learned simultaneously by maximization of a marginal likelihood using an approximate expectation-maximization algorithm. The latter involves a double Laplace approximation at the E-step, and leads to an efficient M-step. Empirical analysis shows that the resulting algorithm is numerically stable, faster than the best existing methods and achieves state-of-the-art accuracy. For illustration, we apply it to an important and challenging problem in the analysis of extremal data.",
            "keywords": [
                "Automatic L2 Regularization",
                "Empirical Bayes",
                "Expectation-maximization     Algorithm",
                "Generalized Additive Model",
                "Laplace Approximation"
            ],
            "author": [
                "Yousra El-Bachir",
                "Anthony C.  Davison"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-659/18-659.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Overcomplete, Low Coherence Dictionaries with Linear Inference",
            "abstract": "Finding overcomplete latent representations of data has applications in data analysis, signal processing, machine learning, theoretical neuroscience and many other fields. In an overcomplete representation, the number of latent features exceeds the data dimensionality, which is useful when the data is undersampled by the measurements (compressed sensing or information bottlenecks in neural systems) or composed from multiple complete sets of linear features, each spanning the data space. Independent Components Analysis (ICA) is a linear technique for learning sparse latent representations, which typically has a lower computational cost than sparse coding, a linear generative model which requires an iterative, nonlinear inference step. While well suited for finding complete representations, we show that overcompleteness poses a challenge to existing ICA algorithms. Specifically, the coherence control used in existing ICA and other dictionary learning algorithms, necessary to prevent the formation of duplicate dictionary features, is ill-suited in the overcomplete case. We show that in the overcomplete case, several existing ICA algorithms have undesirable global minima that maximize coherence. We provide a theoretical explanation of these failures and, based on the theory, propose improved coherence control costs for overcomplete ICA algorithms. Further, by comparing ICA algorithms to the computationally more expensive sparse coding on synthetic data, we show that the limited applicability of overcomplete, linear inference can be extended with the proposed cost functions. Finally, when trained on natural images, we show that the coherence control biases the exploration of the data manifold, sometimes yielding suboptimal, coherent solutions. All told, this study contributes new insights into and methods for coherence control for linear ICA, some of which are applicable to many other nonlinear models.",
            "keywords": [
                "independent components analysis",
                "dictionary learning"
            ],
            "author": [
                "Jesse A. Livezey",
                "Alejandro F. Bujan",
                "Friedrich T. Sommer"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-703/18-703.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DataWig: Missing Value Imputation for Tables",
            "abstract": "With the growing importance of machine learning (ML) algorithms for practical applications, reducing data quality problems in ML pipelines has become a major focus of research. In many cases missing values can break data pipelines which makes completeness one of the most impactful data quality challenges. Current missing value imputation methods are focusing on numerical or categorical data and can be difficult to scale to datasets with millions of rows. We release DataWig, a robust and scalable approach for missing value imputation that can be applied to tables with heterogeneous data types, including unstructured text. DataWig combines deep learning feature extractors with automatic hyperparameter tuning. This enables users without a machine learning background, such as data engineers, to impute missing values with minimal effort in tables with more heterogeneous data types than supported in existing libraries, while requiring less glue code for feature engineering and offering more flexible modelling options. We demonstrate that DataWig compares favourably to existing imputation packages. Source code, documentation, and unit tests for this package are available at: https://github.com/awslabs/datawig",
            "keywords": [
                "missing value imputation",
                "deep learning"
            ],
            "author": [
                "Felix Biessmann",
                "Tammo Rukat",
                "Phillipp Schmidt",
                "Prathik Naidu",
                "Sebastian Schelter",
                "Andrey Taptunov",
                "Dustin Lange",
                "David Salinas"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-753/18-753.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "New Convergence Aspects of Stochastic Gradient Algorithms",
            "abstract": "The classical convergence analysis of SGD is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is violated for cases where the objective function is strongly convex. In Bottou et al. (2018), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. We show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime. We then move on to the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime in the case of diminished learning rate. It is well-known that SGD converges if a sequence of learning rates  satisfies  and . We show the convergence of SGD for strongly convex objective function without using bounded gradient assumption when  is a diminishing sequence and . In other words, we extend the current state-of-the-art class of learning rates satisfying the convergence of SGD.",
            "keywords": [
                "Stochastic Gradient Algorithms",
                "Asynchronous Stochastic Optimization",
                "SGD"
            ],
            "author": [
                "Lam M. Nguyen",
                "Phuong Ha Nguyen",
                "Peter Richtárik",
                "Katya Scheinberg",
                "Martin Takáč",
                "Marten van Dijk"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-759/18-759.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously",
            "abstract": "Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model  with a fixed coefficient vector ) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.",
            "keywords": [
                "Rashomon",
                "permutation importance",
                "conditional variable importance",
                "U-     statistics",
                "transparency"
            ],
            "author": [
                "Aaron Fisher",
                "Cynthia Rudin",
                "Francesca Dominici"
            ],
            "ref": "http://jmlr.org/papers/volume20/18-760/18-760.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning",
            "abstract": "Revealing latent structure in data is an active field of research, having introduced exciting technologies such as variational autoencoders and adversarial networks, and is essential to push machine learning towards unsupervised knowledge discovery. However, a major challenge is the lack of suitable benchmarks for an objective and quantitative evaluation of learned representations. To address this issue we introduce Morpho-MNIST, a framework that aims to answer: “to what extent has my model learned to represent specific factors of variation in the data? We extend the popular MNIST dataset by adding a morphometric analysis enabling quantitative comparison of trained models, identification of the roles of latent variables, and characterisation of sample diversity. We further propose a set of quantifiable perturbations to assess the performance of unsupervised and supervised methods on challenging tasks such as outlier detection and domain adaptation. Data and code are available at https://github.com/dccastro/Morpho-MNIST.",
            "keywords": [
                "representation learning",
                "generative models",
                "empirical evaluation",
                "disentangle-     ment"
            ],
            "author": [
                "Daniel C. Castro",
                "Jeremy Tan",
                "Bernhard Kainz",
                "Ender Konukoglu",
                "Ben Glocker"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-033/19-033.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Differentiable reservoir computing",
            "abstract": "Numerous results in learning and approximation theory have evidenced the importance of differentiability at the time of countering the curse of dimensionality. In the context of reservoir computing, much effort has been devoted in the last two decades to characterize the situations in which systems of this type exhibit the so-called echo state (ESP) and fading memory (FMP) properties. These important features amount, in mathematical terms, to the existence and continuity of global reservoir system solutions. That research is complemented in this paper with the characterization of the differentiability of reservoir filters for very general classes of discrete-time deterministic inputs. This constitutes a novel strong contribution to the long line of research on the ESP and the FMP and, in particular, links to existing research on the input-dependence of the ESP. Differentiability has been shown in the literature to be a key feature in the learning of attractors of chaotic dynamical systems. A Volterra-type series representation for reservoir filters with semi-infinite discrete-time inputs is constructed in the analytic case using Taylor's theorem and corresponding approximation bounds are provided. Finally, it is shown as a corollary of these results that any fading memory filter can be uniformly approximated by a finite Volterra series with finite memory.",
            "keywords": [
                "reservoir computing",
                "fading memory property",
                "finite memory",
                "echo state property",
                "dif-     ferentiable reservoir filter",
                "Volterra series representation",
                "state-space systems",
                "system identification"
            ],
            "author": [
                "Lyudmila Grigoryeva",
                "Juan-Pablo Ortega"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-150/19-150.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DPPy: DPP Sampling with Python",
            "abstract": "Determinantal point processes (DPPs) are specific probability distributions over clouds of points that are used as models and computational tools across physics, probability, statistics, and more recently machine learning. Sampling from DPPs is a challenge and therefore we present DPPy, a Python toolbox that gathers known exact and approximate sampling algorithms for both finite and continuous DPPs. The project is hosted on GitHub, and equipped with an extensive documentation.",
            "keywords": [
                "determinantal point processes",
                "sampling",
                "MCMC",
                "random matrices"
            ],
            "author": [
                "Guillaume Gautier",
                "Guillermo Polito",
                "Rémi Bardenet",
                "Michal Valko"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-179/19-179.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Neural Empirical Bayes",
            "abstract": "We unify kernel density estimation and empirical Bayes and address a set of problems in unsupervised machine learning with a geometric interpretation of those methods, rooted in the concentration of measure phenomenon. Kernel density is viewed symbolically as  where the random variable  is smoothed to , and empirical Bayes is the machinery to denoise in a least-squares sense, which we express as . A learning objective is derived by combining these two, symbolically captured by . Crucially, instead of using the original nonparametric estimators, we parametrize the energy function with a neural network denoted by ; at optimality,  where  is the density of . The optimization problem is abstracted as interactions of high-dimensional spheres which emerge due to the concentration of isotropic Gaussians. We introduce two algorithmic frameworks based on this machinery: (i) a “walk-jump” sampling scheme that combines Langevin MCMC (walks) and empirical Bayes (jumps), and (ii) a probabilistic framework for associative memory, called NEBULA, defined a la Hopfield by the gradient flow of the learned energy to a set of attractors. We finish the paper by reporting the emergence of very rich “creative memories” as attractors of NEBULA for highly-overlapping spheres.",
            "keywords": [
                "empirical Bayes",
                "unnormalized densities",
                "concentration of measure",
                "Langevin     MCMC"
            ],
            "author": [
                "Saeed Saremi",
                "Aapo Hyvärinen"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-216/19-216.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model Selection in Bayesian Neural Networks via Horseshoe Priors",
            "abstract": "The promise of augmenting accurate predictions provided by modern neural networks with well-calibrated predictive uncertainties has reinvigorated interest in Bayesian neural networks. However, model selection---even choosing the number of nodes---remains an open question. Poor choices can severely affect the quality of the produced uncertainties. In this paper, we explore continuous shrinkage priors, the horseshoe, and the regularized horseshoe distributions, for model selection in Bayesian neural networks. When placed over node pre-activations and coupled with appropriate variational approximations, we find that the strong shrinkage provided by the horseshoe is effective at turning off nodes that do not help explain the data. We demonstrate that our approach finds compact network structures even when the number of nodes required is grossly over-estimated. Moreover, the model selection over the number of nodes does not come at the expense of predictive or computational performance; in fact, we learn smaller networks with comparable predictive performance to current approaches. These effects are particularly apparent in sample-limited settings, such as small data sets and reinforcement learning.",
            "keywords": [
                "Bayesian Neural Networks",
                "Model Selection",
                "Horseshoe Priors",
                "Variational     Inference"
            ],
            "author": [
                "Soumya Ghosh",
                "Jiayu Yao",
                "Finale Doshi-Velez"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-236/19-236.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Log-concave sampling: Metropolis-Hastings algorithms are fast",
            "abstract": "We study the problem of sampling from a strongly log-concave density supported on , and prove a non-asymptotic upper bound on the mixing time of the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by simulating a Markov chain obtained from the discretization of an appropriate Langevin diffusion, combined with an accept-reject step. Relative to known guarantees for the unadjusted Langevin algorithm (ULA), our bounds show that the use of an accept-reject step in MALA leads to an exponentially improved dependence on the error-tolerance. Concretely, in order to obtain samples with TV error at most  for a density with condition number , we show that MALA requires  steps from a warm start, as compared to the  steps established in past work on ULA. We also demonstrate the gains of a modified version of MALA over ULA for weakly log-concave densities. Furthermore, we derive mixing time bounds for the Metropolized random walk (MRW) and obtain  mixing time slower than MALA. We provide numerical examples that support our theoretical findings, and demonstrate the benefits of Metropolis-Hastings adjustment for Langevin-type sampling algorithms.",
            "keywords": [
                "Log-concave sampling",
                "Langevin algorithms",
                "MCMC algorithms"
            ],
            "author": [
                "Raaz Dwivedi",
                "Yuansi Chen",
                "Martin J. Wainwright",
                "Bin Yu"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-306/19-306.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Why do deep convolutional networks generalize so poorly to small image transformations?",
            "abstract": "Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network's prediction. In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are sufficient to achieve the desired invariance. Specifically, we show that the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem, and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set. We discuss two possible solutions to this problem: (1) antialiasing the intermediate representations and (2) increasing data augmentation and show that they provide only a partial solution at best. Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.",
            "keywords": [
                "Machine Learning",
                "Deep Convolutional Neural Networks"
            ],
            "author": [
                "Aharon Azulay",
                "Yair Weiss"
            ],
            "ref": "http://jmlr.org/papers/volume20/19-519/19-519.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Low Complexity Algorithm with O(√T) Regret and O(1) Constraint Violations for Online Convex Optimization with Long Term Constraints",
            "abstract": "This paper considers online convex optimization over a complicated constraint set, which typically consists of multiple functional constraints and a set constraint.  The conventional online projection algorithm (Zinkevich, 2003) can be difficult to implement due to the potentially high computation complexity of the projection operation. In this paper, we relax the functional constraints by allowing them to be violated at each round but still requiring them to be satisfied in the long term. This type of relaxed online convex optimization (with long term constraints) was first considered in Mahdavi et al. (2012). That prior work proposes an algorithm to achieve  regret and  constraint violations for general problems and another algorithm to achieve an  bound for both regret and constraint violations when the constraint set can be described by a finite number of linear constraints.  A recent extension in Jenatton et al. (2016) can achieve  regret and  constraint violations where . The current paper proposes a new simple algorithm that yields improved performance in comparison to prior works. The new algorithm achieves an  regret bound with  constraint violations.",
            "keywords": [
                "online convex optimization",
                "long term constraints",
                "regret bounds",
                "constraint     violation bounds"
            ],
            "author": [
                "Hao Yu",
                "Michael J. Neely"
            ],
            "ref": "http://jmlr.org/papers/volume21/16-494/16-494.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Statistical Learning Approach to Modal Regression",
            "abstract": "This paper studies the nonparametric modal regression problem systematically from a statistical learning viewpoint.  Originally motivated by pursuing a theoretical understanding of the maximum correntropy criterion based regression (MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is essentially modal regression. We show that the nonparametric modal regression problem can be approached via the classical empirical risk minimization. Some efforts are then made to develop a framework for analyzing and implementing modal regression. For instance, the modal regression function is described, the modal regression risk is defined explicitly and its Bayes rule is characterized; for the sake of computational tractability, the surrogate modal regression risk, which is termed as the generalization risk in our study, is introduced. On the theoretical side, the excess modal regression risk, the excess generalization risk, the function estimation error, and the relations among the above three quantities are studied rigorously. It turns out that under mild conditions, function estimation consistency and convergence may be pursued in modal regression as in vanilla regression protocols such as mean regression, median regression, and quantile regression. On the practical side, the implementation issues of modal regression including the computational algorithm and the selection of the tuning parameters are discussed. Numerical validations on modal regression are also conducted to verify our findings.",
            "keywords": [
                "Nonparametric modal regression",
                "empirical risk minimization",
                "generalization     bounds",
                "kernel density estimation",
                "statistical learning theoryc 2020 Yunlong Feng",
                "Jun Fan"
            ],
            "author": [
                "Yunlong Feng",
                "Jun Fan",
                "Johan A.K. Suykens"
            ],
            "ref": "http://jmlr.org/papers/volume21/17-068/17-068.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Model of Fake Data in Data-driven Analysis",
            "abstract": "Data-driven analysis has been increasingly used in various decision making processes. With more sources, including reviews, news, and pictures, can now be used for data analysis, the authenticity of data sources is in doubt. While previous literature attempted to detect fake data piece by piece, in the current work, we try to capture the fake data sender's strategic behavior to detect the fake data source. Specifically, we model the tension between a data receiver who makes data-driven decisions and a fake data sender who benefits from misleading the receiver. We propose a potentially infinite horizon continuous time game-theoretic model with asymmetric information to capture the fact that the receiver does not initially know the existence of fake data and learns about it during the course of the game. We use point processes to model the data traffic, where each piece of data can occur at any discrete moment in a continuous time flow. We fully solve the model and employ numerical examples to illustrate the players' strategies and payoffs for insights. Specifically, our results show that maintaining some suspicion about the data sources and understanding that the sender can be strategic are very helpful to the data receiver. In addition, based on our model, we propose a methodology of detecting fake data that is complementary to the previous studies on this topic, which suggested various approaches on analyzing the data piece by piece. We show that after analyzing each piece of data, understanding a source by looking at the its whole history of pushing data can be helpful.",
            "keywords": [
                "data-driven analysis",
                "fake data",
                "game theory"
            ],
            "author": [
                "Xiaofan Li",
                "Andrew B. Whinston"
            ],
            "ref": "http://jmlr.org/papers/volume21/17-360/17-360.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Universal Latent Space Model Fitting for Large Networks with Edge Covariates",
            "abstract": "Latent space models are effective tools for statistical modeling and visualization of network data. Due to their close connection to generalized linear models, it is also natural to incorporate covariate information in them. The current paper presents two universal fitting algorithms for networks with edge covariates: one based on nuclear norm penalization and the other based on projected gradient descent. Both algorithms are motivated by maximizing the likelihood function for an existing class of inner-product models, and we establish their statistical rates of convergence for these models. In addition, the theory informs us that both methods work simultaneously for a wide range of different latent space models that allow latent positions to affect edge formation in flexible ways, such as distance models. Furthermore, the effectiveness of the methods is demonstrated on a number of real world network data sets for different statistical tasks, including community detection with and without edge covariates, and network assisted learning.",
            "keywords": [
                "community detection",
                "network with covariates",
                "non-convex optimization"
            ],
            "author": [
                "Zhuang Ma",
                "Zongming Ma",
                "Hongsong Yuan"
            ],
            "ref": "http://jmlr.org/papers/volume21/17-470/17-470.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Path-Based Spectral Clustering: Guarantees, Robustness to Outliers, and Fast Algorithms",
            "abstract": "We consider the problem of clustering with the longest-leg path distance (LLPD) metric, which is informative for elongated and irregularly shaped clusters. We prove finite-sample guarantees on the performance of clustering with respect to this metric when random samples are drawn from multiple intrinsically low-dimensional clusters in high-dimensional space, in the presence of a large number of high-dimensional outliers.  By combining these results with spectral clustering with respect to LLPD, we provide conditions under which the Laplacian eigengap statistic correctly determines the number of clusters for a large class of data sets,  and prove guarantees on the labeling accuracy of the proposed algorithm.  Our methods are quite general and provide performance guarantees for spectral clustering with any ultrametric.  We also introduce an efficient, easy to implement approximation algorithm for the LLPD based on a multiscale analysis of adjacency graphs, which allows for the runtime of LLPD spectral clustering to be quasilinear in the number of data points.",
            "keywords": [
                "unsupervised learning",
                "spectral clustering",
                "manifold learning",
                "fast al-     gorithms"
            ],
            "author": [
                "Anna Little",
                "Mauro Maggioni",
                "James M. Murphy"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-085/18-085.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Target Propagation in Recurrent Neural Networks",
            "abstract": "Recurrent Neural Networks have been widely used to process sequence data, but have long been criticized for their biological implausibility and training difficulties related to vanishing and exploding gradients. This paper presents a novel algorithm for training recurrent networks, target propagation through time (TPTT), that outperforms standard backpropagation through time (BPTT) on four out of the five problems used for testing. The proposed algorithm is initially tested and compared to BPTT on four synthetic time lag tasks, and its performance is also measured using the sequential MNIST data set. In addition, as TPTT uses target propagation, it allows for discrete nonlinearities and could potentially mitigate the credit assignment problem in more complex recurrent architectures.",
            "keywords": [
                "recurrent neural networks",
                "target propagation"
            ],
            "author": [
                "Nikolay Manchev",
                "Michael Spratling"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-141/18-141.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DESlib: A Dynamic ensemble selection library in Python",
            "abstract": "DESlib is an open-source python library providing the implementation of several dynamic selection techniques. The library is divided into three modules: (i) dcs, containing the implementation of dynamic classifier selection methods (DCS); (ii) des, containing the implementation of dynamic ensemble selection methods (DES); (iii) static, with the implementation of static ensemble techniques. The library is fully documented (documentation available online on Read the Docs), has a high test coverage (codecov.io) and is part of the scikit-learn-contrib supported projects. Documentation, code and examples can be found on its GitHub page: https://github.com/scikit-learn-contrib/DESlib.",
            "keywords": [
                "Multiple classifier systems",
                "Ensemble of Classifiers",
                "Dynamic classifier selection",
                "Dynamic ensemble selection",
                "Machine learning"
            ],
            "author": [
                "Rafael M. O. Cruz",
                "Luiz G. Hafemann",
                "Robert Sabourin",
                "George D. C. Cavalcanti"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-144/18-144.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Mahalanobis Distance in Functional Settings",
            "abstract": "Mahalanobis distance is a classical tool in multivariate analysis. We suggest here an extension of this concept to the case of functional data. More precisely, the proposed definition concerns those statistical problems where the sample data are real functions defined on a compact interval of the real line. The obvious difficulty for such a functional extension is the non-invertibility of the covariance operator in infinite-dimensional cases. Unlike other recent proposals, our definition is suggested and motivated in terms of the Reproducing Kernel Hilbert Space (RKHS) associated with the stochastic process that generates the data. The proposed distance is a true metric; it depends on a unique real smoothing parameter which is fully motivated in RKHS terms. Moreover, it shares some properties of its finite dimensional counterpart: it is invariant under isometries, it can be consistently estimated from the data and its sampling distribution is known under Gaussian models. An empirical study for two statistical applications, outliers detection and binary classification, is included. The results are quite competitive when compared to other recent proposals in the literature.",
            "keywords": [
                "Functional data",
                "Mahalanobis distance",
                "reproducing kernel Hilbert spaces",
                "kernel methods in statistics"
            ],
            "author": [
                "José R. Berrendero",
                "Beatriz Bueno-Larraz",
                "Antonio Cuevas"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-156/18-156.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Sufficient Dimension Reduction Through Sliced Inverse Regression",
            "abstract": "Sliced inverse regression is an effective paradigm that achieves the goal of dimension reduction through replacing  high dimensional covariates with a small number of linear combinations. It does not impose  parametric assumptions on the dependence structure. More importantly, such a reduction of dimension  is sufficient in that it  does  not cause  loss of  information. In this paper, we adapt the stationary sliced inverse regression to cope with the rapidly  changing environments. We propose to implement sliced inverse regression in an online fashion. This online  learner consists of two steps. In the first step we construct an online estimate for the  kernel matrix; in the second step we propose two online algorithms, one is motivated by the perturbation method and the other is originated from the gradient descent optimization, to perform   online singular value decomposition. The  theoretical properties of this   online learner are established. We demonstrate the numerical performance of  this  online learner  through simulations and  real world applications. All numerical studies confirm that this  online learner  performs as well as the batch learner.",
            "keywords": [
                "Dimension reduction",
                "online learning",
                "perturbation",
                "singular value decompo-     sition",
                "sliced inverse regression"
            ],
            "author": [
                "Zhanrui Cai",
                "Runze Li",
                "Liping Zhu"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-567/18-567.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information",
            "abstract": "We study the misclassification error for community detection in general heterogeneous stochastic block models (SBM) with noisy or partial label information. We establish a connection between the misclassification rate and the notion of minimum energy on the local neighborhood of the SBM. We develop an optimally weighted message passing algorithm to reconstruct labels for SBM based on the minimum energy flow and the eigenvectors of a certain Markov transition matrix. The general SBM considered in this paper allows for unequal-size communities, degree heterogeneity, and different connection probabilities among blocks. We focus on how to optimally weigh the message passing to improve misclassification.",
            "keywords": [
                "semi-supervised learning",
                "general stochastic block models",
                "misclassification",
                "weighted message passing",
                "minimum energy flow"
            ],
            "author": [
                "T. Tony Cai",
                "Tengyuan Liang",
                "Alexander Rakhlin"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-573/18-573.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Neyman-Pearson classification: parametrics and sample size requirement",
            "abstract": "The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers that achieve a minimal type II error while enforcing the prioritized type I error controlled under some user-specified level . This paradigm serves naturally in applications such as severe disease diagnosis and spam detection, where people have clear priorities among the two error types. Recently, Tong, Feng, and Li (2018) proposed a nonparametric umbrella algorithm that adapts all scoring-type classification methods (e.g., logistic regression, support vector machines, random forest) to respect the given type I error (i.e., conditional probability of classifying a class  observation as class  under the 0-1 coding) upper bound  with high probability, without specific  distributional assumptions on the features and the responses. Universal the umbrella algorithm is, it demands an explicit minimum sample size requirement on class , which is often the more scarce class, such as in rare disease diagnosis applications. In this work, we employ the parametric linear discriminant analysis (LDA) model and propose a new parametric thresholding algorithm, which does not need the minimum sample size requirements on class  observations and thus is suitable for small sample applications such as rare disease diagnosis. Leveraging both the existing nonparametric and the newly proposed parametric thresholding rules, we propose four LDA-based NP classifiers, for both low- and high-dimensional settings. On the theoretical front, we prove NP oracle inequalities for one proposed classifier, where the rate for excess type II error benefits from the explicit parametric model assumption. Furthermore, as NP classifiers involve a sample splitting step of class  observations,  we construct a new adaptive sample splitting scheme that can be applied universally to NP classifiers, and this adaptive strategy reduces the type II error of these classifiers. The proposed NP classifiers are implemented in the R package nproc.",
            "keywords": [],
            "author": [
                "Xin Tong",
                "Lucy Xia",
                "Jiacheng Wang",
                "Yang Feng"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-577/18-577.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized probabilistic principal component analysis of correlated data",
            "abstract": "Principal component analysis (PCA) is a well-established tool in machine learning and data processing. The principal axes in PCA were shown to be equivalent to the maximum marginal likelihood estimator of the factor loading matrix in a latent factor model for the observed data, assuming that the latent factors are independently distributed as standard normal distributions. However, the independence assumption may be unrealistic for many scenarios such as modeling multiple time series, spatial processes, and functional data, where the outcomes are correlated.  In this paper, we introduce the generalized probabilistic principal component analysis (GPPCA) to study the latent factor model for multiple correlated outcomes, where each factor is modeled by a Gaussian process. Our method generalizes the previous probabilistic formulation of PCA (PPCA)  by providing the closed-form maximum marginal likelihood estimator of the factor loadings and other parameters.  Based on the explicit expression of the precision matrix in the marginal likelihood that we derived, the number of the computational operations is linear to the number of output variables. Furthermore, we also provide the closed-form expression of the marginal likelihood when   other covariates are included in the mean structure. We highlight the advantage of GPPCA in terms of the practical relevance, estimation accuracy and computational convenience.  Numerical studies of simulated and real data confirm the excellent finite-sample performance of the proposed approach.",
            "keywords": [
                "Gaussian process",
                "maximum marginal likelihood estimator",
                "kernel method",
                "principal component analysis"
            ],
            "author": [
                "Mengyang Gu",
                "Weining Shen"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-595/18-595.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On lp-Support Vector Machines and Multidimensional Kernels",
            "abstract": "In this paper, we extend the  methodology developed for Support Vector Machines (SVM) using the -norm  (-SVM)  to the more general case of  -norms with  (-SVM).  We derive second order cone formulations for the resulting dual and primal problems.  The concept of kernel function, widely applied in -SVM,  is extended to the more general case of -norms with  by defining a new operator called multidimensional kernel. This object gives rise to reformulations of dual problems, in a transformed space of the original data, where the dependence on the original data always appear as homogeneous polynomials. We adapt known solution algorithms to efficiently solve the primal and dual resulting problems and some computational experiments on real-world datasets are presented showing rather good behavior in terms of the accuracy of -SVM with .",
            "keywords": [
                "Support Vector Machines",
                "Kernel functions"
            ],
            "author": [
                "Victor Blanco",
                "Justo Puerto",
                "Antonio M. Rodriguez-Chia"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-601/18-601.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Perturbation Bounds for Procrustes, Classical Scaling, and Trilateration, with Applications to Manifold Learning",
            "abstract": "One of the common tasks in unsupervised learning is dimensionality reduction, where the goal is to find meaningful low-dimensional structures hidden in high-dimensional data.  Sometimes referred to as manifold learning, this problem is closely related to the problem of localization, which aims at embedding a weighted graph into a low-dimensional Euclidean space.  Several methods have been proposed for localization, and also manifold learning. Nonetheless, the robustness property of most of them is little understood. In this paper, we obtain perturbation bounds for classical scaling and trilateration, which are then applied to derive performance bounds for Isomap, Landmark Isomap, and Maximum Variance Unfolding.  A new perturbation bound for procrustes analysis plays a key role.",
            "keywords": [],
            "author": [
                "Ery Arias-Castro",
                "Adel Javanmard",
                "Bruno Pelletier"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-720/18-720.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Practical Locally Private Heavy Hitters",
            "abstract": "We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error and running time -- TreeHist and Bitstogram. In both algorithms, server running time is  and user running time is , hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring  server time and  user time. With a typically large number of participants in local algorithms (in the millions), this reduction in time complexity, in particular at the user side, is crucial for making locally private heavy hitters algorithms usable in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code.",
            "keywords": [
                "Differential privacy",
                "local differential privacy",
                "heavy hitters",
                "histograms"
            ],
            "author": [
                "Raef Bassily",
                "Kobbi Nissim",
                "Uri Stemmer",
                "Abhradeep Thakurta"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-786/18-786.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Expectation Propagation as a Way of Life: A Framework for Bayesian Inference on Partitioned Data",
            "abstract": "A common divide-and-conquer approach for Bayesian computation with big data is to partition the data, perform local inference for each piece separately, and combine the results to obtain a global posterior approximation. While being conceptually and computationally appealing, this method involves the problematic need to also split the prior for the local inferences; these weakened priors may not provide enough regularization for each separate computation, thus eliminating one of the key advantages of Bayesian methods. To resolve this dilemma while still retaining the generalizability of the underlying local inference method, we apply the idea of expectation propagation (EP) as a framework for distributed Bayesian inference. The central idea is to iteratively update approximations to the local likelihoods given the state of the other approximations and the prior. The present paper has two roles: we review the steps that are needed to keep EP algorithms numerically stable, and we suggest a general approach, inspired by EP, for approaching data partitioning problems in a way that achieves the computational benefits of parallelism while allowing each local update to make use of relevant information from the other sites. In addition, we demonstrate how the method can be applied in a hierarchical context to make use of partitioning of both data and parameters. The paper describes a general algorithmic framework, rather than a specific algorithm, and presents an example implementation for it.",
            "keywords": [],
            "author": [
                "Aki Vehtari",
                "Andrew Gelman",
                "Tuomas Sivula",
                "Pasi Jylänki",
                "Dustin Tran",
                "Swupnil Sahai",
                "Paul Blomstedt",
                "John P. Cunningham",
                "David Schiminovich",
                "Christian P. Robert"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-817/18-817.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Connecting Spectral Clustering to Maximum Margins and Level Sets",
            "abstract": "We study the connections between spectral clustering and the problems of maximum margin clustering, and estimation of the components of level sets of a density function. Specifically, we obtain bounds on the eigenvectors of graph Laplacian matrices in terms of the between cluster separation, and within cluster connectivity. These bounds ensure that the spectral clustering solution converges to the maximum margin clustering solution as the scaling parameter is reduced towards zero. The sensitivity of maximum margin clustering solutions to outlying points is well known, but can be mitigated by first removing such outliers, and applying maximum margin clustering to the remaining points. If outliers are identified using an estimate of the underlying probability density, then the remaining points may be seen as an estimate of a level set of this density function. We show that such an approach can be used to consistently estimate the components of the level sets of a density function under very mild assumptions.",
            "keywords": [
                "spectral clustering",
                "maximum margin clustering",
                "density clustering",
                "level     sets",
                "convergence",
                "asymptotics"
            ],
            "author": [
                "David P. Hofmeyr"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-850/18-850.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-Dimensional Interactions Detection with Sparse Principal Hessian Matrix",
            "abstract": "In statistical learning framework with regressions, interactions are the contributions to the response variable from the products of the explanatory variables. In high-dimensional problems, detecting interactions is challenging due to  combinatorial complexity and limited data information. We consider detecting interactions by exploring their connections with the principal Hessian matrix. Specifically, we propose a one-step synthetic approach for estimating the principal Hessian matrix by a penalized M-estimator. An alternating direction method of multipliers (ADMM) is proposed to efficiently solve the encountered regularized optimization problem. Based on the sparse estimator, we  detect the interactions by identifying its nonzero components. Our method directly targets at the interactions, and it requires no structural assumption on the hierarchy of the interactions effects.  We show that our estimator is theoretically valid, computationally efficient, and practically useful for detecting the interactions in a broad spectrum of scenarios.",
            "keywords": [],
            "author": [
                "Cheng Yong Tang",
                "Ethan X. Fang",
                "Yuexiao Dong"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-071/19-071.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergences of Regularized Algorithms and Stochastic Gradient Methods with Random Projections",
            "abstract": "We study the least-squares regression problem over a Hilbert space, covering nonparametric regression over a reproducing kernel Hilbert space as a special case.  We first investigate regularized algorithms adapted to a projection operator on a closed subspace of the Hilbert space. We prove convergence results with respect to variants of norms, under a capacity assumption on the hypothesis space and a regularity condition on the target function.  As a result, we obtain optimal rates for regularized algorithms with randomized sketches, provided that the sketch dimension is proportional to the effective dimension up to a logarithmic factor. As a byproduct, we obtain similar results for Nystr\\\"{o}m regularized algorithms.  Our results provide optimal, distribution-dependent rates that do not have any saturation effect for sketched/Nystr\\\"{o}m regularized algorithms, considering both the attainable and non-attainable cases, in the well-conditioned regimes. We then study stochastic gradient methods with projection over the subspace, allowing multi-pass over the data and minibatches, and we derive similar optimal statistical convergence results.",
            "keywords": [
                "kernel methods",
                "regularized algorithms",
                "stochastic gradient methods",
                "random     projection"
            ],
            "author": [
                "Junhong Lin",
                "Volkan Cevher"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-083/19-083.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Derivative-Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems",
            "abstract": "We study derivative-free methods for policy optimization over the class of linear policies. We focus on characterizing the convergence rate of these methods when applied to linear-quadratic systems, and study various settings of driving noise and reward feedback.  Our main theoretical result provides an explicit bound on the sample or evaluation complexity: we show that these methods are guaranteed to converge to within any pre-specified tolerance of the optimal policy with a number of zero-order evaluations that is an explicit polynomial of the error tolerance, dimension, and curvature properties of the problem.  Our analysis reveals some interesting differences between the settings of additive driving noise and random initialization, as well as the settings of one-point and two-point reward feedback. Our theory is corroborated by simulations of derivative-free methods in application to these systems. Along the way, we derive convergence rates for stochastic zero-order optimization algorithms when applied to a certain class of non-convex problems.",
            "keywords": [
                "Derivative-Free Optimization",
                "Linear Quadratic Control"
            ],
            "author": [
                "Dhruv Malik",
                "Ashwin Pananjady",
                "Kush Bhatia",
                "Koulik Khamaru",
                "Peter L. Bartlett",
                "Martin J. Wainwright"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-198/19-198.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unified Framework for Structured Graph Learning via Spectral Constraints",
            "abstract": "Graph learning from data is a canonical problem that has received substantial attention in the literature. Learning a structured graph is essential for interpretability and identification of the relationships among data. In general, learning a graph with a specific structure is an NP-hard combinatorial problem and thus designing a general tractable algorithm is challenging. Some useful structured graphs include connected, sparse, multi-component, bipartite, and regular graphs. In this paper, we introduce a unified framework for structured graph learning that combines Gaussian graphical model and spectral graph theory. We propose to convert combinatorial structural constraints into spectral constraints on graph matrices and develop an optimization framework based on block majorization-minimization to solve structured graph learning problem. The proposed algorithms are provably convergent and practically amenable for a number of graph based applications such as data clustering. Extensive numerical experiments with both synthetic and real data sets illustrate the effectiveness of the proposed algorithms. An open source R package containing the code for all the experiments is available at https://CRAN.R-project.org/package=spectralGraphTopology.",
            "keywords": [
                "Structured graph learning",
                "spectral graph theory",
                "Markov random field",
                "Gaussian graphical model",
                "Laplacian matrix",
                "clustering",
                "adjacency matrix",
                "bipartite struc-     ture"
            ],
            "author": [
                "Sandeep Kumar",
                "Jiaxi Ying",
                "José Vinícius de M. Cardoso",
                "Daniel P. Palomar"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-276/19-276.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing",
            "abstract": "We present GluonCV and GluonNLP, the deep learning toolkits for computer vision and natural language processing based on Apache MXNet (incubating). These toolkits provide state-of-the-art pre-trained models, training scripts, and training logs, to facilitate rapid prototyping and promote reproducible research. We also provide modular APIs with flexible building blocks to enable efficient customization. Leveraging the MXNet ecosystem, the deep learning models in GluonCV and GluonNLP can be deployed onto a variety of platforms with different programming languages. The Apache 2.0 license has been adopted by GluonCV and GluonNLP to allow for software distribution, modification, and usage.",
            "keywords": [
                "Machine Learning",
                "Deep Learning",
                "Apache MXNet",
                "Computer Vision"
            ],
            "author": [
                "Jian Guo",
                "He He",
                "Tong He",
                "Leonard Lausen",
                "Mu Li",
                "Haibin Lin",
                "Xingjian Shi",
                "Chenguang Wang",
                "Junyuan Xie",
                "Sheng Zha",
                "Aston Zhang",
                "Hang Zhang",
                "Zhi Zhang",
                "Zhongyue Zhang",
                "Shuai Zheng",
                "Yi Zhu"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-429/19-429.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Feature Screening via Componentwise Debiasing",
            "abstract": "Feature screening is a powerful tool in processing high-dimensional data. When the sample size N and the number of features p are both large, the implementation of classic screening methods can be numerically challenging. In this paper, we propose a distributed screening framework for big data setup. In the spirit of 'divide-and-conquer', the proposed framework expresses a correlation measure as a function of several component parameters, each of which can be distributively estimated using a natural U-statistic from data segments. With the component estimates aggregated, we obtain a final correlation estimate that can be readily used for screening features. This framework enables distributed storage and parallel computing and thus is computationally attractive. Due to the unbiased distributive estimation of the component parameters, the final aggregated estimate achieves a high accuracy that  is insensitive to the number of data segments m. Under mild conditions, we show that the aggregated correlation estimator is as efficient as the centralized estimator in terms of the probability convergence bound and the mean squared error rate; the corresponding screening procedure enjoys sure screening property for a wide range of correlation measures. The promising performances of the new method are supported by extensive numerical examples.",
            "keywords": [
                "Feature screening",
                "Big data",
                "Divide-and-conquer",
                "Aggregated correlation"
            ],
            "author": [
                "Xingxiang Li",
                "Runze Li",
                "Zhiming Xia",
                "Chen Xu"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-537/19-537.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lower Bounds for Testing Graphical Models: Colorings and Antiferromagnetic Ising Models",
            "abstract": "We study the identity testing problem in the context of spin systems or undirected graphical models, where it takes the following form: given the parameter specification of the model  and a sampling oracle for the distribution  of an unknown model , can we efficiently determine if the two models  and  are the same? We consider identity testing for both soft-constraint and hard-constraint systems. In particular, we prove hardness results in two prototypical cases, the Ising model and proper colorings, and explore whether identity testing is any easier than structure learning. For the ferromagnetic (attractive) Ising model, Daskalakis et al. (2018) presented a polynomial-time algorithm for identity testing. We prove hardness results in the antiferromagnetic (repulsive) setting in the same regime of parameters where structure learning is known to require a super-polynomial number of samples. Specifically, for -vertex graphs of maximum degree , we prove that if  (where  is the inverse temperature parameter), then there is no polynomial running time identity testing algorithm unless . In the hard-constraint setting, we present hardness results for identity testing for proper colorings. Our results are based on the presumed hardness of #BIS, the problem of (approximately) counting independent sets in bipartite graphs.",
            "keywords": [
                "distribution testing",
                "structure learning",
                "graphical models",
                "Ising model"
            ],
            "author": [
                "Ivona Bezáková",
                "Antonio Blanca",
                "Zongchen Chen",
                "Daniel Štefankovič",
                "Eric Vigoda"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-580/19-580.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Targeted Fused Ridge Estimation of Inverse Covariance Matrices from Multiple High-Dimensional Data Classes",
            "abstract": "We consider the problem of jointly estimating multiple inverse covariance matrices from high-dimensional data consisting of distinct classes. An -penalized maximum likelihood approach is employed. The suggested approach is flexible and generic, incorporating several other -penalized estimators as special cases. In addition, the approach allows specification of target matrices through which prior knowledge may be incorporated and which can stabilize the estimation procedure in high-dimensional settings. The result is a targeted fused ridge estimator that is of use when the precision matrices of the constituent classes are believed to chiefly share the same structure while potentially differing in a number of locations of interest. It has many applications in (multi)factorial study designs. We focus on the graphical interpretation of precision matrices with the proposed estimator then serving as a basis for integrative or meta-analytic Gaussian graphical modeling. Situations are considered in which the classes are defined by data sets and subtypes of diseases. The performance of the proposed estimator in the graphical modeling setting is assessed through extensive simulation experiments. Its practical usability is illustrated by the differential network modeling of 12 large-scale gene expression data sets of diffuse large B-cell lymphoma subtypes. The estimator and its related procedures are incorporated into the R-package rags2ridges.",
            "keywords": [],
            "author": [
                "Anders Ellern Bilgrau",
                "Carel F.W. Peeters",
                "Poul Svante Eriksen",
                "Martin Boegsted",
                "Wessel N. van Wieringen"
            ],
            "ref": "http://jmlr.org/papers/volume21/15-509/15-509.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A New Class of Time Dependent Latent Factor Models with Applications",
            "abstract": "In many applications, observed data are influenced by some combination of latent causes. For example, suppose sensors are placed inside a building to record responses such as temperature, humidity, power consumption and noise levels. These random, observed responses are typically affected by many unobserved, latent factors (or features) within the building such as the number of individuals, the turning on and off of electrical devices, power surges, etc. These latent factors are usually present for a contiguous period of time before disappearing; further, multiple factors could be present at a time. \n\nThis paper develops new probabilistic methodology and inference methods for random object generation influenced by latent features exhibiting temporal persistence. Every datum is associated with subsets of a potentially infinite number of hidden, persistent features that account for temporal dynamics in an observation. The ensuing class of dynamic models constructed by adapting the Indian Buffet Process — a probability measure on the space of random, unbounded binary matrices — finds use in a variety of applications arising in operations, signal processing, biomedicine, marketing, image analysis, etc. Illustrations using synthetic and real data are provided.",
            "keywords": [
                "Bayesian Nonparametrics",
                "Latent Factor Models"
            ],
            "author": [
                "Sinead A. Williamson",
                "Michael Minyi Zhang",
                "Paul Damien"
            ],
            "ref": "http://jmlr.org/papers/volume21/16-639/16-639.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the consistency of graph-based Bayesian semi-supervised learning and the scalability of sampling algorithms",
            "abstract": "This paper considers a Bayesian approach to graph-based semi-supervised learning. We show that if the graph parameters are suitably scaled, the graph-posteriors converge to a continuum limit as the size of the unlabeled data set grows. This consistency result has profound algorithmic implications: we prove that when consistency holds, carefully designed Markov chain Monte Carlo algorithms have a uniform spectral gap, independent of the number of unlabeled inputs. Numerical experiments illustrate and complement the theory.",
            "keywords": [
                "semi-supervised learning",
                "graph-based learning",
                "Markov chain Monte Carlo"
            ],
            "author": [
                "Nicolas Garcia Trillos",
                "Zachary Kaplan",
                "Thabo Samakhoana",
                "Daniel Sanz-Alonso"
            ],
            "ref": "http://jmlr.org/papers/volume21/17-698/17-698.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Maximum Separation Subspace in Sufficient Dimension Reduction with Categorical Response",
            "abstract": "Sufficient dimension reduction (SDR) is a very useful concept for exploratory analysis and data visualization in regression, especially when the number of covariates is large. Many SDR methods have been proposed for regression with a continuous response, where the central subspace (CS) is the target of estimation. Various conditions, such as the linearity condition and the constant covariance condition, are imposed so that these methods can estimate at least a portion of the CS. In this paper we study SDR for regression and discriminant analysis with categorical response. Motivated by the exploratory analysis and data visualization aspects of SDR,  we propose a new geometric framework to reformulate the SDR problem in terms of manifold optimization and introduce a new concept called Maximum Separation Subspace (MASES). The MASES naturally preserves the “sufficiency” in SDR without imposing additional conditions on the predictor distribution, and directly inspires a semi-parametric estimator. Numerical studies show MASES exhibits superior performance as compared with competing SDR methods in specific settings.",
            "keywords": [],
            "author": [
                "Xin Zhang",
                "Qing Mai",
                "Hui Zou"
            ],
            "ref": "http://jmlr.org/papers/volume21/17-788/17-788.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tensor Train Decomposition on TensorFlow (T3F)",
            "abstract": "Tensor Train decomposition is used across many branches of machine learning. We present T3F—a library for Tensor Train decomposition based on TensorFlow. T3F supports GPU execution, batch processing, automatic differentiation, and versatile functionality for the Riemannian optimization framework, which takes into account the underlying manifold structure to construct efficient optimization methods. The library makes it easier to implement machine learning papers that rely on the Tensor Train decomposition. T3F includes documentation, examples and 94% test coverage.",
            "keywords": [
                "tensor decomposition",
                "tensor train",
                "software",
                "gpu"
            ],
            "author": [
                "Alexander Novikov",
                "Pavel Izmailov",
                "Valentin Khrulkov",
                "Michael Figurnov",
                "Ivan Oseledets"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-008/18-008.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Nonbacktracking Bounds on the Influence",
            "abstract": "This paper develops deterministic upper and lower bounds on the influence measure in a network, more precisely, the expected number of nodes that a seed set can influence in the independent cascade model. In particular, our bounds exploit r-nonbacktracking walks and Fortuin-Kasteleyn-Ginibre (FKG) type inequalities, and are computed by message passing algorithms. Further, we provide parameterized versions of the bounds that control the trade-off between efficiency and accuracy. Finally, the tightness of the bounds is illustrated on various network models.",
            "keywords": [
                "Influence Estimation",
                "Nonbacktracking Walk",
                "Message Passing",
                "Social Net-     works"
            ],
            "author": [
                "Emmanuel Abbe",
                "Sanjeev Kulkarni",
                "Eun Jee Lee"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-112/18-112.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Provably robust estimation of modulo 1 samples of a smooth function with applications to phase unwrapping",
            "abstract": "Consider an unknown smooth function  , and assume we are given  noisy mod 1 samples of , i.e., , for  , where  denotes the noise. Given the samples , our goal is to recover smooth, robust estimates of the clean  samples . We formulate a natural approach for solving this problem, which works with angular embeddings of  the noisy mod 1 samples over the unit circle, inspired by the angular synchronization framework. This amounts to solving a smoothness regularized least-squares problem -- a quadratically constrained quadratic program (QCQP) -- where the variables are constrained to lie on the unit circle. Our proposed approach is based on solving its relaxation, which is a trust-region sub-problem and hence solvable efficiently. We provide theoretical guarantees demonstrating its robustness to noise for adversarial, as well as random Gaussian and Bernoulli noise models. To the best of our knowledge, these are the first such theoretical results for this problem. We demonstrate the robustness and efficiency of our proposed approach via extensive numerical simulations on synthetic data, along with a simple least-squares based solution for the unwrapping stage, that recovers the original samples of  (up to a global shift). It is shown to perform well at high levels of noise, when taking as input the denoised modulo  samples. Finally, we also consider two other approaches for denoising the modulo 1 samples that leverage tools from Riemannian optimization on manifolds, including a Burer-Monteiro approach for a semidefinite programming relaxation of our formulation. For the two-dimensional version of the problem, which has applications in synthetic aperture radar interferometry (InSAR), we are able to solve instances of real-world data with a million sample points in under 10 seconds, on a personal laptop.",
            "keywords": [],
            "author": [
                "Mihai Cucuringu",
                "Hemant Tyagi"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-143/18-143.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Complexity Analysis of the Primal Solutions for the Accelerated Randomized Dual Coordinate Ascent",
            "abstract": "Dual first-order methods are essential techniques for large-scale constrained convex optimization. However, when recovering the primal solutions, we need  iterations to achieve an -optimal primal solution when we apply an algorithm to the non-strongly convex dual problem with  iterations to achieve an -optimal dual solution, where  can be  or . In this paper, we prove that the iteration complexity of the primal solutions and dual solutions have the same  order of magnitude for the accelerated randomized dual coordinate ascent. When the dual function further satisfies the quadratic functional growth condition, by restarting the algorithm at any period, we establish the linear iteration complexity for both the primal solutions and dual solutions even if the condition number is unknown. When applied to the regularized empirical risk minimization problem, we prove the iteration complexity of  in both primal space and dual space, where  is the number of samples. Our result takes out the  factor compared with the methods based on smoothing/regularization or Catalyst reduction. As far as we know, this is the first time that the optimal  iteration complexity in the primal space is established for the dual coordinate ascent based stochastic algorithms. We also establish the accelerated linear complexity for some problems with nonsmooth loss, e.g., the least absolute deviation and SVM.",
            "keywords": [
                "accelerated randomized dual coordinate ascent",
                "restart at any period",
                "iteration     complexity",
                "primal solutions"
            ],
            "author": [
                "Huan Li",
                "Zhouchen Lin"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-425/18-425.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graph-Dependent Implicit Regularisation for Distributed Stochastic Subgradient Descent",
            "abstract": "We propose graph-dependent implicit regularisation strategies for synchronised distributed stochastic subgradient descent (Distributed SGD) for convex problems in multi-agent learning. Under the standard assumptions of convexity, Lipschitz continuity, and smoothness, we establish statistical learning rates that retain, up to logarithmic terms, single-machine serial statistical guarantees through implicit regularisation (step size tuning and early stopping) with appropriate dependence on the graph topology. Our approach avoids the need for explicit regularisation in  decentralised learning problems, such as adding constraints to the empirical risk minimisation rule. Particularly for distributed methods, the use of implicit regularisation allows the algorithm to remain simple, without projections or dual methods. To prove our results, we establish graph-independent generalisation bounds for Distributed SGD that match the single-machine serial SGD setting (using algorithmic stability), and we establish graph-dependent optimisation bounds that are of independent interest. We present numerical experiments to show that the qualitative nature of the upper bounds we derive can be representative of real behaviours.",
            "keywords": [
                "Distributed machine learning",
                "implicit regularisation",
                "generalisation bounds",
                "algorith-     mic stability"
            ],
            "author": [
                "Dominic Richards",
                "Patrick Rebeschini"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-638/18-638.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Noise Accumulation in High Dimensional Classification and Total Signal Index",
            "abstract": "Great attention has been paid to Big Data in recent years. Such data hold promise for scientific discoveries but also pose challenges to analyses. One potential challenge is noise accumulation. In this paper, we explore noise accumulation in high dimensional two-group classification. First, we revisit a previous assessment of noise accumulation with principal component analyses, which yields a different threshold for discriminative ability than originally identified. Then we extend our scope to its impact on classifiers developed with three common machine learning approaches---random forest, support vector machine, and boosted classification trees. We simulate four scenarios with differing amounts of signal strength to evaluate each method. After determining noise accumulation may affect the performance of these classifiers, we assess factors that impact it. We conduct simulations by varying sample size, signal strength, signal strength proportional to the number predictors, and signal magnitude with random forest classifiers. These simulations suggest that noise accumulation affects the discriminative ability of high-dimensional classifiers developed using common machine learning methods, which can be modified by sample size, signal strength, and signal magnitude. We developed the measure total signal index (TSI) to track the trends of total signal and noise accumulation.",
            "keywords": [
                "Noise Accumulation",
                "Classification",
                "High Dimensional",
                "Random Forest",
                "Asymptotic"
            ],
            "author": [
                "Miriam R. Elman",
                "Jessica Minnier",
                "Xiaohui Chang",
                "Dongseok Choi"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-117/19-117.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Discovery Toolbox: Uncovering causal relationships in Python",
            "abstract": "This paper presents a new open source Python framework for causal discovery from observational data and domain background knowledge, aimed at causal graph and causal mechanism modeling. The cdt package implements an end-to-end approach, recovering the direct dependencies (the skeleton of the causal graph) and the causal relationships between variables. It includes algorithms from the `Bnlearn' and `Pcalg' packages, together with algorithms for pairwise causal discovery such as ANM.",
            "keywords": [
                "Causal Discovery",
                "Graph recovery",
                "open source",
                "constraint-based methods",
                "score-based methods",
                "pairwise causality"
            ],
            "author": [
                "Diviyan Kalainathan",
                "Olivier Goudet",
                "Ritik Dutta"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-187/19-187.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Linear Non-Gaussian Causal Models in the Presence of Latent Variables",
            "abstract": "We consider the problem of learning causal models from observational data generated by linear non-Gaussian acyclic causal models with latent variables. Without considering the effect of latent variables, the inferred causal relationships among the observed variables are often wrong. Under faithfulness assumption, we propose a method to check whether there exists a causal path between any two observed variables. From this information, we can obtain the causal order among the observed variables. The next question is whether the causal effects can be uniquely identified as well. We show that causal effects among observed variables cannot be identified uniquely under mere assumptions of faithfulness and non-Gaussianity of exogenous noises. However, we are able to propose an efficient method that identifies the set of all possible causal effects that are compatible with the observational data. We present additional structural conditions on the causal graph under which causal effects among observed variables can be determined uniquely.  Furthermore, we provide necessary and sufficient graphical conditions for unique identification of the number of variables in the system. Experiments on synthetic data and real-world data show the effectiveness of our proposed algorithm for learning causal models.",
            "keywords": [
                "Causal Discovery",
                "Structural Equation Models",
                "Non-Gaussianity",
                "Latent Variables"
            ],
            "author": [
                "Saber Salehkaleybar",
                "AmirEmad Ghassami",
                "Negar Kiyavash",
                "Kun Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-260/19-260.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Bipartite Network Clustering",
            "abstract": "We study bipartite community detection in networks, or more generally the network  biclustering problem. We present a fast two-stage procedure based on spectral initialization followed by the application of a pseudo-likelihood  classifier twice. Under mild regularity conditions, we establish the weak consistency of the procedure (i.e., the convergence of the misclassification rate to zero) under a general bipartite stochastic block model. We show that the procedure is optimal in the sense that it achieves the optimal convergence rate that is achievable by a biclustering oracle, adaptively over the whole class, up to constants. This is further formalized by deriving a minimax lower bound over a class of biclustering problems.  The optimal rate we obtain sharpens some of the existing results and generalizes others to a wide regime of  average degree growth, from sparse networks with average degrees growing arbitrarily slowly  to fairly dense networks with average degrees of order . As a special case, we recover the known exact recovery threshold in the  regime of sparsity. To obtain the consistency result, as part of the provable version of the algorithm, we introduce a sub-block partitioning scheme that is also computationally attractive, allowing for distributed implementation of the algorithm without sacrificing optimality. The provable  algorithm is derived from a general class of pseudo-likelihood biclustering algorithms that employ simple EM type updates. We show the effectiveness of this general class  by numerical simulations.",
            "keywords": [],
            "author": [
                "Zhixin Zhou",
                "Arash A. Amini"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-299/19-299.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Switching Regression Models and Causal Inference in the Presence of Discrete Latent Variables",
            "abstract": "Given a response  and a vector  of  predictors, we investigate the problem of inferring direct causes of  among the vector . Models for  that use all of its causal covariates as predictors enjoy the property of being invariant across different environments or interventional settings. Given data from such environments, this property has been exploited for causal discovery. Here, we extend this inference principle to situations in which some (discrete-valued) direct causes of  are unobserved. Such cases naturally give rise to switching regression models. We provide sufficient conditions for the existence, consistency and asymptotic normality of the MLE in linear switching regression models with Gaussian noise, and construct a test for the equality of such models. These results allow us to prove that the proposed causal discovery method obtains asymptotic false discovery control under mild conditions. We provide an algorithm, make available code, and test our method on simulated data. It is robust against model violations and outperforms state-of-the-art approaches. We further apply our method to a real data set, where we show that it does not only output causal predictors, but also a process-based clustering of data points, which could be of additional interest to practitioners.",
            "keywords": [
                "causal discovery",
                "invariance",
                "switching regression models",
                "hidden Markov     models"
            ],
            "author": [
                "Rune Christiansen",
                "Jonas Peters"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-407/19-407.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Branch and Bound for Piecewise Linear Neural Network Verification",
            "abstract": "The success of Deep Learning and its potential use in many safety-critical applicationshas motivated research on formal verification of Neural Network (NN) models. In thiscontext, verification involves proving or disproving that an NN model satisfies certaininput-output properties. Despite the reputation of learned NN models as black boxes,and the theoretical hardness of proving useful properties about them, researchers havebeen successful in verifying some classes of models by exploiting their piecewise linearstructure and taking insights from formal methods such as Satisifiability Modulo Theory.However, these methods are still far from scaling to realistic neural networks. To facilitateprogress on this crucial area, we exploit the Mixed Integer Linear Programming (MIP) formulation of verification to propose a family of algorithms based on Branch-and-Bound (BaB). We show that our family contains previous verification methods as special cases.With the help of the BaB framework, we make three key contributions. Firstly, we identifynew methods that combine the strengths of multiple existing approaches, accomplishingsignificant performance improvements over previous state of the art. Secondly, we introducean effective branching strategy on ReLU non-linearities. This branching strategy allows usto efficiently and successfully deal with high input dimensional problems with convolutionalnetwork architecture, on which previous methods fail frequently.  Finally, we proposecomprehensive test data sets and benchmarks which includes a collection of previouslyreleased testcases. We use the data sets to conduct a thorough experimental comparison ofexisting and new algorithms and to provide an inclusive analysis of the factors impactingthe hardness of verification problems.",
            "keywords": [
                "Formal Verification",
                "Branch and Bound"
            ],
            "author": [
                "Rudy Bunel",
                "Jingyue Lu",
                "Ilker Turkaslan",
                "Philip H.S. Torr",
                "Pushmeet Kohli",
                "M. Pawan Kumar"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-468/19-468.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data",
            "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.",
            "keywords": [],
            "author": [
                "Puyudi Yang",
                "Jianbo Chen",
                "Cho-Jui Hsieh",
                "Jane-Ling Wang",
                "Michael I. Jordan"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-569/19-569.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamical Systems as Temporal Feature Spaces",
            "abstract": "Parametrised state space models in the form of recurrent networks are often used in machine learning to learn from data streams exhibiting temporal dependencies. To break the black box nature of such models it is important to understand the dynamical features of the input-driving time series that are formed in the state space. We propose a framework for rigorous analysis of such state representations in vanishing memory state space models such as echo state networks (ESN). In particular, we consider the state space a temporal feature space and the readout mapping from the state space a kernel machine operating in that feature space. We show that: (1) The usual ESN strategy of randomly generating input-to-state, as well as state coupling leads to shallow memory time series representations, corresponding to cross-correlation operator with fast exponentially decaying coefficients; (2) Imposing symmetry on dynamic coupling yields a constrained dynamic kernel matching the input time series with straightforward exponentially decaying motifs or exponentially decaying motifs of the highest frequency; (3) Simple ring (cycle) high-dimensional reservoir topology specified only through two free parameters can implement deep memory dynamic kernels with a rich variety of matching motifs. We quantify richness of feature representations imposed by dynamic kernels and demonstrate that for dynamic kernel associated with cycle reservoir topology, the kernel richness undergoes a phase transition close to the edge of stability.",
            "keywords": [
                "Recurrent Neural Network",
                "Echo State Network",
                "Dynamical Systems",
                "Time     Series"
            ],
            "author": [
                "Peter Tino"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-589/19-589.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Convex Parametrization of a New Class of Universal Kernel Functions",
            "abstract": "The accuracy and complexity of kernel learning algorithms is determined by the set of kernels over which it is able to optimize. An ideal set of kernels should: admit a linear parameterization (tractability); be dense in the set of all kernels (accuracy); and every member should be universal so that the hypothesis space is infinite-dimensional (scalability). Currently, there is no class of kernel that meets all three criteria - e.g. Gaussians are not tractable or accurate; polynomials are not scalable. We propose a new class that meet all three criteria - the Tessellated Kernel (TK) class. Specifically, the TK class: admits a linear parameterization using positive matrices; is dense in all kernels; and every element in the class is universal. This implies that the use of TK kernels for learning the kernel can obviate the need for selecting candidate kernels in algorithms such as SimpleMKL and parameters such as the bandwidth.  Numerical testing on soft margin Support Vector Machine (SVM) problems show that algorithms using TK kernels outperform other kernel learning algorithms and neural networks. Furthermore, our results show that when the ratio of the number of training data to features is high, the improvement of TK over MKL increases significantly.",
            "keywords": [
                "kernel functions",
                "multiple kernel learning",
                "semi-definite programming",
                "super-     vised learning"
            ],
            "author": [
                "Brendon K. Colbert",
                "Matthew M. Peet"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-594/19-594.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "pyts: A Python Package for Time Series Classification",
            "abstract": "pyts is an open-source Python package for time series classification. This versatile toolbox provides implementations of many algorithms published in the literature, preprocessing functionalities, and data set loading utilities. pyts relies on the standard scientific Python packages numpy, scipy, scikit-learn, joblib, and numba, and is distributed under the BSD-3-Clause license. Documentation contains installation instructions, a detailed user guide, a full API description, and concrete self-contained examples.",
            "keywords": [
                "time series",
                "classification",
                "machine learning"
            ],
            "author": [
                "Johann Faouzi",
                "Hicham Janati"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-763/19-763.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Skill Rating for Multiplayer Games. Introducing Hypernode Graphs and their Spectral Theory",
            "abstract": "We consider the skill rating problem for multiplayer games, that is how to infer player skills from game outcomes in multiplayer games. We formulate the problem as a minimization problem  where  is a positive semidefinite matrix and  a real-valued function, of which some entries are the skill values to be inferred and other entries are constrained by the game outcomes. We leverage graph-based semi-supervised learning (SSL) algorithms for this problem. We apply our algorithms on several data sets of multiplayer games and obtain very promising results compared to Elo Duelling (see Elo, 1978) and TrueSkill (see Herbrich et al., 2006).. As we leverage graph-based SSL algorithms and because games can be seen as relations between sets of players, we then generalize the approach. For this aim, we introduce a new finite model, called hypernode graph, defined to be a set of weighted binary relations between sets of nodes. We define Laplacians of hypernode graphs. Then, we show that the skill rating problem for multiplayer games can be formulated as  where  is the Laplacian of a hypernode graph constructed from a set of games.  From a fundamental perspective, we show that hypernode graph Laplacians are symmetric positive semidefinite matrices with constant functions in their null space. We show that problems on hypernode graphs can not be solved with graph constructions and graph kernels. We relate hypernode graphs to signed graphs showing that positive relations between groups can lead to negative relations between individuals.",
            "keywords": [
                "Hypergraphs",
                "Graph Laplacians",
                "Graph Kernels",
                "Spectral Learning",
                "Semi-     supervised Learning",
                "Multiplayer Games"
            ],
            "author": [
                "Thomas Ricatte",
                "Rémi Gilleron",
                "Marc Tommasi"
            ],
            "ref": "http://jmlr.org/papers/volume21/13-561/13-561.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ensemble Learning for Relational Data",
            "abstract": "We present a theoretical analysis framework for relational ensemble models. We show that ensembles of collective classifiers can improve predictions for graph data by reducing errors due to variance in both learning and inference. In addition, we propose a relational ensemble framework that combines a relational ensemble learning approach with a relational ensemble inference approach for collective classification. The proposed ensemble techniques are applicable for both single and multiple graph settings. Experiments on both synthetic and real-world data demonstrate the effectiveness of the proposed framework. Finally, our experimental results support the theoretical analysis and confirm that ensemble algorithms that explicitly focus on both learning and inference processes and aim at reducing errors associated with both, are the best performers.",
            "keywords": [
                "Ensemble learning",
                "relational ensemble",
                "collective classification",
                "collective     inference",
                "bias-variance decomposition",
                "relational machine learning"
            ],
            "author": [
                "Hoda Eldardiry",
                "Jennifer Neville",
                "Ryan A. Rossi"
            ],
            "ref": "http://jmlr.org/papers/volume21/14-368/14-368.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse and low-rank multivariate Hawkes processes",
            "abstract": "We consider the problem of unveiling the implicit network structure of node interactions (such as user interactions in a social network), based only on high-frequency timestamps. Our inference is based on the minimization of the least-squares loss associated with a multivariate Hawkes model, penalized by  and trace norm of the interaction tensor. We provide a first theoretical analysis for this problem, that includes sparsity and low-rank inducing penalizations. This result involves a new data-driven concentration inequality for matrix martingales in continuous time with observable variance, which is a result of independent interest and a broad range of possible applications since it extends to matrix martingales former results restricted to the scalar case. A consequence of our analysis is the construction of sharply tuned  and trace-norm penalizations, that leads to a data-driven scaling of the variability of information available for each users. Numerical experiments illustrate the significant improvements achieved by the use of such data-driven penalizations.",
            "keywords": [],
            "author": [
                "Emmanuel Bacry",
                "Martin Bompaire",
                "Stéphane Gaïffas",
                "Jean-Francois Muzy"
            ],
            "ref": "http://jmlr.org/papers/volume21/15-114/15-114.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Causal Networks via Additive Faithfulness",
            "abstract": "In this paper we introduce a statistical model, called additively faithful directed acyclic graph (AFDAG), for causal learning from observational data. Our approach is based on additive conditional independence (ACI), a recently proposed three-way statistical relation that shares many similarities with conditional independence but without resorting to multi-dimensional kernels. This distinct feature strikes a balance between a parametric model and a fully nonparametric model, which makes the proposed model attractive for handling large networks. We develop an estimator for AFDAG based on a linear operator that characterizes ACI, and establish the consistency and convergence rates of this estimator, as well as the uniform consistency of the estimated DAG. Moreover, we introduce a modified PC-algorithm to implement the estimating procedure efficiently, so that its complexity is determined by the level of sparseness rather than the dimension of the network. Through simulation studies we show that our method outperforms existing methods when commonly assumed conditions such as Gaussian or Gaussian copula distributions do not hold. Finally, the usefulness of AFDAG formulation is demonstrated through an application to a proteomics data set.",
            "keywords": [
                "additive conditional independence",
                "additive reproducing kernel Hilbert space",
                "directed acyclic graph",
                "global Markov property",
                "normalized additive conditional covariance     operator"
            ],
            "author": [
                "Kuang-Yao Lee",
                "Tianqi Liu",
                "Bing Li",
                "Hongyu Zhao"
            ],
            "ref": "http://jmlr.org/papers/volume21/16-252/16-252.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Expected Policy Gradients for Reinforcement Learning",
            "abstract": "We propose expected policy gradients (EPG), which unify stochastic policy gradients (SPG) and deterministic policy gradients (DPG) for reinforcement learning. Inspired by expected sarsa, EPG integrates (or sums) across actions when estimating the gradient, instead of relying only on the action in the sampled trajectory. For continuous action spaces, we first derive a practical result for Gaussian policies and quadratic critics and then extend it to a universal analytical method, covering a broad class of actors and critics, including Gaussian, exponential families, and policies with bounded support. For Gaussian policies, we introduce an exploration method that uses covariance proportional to the matrix exponential of the scaled Hessian of the critic with respect to the actions. For discrete action spaces, we derive a variant of EPG based on softmax policies. We also establish a new general policy gradient theorem, of which the stochastic and deterministic policy gradient theorems are special cases. Furthermore, we prove that EPG reduces the variance of the gradient estimates without requiring deterministic policies and with little computational overhead. Finally, we provide an extensive experimental evaluation of EPG and show that it outperforms existing approaches on multiple challenging control domains.",
            "keywords": [
                "policy gradients",
                "exploration",
                "bounded actions",
                "reinforcement learning"
            ],
            "author": [
                "Kamil Ciosek",
                "Shimon Whiteson"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-012/18-012.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-Dimensional Inference for Cluster-Based Graphical Models",
            "abstract": "Motivated by modern applications in which one constructs graphical models based on a very large number of features, this paper introduces a new class of cluster-based graphical models, in which variable clustering is applied as an initial step for reducing the dimension of the feature space. We employ model assisted clustering, in which the clusters contain features that are similar to the same unobserved latent variable. Two different cluster-based Gaussian graphical models are considered: the latent variable graph, corresponding to the graphical model associated with the unobserved latent variables, and the cluster-average graph, corresponding to the vector of features averaged over clusters. Our study reveals that likelihood based inference for the latent graph, not analyzed previously,  is analytically intractable. Our main contribution is the development and analysis of  alternative estimation and inference strategies, for  the precision matrix of an unobservable latent vector Z. We replace the likelihood of the data by an appropriate class of empirical risk functions, that can be specialized  to the latent graphical model and to the simpler, but under-analyzed, cluster-average graphical model. The estimators thus derived can be used  for inference on the graph structure, for instance  on edge strength or pattern recovery. Inference is based on the asymptotic limits of the entry-wise estimates of the precision matrices associated with the conditional independence  graphs under consideration. While taking the uncertainty induced by the clustering step into account, we establish  Berry-Esseen central limit theorems for the proposed estimators. It is noteworthy that, although the clusters are estimated adaptively from the data, the central limit theorems regarding the entries of the estimated graphs  are proved under the same conditions one would use if the clusters were known in advance.  As an illustration  of the usage of  these newly developed inferential tools, we show that they can be reliably used for recovery of the sparsity pattern of the graphs we study, under FDR control, which is verified via simulation studies and an fMRI data analysis. These experimental results confirm the theoretically established difference between the two graph structures. Furthermore, the data analysis suggests that the  latent variable graph, corresponding to  the unobserved cluster centers, can help provide more insight into the understanding of the brain connectivity networks relative to the simpler, average-based, graph.",
            "keywords": [],
            "author": [
                "Carson Eisenach",
                "Florentina Bunea",
                "Yang Ning",
                "Claudiu Dinicu"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-357/18-357.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GraKeL: A Graph Kernel Library in Python",
            "abstract": "The problem of accurately measuring the similarity between graphs is at the core of many applications in a variety of disciplines. Graph kernels have recently emerged as a promising approach to this problem. There are now many kernels, each focusing on different structural aspects of graphs. Here, we present GraKeL, a library that unifies several graph kernels into a common framework. The library is written in Python and adheres to the scikit-learn interface. It is simple to use and can be naturally combined with scikit-learn's modules to build a complete machine learning pipeline for tasks such as graph classification and clustering. The code is BSD licensed and is available at: https://github.com/ysig/GraKeL.",
            "keywords": [
                "graph similarity",
                "graph kernels",
                "scikit-learn"
            ],
            "author": [
                "Giannis Siglidis",
                "Giannis Nikolentzos",
                "Stratis Limnios",
                "Christos Giatsidis",
                "Konstantinos Skianis",
                "Michalis Vazirgiannis"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-370/18-370.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Conjugate Gradients for Kernel Machines",
            "abstract": "Regularized least-squares (kernel-ridge / Gaussian process) regression is a fundamental algorithm of statistics and machine learning. Because generic algorithms for the exact solution have cubic complexity in the number of datapoints, large datasets require to resort to approximations. In this work, the computation of the least-squares prediction is itself treated as a probabilistic inference problem. We propose a structured Gaussian regression model on the kernel function that uses projections of the kernel matrix to obtain a low-rank approximation of the kernel and the matrix. A central result is an enhanced way to use the method of conjugate gradients for the specific setting of least-squares regression as encountered in machine learning.",
            "keywords": [
                "Gaussian processes",
                "kernel methods",
                "low-rank approximation",
                "conjugate     gradients"
            ],
            "author": [
                "Simon Bartels",
                "Philipp Hennig"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-473/18-473.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Self-paced Multi-view Co-training",
            "abstract": "Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a 'draw without replacement' strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.",
            "keywords": [],
            "author": [
                "Fan Ma",
                "Deyu Meng",
                "Xuanyi Dong",
                "Yi Yang"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-794/18-794.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Asynchronous Stochastic Gradient-Push: Asymptotically Optimal and Network-Independent Performance for Strongly Convex Functions",
            "abstract": "We consider the standard model of distributed optimization of a  sum of functions , where node  in a network holds the function . We allow for a harsh network model characterized by asynchronous updates, message delays, unpredictable message losses, and directed communication among nodes.  In this setting, we analyze a modification of the Gradient-Push method for distributed optimization, assuming that (i) node  is capable of generating gradients of its function  corrupted by zero-mean bounded-support additive noise at each step, (ii)  is strongly convex, and (iii) each  has Lipschitz gradients. We show that our proposed  method asymptotically performs  as well as the best bounds on centralized  gradient descent that takes steps in the direction of the  sum of the noisy gradients of all the functions  at each step.",
            "keywords": [
                "distributed optimization"
            ],
            "author": [
                "Artin Spiridonoff",
                "Alex Olshevsky",
                "Ioannis Ch. Paschalidis"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-813/18-813.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exact Guarantees on the Absence of Spurious Local Minima for Non-negative Rank-1 Robust Principal Component Analysis",
            "abstract": "This work is concerned with the non-negative rank-1 robust principal component analysis (RPCA), where the goal is to recover the dominant non-negative principal components of a data matrix precisely, where a number of measurements could be grossly corrupted with sparse and arbitrary large noise. Most of the known techniques for solving the RPCA rely on convex relaxation methods by lifting the problem to a higher dimension, which significantly increase the number of variables. As an alternative, the well-known Burer-Monteiro approach can be used to cast the RPCA as a non-convex and non-smooth  optimization problem with a significantly smaller number of variables. In this work, we show that the low-dimensional formulation of the symmetric and asymmetric positive rank-1 RPCA based on the Burer-Monteiro approach has benign landscape, i.e., 1) it does not have any spurious local solution, 2) has a unique global solution, and 3) its unique global solution coincides with the true components. An implication of this result is that simple local search algorithms are guaranteed to achieve a zero global optimality gap when directly applied to the low-dimensional formulation. Furthermore, we provide strong deterministic and probabilistic guarantees for the exact recovery of the true principal components. In particular, it is shown that a constant fraction of the measurements could be grossly corrupted and yet they would not create any spurious local solution.",
            "keywords": [],
            "author": [
                "Salar Fattahi",
                "Somayeh Sojoudi"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-884/18-884.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kymatio: Scattering Transforms in Python",
            "abstract": "The wavelet scattering transform is an invariant and stable signal representation suitable for many signal processing and machine learning applications. We present the Kymatio software package, an easy-to-use, high-performance Python implementation of the scattering transform in 1D, 2D, and 3D that is compatible with modern deep learning frameworks, including PyTorch and TensorFlow/Keras. The transforms are implemented on both CPUs and GPUs, the latter offering a significant speedup over the former. The package also has a small memory footprint. Source code, documentation, and examples are available under a BSD license at https://www.kymat.io.",
            "keywords": [],
            "author": [
                "Mathieu Andreux",
                "Tomás Angles",
                "Georgios Exarchakis",
                "Roberto Leonarduzzi",
                "Gaspar Rochette",
                "Louis Thiry",
                "John Zarka",
                "Stéphane Mallat",
                "Joakim Andén",
                "Eugene Belilovsky",
                "Joan Bruna",
                "Vincent Lostanlen",
                "Muawiz Chaudhary",
                "Matthew J. Hirn",
                "Edouard Oyallon",
                "Sixin Zhang",
                "Carmine Cella",
                "Michael Eickenberg"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-047/19-047.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiparameter Persistence Landscapes",
            "abstract": "An important problem in the field of Topological Data Analysis is defining topological summaries which can be combined with traditional data analytic tools. In recent work Bubenik introduced the persistence landscape, a stable representation of persistence diagrams amenable to statistical analysis and machine learning tools. In this paper we generalise the persistence landscape to multiparameter persistence modules providing a stable representation of the rank invariant. We show that multiparameter landscapes are stable with respect to the interleaving distance and persistence weighted Wasserstein distance, and that the collection of multiparameter landscapes faithfully represents the rank invariant. Finally we provide example calculations and statistical tests to demonstrate a range of potential applications and how one can interpret the landscapes associated to a multiparameter module.",
            "keywords": [
                "Topological Data Analysis",
                "Multiparameter Persistence",
                "Persistence Land-     scapes",
                "Machine Learning"
            ],
            "author": [
                "Oliver Vipond"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-054/19-054.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Optimal Matching Methods for Causal Inference",
            "abstract": "We develop an encompassing framework for matching, covariate balancing, and doubly-robust methods for causal inference from observational data called generalized optimal matching (GOM). The framework is given by generalizing a new functional-analytical formulation of optimal matching, giving rise to the class of GOM methods, for which we provide a single unified theory to analyze tractability and consistency. Many commonly used existing methods are included in GOM and, using their GOM interpretation, can be extended to optimally and automatically trade off balance for variance and outperform their standard counterparts. As a subclass, GOM gives rise to kernel optimal matching (KOM), which, as supported by new theoretical and empirical results, is notable for combining many of the positive properties of other methods in one. KOM, which is solved as a linearly-constrained convex-quadratic optimization problem, inherits both the interpretability and model-free consistency of matching but can also achieve the -consistency of well-specified regression and the bias reduction and robustness of doubly robust methods. In settings of limited overlap, KOM enables a very transparent method for interval estimation for partial identification and robust coverage. We demonstrate this in examples with both synthetic and real data.",
            "keywords": [
                "Causal inference",
                "optimal covariate balance",
                "embeddings",
                "matching"
            ],
            "author": [
                "Nathan Kallus"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-120/19-120.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Unique Sharp Local Minimum in L1-minimization Complete Dictionary Learning",
            "abstract": "We study the problem of globally recovering a dictionary from a set of signals via -minimization. We assume that the signals are generated as i.i.d. random linear combinations of the  atoms from a complete reference dictionary , where the linear combination coefficients are from either a Bernoulli type model or exact sparse model. First, we obtain a necessary and sufficient norm condition for the reference dictionary  to be a sharp local minimum of the expected  objective function. Our result substantially extends that of Wu and Yu (2015) and allows the combination coefficient to be non-negative. Secondly, we obtain an explicit bound on the region within which the objective value of the reference dictionary is minimal. Thirdly, we show that the reference dictionary is the unique sharp local minimum, thus establishing the first known global property of -minimization dictionary learning. Motivated by the theoretical results, we introduce a perturbation based test to determine whether a dictionary is a sharp local minimum of the objective function. In addition, we also propose a new dictionary learning algorithm based on Block Coordinate Descent, called DL-BCD, which is guaranteed to decrease the obective function monotonically. Simulation studies show that DL-BCD has competitive performance in terms of recovery rate compared to other state-of-the-art dictionary learning algorithms when the reference dictionary is generated from random Gaussian matrices.",
            "keywords": [
                "dictionary learning"
            ],
            "author": [
                "Yu Wang",
                "Siqi Wu",
                "Bin Yu"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-169/19-169.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Community-Based Group Graphical Lasso",
            "abstract": "A new strategy for probabilistic graphical modeling is developed that draws parallels to community detection analysis. The method jointly estimates an undirected graph and homogeneous communities of nodes. The structure of the communities is taken into account when estimating the graph and at the same time, the structure of the graph is accounted for when estimating communities of nodes. The procedure uses a joint group graphical lasso approach with community detection-based grouping, such that some groups of edges co-occur in the estimated graph. The grouping structure is unknown and is estimated based on community detection algorithms. Theoretical derivations regarding graph convergence and sparsistency, as well as accuracy of community recovery are included, while the method's empirical performance is illustrated in an fMRI context, as well as with simulated examples.",
            "keywords": [],
            "author": [
                "Eugen Pircalabelu",
                "Gerda Claeskens"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-181/19-181.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Smoothed Nonparametric Derivative Estimation using Weighted Difference Quotients",
            "abstract": "Derivatives play an important role in bandwidth selection methods (e.g., plug-ins), data analysis and bias-corrected confidence intervals. Therefore, obtaining accurate derivative information is crucial. Although many derivative estimation methods exist, the majority require a fixed design assumption. In this paper, we propose an effective and fully data-driven framework to estimate the first and second order derivative in random design. We establish the asymptotic properties of the proposed derivative estimator, and also propose a fast selection method for the tuning parameters. The performance and flexibility of the method is illustrated via an extensive simulation study.",
            "keywords": [
                "derivative estimation",
                "asymptotic properties"
            ],
            "author": [
                "Yu Liu",
                "Kris De Brabanter"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-246/19-246.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "WONDER: Weighted One-shot Distributed Ridge Regression in High Dimensions",
            "abstract": "In many areas, practitioners need to analyze large data sets that challenge conventional single-machine computing. To scale up data analysis, distributed and parallel computing approaches are increasingly needed. Here we study a fundamental and highly important problem in this area: How to do ridge regression in a distributed computing environment? Ridge regression is an extremely popular method for supervised learning, and has several optimality properties, thus it is important to study. We study one-shot methods that construct weighted combinations of ridge regression estimators computed on each machine. By analyzing the mean squared error in a high-dimensional random-effects model where each predictor has a small effect, we discover several new phenomena. Infinite-worker limit: The distributed estimator works well for very large numbers of machines, a phenomenon we call 'infinite-worker limit'. Optimal weights: The optimal weights for combining local estimators sum to more than unity, due to the downward bias of ridge. Thus, all averaging methods are suboptimal. We also propose a new Weighted ONe-shot DistributEd Ridge regression algorithm (WONDER). We test WONDER in simulation studies and using the Million Song Dataset as an example. There it can save at least 100x in computation time, while nearly preserving test accuracy.",
            "keywords": [
                "distributed learning",
                "ridge regression",
                "high-dimensional statistics"
            ],
            "author": [
                "Edgar Dobriban",
                "Yue Sheng"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-277/19-277.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The weight function in the subtree kernel is decisive",
            "abstract": "Tree data are ubiquitous because they model a large variety of situations, e.g., the architecture of plants, the secondary structure of RNA, or the hierarchy of XML files. Nevertheless, the analysis of these non-Euclidean data is difficult per se. In this paper, we focus on the subtree kernel that is a convolution kernel for tree data introduced by Vishwanathan and Smola in the early 2000's. More precisely, we investigate the influence of the weight function from a theoretical perspective and in real data applications. We establish on a 2-classes stochastic model that the performance of the subtree kernel is improved when the weight of leaves vanishes, which motivates the definition of a new weight function, learned from the data and not fixed by the user as usually done. To this end, we define a unified framework for computing the subtree kernel from ordered or unordered trees, that is particularly suitable for tuning parameters. We show through eight real data classification problems the great efficiency of our approach, in particular for small data sets, which also states the high importance of the weight function. Finally, a visualization tool of the significant features is derived.",
            "keywords": [],
            "author": [
                "Romain Azaïs",
                "Florian Ingels"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-290/19-290.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Union of Low-Rank Tensor Spaces: Clustering and Completion",
            "abstract": "We consider the problem of clustering and completing a set of tensors with missing data that are drawn from a union of low-rank tensor spaces. In the clustering problem, given a partially sampled tensor data that is composed of a number of subtensors, each chosen from one of a certain number of unknown tensor spaces, we need to group the subtensors that belong to the same tensor space. We provide a geometrical analysis on the sampling pattern and subsequently derive the sampling rate that guarantees the correct clustering under some assumptions with high probability. Moreover, we investigate the fundamental conditions for finite/unique completability for the union of tensor spaces completion problem. Both deterministic and probabilistic conditions on the sampling pattern to ensure finite/unique completability are obtained. For both the clustering and completion problems, our tensor analysis provides significantly better bound than the bound given by the matrix analysis applied to any unfolding of the tensor data.",
            "keywords": [
                "Low-rank tensor completion"
            ],
            "author": [
                "Morteza Ashraphijuo",
                "Xiaodong Wang"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-347/19-347.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Representation Learning for Dynamic Graphs: A Survey",
            "abstract": "Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research.",
            "keywords": [
                "graph representation learning",
                "dynamic graphs",
                "knowledge graph embedding"
            ],
            "author": [
                "Seyed Mehran Kazemi",
                "Rishab Goel",
                "Kshitij Jain",
                "Ivan Kobyzev",
                "Akshay Sethi",
                "Peter Forsyth",
                "Pascal Poupart"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-447/19-447.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimation of a Low-rank Topic-Based Model for Information Cascades",
            "abstract": "We consider the problem of estimating the latent structure of a social network based on the observed information diffusion events, or cascades, where the observations for a given cascade consist of only the timestamps of infection for infected nodes but not the source of the infection. Most of the existing work on this problem has focused on estimating a diffusion matrix without any structural assumptions on it.  In this paper, we propose a novel model based on the intuition that an information is more likely to propagate among two nodes if they are interested in similar topics which are also prominent in the information content. In particular, our model endows each node with an influence vector (which measures how authoritative the node is on each topic) and a receptivity vector (which measures how susceptible the node is for each topic). We show how this node-topic structure can be estimated from the observed cascades, and prove the consistency of the estimator.  Experiments on synthetic and real data demonstrate the improved performance and better interpretability of our model compared to existing state-of-the-art methods.",
            "keywords": [
                "alternating gradient descent",
                "low-rank models",
                "information diffusion",
                "influence-     receptivity model",
                "network science"
            ],
            "author": [
                "Ming Yu",
                "Varun Gupta",
                "Mladen Kolar"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-496/19-496.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "(1 + epsilon)-class Classification: an Anomaly Detection Method for Highly Imbalanced or Incomplete Data Sets",
            "abstract": "Anomaly detection is not an easy problem since distribution of anomalous samples is unknown a priori. We explore a novel method that gives a trade-off possibility between one-class and two-class approaches, and leads to a better performance on anomaly detection problems with small or non-representative anomalous samples. The method is evaluated using several data sets and compared to a set of conventional one-class and two-class approaches.",
            "keywords": [
                "Anomaly Detection",
                "Imbalanced Data Sets",
                "Neural Networks",
                "One-class     Classification"
            ],
            "author": [
                "Maxim Borisyak",
                "Artem Ryzhikov",
                "Andrey Ustyuzhanin",
                "Denis Derkach",
                "Fedor Ratnikov",
                "Olga Mineeva"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-514/19-514.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Approximate MCMC Algorithms for the Horseshoe Prior",
            "abstract": "The horseshoe prior is frequently employed in Bayesian analysis of high-dimensional models, and has been shown to achieve minimax optimal risk properties when the truth is sparse. While optimization-based algorithms for the extremely popular Lasso and elastic net procedures can scale to dimension in the hundreds of thousands, algorithms for the horseshoe that use Markov chain Monte Carlo (MCMC) for computation are limited to problems an order of magnitude smaller. This is due to high computational cost per step and growth of the variance of time-averaging estimators as a function of dimension. We propose two new MCMC algorithms for computation in these models that have significantly improved performance compared to existing alternatives. One of the algorithms also approximates an expensive matrix product to give orders of magnitude speedup in high-dimensional applications. We prove guarantees for the accuracy of the approximate algorithm, and show that gradually decreasing the approximation error as the chain extends results in an exact algorithm. The scalability of the algorithm is illustrated in simulations with problem size as large as  observations and  predictors, and an application to a genome-wide association study with  and . The empirical results also show that the new algorithm yields estimates with lower mean squared error, intervals with better coverage, and elucidates features of the posterior that were often missed by previous algorithms in high dimensions, including bimodality of posterior marginals indicating uncertainty about which covariates belong in the model.",
            "keywords": [],
            "author": [
                "James Johndrow",
                "Paulo Orenstein",
                "Anirban Bhattacharya"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-536/19-536.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-dimensional Gaussian graphical models on network-linked data",
            "abstract": "Graphical models are commonly used to represent conditional dependence relationships between variables. There are multiple methods available for exploring them from high-dimensional data, but almost all of them rely on the assumption that the observations are independent and identically distributed.  At the same time, observations connected by a network are becoming increasingly common, and tend to violate these assumptions.  Here we develop a Gaussian graphical model for observations connected by a network with potentially different mean vectors, varying smoothly over the network. We propose an efficient estimation algorithm and demonstrate its effectiveness on both simulated and real data, obtaining meaningful and interpretable results on a statistics coauthorship network. We also prove that our method estimates both the inverse covariance matrix and the corresponding graph structure correctly under the assumption of network “cohesion”, which refers to the empirically observed phenomenon of network neighbors sharing similar traits.",
            "keywords": [
                "High-dimensional statistics",
                "Gaussian graphical model",
                "network analysis",
                "network cohesion"
            ],
            "author": [
                "Tianxi Li",
                "Cheng Qian",
                "Elizaveta Levina",
                "Ji Zhu"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-563/19-563.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Identifiability of Additive Noise Models Using Conditional Variances",
            "abstract": "This paper considers a new identifiability condition for additive noise models (ANMs) in which each variable is determined by an arbitrary Borel measurable function of its parents plus an independent error. It has been shown that ANMs are fully recoverable under some identifiability conditions, such as when all error variances are equal. However, this identifiable condition could be restrictive, and hence, this paper focuses on a relaxed identifiability condition that involves not only error variances, but also the influence of parents. This new class of identifiable ANMs does not put any constraints on the form of dependencies, or distributions of errors, and allows different error variances. It further provides a statistically consistent and computationally feasible structure learning algorithm for the identifiable ANMs based on the new identifiability condition. The proposed algorithm assumes that all relevant variables are observed, while it does not assume faithfulness or a sparse graph. Demonstrated through extensive simulated and real multivariate data is that the proposed algorithm successfully recovers directed acyclic graphs.",
            "keywords": [
                "Bayesian Network",
                "Causal Inference",
                "Directed Acyclic Graph",
                "Identifiability",
                "Structural Equation Modeling"
            ],
            "author": [
                "Gunwoong Park"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-664/19-664.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GADMM: Fast and Communication Efficient Framework for Distributed Machine Learning",
            "abstract": "When the data is distributed across multiple servers, lowering the communication cost between the servers (or workers) while solving the distributed learning problem is an important problem and is the focus of this paper. In particular, we propose a fast, and communication-efficient decentralized framework to solve the distributed machine learning (DML) problem. The proposed algorithm, Group Alternating Direction Method of Multipliers (GADMM) is based on the Alternating Direction Method of Multipliers (ADMM) framework. The key novelty in GADMM is that it solves the problem in a decentralized topology where at most half of the workers are competing for the limited communication resources at any given time. Moreover, each worker exchanges the locally trained model only with two neighboring workers, thereby training a global model with a lower amount of communication overhead in each exchange. We prove that GADMM converges to the optimal solution for convex loss functions, and numerically show that it converges faster and more communication-efficient than the state-of-the-art communication-efficient algorithms such as the Lazily Aggregated Gradient (LAG) and dual averaging, in linear and logistic regression tasks on synthetic and real datasets. Furthermore, we propose Dynamic GADMM (D-GADMM), a variant of GADMM, and prove its convergence under the time-varying network topology of the workers.",
            "keywords": [],
            "author": [
                "Anis Elgabli",
                "Jihong Park",
                "Amrit S. Bedi",
                "Mehdi Bennis",
                "Vaneet Aggarwal"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-718/19-718.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Player Bandits: The Adversarial Case",
            "abstract": "We consider a setting where multiple players sequentially choose among a common set of actions (arms). Motivated by an application to cognitive radio networks, we assume that players incur a loss upon colliding, and that communication between players is not possible. Existing approaches assume that the system is stationary. Yet this assumption is often violated in practice, e.g., due to signal strength fluctuations. In this work, we design the first multi-player Bandit algorithm that provably works in arbitrarily changing environments, where the losses of the arms may even be chosen by an adversary. This resolves an open problem posed by Rosenski et al. (2016).",
            "keywords": [
                "Multi-Armed Bandits",
                "Multi-Player Problems",
                "Online Learning",
                "Sequential Decision     Making"
            ],
            "author": [
                "Pragnya Alatur",
                "Kfir Y. Levy",
                "Andreas Krause"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-912/19-912.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Harmless Overfitting: Using Denoising Autoencoders in Estimation of Distribution Algorithms",
            "abstract": "Estimation of Distribution Algorithms (EDAs) are metaheuristics where learning a model and sampling new solutions replaces the variation operators recombination and mutation used in standard Genetic Algorithms. The choice of these models as well as the corresponding training processes are subject to the bias/variance tradeoff, also known as under- and overfitting: simple models cannot capture complex interactions between problem variables, whereas complex models are susceptible to modeling random noise. This paper suggests using Denoising Autoencoders (DAEs) as generative models within EDAs (DAE-EDA). The resulting DAE-EDA is able to model complex probability distributions. Furthermore, overfitting is less harmful, since DAEs overfit by learning the identity function. This overfitting behavior introduces unbiased random noise into the samples, which is no major problem for the EDA but just leads to higher population diversity. As a result, DAE-EDA runs for more generations before convergence and searches promising parts of the solution space more thoroughly. We study the performance of DAE-EDA on several combinatorial single-objective optimization problems. In comparison to the Bayesian Optimization Algorithm, DAE-EDA requires a similar number of evaluations of the objective function but is much faster and can be parallelized efficiently, making it the preferred choice especially for large and difficult optimization problems.",
            "keywords": [],
            "author": [
                "Malte Probst",
                "Franz Rothlauf"
            ],
            "ref": "http://jmlr.org/papers/volume21/16-543/16-543.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantile Graphical Models: a Bayesian Approach",
            "abstract": "Graphical models are ubiquitous tools to describe the interdependence between variables measured simultaneously such as large-scale gene or protein expression data. Gaussian graphical models (GGMs) are well-established tools for probabilistic exploration of dependence structures using precision matrices and they are generated under a multivariate normal joint distribution. However, they suffer from several shortcomings since they are based on Gaussian distribution assumptions. In this article, we propose a Bayesian quantile based approach for sparse estimation of graphs. We demonstrate that the resulting graph estimation is robust to outliers and applicable under general distributional assumptions. Furthermore, we develop efficient variational Bayes approximations to scale the methods for large data sets. Our methods are applied to a novel cancer proteomics data dataset where-in multiple proteomic antibodies are simultaneously assessed on tumor samples using reverse-phase protein arrays (RPPA) technology.",
            "keywords": [],
            "author": [
                "Nilabja Guha",
                "Veera Baladandayuthapani",
                "Bani K. Mallick"
            ],
            "ref": "http://jmlr.org/papers/volume21/17-231/17-231.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Memoryless Sequences for General Losses",
            "abstract": "One way to define the randomness of a fixed individual sequence is to ask how hard it is to predict relative to a given loss function.  A sequence is memoryless if, with respect to average loss, no continuous function can predict the next entry of the sequence from a finite window of previous entries better than a constant prediction.  For squared loss, memoryless sequences are known to have stochastic attributes analogous to those of truly random sequences.  In this paper, we address the question of how changing the loss function changes the set of memoryless sequences, and in particular, the stochastic attributes they possess.  For convex differentiable losses we establish that the statistic or property elicited by the loss determines the identity and stochastic attributes of the corresponding memoryless sequences.  We generalize these results to convex non-differentiable losses, under additional assumptions, and to non-convex Bregman divergences.  In particular, our results show that any Bregman divergence has the same set of memoryless sequences as squared loss.  We apply our results to price calibration in prediction markets.",
            "keywords": [
                "algorithmic randomness",
                "property elicitation"
            ],
            "author": [
                "Rafael Frongillo",
                "Andrew Nobel"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-042/18-042.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly",
            "abstract": "Bayesian Optimisation (BO) refers to a suite of techniques for global optimisation of expensive black box functions, which use introspective Bayesian models of the function to efficiently search for the optimum. While BO has been applied successfully in many applications, modern optimisation tasks usher in new challenges where conventional methods fail spectacularly. In this work, we present Dragonfly, an open source Python library for scalable and robust BO. Dragonfly incorporates multiple recently developed methods that allow BO to be applied in challenging real world settings; these include better methods for handling higher dimensional domains, methods for handling multi-fidelity evaluations when cheap approximations of an expensive function are available, methods for optimising over structured combinatorial spaces, such as the space of neural network architectures, and methods for handling parallel evaluations. Additionally, we develop new methodological improvements in BO for selecting the Bayesian model, selecting the acquisition function, and optimising over complex domains with different variable types and additional constraints. We compare Dragonfly to a suite of other packages and algorithms for global optimisation and demonstrate that when the above methods are integrated, they enable significant improvements in the performance of BO. The Dragonfly library is available at dragonfly.github.io.",
            "keywords": [],
            "author": [
                "Kirthevasan Kandasamy",
                "Karun Raju Vysyaraju",
                "Willie Neiswanger",
                "Biswajit Paria",
                "Christopher R. Collins",
                "Jeff Schneider",
                "Barnabas Poczos",
                "Eric P. Xing"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-223/18-223.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sequential change-point detection in high-dimensional Gaussian graphical models",
            "abstract": "High dimensional piecewise stationary graphical models represent a versatile class for modelling time varying networks arising in diverse application areas, including biology, economics, and social sciences. There has been recent work in offline detection and estimation of regime changes in the topology of sparse graphical models. However, the online setting remains largely unexplored, despite its high relevance to applications in sensor networks and other engineering monitoring systems, as well as financial markets. To that end, this work introduces a novel scalable online algorithm for detecting an unknown number of abrupt changes in the inverse covariance matrix of sparse Gaussian graphical models with small delay. The proposed algorithm is based upon monitoring the conditional log-likelihood of all nodes in the network and can be extended to a large class of continuous and discrete graphical models. We also investigate asymptotic properties of our procedure under certain mild regularity conditions on the graph size, sparsity level, number of samples, and pre- and post-changes in the topology of the network. Numerical works on both synthetic and real data illustrate the good performance of the proposed methodology both in terms of computational and statistical efficiency across numerous experimental settings.",
            "keywords": [
                "Sequential change-point detection",
                "Gaussian graphical models",
                "Pseudo-     likelihood",
                "Mini-batch update"
            ],
            "author": [
                "Hossein Keshavarz",
                "George Michaildiis",
                "Yves Atchade"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-410/18-410.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model-Preserving Sensitivity Analysis for Families of Gaussian Distributions",
            "abstract": "The accuracy of probability distributions inferred using machine-learning algorithms heavily depends on data availability and quality. In practical applications it is therefore fundamental to investigate the robustness of a statistical model to misspecification of some of its underlying probabilities. In the context of graphical models, investigations of robustness fall under the notion of sensitivity analyses. These analyses consist in varying some of the model's probabilities or parameters and then assessing how far apart the original and the varied distributions are.  However, for Gaussian graphical models, such variations usually make the original graph an incoherent  representation of the model's conditional independence structure. Here we develop an approach to sensitivity analysis which guarantees the original graph remains valid after any probability variation and we quantify the effect of such variations using different measures. To achieve this we take advantage of algebraic techniques to both concisely represent conditional independence and to provide a straightforward way of checking the validity of such relationships. Our methods are demonstrated to be robust and comparable to standard ones, which can break the conditional independence structure of the model, using an artificial example and a medical real-world application.",
            "keywords": [
                "Conditional independence",
                "Gaussian models",
                "Graphical models",
                "Kullback-     Leibler divergence"
            ],
            "author": [
                "Christiane Görgen",
                "Manuele Leonelli"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-668/18-668.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Effective Ways to Build and Evaluate Individual Survival Distributions",
            "abstract": "An accurate model of a patient’s individual survival distribution can help determine the appropriate treatment for terminal patients. Unfortunately, risk scores (for example from Cox Proportional Hazard models) do not provide survival probabilities, single-time probability models (for instance the Gail model, predicting 5 year probability) only provide for a single time point, and standard Kaplan-Meier survival curves provide only population averages for a large class of patients, meaning they are not specific to individual patients. This motivates an alternative class of tools that can learn a model that provides an individual survival distribution for each subject, which gives survival probabilities across all times, such as extensions to the Cox model, Accelerated Failure Time, an extension to Random Survival Forests, and Multi-Task Logistic Regression. This paper first motivates such 'individual survival distribution' (ISD) models, and explains how they differ from standard models. It then discusses ways to evaluate such models — namely Concordance, 1-Calibration, Integrated Brier score, and versions of L1-loss — then motivates and defines a novel approach, 'D-Calibration', which determines whether a model's probability estimates are meaningful. We also discuss how these measures differ, and use them to evaluate several ISD prediction tools over a range of survival data sets. We also provide a code base for all of these survival models and evaluation measures, at https://github.com/haiderstats/ISDEvaluation.",
            "keywords": [
                "Survival analysis",
                "risk model",
                "patient-specific survival prediction",
                "calibration"
            ],
            "author": [
                "Humza Haider",
                "Bret Hoehn",
                "Sarah Davis",
                "Russell Greiner"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-772/18-772.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Rate of Optimal Quantization and Application to the Clustering Performance of the Empirical Measure",
            "abstract": "We study the convergence rate of the optimal quantization for a probability measure sequence  on  converging in the Wasserstein distance in two aspects: the first one is the convergence rate of optimal quantizer  of  at level ; the other one is the convergence rate of the distortion function valued at , called the “performance” of . Moreover, we also study the mean performance of the optimal quantization for the empirical measure of a distribution  with finite second moment but possibly unbounded support. As an application, we show an upper bound with a convergence rate   of the mean performance for the empirical measure of the multidimensional normal distribution  and of distributions with hyper-exponential tails.  This extends the results from Biau et al. (2008) obtained for compactly supported distribution. We also derive an upper bound which is sharper in the quantization level  but suboptimal in  by applying results in Fournier and Guillin (2015).",
            "keywords": [
                "clustering performance",
                "convergence rate of optimal quantization",
                "distortion     function",
                "empirical measure"
            ],
            "author": [
                "Yating Liu",
                "Gilles Pagès"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-804/18-804.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Constrained Dynamic Programming and Supervised Penalty Learning Algorithms for Peak Detection in Genomic Data",
            "abstract": "Peak detection in genomic data involves segmenting counts of DNA sequence reads aligned to different locations of a chromosome. The goal is to detect peaks with higher counts, and filter out background noise with lower counts. Most existing algorithms for this problem are unsupervised heuristics tailored to patterns in specific data types. We propose a supervised framework for this problem, using optimal changepoint detection models with learned penalty functions. We propose the first dynamic programming algorithm that is guaranteed to compute the optimal solution to changepoint detection problems with constraints between adjacent segment mean parameters. Implementing this algorithm requires the choice of penalty parameter that determines the number of segments that are estimated. We show how the supervised learning ideas of Rigaill et al. (2013) can be used to choose this penalty. We compare the resulting implementation of our algorithm to several baselines in a benchmark of labeled ChIP-seq data sets with two different patterns (broad H3K36me3 data and sharp H3K4me3 data). Whereas baseline unsupervised methods only provide accurate peak detection for a single pattern, our supervised method achieves state-of-the-art accuracy in all data sets. The log-linear timings of our proposed dynamic programming algorithm make it scalable to the large genomic data sets that are now common. Our implementation is available in the PeakSegOptimal R package on CRAN.",
            "keywords": [
                "Non-convex",
                "constrained",
                "optimization",
                "changepoint"
            ],
            "author": [
                "Toby Dylan Hocking",
                "Guillem Rigaill",
                "Paul Fearnhead",
                "Guillaume Bourque"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-843/18-843.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Target–Aware Bayesian Inference: How to Beat Optimal Conventional Estimators",
            "abstract": "Standard approaches for Bayesian inference focus solely on approximating the posterior distribution. Typically, this approximation is, in turn, used to calculate expectations for one or more target functions—a computational pipeline that is inefficient when the target function(s) are known upfront. We address this inefficiency by introducing a framework for target–aware Bayesian inference (TABI) that estimates these expectations directly. While conventional Monte Carlo estimators have a fundamental limit on the error they can achieve for a given sample size, our TABI framework is able to breach this limit; it can theoretically produce arbitrarily accurate estimators using only three samples, while we show empirically that it can also breach this limit in practice. We utilize our TABI framework by combining it with adaptive importance sampling approaches and show both theoretically and empirically that the resulting estimators are capable of converging faster than the standard  Monte Carlo rate, potentially producing rates as fast as . We further combine our TABI framework with amortized inference methods, to produce a method for amortizing the cost of calculating expectations. Finally, we show how TABI can be used to convert any marginal likelihood estimator into a target aware inference scheme and demonstrate the substantial benefits this can yield.",
            "keywords": [
                "Bayesian inference",
                "Monte Carlo methods",
                "importance sampling",
                "adaptive sampling"
            ],
            "author": [
                "Tom Rainforth",
                "Adam Golinski",
                "Frank Wood",
                "Sheheryar Zaidi"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-102/19-102.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Probabilistic Symmetries and Invariant Neural Networks",
            "abstract": "Treating neural network inputs and outputs as random variables, we characterize the structure of neural networks that can be used to model data that are invariant or equivariant under the action of a compact group. Much recent research has been devoted to encoding invariance under symmetry transformations into neural network architectures, in an effort to improve the performance of deep neural networks in data-scarce, non-i.i.d., or unsupervised settings. By considering group invariance from the perspective of probabilistic symmetry, we establish a link between functional and probabilistic symmetry, and obtain generative functional representations of probability distributions that are invariant or equivariant under the action of a compact group. Our representations completely characterize the structure of neural networks that can be used to model such distributions and yield a general program for constructing invariant stochastic or deterministic neural networks. We demonstrate that examples from the recent literature are special cases, and develop the details of the general program for exchangeable sequences and arrays.",
            "keywords": [
                "probabilistic symmetry",
                "convolutional neural networks",
                "exchangeability",
                "neu-     ral architectures",
                "invariance",
                "equivariance",
                "sufficiency",
                "adequacy"
            ],
            "author": [
                "Benjamin Bloem-Reddy",
                "{ Yee Whye } Teh"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-322/19-322.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simultaneous Inference for Pairwise Graphical Models with Generalized Score Matching",
            "abstract": "Probabilistic graphical models provide a flexible yet parsimonious framework for modeling dependencies among nodes in networks.  There is a vast literature on parameter estimation and consistent model selection for graphical models.  However, in many of the applications, scientists are also interested in quantifying the uncertainty associated with the estimated parameters and selected models, which current literature has not addressed thoroughly.  In this paper, we propose a novel estimator for statistical inference on edge parameters in pairwise graphical models based on generalized Hyvarinen scoring rule.  Hyvarinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form, which is a common problem for graphical models, including Ising models and truncated Gaussian graphical models.  Our estimator allows us to perform statistical inference for general graphical models whereas the existing works mostly focus on statistical inference for Gaussian graphical models where finding normalizing constant is computationally tractable.  Under mild conditions that are typically assumed in the literature for consistent estimation, we prove that our proposed estimator is -consistent and asymptotically normal, which allows us to construct confidence intervals and build hypothesis tests for edge parameters. Moreover, we show how our proposed method can be applied to test hypotheses that involve a large number of model parameters simultaneously.  We illustrate validity of our estimator through extensive simulation studies on a diverse collection of data-generating processes.",
            "keywords": [
                "generalized score matching",
                "high-dimensional inference",
                "probabilistic graph-     ical models"
            ],
            "author": [
                "Ming Yu",
                "Varun Gupta",
                "Mladen Kolar"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-383/19-383.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast mixing of Metropolized Hamiltonian Monte Carlo: Benefits of multi-step gradients",
            "abstract": "Hamiltonian Monte Carlo (HMC) is a state-of-the-art Markov chain Monte Carlo sampling algorithm for drawing samples from smooth probability densities over continuous spaces. We study the variant most widely used in practice, Metropolized HMC with the Stormer-Verlet or leapfrog integrator, and make two primary contributions. First, we provide a non-asymptotic upper bound on the mixing time of the Metropolized HMC with explicit choices of step-size and number of leapfrog steps. This bound gives a precise quantification of the faster convergence of Metropolized HMC relative to simpler MCMC algorithms such as the Metropolized random walk, or Metropolized Langevin algorithm. Second, we provide a general framework for sharpening mixing time bounds of Markov chains initialized at a substantial distance from the target distribution over continuous spaces. We apply this sharpening device to the Metropolized random walk and Langevin algorithms, thereby obtaining improved mixing time bounds from a non-warm initial distribution.",
            "keywords": [],
            "author": [
                "Yuansi Chen",
                "Raaz Dwivedi",
                "Martin J. Wainwright",
                "Bin Yu"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-441/19-441.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Kernel Ridge Regression with Communications",
            "abstract": "This paper focuses on generalization performance analysis for distributed algorithms in the framework of learning theory. Taking distributed kernel ridge regression (DKRR) for example, we succeed in deriving its optimal learning rates in expectation and providing theoretically optimal ranges of the number of local processors. Due to the gap between theory and experiments, we also deduce optimal learning rates for DKRR in probability to essentially reflect the generalization performance and limitations of DKRR. Furthermore, we propose a communication strategy to improve the learning performance of DKRR and demonstrate the power of communications in DKRR via both theoretical assessments and numerical experiments.",
            "keywords": [
                "learning theory",
                "distributed learning",
                "kernel ridge regression"
            ],
            "author": [
                "Shao-Bo Lin",
                "Di Wang",
                "Ding-Xuan Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-592/19-592.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Nonparametric Parallelism Test",
            "abstract": "Testing the hypothesis of parallelism is a fundamental statistical problem arising from many applied sciences.  In this paper, we develop a nonparametric parallelism test for inferring whether the trends are parallel in treatment and control groups.  In particular, the proposed nonparametric parallelism test is a Wald type test based on a smoothing spline ANOVA (SSANOVA) model which can characterize the complex patterns of the data. We derive that the asymptotic null distribution of the test statistic is a Chi-square distribution, unveiling a new version of Wilks phenomenon. Notably, we establish the minimax sharp lower bound of the distinguishable rate for the nonparametric parallelism test by using the information theory, and further prove that the proposed test is minimax optimal. Simulation studies are conducted to investigate the empirical performance of the proposed test. DNA methylation and neuroimaging studies are presented to illustrate potential applications of the test. The software is available at https://github.com/BioAlgs/Parallelism.",
            "keywords": [
                "asymptotic distribution",
                "minimax optimality",
                "nonparametric inference",
                "parallelism test",
                "penalized least squares",
                "smoothing spline ANOVA"
            ],
            "author": [
                "Xin Xing",
                "Meimei Liu",
                "Ping Ma",
                "Wenxuan Zhong"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-800/19-800.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cornac: A Comparative Framework for Multimodal Recommender Systems",
            "abstract": "Cornac is an open-source Python framework for multimodal recommender systems. In addition to core utilities for accessing, building, evaluating, and comparing recommender models, Cornac is distinctive in putting emphasis on recommendation models that leverage auxiliary information in the form of a social network, item textual descriptions, product images, etc. Such multimodal auxiliary data supplement user-item interactions (e.g., ratings, clicks), which tend to be sparse in practice. To facilitate broad adoption and community contribution, Cornac is publicly available at https://github.com/PreferredAI/cornac, and it can be installed via Anaconda or the Python Package Index (pip). Not only is it well-covered by unit tests to ensure code quality, but it is also accompanied with a detailed documentation, tutorials, examples, and several built-in benchmarking data sets.",
            "keywords": [
                "comparison",
                "multimodality",
                "recommendation algorithms"
            ],
            "author": [
                "Aghiles Salah",
                "Quoc-Tuan Truong",
                "Hady W. Lauw"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-805/19-805.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "pyDML: A Python Library for Distance Metric Learning",
            "abstract": "pyDML is an open-source python library that provides a wide range of distance metric learning algorithms. Distance metric learning can be useful to improve similarity learning algorithms, such as the nearest neighbors classifier, and also has other applications, like dimensionality reduction. The pyDML package currently provides more than 20 algorithms, which can be categorized, according to their purpose, in: dimensionality reduction algorithms, algorithms to improve nearest neighbors or nearest centroids classifiers, information theory based algorithms or kernel based algorithms, among others. In addition, the library also provides some utilities for the visualization of classifier regions, parameter tuning and a stats website with the performance of the implemented algorithms. The package relies on the scipy ecosystem, it is fully compatible with scikit-learn, and is distributed under GPLv3 license. Source code and documentation can be found at https://github.com/jlsuarezdiaz/pyDML.",
            "keywords": [
                "Distance Metric Learning",
                "Classification",
                "Mahalanobis Distance",
                "Dimension-     ality"
            ],
            "author": [
                "Juan Luis Suárez",
                "Salvador García",
                "Francisco Herrera"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-864/19-864.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Loss Control with Rank-one Covariance Estimate for Short-term Portfolio Optimization",
            "abstract": "In short-term portfolio optimization (SPO), some financial characteristics like the expected return and the true covariance might be dynamic. Then there are only a small window size  of observations that are sufficiently close to the current moment and reliable to make estimations.  is usually much smaller than the number of assets , which leads to a typical undersampled problem. Worse still, the asset price relatives are not likely subject to any proper distributions. These facts violate the statistical assumptions of the traditional covariance estimates and invalidate their statistical efficiency and consistency in risk measurement. In this paper, we propose to reconsider the function of covariance estimates in the perspective of operators, and establish a rank-one covariance estimate in the principal rank-one tangent space at the observation matrix. Moreover, we propose a loss control scheme with this estimate, which effectively catches the instantaneous risk structure and avoids extreme losses. We conduct extensive experiments on  real-world benchmark daily or monthly data sets with stocks, funds and portfolios from diverse regional markets to show that the proposed method achieves state-of-the-art performance in comprehensive downside risk metrics and gains good investing incomes as well. It offers a novel perspective of rank-related approaches for undersampled estimations in SPO.",
            "keywords": [
                "rank-one covariance estimate",
                "short-term portfolio optimization",
                "undersam-     pled condition",
                "loss control"
            ],
            "author": [
                "Zhao-Rong Lai",
                "Liming Tan",
                "Xiaotian Wu",
                "Liangda Fang"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-959/19-959.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A General Framework for Consistent Structured Prediction with Implicit Loss Embeddings",
            "abstract": "We propose and analyze a novel theoretical and algorithmic framework for structured prediction. While so far the term has referred to discrete output spaces, here we consider more general settings, such as manifolds or spaces of probability measures. We define structured prediction as a problem where the output space lacks a vectorial structure. We identify and study a large class of loss functions that implicitly defines a suitable geometry on the problem. The latter is the key to develop an algorithmic framework amenable to a sharp statistical analysis and yielding efficient computations. When dealing with output spaces with infinite cardinality, a suitable implicit formulation of the estimator is shown to be crucial.",
            "keywords": [
                "Structured Prediction",
                "Statistical Learning Theory"
            ],
            "author": [
                "Carlo Ciliberto",
                "Lorenzo Rosasco",
                "Alessandro Rudi"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-097/20-097.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Joint Causal Inference from Multiple Contexts",
            "abstract": "The gold standard for discovering causal relations is by means of experimentation. Over the last decades, alternative methods have been proposed that can infer causal relations between variables from certain statistical patterns in purely observational data. We introduce Joint Causal Inference (JCI), a novel approach to causal discovery from multiple data sets from different contexts that elegantly unifies both approaches. JCI is a causal modeling framework rather than a specific algorithm, and it can be implemented using any causal discovery algorithm that can take into account certain background knowledge. JCI can deal with different types of interventions (e.g., perfect, imperfect, stochastic, etc.) in a unified fashion, and does not require knowledge of intervention targets or types in case of interventional data. We explain how several well-known causal discovery algorithms can be seen as addressing special cases of the JCI framework, and we also propose novel implementations that extend existing causal discovery methods for purely observational data to the JCI setting. We evaluate different JCI implementations on synthetic data and on flow cytometry protein expression data and conclude that JCI implementations can considerably outperform state-of-the-art causal discovery algorithms.",
            "keywords": [
                "causal discovery",
                "causal modeling",
                "causal inference",
                "observational and exper-     imental data",
                "interventions"
            ],
            "author": [
                "Joris M. Mooij",
                "Sara Magliacane",
                "Tom Claassen"
            ],
            "ref": "http://jmlr.org/papers/volume21/17-123/17-123.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "General Latent Feature Models for Heterogeneous Datasets",
            "abstract": "Latent variable models allow capturing the hidden structure underlying the data. In particular, feature allocation models represent each observation by a linear combination of latent variables. These models are often used to make predictions either for new observations or for missing information in the original data, as well as to perform exploratory data analysis. Although there is an extensive literature on latent feature allocation models for homogeneous datasets, where all the attributes that describe each object are of the same (continuous or discrete) type, there is no general framework for practical latent feature modeling for heterogeneous datasets. In this paper, we introduce a general Bayesian nonparametric latent feature allocation model suitable for heterogeneous datasets, where the attributes describing each object can be arbitrary combinations of real-valued, positive real-valued, categorical, ordinal and count variables. The proposed model presents several important properties. First, it is suitable for heterogeneous data while keeping the properties of conjugate models, which enables us to develop an inference algorithm that presents linear complexity with respect to the number of objects and attributes per MCMC iteration. Second, the Bayesian nonparametric component allows us to place a prior distribution on the number of features required to capture the latent structure in the data. Third, the latent features in the model are binary-valued, which facilitates the interpretability of the obtained latent features in exploratory data analysis. Finally, a software package, called GLFM toolbox, is made publicly available for other researchers to use and extend. It is available at https://ivaleram.github.io/GLFM/. We show the flexibility of the proposed model by solving both prediction and data analysis tasks on several real-world datasets.",
            "keywords": [],
            "author": [
                "Isabel Valera",
                "Melanie F. Pradier",
                "Maria Lomeli",
                "Zoubin Ghahramani"
            ],
            "ref": "http://jmlr.org/papers/volume21/17-328/17-328.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Gaussian Belief Propagation with Nodes of Arbitrary Size",
            "abstract": "Gaussian belief propagation (GaBP) is a message-passing algorithm that can be used to perform approximate inference on a pairwise Markov graph (MG) constructed from a multivariate Gaussian distribution in canonical parameterization. The output of GaBP is a set of approximate univariate marginals for each variable in the pairwise MG. An extension of GaBP (labeled GaBP-m), allowing for the approximation of higher-dimensional marginal distributions, was explored by Kamper et al. (2019). The idea is to create an MG in which each node is allowed to receive more than one variable. As in the univariate case, the multivariate extension does not necessarily converge in loopy graphs and, even if convergence occurs, is not guaranteed to provide exact inference. To address the problem of convergence, we consider a multivariate extension of the principle of node regularization proposed by Kamper et al. (2018). We label this algorithm slow GaBP-m (sGaBP-m), where the term 'slow' relates to the damping effect of the regularization on the message passing. We prove that, given sufficient regularization, this algorithm will converge and provide the exact marginal means at convergence, regardless of the way variables are assigned to nodes. The selection of the degree of regularization is addressed through the use of a heuristic, which is based on a tree representation of sGaBP-m. As a further contribution, we extend other GaBP variants in the literature to allow for higher-dimensional marginalization. We show that our algorithm compares favorably with these variants, both in terms of convergence speed and inference quality.",
            "keywords": [
                "belief propagation",
                "Gaussian distributions",
                "regularization",
                "inference quality"
            ],
            "author": [
                "Francois Kamper",
                "Sarel J. Steel",
                "Johan A. du Preez"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-101/18-101.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "AI-Toolbox: A C++ library for Reinforcement Learning and Planning (with Python Bindings)",
            "abstract": "This paper describes AI-Toolbox, a C++ software library that contains reinforcement learning and planning algorithms, and supports both single and multi agent problems, as well as partial observability. It is designed for simplicity and clarity, and contains extensive documentation of its API and code. It supports Python to enable users not comfortable with C++ to take advantage of the library's speed and functionality. AI-Toolbox is free software, and is hosted online at https://github.com/Svalorzen/AI-Toolbox.",
            "keywords": [
                "MDP",
                "POMDP",
                "multiagent",
                "reinforcement learning",
                "software"
            ],
            "author": [
                "Eugenio Bargiacchi",
                "Diederik M. Roijers",
                "Ann Nowé"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-402/18-402.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Nested Variance Reduction for Nonconvex Optimization",
            "abstract": "We study nonconvex optimization problems, where the objective function is either an average of  nonconvex functions or the expectation of some stochastic function. We propose a new stochastic gradient descent algorithm based on nested variance reduction, namely, Stochastic Nested Variance-Reduced Gradient descent (). Compared with conventional stochastic variance reduced gradient () algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each iteration, our algorithm uses  nested reference points to build a semi-stochastic gradient to further reduce its variance in each iteration. For smooth nonconvex functions,  converges to an -approximate first-order stationary point within  number of stochastic gradient evaluations. This improves the best known gradient complexity of   and that of  . For gradient dominated functions,  also achieves better gradient complexity than the state-of-the-art algorithms.  \n \n\nBased on , we further propose two algorithms that can find local minima faster than state-of-the-art algorithms in both finite-sum and general stochastic (online) nonconvex optimization. In particular, for finite-sum optimization problems, the proposed  algorithm achieves  gradient complexity to converge to an -second-order stationary point, which outperforms  (Allen-Zhu and Li, 2018), the best existing algorithm, in a wide regime. For general stochastic optimization problems, the proposed  achieves  gradient complexity, which is better than both  (Allen-Zhu and Li, 2018) and  (Allen-Zhu, 2018a) in certain regimes. Thorough experimental results on different nonconvex optimization problems back up our theory.",
            "keywords": [
                "Nonconvex Optimization",
                "Finding Local Minima",
                "Variance Reduction   e hides the logarithmic factors"
            ],
            "author": [
                "Dongruo Zhou",
                "Pan Xu",
                "Quanquan Gu"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-447/18-447.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Projection Oblique Randomer Forests",
            "abstract": "Decision forests, including Random Forests and Gradient Boosting Trees, have recently demonstrated state-of-the-art performance in a variety of machine learning settings. Decision forests are typically ensembles of axis-aligned decision trees; that is, trees that split only along feature dimensions. In contrast, many recent extensions to decision forests  are based on axis-oblique splits. Unfortunately, these extensions forfeit one or more of the favorable properties of decision forests based on axis-aligned splits, such as robustness to many noise dimensions, interpretability, or computational efficiency. We introduce yet another decision forest, called “Sparse Projection Oblique Randomer Forests” (SPORF). SPORF trees recursively split along very sparse random projections. Our method significantly improves accuracy over existing state-of-the-art algorithms on a standard benchmark suite for classification with  problems of varying dimension, sample size, and number of classes. To illustrate how SPORF addresses the limitations of both axis-aligned and existing oblique decision forest methods, we conduct extensive simulated experiments. SPORF typically yields improved performance over existing decision forest methods, while mitigating  computational efficiency and scalability and maintaining interpretability. Very sparse random projections can be incorporated into gradient boosted trees to obtain potentially similar gains.",
            "keywords": [],
            "author": [
                "Tyler M. Tomita",
                "James Browne",
                "Cencheng Shen",
                "Jaewon Chung",
                "Jesse L. Patsolic",
                "Benjamin Falk",
                "Carey E. Priebe",
                "Jason Yim",
                "Randal Burns",
                "Mauro Maggioni",
                "Joshua T. Vogelstein"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-664/18-664.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization",
            "abstract": "This paper considers stochastic optimization problems for a large class of objective functions, including convex and continuous submodular. Stochastic proximal gradient methods have been widely used to solve such problems; however, their applicability remains limited when the problem dimension is large and the projection onto a convex set is computationally costly. Instead, stochastic conditional gradient algorithms are proposed as an alternative solution which rely on (i) Approximating gradients via a simple averaging technique requiring a single stochastic gradient evaluation per iteration; (ii) Solving a linear program to compute the descent/ascent direction. The gradient averaging technique reduces the noise of gradient approximations as time progresses, and replacing projection step in proximal methods by a linear program lowers the computational complexity of each iteration. We show that under convexity and smoothness assumptions, our proposed stochastic conditional gradient method converges to the optimal objective function value at a sublinear rate of . Further, for a monotone and continuous DR-submodular function and subject to a general convex body constraint, we prove that our proposed method achieves a  guarantee (in expectation) with    stochastic gradient computations. This guarantee matches the known hardness results and closes the gap between deterministic and stochastic continuous submodular maximization. Additionally, we achieve  guarantee after operating on  stochastic gradients for the case that the objective function is continuous DR-submodular but non-monotone and the constraint set is a down-closed convex body. By using stochastic continuous optimization as an interface, we also provide the first  tight approximation guarantee for maximizing  a monotone but stochastic submodular set function subject to a general matroid constraint and  approximation guarantee for the non-monotone case.",
            "keywords": [
                "Stochastic optimization",
                "conditional gradient methods",
                "convex minimization",
                "submodular maximization",
                "gradient averaging",
                "Frank-Wolfe algorithm",
                "greedy algorithmc 2020 Aryan Mokhtari",
                "Hamed Hassani"
            ],
            "author": [
                "Aryan Mokhtari",
                "Hamed Hassani",
                "Amin Karbasi"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-764/18-764.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quadratic Decomposable Submodular Function Minimization: Theory and Practice",
            "abstract": "We introduce a new convex optimization problem, termed quadratic decomposable submodular function minimization (QDSFM), which allows to model a number of learning tasks on graphs and hypergraphs. The problem exhibits close ties to decomposable submodular function minimization (DSFM) yet is much more challenging to solve. We approach the problem via a new dual strategy and formulate an objective that can be optimized through a number of double-loop algorithms. The outer-loop uses either random coordinate descent (RCD) or alternative projection (AP) methods, for both of which we prove linear convergence rates. The inner-loop computes projections onto cones generated by base polytopes of the submodular functions via the modified min-norm-point or Frank-Wolfe algorithms. We also describe two new applications of QDSFM: hypergraph-adapted PageRank and semi-supervised learning. The proposed hypergraph-based PageRank algorithm can be used for local hypergraph partitioning and comes with provable performance guarantees. For hypergraph-adapted semi-supervised learning, we provide numerical experiments demonstrating the efficiency of our QDSFM solvers and their significant improvements on prediction accuracy when compared to state-of-the-art methods.",
            "keywords": [
                "Submodular functions"
            ],
            "author": [
                "Pan Li",
                "Niao He",
                "Olgica Milenkovic"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-790/18-790.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Change Point Estimation in a Dynamic Stochastic Block Model",
            "abstract": "We consider the problem of estimating the location of a single change point in a network generated by a dynamic stochastic block model mechanism. This model produces community structure in the network that exhibits change at a single time epoch. We propose two methods of estimating the change point, together with the model parameters,  before and after its occurrence. The first employs a least-squares criterion function and takes into consideration the full structure of the stochastic block model and is evaluated at each point in time. Hence, as an intermediate step, it requires estimating the community structure based on a clustering algorithm at every time point. The second method comprises the following two steps: in the first one, a least-squares function is used and evaluated at each time point, but ignoring the community structure and only considering a random graph generating mechanism exhibiting a change point. Once the change point is identified, in the second step, all network data before and after it are used together with a clustering algorithm to obtain the corresponding community structures and subsequently estimate the generating stochastic block model parameters. The first method, since it requires knowledge of the community structure and hence clustering at every point in time,  is significantly more computationally expensive than the second one. On the other hand, it requires a significantly less stringent identifiability condition for consistent estimation of the change point and the model parameters than the second method; however, it also requires a condition on the misclassification rate of misallocating network nodes to their respective communities that may fail to hold in many realistic settings. Despite the apparent stringency of the identifiability condition for the second method, we show that networks generated by a stochastic block mechanism exhibiting a change in their structure can easily satisfy this condition under a multitude of scenarios, including merging/splitting communities, nodes joining another community, etc. Further, for both methods under their respective identifiability and certain additional regularity conditions,  we establish rates of convergence and derive the asymptotic distributions of the change point estimators. The results are illustrated on synthetic data. In summary, this work provides an in-depth investigation of the novel problem of change point analysis for networks generated by stochastic block models, identifies key conditions for the consistent estimation of the change point, and proposes a computationally fast algorithm that solves the problem in many settings that occur in applications. Finally, it discusses challenges posed by employing clustering algorithms in this problem, that require additional investigation for their full resolution.",
            "keywords": [],
            "author": [
                "Monika Bhattacharjee",
                "Moulinath Banerjee",
                "George Michailidis"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-814/18-814.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ThunderGBM: Fast GBDTs and Random Forests on GPUs",
            "abstract": "Gradient Boosting Decision Trees (GBDTs) and Random Forests (RFs) have been used in many real-world applications. They are often a standard recipe for building state-of-the-art solutions to machine learning and data mining problems. However, training and prediction are very expensive computationally for large and high dimensional problems. This article presents an efficient and open source software toolkit called ThunderGBM which exploits the high-performance Graphics Processing Units (GPUs) for GBDTs and RFs. ThunderGBM supports classification, regression and ranking, and can run on single or multiple GPUs of a machine. Our experimental results show that ThunderGBM outperforms the existing libraries while producing similar models, and can handle high dimensional problems where existing GPU-based libraries fail. Documentation, examples, and more details about ThunderGBM are available at https://github.com/xtra-computing/thundergbm.",
            "keywords": [
                "Gradient Boosting Decision Trees",
                "Random Forests",
                "GPUs"
            ],
            "author": [
                "Zeyi Wen",
                "Hanfeng Liu",
                "Jiashuai Shi",
                "Qinbin Li",
                "Bingsheng He",
                "Jian Chen"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-095/19-095.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Model Selection with Graph Structured Sparsity",
            "abstract": "We propose a general algorithmic framework for Bayesian model selection. A spike-and-slab Laplacian prior is introduced to model the underlying structural assumption. Using the notion of effective resistance, we derive an EM-type algorithm with closed-form iterations to efficiently explore possible candidates for Bayesian model selection. The deterministic nature of the proposed algorithm makes it more scalable to large-scale and high-dimensional data sets compared with existing stochastic search algorithms. When applied to sparse linear regression, our framework recovers the EMVS algorithm by Ročková and George (2014) as a special case. We also discuss extensions of our framework using tools from graph algebra to incorporate complex Bayesian models such as biclustering and submatrix localization. Extensive simulation studies and real data applications are conducted to demonstrate the superior performance of our methods over its frequentist competitors such as  or  penalization.",
            "keywords": [
                "spike-and-slab prior",
                "graph laplacian",
                "variational inference",
                "expectation     maximization",
                "sparse linear regression"
            ],
            "author": [
                "Youngseok Kim",
                "Chao Gao"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-123/19-123.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ProxSARAH: An Efficient Algorithmic Framework for Stochastic Composite Nonconvex Optimization",
            "abstract": "We propose a new stochastic first-order algorithmic framework to solve stochastic  composite nonconvex optimization problems that covers both finite-sum and expectation settings. Our algorithms rely on the SARAH estimator and consist of two steps: a proximal gradient and an averaging step making them different from existing nonconvex proximal-type algorithms. The algorithms only require an average smoothness assumption of the nonconvex objective term and additional bounded variance assumption if applied to expectation problems. They work with both constant and dynamic step-sizes, while allowing single sample and mini-batches. In all these cases, we prove that our algorithms can achieve the best-known complexity bounds in terms of stochastic first-order oracle. One key step of our methods is the new constant and dynamic step-sizes resulting in the desired complexity bounds while improving practical performance. Our constant step-size is much larger than existing methods including proximal SVRG scheme in the single sample case. We also specify our framework to the non-composite case that covers existing state-of-the-arts in terms of oracle complexity bounds. Our update also allows one to trade-off between step-sizes and mini-batch sizes to improve performance. We test the proposed algorithms on two composite nonconvex problems and neural networks using several well-known data sets.",
            "keywords": [],
            "author": [
                "Nhan H. Pham",
                "Lam M. Nguyen",
                "Dzung T. Phan",
                "Quoc Tran-Dinh"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-248/19-248.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MFE: Towards reproducible meta-feature extraction",
            "abstract": "Automated recommendation of machine learning algorithms is receiving a large deal of attention, not only because they can recommend the most suitable algorithms for a new task, but also because they can support efficient hyper-parameter tuning, leading to better machine learning solutions. The automated recommendation can be implemented using meta-learning, learning from previous learning experiences, to create a meta-model able to associate a data set to the predictive performance of machine learning algorithms. Although a large number of publications report the use of meta-learning, reproduction and comparison of meta-learning experiments is a difficult task.  The literature lacks extensive and comprehensive public tools that enable the reproducible investigation of the different meta-learning approaches. An alternative to deal with this difficulty is to develop a meta-feature extractor package with the main characterization measures, following uniform guidelines that facilitate the use and inclusion of new meta-features.  In this paper, we propose two Meta-Feature Extractor (MFE) packages, written in both Python and R, to fill this lack. The packages follow recent frameworks for meta-feature extraction, aiming to facilitate the reproducibility of meta-learning experiments.",
            "keywords": [
                "Machine Learning",
                "AutoML",
                "Meta-Learning"
            ],
            "author": [
                "Edesio Alcobaça",
                "Felipe Siqueira",
                "Adriano Rivolli",
                "Luís P. F. Garcia",
                "Jefferson T. Oliva",
                "André C. P. L. F. de Carvalho"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-348/19-348.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-dimensional Linear Discriminant Analysis Classifier for Spiked Covariance Model",
            "abstract": "Linear discriminant analysis (LDA) is a popular classifier that is built on the assumption of common population covariance matrix across classes. The performance of LDA depends heavily on the quality of estimating the mean vectors and the population covariance matrix. This issue becomes more challenging in high-dimensional settings where the number of features is of the same order as the number of training samples. Several techniques for estimating the covariance matrix can be found in the literature. One of the most popular approaches are estimators based on using a regularized sample covariance matrix, giving the name regularized LDA (R-LDA) to the corresponding classifier. These estimators are known to be more resilient to the sample noise than the traditional sample covariance matrix estimator. However, the main challenge of the regularization approach is the choice of the optimal regularization parameter, as an arbitrary choice could lead to severe degradation of the classifier performance. In this work, we propose an improved LDA classifier based on the assumption that the covariance matrix follows a spiked covariance model. The main principle of our proposed technique is the design of a parametrized inverse covariance matrix estimator, the parameters of which are shown to be easily optimized. Numerical simulations, using both real and synthetic data, show that the proposed classifier yields better classification performance than the classical R-LDA while requiring lower computational complexity.",
            "keywords": [
                "Linear Discriminant Analysis",
                "Spiked Covariance Models",
                "High-Dimensional     Data"
            ],
            "author": [
                "Houssem Sifaou",
                "Abla Kammoun",
                "Mohamed-Slim Alouini"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-428/19-428.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "NEVAE: A Deep Generative Model for Molecular Graphs",
            "abstract": "Deep generative models have been praised for their ability to learn smooth latent representations of images, text, and audio, which can then be used to generate new, plausible data. Motivated by these success stories, there has been a surge of interest in developing deep generative models for automated molecule design. However, these models face several difficulties due to the unique characteristics of molecular graphs—their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes’ labels, and they come with a different number of nodes and edges. In this paper, we first propose a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. Moreover, in contrast with the state of the art, our decoder is able to provide the spatial coordinates of the atoms of the molecules it generates. Then, we develop a gradient-based algorithm to optimize the decoder of our model so that it learns to generate molecules that maximize the value of certain property of interest and, given any arbitrary molecule, it is able to optimize the spatial configuration of its atoms for greater stability. Experiments reveal that our variational autoencoder can discover plausible, diverse and novel molecules more effectively than several state of the art models. Moreover, for several properties of interest, our optimized decoder is able to identify molecules with property values 121% higher than those identified by several state of the art methods based on Bayesian optimization and reinforcement learning.",
            "keywords": [
                "Drug design",
                "Molecule discovery",
                "Deep generative models",
                "Variational autoencoders"
            ],
            "author": [
                "Bidisha Samanta",
                "Abir De",
                "Gourhari Jana",
                "Vicenç Gómez",
                "Pratim Chattaraj",
                "Niloy Ganguly",
                "Manuel Gomez-Rodriguez"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-671/19-671.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Identifiability and Consistent Estimation of Nonparametric Translation Hidden Markov Models with General State Space",
            "abstract": "This paper considers hidden Markov models where the observations are given as the sum of a latent state which lies in a general state space and some independent noise with unknown distribution. It is shown that these fully nonparametric translation models are identifiable with respect to both the distribution of the latent variables and the distribution of the noise, under mostly a light tail assumption on the latent variables. Two nonparametric estimation methods are proposed and we prove that the corresponding estimators are consistent for the weak convergence topology. These results are illustrated with numerical experiments.",
            "keywords": [
                "Nonparametric estimation",
                "latent variable models"
            ],
            "author": [
                "Elisabeth Gassiat",
                "Sylvain Le Corff",
                "Luc Lehéricy"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-802/19-802.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GluonTS: Probabilistic and Neural Time Series Modeling in Python",
            "abstract": "We introduce the Gluon Time Series Toolkit (GluonTS), a Python library for deep learning based time series modeling for ubiquitous tasks, such as forecasting and anomaly detection. GluonTS simplifies the time series modeling pipeline by providing the necessary components and tools for quick model development, efficient experimentation and evaluation. In addition, it contains reference implementations of state-of-the-art time series models that enable simple benchmarking of new algorithms.",
            "keywords": [
                "time series",
                "deep learning",
                "Python",
                "scientific toolkit"
            ],
            "author": [
                "Alexander Alexandrov",
                "Konstantinos Benidis",
                "Michael Bohlke-Schneider",
                "Valentin Flunkert",
                "Jan Gasthaus",
                "Tim Januschowski",
                "Danielle C. Maddix",
                "Syama Rangapuram",
                "David Salinas",
                "Jasper Schulz",
                "Lorenzo Stella",
                "Ali Caner Türkmen",
                "Yuyang Wang"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-820/19-820.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Estimation of High-dimensional Factor-Augmented Vector Autoregressive (FAVAR) Models",
            "abstract": "A factor-augmented vector autoregressive (FAVAR) model is defined by a VAR equation that captures lead-lag correlations amongst a set of observed variables  and latent factors , and a calibration equation that relates another set of observed variables  with  and . The latter equation is used to estimate the factors that are subsequently used in estimating the parameters of the VAR system. The FAVAR model has become popular in applied economic research, since it can summarize a large number of variables of interest as a few factors through the calibration equation and subsequently examine their influence on core variables of primary interest through the VAR equation. However, there is increasing need for examining lead-lag relationships between a large number of time series, while incorporating information from another high-dimensional set of variables. Hence, in this paper we investigate the FAVAR model under high-dimensional scaling. We introduce an  appropriate identification constraint for the model parameters, which when incorporated into the formulated optimization problem yields estimates with good statistical properties. Further, we address a number of technical challenges introduced by the fact that estimates of the VAR system model parameters are based on estimated rather than directly observed quantities. The performance of the proposed estimators is evaluated on synthetic data. Further, the model is applied to commodity prices and reveals interesting and interpretable relationships between the prices and the factors extracted from a set of global macroeconomic indicators.",
            "keywords": [],
            "author": [
                "Jiahe Lin",
                "George Michailidis"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-874/19-874.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tslearn, A Machine Learning Toolkit for Time Series Data",
            "abstract": "tslearn is a general-purpose Python machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression. It follows scikit-learn's Application Programming Interface for transformers and estimators, allowing the use of standard pipelines and model selection tools on top of tslearn objects. It is distributed under the BSD-2-Clause license, and its source code is available at https://github.com/tslearn-team/tslearn.",
            "keywords": [
                "time series",
                "clustering",
                "classification",
                "pre-processing"
            ],
            "author": [
                "Romain Tavenard",
                "Johann Faouzi",
                "Gilles Vandewiele",
                "Felix Divo",
                "Guillaume Androz",
                "Chester Holtz",
                "Marie Payne",
                "Roman Yurchak",
                "Marc Rußwurm",
                "Kushal Kolar",
                "Eli Woods"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-091/20-091.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Closed Surface Fitting Through Tensor Products",
            "abstract": "Closed surfaces provide a useful model for -d shapes, with the data typically consisting of a cloud of points in . The existing literature on closed surface modeling focuses on frequentist point estimation methods that join surface patches along the edges, with surface patches created via Bézier surfaces or tensor products of B-splines. However, the resulting surfaces are not smooth along the edges and the geometric constraints required to join the surface patches lead to computational drawbacks. In this article, we develop a Bayesian model for closed surfaces based on tensor products of a cyclic basis resulting in infinitely smooth surface realizations. We impose sparsity on the control points through a double-shrinkage prior. Theoretical properties of the support of our proposed prior are studied and it is shown that the posterior achieves the optimal rate of convergence under reasonable assumptions on the prior. The proposed approach is illustrated with some examples.",
            "keywords": [],
            "author": [
                "Olivier Binette",
                "Debdeep Pati",
                "David B. Dunson"
            ],
            "ref": "http://jmlr.org/papers/volume21/13-233/13-233.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Class of Parallel Doubly Stochastic Algorithms for Large-Scale Learning",
            "abstract": "We consider learning problems over training sets in which both, the number of training examples and the dimension of the feature vectors, are large. To solve these problems we propose the random parallel stochastic algorithm (RAPSA). We call the algorithm random parallel because it utilizes multiple parallel processors to operate on a randomly chosen subset of blocks of the feature vector. RAPSA is doubly stochastic since each processor utilizes a random set of functions to compute the stochastic gradient associated with a randomly chosen sets of variable coordinates. Algorithms that are parallel in either of these dimensions exist, but RAPSA is the first attempt at a methodology that is parallel in both the selection of blocks and the selection of elements of the training set. In RAPSA, processors utilize the randomly chosen functions to compute the stochastic gradient component associated with a randomly chosen block. The technical contribution of this paper is to show that this minimally coordinated algorithm converges to the optimal classifier when the training objective is strongly convex. Moreover, we present an accelerated version of RAPSA (ARAPSA) that incorporates the objective function curvature information by premultiplying the descent direction by a Hessian approximation matrix. We further extend the results for asynchronous settings and show that if the processors perform their updates without any coordination the algorithms are still convergent to the optimal argument. RAPSA and its extensions are then numerically evaluated on a linear estimation problem and a binary image classification task using the MNIST handwritten digit dataset.",
            "keywords": [
                "Stochastic optimization",
                "large-scale learning",
                "asynchronous methods",
                "parallel     algorithmsc 2020 Aryan Mokhtari",
                "Alec Koppel"
            ],
            "author": [
                "Aryan Mokhtari",
                "Alec Koppel",
                "Martin Takac",
                "Alejandro Ribeiro"
            ],
            "ref": "http://jmlr.org/papers/volume21/16-311/16-311.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Agnostic Estimation for Phase Retrieval",
            "abstract": "The goal of noisy high-dimensional phase retrieval is to estimate an -sparse parameter  from  realizations of the model . Based on this model, we propose a significant semi-parametric generalization called  misspecified phase retrieval (MPR), in which  with unknown  and . For example, MPR encompasses  with increasing  as a special case. Despite the generality of the MPR model, it eludes the reach of most existing semi-parametric estimators. In this paper, we propose an estimation procedure, which consists of solving a cascade of two convex programs and provably recovers the direction of . Furthermore, we prove that our procedure is minimax optimal over the class of MPR models. Interestingly, our minimax analysis characterizes the statistical price of misspecifying the link function in phase retrieval models. Our theory is backed up by thorough numerical results.",
            "keywords": [],
            "author": [
                "Matey Neykov",
                "Zhaoran Wang",
                "Han Liu"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-259/18-259.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel-estimated Nonparametric Overlap-Based Syncytial Clustering",
            "abstract": "Commonly-used clustering algorithms usually find ellipsoidal, spherical or other regular-structured clusters, but are more challenged  when the underlying groups lack formal structure or definition. Syncytial clustering is the name that we introduce for methods that merge groups obtained from standard clustering algorithms in order to reveal complex group structure in the data. Here, we develop a distribution-free fully-automated syncytial clustering algorithm that can be used with -means and other algorithms. Our approach estimates the cumulative distribution function of the normed residuals from an appropriately fit -groups model and calculates  the estimated nonparametric overlap between each pair of clusters. Groups with high pairwise overlap are merged  as long as the estimated generalized overlap decreases. Our methodology is always a top performer in identifying groups with regular and irregular structures in several datasets and can be applied to datasets with scatter or incomplete records. The approach is also used to identify the distinct kinds of gamma ray bursts in the Burst and Transient Source Experiment 4Br catalog and the distinct kinds of activation in a functional Magnetic Resonance Imaging study.",
            "keywords": [
                "BATSE",
                "DEMP"
            ],
            "author": [
                "Israel A. Almodóvar-Rivera",
                "Ranjan Maitra"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-435/18-435.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tensor Regression Networks",
            "abstract": "Convolutional neural networks typically consist of many convolutional layers followed by one or more fully connected layers. While convolutional layers map between high-order activation tensors, the fully connected layers operate on flattened activation vectors. Despite empirical success, this approach has notable drawbacks. Flattening followed by fully connected layers discards multilinear structure in the activations and requires many parameters. We address these problems by incorporating tensor algebraic operations that preserve multilinear structure at every layer. First, we introduce Tensor Contraction Layers (TCLs) that reduce the dimensionality of their input while preserving their multilinear structure using tensor contraction. Next, we introduce Tensor Regression Layers (TRLs), which express outputs through a low-rank multilinear mapping from a high-order activation tensor to an output tensor of arbitrary order. We learn the contraction and regression factors end-to-end, and produce accurate nets with fewer parameters. Additionally, our layers regularize networks by imposing low-rank constraints on the activations (TCL) and regression weights (TRL). Experiments on ImageNet show that, applied to VGG and ResNet architectures, TCLs and TRLs reduce the number of parameters compared to fully connected layers by more than 65% while maintaining or increasing accuracy. In addition to the space savings, our approach's ability to leverage topological structure can be crucial for structured data such as MRI. In particular, we demonstrate significant performance improvements over comparable architectures on three tasks associated with the UK Biobank dataset.",
            "keywords": [
                "Machine Learning",
                "Tensor Methods",
                "Tensor Regression Networks",
                "Low-Rank     Regression",
                "Tensor Regression Layers",
                "Deep Learning"
            ],
            "author": [
                "Jean Kossaifi",
                "Zachary C. Lipton",
                "Arinbjorn Kolbeinsson",
                "Aran Khanna",
                "Tommaso Furlanello",
                "Anima Anandkumar"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-503/18-503.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Bayesian Inference of Sparse Networks with Automatic Sparsity Determination",
            "abstract": "Structure learning of Gaussian graphical models typically involves careful tuning of penalty parameters, which balance the tradeoff between data fidelity and graph sparsity. Unfortunately, this tuning is often a “black art” requiring expert experience or brute-force search. It is therefore tempting to develop tuning-free algorithms that can determine the sparsity of the graph adaptively from the observed data in an automatic fashion. In this paper, we propose a novel approach, named BISN (Bayesian inference of Sparse Networks), for automatic Gaussian graphical model selection. Specifically, we regard the off-diagonal entries in the precision matrix as random variables and impose sparse-promoting horseshoe priors on them, resulting in automatic sparsity determination. With the help of stochastic gradients, an efficient variational Bayes algorithm is derived to learn the model. We further propose a decaying recursive stochastic gradient (DRSG) method to reduce the variance of the stochastic gradients and to accelerate the convergence. Our theoretical analysis shows that the time complexity of BISN scales only quadratically with the dimension, whereas the theoretical time complexity of the state-of-the-art methods for automatic graphical model selection is typically a third-order function of the dimension. Furthermore, numerical results show that BISN can achieve comparable or better performance than the state-of-the-art methods in terms of structure recovery, and yet its computational time is several orders of magnitude shorter, especially for large dimensions.",
            "keywords": [
                "Gaussian Graphical Models",
                "Structure Learning",
                "Tuning Free",
                "Time Com-    plexity",
                "Variational Bayes",
                "Variance Reduction"
            ],
            "author": [
                "Hang Yu",
                "Songwei Wu",
                "Luyin Xin",
                "Justin Dauwels"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-514/18-514.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Algorithms for Continuous Non-monotone Submodular and DR-Submodular Maximization",
            "abstract": "In this paper we study the fundamental problems of maximizing a continuous non-monotone submodular function over the hypercube, both with and without coordinate-wise concavity. This family of optimization problems has several applications in machine learning, economics, and communication systems. Our main result is the first -approximation algorithm for continuous submodular function maximization; this approximation factor of  is the best possible for algorithms that only query the objective function at polynomially many points.  For the special case of DR-submodular maximization, i.e. when the submodular function is also coordinate-wise concave along all coordinates, we provide a different -approximation algorithm that runs in quasi-linear time. Both these results improve upon prior work (Bian et al. 2017; Soma and Yoshida, 2017). Our first algorithm uses novel ideas such as reducing the guaranteed approximation problem to analyzing a zero-sum game for each coordinate, and incorporates the geometry of this zero-sum game to fix the value at this coordinate. Our second algorithm exploits coordinate-wise concavity to identify a monotone equilibrium condition sufficient for getting the required approximation guarantee, and hunts for the equilibrium point using binary search. We further run experiments to verify the performance of our proposed algorithms in related machine learning applications.",
            "keywords": [
                "Continuous submodularity",
                "non-monotone submodular maximization"
            ],
            "author": [
                "Rad Niazadeh",
                "Tim Roughgarden",
                "Joshua R. Wang"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-527/18-527.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Minimum Error Entropy Algorithms",
            "abstract": "Minimum Error Entropy (MEE) principle is an important approach in Information Theoretical Learning (ITL). It is widely applied and studied in various fields for its robustness to noise. In this paper, we study a reproducing kernel-based distributed MEE algorithm, DMEE, which is designed to work with both fully supervised data and semi-supervised data. The divide-and-conquer approach is employed, so there is no inter-node communication overhead. Similar as other distributed algorithms, DMEE significantly reduces the computational complexity and memory requirement on single computing nodes. With fully supervised data, our proved learning rates equal the minimax optimal learning rates of the classical pointwise kernel-based regressions. Under the semi-supervised learning scenarios, we show that DMEE exploits unlabeled data effectively, in the sense that first, under the settings with weak regularity assumptions, additional unlabeled data significantly improves the learning rates of DMEE. Second, with sufficient unlabeled data, labeled data can be distributed to many more computing nodes, that each node takes only O(1) labels, without spoiling the learning rates in terms of the number of labels. This conclusion overcomes the saturation phenomenon in unlabeled data size. It parallels a recent results for regularized least squares (Lin and Zhou, 2018), and suggests that an inflation of unlabeled data is a solution to the MEE learning problems with decentralized data source for the concerns of privacy protection. Our work refers to pairwise learning and non-convex loss. The theoretical analysis is achieved by distributed U-statistics and error decomposition techniques in integral operators.",
            "keywords": [
                "Information theoretic learning",
                "minimum error entropy",
                "distributed method",
                "semi-     supervised data"
            ],
            "author": [
                "Xin Guo",
                "Ting Hu",
                "Qiang Wu"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-696/18-696.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Apache Mahout: Machine Learning on Distributed Dataflow Systems",
            "abstract": "Apache Mahout is a library for scalable machine learning (ML) on distributed dataflow systems, offering various implementations of classification, clustering, dimensionality reduction and recommendation algorithms. Mahout was a pioneer in large-scale machine learning in 2008, when it started  and targeted MapReduce, which was the predominant abstraction for scalable computing in industry at that time. Mahout has been widely used by leading web companies and is part of several commercial cloud offerings. In recent years, Mahout migrated to a general framework enabling a mix of dataflow programming and linear algebraic computations on backends such as Apache Spark and Apache Flink. This design allows users to execute data preprocessing and model training in a single, unified dataflow system, instead of requiring a complex integration of several specialized systems. Mahout is maintained as a community-driven open source project at the Apache Software Foundation, and is available under https://mahout.apache.org.",
            "keywords": [],
            "author": [
                "Robin Anil",
                "Gokhan Capan",
                "Isabel Drost-Fromm",
                "Ted Dunning",
                "Ellen Friedman",
                "Trevor Grant",
                "Shannon Quinn",
                "Paritosh Ranjan",
                "Sebastian Schelter",
                "Özgür Yılmazel"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-800/18-800.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Regularization-Based Adaptive Test for High-Dimensional GLMs",
            "abstract": "In spite of its urgent importance in the era of big data, testing high-dimensional parameters in generalized linear models (GLMs) in the presence of high-dimensional nuisance parameters has been largely under-studied, especially with regard to constructing powerful tests for general (and unknown) alternatives. Most existing tests are powerful only against certain alternatives and may yield incorrect Type 1 error rates under high-dimensional nuisance parameter situations. In this paper, we propose the adaptive interaction sum of powered score (aiSPU) test in the framework of penalized regression with a non-convex penalty, called truncated Lasso penalty (TLP), which can maintain correct Type 1 error rates while yielding high statistical power across a wide range of alternatives. To calculate its p-values analytically, we derive its asymptotic null distribution. Via simulations, its superior finite-sample performance is demonstrated over several representative existing methods. In addition, we apply it and other representative tests to an Alzheimer's Disease Neuroimaging Initiative (ADNI) data set, detecting possible gene-gender interactions for Alzheimer's disease. We also put R package “aispu” implementing the proposed test on GitHub.",
            "keywords": [
                "Adaptive Test",
                "Truncated Lasso Penalty"
            ],
            "author": [
                "Chong Wu",
                "Gongjun Xu",
                "Xiaotong Shen",
                "Wei Pan"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-807/18-807.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A General System of Differential Equations to Model First-Order Adaptive Algorithms",
            "abstract": "First-order optimization algorithms play a major role in large scale machine learning. A new class of methods, called adaptive algorithms, was recently introduced to adjust iteratively the learning rate for each coordinate. Despite great practical success in deep learning, their behavior and performance on more general loss functions are not well understood. In this paper, we derive a non-autonomous system of differential equations, which is the continuous-time limit of adaptive optimization methods. We study the convergence of its trajectories and give conditions under which the differential system, underlying all adaptive algorithms, is suitable for optimization. We discuss convergence to a critical point in the non-convex case and give conditions for the dynamics to avoid saddle points and local maxima. For convex loss function, we introduce a suitable Lyapunov functional which allows us to study its rate of convergence. Several other properties of both the continuous and discrete systems are briefly discussed. The differential system studied in the paper is general enough to encompass many other classical algorithms (such as Heavy Ball and Nesterov's accelerated method) and allow us to recover several known results for these algorithms.",
            "keywords": [
                "Adaptive algorithms",
                "convex and non-convex optimization",
                "first-order meth-     ods",
                "differential equation"
            ],
            "author": [
                "Andre Belotto da Silva",
                "Maxime Gazeau"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-808/18-808.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "AI Explainability 360: An Extensible Toolkit for Understanding Data and Machine Learning Models",
            "abstract": "As artificial intelligence algorithms make further inroads in high-stakes societal applications, there are increasing calls from multiple stakeholders for these algorithms to explain their outputs.  To make matters more challenging, different personas of consumers of explanations have different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360, an open-source Python toolkit featuring ten diverse and state-of-the-art explainability methods and two evaluation metrics. Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of interpretation and explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline.  The toolkit is not only the software, but also guidance material, tutorials, and an interactive web demo to introduce AI explainability to different audiences. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed.",
            "keywords": [
                "explainability",
                "interpretability",
                "transparency",
                "taxonomy"
            ],
            "author": [
                "Vijay Arya",
                "Rachel K. E. Bellamy",
                "Pin-Yu Chen",
                "Amit Dhurandhar",
                "Michael Hind",
                "Samuel C. Hoffman",
                "Stephanie Houde",
                "Q. Vera Liao",
                "Ronny Luss",
                "Aleksandra Mojsilović",
                "Sami Mourad",
                "Pablo Pedemonte",
                "Ramya Raghavendra",
                "John T. Richards",
                "Prasanna Sattigeri",
                "Karthikeyan Shanmugam",
                "Moninder Singh",
                "Kush R. Varshney",
                "Dennis Wei",
                "Yunfeng Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-1035/19-1035.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence of Sparse Variational Inference in Gaussian Processes Regression",
            "abstract": "Gaussian processes are distributions over functions that are versatile and mathematically convenient priors in Bayesian modelling. However, their use is often impeded for data with large numbers of observations, , due to the cubic (in ) cost of matrix operations used in exact inference. Many solutions have been proposed that rely on  inducing variables to form an approximation at a cost of . While the computational cost appears linear in , the true complexity depends on how  must scale with  to ensure a certain quality of the approximation. In this work, we investigate upper and lower bounds on how  needs to grow with  to ensure high quality approximations. We show that we can make the KL-divergence between the approximate model and the exact posterior arbitrarily small for a Gaussian-noise regression model with . Specifically, for the popular squared exponential kernel and -dimensional Gaussian distributed covariates,  suffice and a method with an overall computational cost of  can be used to perform inference.",
            "keywords": [
                "Gaussian processes",
                "approximate inference",
                "variational methods",
                "Bayesian     non-parameterics"
            ],
            "author": [
                "David R. Burt",
                "Carl Edward Rasmussen",
                "Mark van der Wilk"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-1015/19-1015.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Monte Carlo Gradient Estimation in Machine Learning",
            "abstract": "This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies---the pathwise, score function, and measure-valued gradient estimators---exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.",
            "keywords": [
                "gradient estimation",
                "Monte Carlo",
                "sensitivity analysis",
                "score-function estimator",
                "pathwise estimator",
                "measure-valued estimator"
            ],
            "author": [
                "Shakir Mohamed",
                "Mihaela Rosca",
                "Michael Figurnov",
                "Andriy Mnih"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-346/19-346.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced Aggregation of Sparsely Interacting Workers",
            "abstract": "We consider worker skill estimation for the single-coin Dawid-Skene crowdsourcing model. In practice, skill-estimation is challenging because worker assignments are sparse and irregular due to the arbitrary and uncontrolled availability of workers. We formulate skill estimation as a rank-one correlation-matrix completion problem, where the observed components correspond to observed label correlation between workers. We show that the correlation matrix can be successfully recovered and skills are identifiable if and only if the sampling matrix (observed components) does not have a bipartite connected component. We then propose a projected gradient descent scheme and show that skill estimates converge to the desired global optima for such sampling matrices. Our proof is original and the results are surprising in light of the fact that even the weighted rank-one matrix factorization problem is NP-hard in general. Next, we derive sample complexity bounds in terms of spectral properties of the  signless Laplacian of the sampling matrix. Our proposed scheme achieves state-of-art performance on a number of real-world datasets.",
            "keywords": [
                "distributed optimization"
            ],
            "author": [
                "Yao Ma",
                "Alex Olshevsky",
                "Csaba Szepesvari",
                "Venkatesh Saligrama"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-359/19-359.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Probabilistic Learning on Graphs via Contextual Architectures",
            "abstract": "We propose a novel methodology for representation learning on graph-structured data, in which a stack of Bayesian Networks learns different distributions of a vertex's neighbourhood. Through an incremental construction policy and layer-wise training, we can build deeper architectures with respect to typical graph convolutional neural networks, with benefits in terms of context spreading between vertices. First, the model learns from graphs via maximum likelihood estimation without using target labels. Then, a supervised readout is applied to the learned graph embeddings to deal with graph classification and vertex classification tasks, showing competitive results against neural models for graphs. The computational complexity is linear in the number of edges, facilitating learning on large scale data sets. By studying how depth affects the performances of our model, we discover that a broader context generally improves performances. In turn, this leads to a critical analysis of some benchmarks used in literature.",
            "keywords": [
                "Structured domains",
                "deep graph networks",
                "graph neural networks",
                "deep     learning",
                "maximum likelihood",
                "graph classification"
            ],
            "author": [
                "Davide Bacciu",
                "Federico Errica",
                "Alessio Micheli"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-470/19-470.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Rates for the Stochastic Gradient Descent Method for Non-Convex Objective Functions",
            "abstract": "We prove the convergence to minima and estimates on the rate of convergence for the stochastic gradient descent method in the case of not necessarily locally convex nor contracting objective functions.  In particular, the analysis relies on a quantitative use of mini-batches to control the loss of iterates to non-attracted regions.  The applicability of the results to simple objective functions arising in machine learning is shown.",
            "keywords": [
                "stochastic gradient descent",
                "mini-batch algorithm",
                "machine learning"
            ],
            "author": [
                "Benjamin Fehrman",
                "Benjamin Gess",
                "Arnulf Jentzen"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-636/19-636.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Contextual Bandits with Continuous Actions: Smoothing, Zooming, and Adapting",
            "abstract": "We study contextual bandit learning with an abstract policy class and continuous action space. We obtain two qualitatively different regret bounds: one competes with a smoothed version of the policy class under no continuity assumptions, while the other requires standard Lipschitz assumptions. Both bounds exhibit data-dependent “zooming” behavior and, with no tuning, yield improved guarantees for benign problems. We also study adapting to unknown smoothness parameters, establishing a price-of-adaptivity and deriving optimal adaptive algorithms that require no additional information.",
            "keywords": [
                "Contextual bandits"
            ],
            "author": [
                "Akshay Krishnamurthy",
                "John Langford",
                "Aleksandrs Slivkins",
                "Chicheng Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-650/19-650.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Chaining Meets Chain Rule: Multilevel Entropic Regularization and Training of Neural Networks",
            "abstract": "We derive generalization and excess risk bounds for neural networks using a family of complexity measures based on a multilevel relative entropy. The bounds are obtained by introducing the notion of generated hierarchical coverings of neural networks and by using the technique of chaining mutual information introduced by Asadi et al. '18. The resulting bounds are algorithm-dependent and multiscale: they exploit the multilevel structure of neural networks. This, in turn, leads to an empirical risk minimization problem with a multilevel entropic regularization. The minimization problem is resolved by introducing a multiscale extension of the celebrated Gibbs posterior distribution, proving that the derived distribution achieves the unique minimum. This leads to a new training procedure for neural networks with performance guarantees, which exploits the chain rule of relative entropy rather than the chain rule of derivatives (as in backpropagation), and which takes into account the interactions between different scales of the hypothesis sets of neural networks corresponding to different depths of the hidden layers. To obtain an efficient implementation of the latter, we further develop a multilevel Metropolis algorithm simulating the multiscale Gibbs distribution, with an experiment for a two-layer neural network on the MNIST data set.",
            "keywords": [
                "neural networks",
                "multilevel relative entropy",
                "chaining mutual information",
                "multiscale generalization bound"
            ],
            "author": [
                "Amir R. Asadi",
                "Emmanuel Abbe"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-794/19-794.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
            "keywords": [
                "transfer learning",
                "natural language processing",
                "multi-task learning",
                "attention-     based models"
            ],
            "author": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-074/20-074.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Importance Sampling Techniques for Policy Optimization",
            "abstract": "How can we effectively exploit the collected samples when solving a continuous control task with Reinforcement Learning? Recent results have empirically demonstrated that multiple policy optimization steps can be performed with the same batch by using off-distribution techniques based on importance sampling. However, when dealing with off-distribution optimization, it is essential to take into account the uncertainty introduced by the importance sampling process. In this paper, we propose and analyze a class of model-free, policy search algorithms that extend the recent Policy Optimization via Importance Sampling (Metelli et al., 2018) by incorporating two advanced variance reduction techniques: per-decision and multiple importance sampling. For both of them, we derive a high-probability bound, of independent interest, and then we show how to employ it to define a suitable surrogate objective function that can be used for both action-based and parameter-based settings. The resulting algorithms are finally evaluated on a set of continuous control tasks, using both linear and deep policies, and compared with modern policy optimization methods.",
            "keywords": [
                "Reinforcement Learning",
                "Policy Optimization",
                "Importance Sampling"
            ],
            "author": [
                "Alberto Maria Metelli",
                "Matteo Papini",
                "Nico Montali",
                "Marcello Restelli"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-124/20-124.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nesterov's Acceleration for Approximate Newton",
            "abstract": "Optimization plays a key role in machine learning. Recently, stochastic second-order methods have attracted considerable attention because of their low computational cost in each iteration. However, these methods might suffer from poor performance when the Hessian is hard to be approximate well in a computation-efficient way. To overcome this dilemma, we resort to Nesterov's acceleration to improve the convergence performance of these second-order methods and propose accelerated approximate Newton. We give the theoretical convergence analysis of accelerated approximate Newton and show that Nesterov's acceleration can improve the convergence rate. Accordingly, we propose an accelerated regularized sub-sampled Newton (ARSSN) which performs much better than the conventional regularized sub-sampled Newton  empirically and theoretically. Moreover, we show that ARSSN has better performance  than classical first-order methods empirically.",
            "keywords": [],
            "author": [
                "Haishan Ye",
                "Luo Luo",
                "Zhihua Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-265/19-265.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Data Efficient and Feasible Level Set Method for Stochastic Convex Optimization with Expectation Constraints",
            "abstract": "Stochastic convex optimization problems with expectation constraints (SOECs) are encountered in statistics and machine learning, business, and engineering. The SOEC objective and constraints contain expectations defined with respect to complex distributions or large data sets, leading to high computational complexity when solved by the algorithms that use exact functions and their gradients. Recent stochastic first order methods exhibit low computational complexity when handling SOECs but guarantee near-feasibility and near-optimality only at convergence. These methods may thus return highly infeasible solutions when heuristically terminated, as is often the case, due to theoretical convergence criteria being highly conservative. This issue limits the use of first order methods in several applications where the SOEC constraints encode implementation requirements. We design a stochastic feasible level set method (SFLS) for SOECs that has low complexity and emphasizes feasibility before convergence. Specifically, our level-set method solves a root-finding problem by calling a novel first order oracle that computes a stochastic upper bound on the level-set function by extending mirror descent and online validation techniques. We establish that SFLS maintains a high-probability feasible solution at each root-finding iteration and exhibits favorable complexity compared to state-of-the-art deterministic feasible level set and stochastic subgradient methods. Numerical experiments on three diverse applications highlight how SFLS finds feasible solutions with small optimality gaps with lower complexity than the former approaches.",
            "keywords": [],
            "author": [
                "Qihang Lin",
                "Selvaprabu Nadarajah",
                "Negar Soheili",
                "Tianbao Yang"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-1022/19-1022.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Empirical Priors for Prediction in Sparse High-dimensional Linear Regression",
            "abstract": "In this paper we adopt the familiar sparse, high-dimensional linear regression model and focus on the important but often overlooked task of prediction.  In particular, we consider a new empirical Bayes framework that incorporates data in the prior in two ways: one is to center the prior for the non-zero regression coefficients and the other is to provide some additional regularization.  We show that, in certain settings, the asymptotic concentration of the proposed empirical Bayes posterior predictive distribution is very fast, and we establish a Bernstein--von Mises theorem which ensures that the derived empirical Bayes prediction intervals achieve the targeted frequentist coverage probability.  The empirical prior has a convenient conjugate form, so posterior computations are relatively simple and fast.  Finally, our numerical results demonstrate the proposed method's strong finite-sample performance in terms of prediction accuracy, uncertainty quantification, and computation time compared to existing Bayesian methods.",
            "keywords": [
                "Bayesian inference",
                "data-dependent prior",
                "model averaging",
                "predictive distri-     bution"
            ],
            "author": [
                "Ryan Martin",
                "Yiqi Tang"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-152/19-152.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Orlicz Random Fourier Features",
            "abstract": "Kernel techniques are among the most widely-applied and influential tools in machine learning with applications at virtually all areas of the field. To combine this expressive power with computational efficiency numerous randomized schemes have been proposed in the literature, among which probably random Fourier features (RFF) are the simplest and most popular. While RFFs were originally designed for the approximation of kernel values, recently they have been adapted to kernel derivatives, and hence to the solution of large-scale tasks involving function derivatives. Unfortunately, the understanding of the RFF scheme for the approximation of higher-order kernel derivatives is quite limited due to the challenging polynomial growing nature of the underlying function class in the empirical process. To tackle this difficulty, we establish a finite-sample deviation bound for a general class of polynomial-growth functions under -exponential Orlicz condition on the distribution of the sample. Instantiating this result for RFFs, our finite-sample uniform guarantee implies a.s. convergence with tight rate for arbitrary kernel with -exponential Orlicz spectrum and any order of derivative.",
            "keywords": [
                "random Fourier features",
                "kernel derivative",
                "polynomial-growth functions",
                "α-exponential Orlicz norm"
            ],
            "author": [
                "Linda Chamakh",
                "Emmanuel Gobet",
                "Zoltán Szabó"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-1031/19-1031.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "New Insights and Perspectives on the Natural Gradient Method",
            "abstract": "Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of 2nd-order optimization method, with the Fisher information matrix acting as a substitute for the Hessian. In many important cases, the Fisher information matrix is shown to be equivalent to the Generalized Gauss-Newton matrix, which both approximates the Hessian, but also has certain properties that favor its use over the Hessian. This perspective turns out to have significant implications for the design of a practical and robust natural gradient optimizer, as it motivates the use of techniques like trust regions and Tikhonov regularization. Additionally, we make a series of contributions to the understanding of natural gradient and 2nd-order methods, including: a thorough analysis of the convergence speed of stochastic natural gradient descent (and more general stochastic 2nd-order methods) as applied to convex quadratics, a critical examination of the oft-used 'empirical' approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by natural gradient methods (which we show also holds for certain other curvature matrices, but notably not the Hessian).",
            "keywords": [
                "natural gradient methods",
                "2nd-order optimization",
                "neural networks",
                "conver-     gence rate"
            ],
            "author": [
                "James Martens"
            ],
            "ref": "http://jmlr.org/papers/volume21/17-678/17-678.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Convergence for Distributed Learning with Stochastic Gradient Methods and Spectral Algorithms",
            "abstract": "We study generalization properties of distributed algorithms in the setting of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We first investigate distributed stochastic gradient methods (SGM), with mini-batches and multi-passes over the data. We show that optimal generalization error bounds (up to logarithmic factor) can be retained for distributed SGM  provided that the partition level is not too large.  We then extend our results to spectral algorithms (SA), including kernel ridge regression (KRR), kernel principal component regression, and gradient methods.  Our results show that distributed SGM has a smaller theoretical computational complexity, compared with distributed KRR and classic SGM. Moreover, even for a general non-distributed SA, they provide optimal, capacity-dependent convergence rates, for the case that the regression function may not be in the RKHS in the well-conditioned regimes.",
            "keywords": [
                "Kernel Methods",
                "Stochastic Gradient Methods",
                "Regularization"
            ],
            "author": [
                "Junhong Lin",
                "Volkan Cevher"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-041/18-041.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Causal Network Learning for Finding Pairs of Total and Direct Effects",
            "abstract": "In observational studies, it is important to evaluate not only the total effect but also the direct and indirect effects of a treatment variable on a response variable. In terms of local structural learning of causal networks, we try to find all possible pairs of total and direct causal effects, which can further be used to calculate indirect causal effects. An intuitive global learning approach is first to find an essential graph over all variables representing all Markov equivalent causal networks, and then enumerate all equivalent networks and estimate a pair of the total and direct effects for each of them. However, it could be inefficient to learn an essential graph and enumerate equivalent networks when the true causal graph is large. In this paper, we propose a local learning approach instead. In the local learning approach, we first learn locally a chain component containing the treatment. Then, if necessary, we learn locally a chain component containing the response. Next, we locally enumerate all possible pairs of the treatment's parents and the response's parents. Finally based on these pairs, we find all possible pairs of total and direct effects of the treatment on the response.",
            "keywords": [
                "causal networks",
                "directed acyclic graphs",
                "total effects",
                "direct effects"
            ],
            "author": [
                "Yue Liu",
                "Zhuangyan Fang",
                "Yangbo He",
                "Zhi Geng",
                "Chunchen Liu"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-083/18-083.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributionally Ambiguous Optimization for Batch Bayesian Optimization",
            "abstract": "We propose a novel, theoretically-grounded, acquisition function for Batch Bayesian Optimization informed by insights from distributionally ambiguous optimization. Our acquisition function is a lower bound on the well-known Expected Improvement function, which requires evaluation of a Gaussian expectation over a multivariate piecewise affine function. Our bound is computed instead by evaluating the best-case expectation over all probability distributions consistent with the same mean and variance as the original Gaussian distribution. Unlike alternative approaches, including Expected Improvement, our proposed acquisition function avoids multi-dimensional integrations entirely, and can be computed exactly - even on large batch sizes - as the solution of a tractable convex optimization problem. Our suggested acquisition function can also be optimized efficiently, since first and second derivative information can be calculated inexpensively as by-products of the acquisition function calculation itself. We derive various novel theorems that ground our work theoretically and we demonstrate superior performance via simple motivating examples, benchmark functions and real-world problems.",
            "keywords": [
                "Bayesian Optimization",
                "Convex Optimization",
                "Distributionally Robust Op-     timization",
                "Batch Optimization"
            ],
            "author": [
                "Nikitas Rontsis",
                "Michael A. Osborne",
                "Paul J. Goulart"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-211/18-211.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Kalai-Smorodinsky solution for many-objective Bayesian optimization",
            "abstract": "An ongoing aim of research in multiobjective Bayesian optimization is to extend its applicability to a large number of objectives.  While coping with a limited budget of evaluations, recovering the set of optimal compromise solutions generally requires numerous observations and is less interpretable since this set tends to grow larger with the number of objectives. We thus propose to focus on a specific solution originating from game theory, the Kalai-Smorodinsky solution, which possesses attractive properties. In particular, it ensures equal marginal gains over all objectives. We further make it insensitive to a monotonic transformation of the objectives by considering the objectives in the copula space.  A novel tailored algorithm is proposed to search for the solution, in the form of a Bayesian optimization algorithm: sequential sampling decisions are made based on acquisition functions that derive from an instrumental Gaussian process prior. Our approach is tested on four problems with respectively four, six, eight, and nine objectives. The method is available in the R package GPGame.",
            "keywords": [
                "Gaussian process",
                "Game theory"
            ],
            "author": [
                "Mickael Binois",
                "Victor Picheny",
                "Patrick Taillandier",
                "Abderrahmane Habbal"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-212/18-212.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Reinforcement Learning with Bayesian Optimisation and Quadrature",
            "abstract": "Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables: state features that are unobservable and randomly determined by the environment in a physical setting but are controllable in a simulator. This article considers the problem of finding a robust policy while taking into account the impact of environment variables. We present Alternating Optimisation and Quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. We also present Transferable ALOQ (TALOQ), for settings where simulator inaccuracies lead to difficulty in transferring the learnt policy to the physical system. We show that our algorithms are robust to the presence of significant rare events, which may not be observable under random sampling but play a substantial role in determining the optimal policy. Experimental results across different domains show that our algorithms learn robust policies efficiently.",
            "keywords": [
                "Reinforcement Learning",
                "Bayesian Optimisation",
                "Bayesian Quadrature",
                "Sig-     nificant rare events"
            ],
            "author": [
                "Supratik Paul",
                "Konstantinos Chatzilygeroudis",
                "Kamil Ciosek",
                "Jean-Baptiste Mouret",
                "Michael A. Osborne",
                "Shimon Whiteson"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-216/18-216.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dual Iterative Hard Thresholding",
            "abstract": "Iterative Hard Thresholding (IHT) is a popular class of first-order greedy selection methods for loss minimization under cardinality constraint. The existing IHT-style algorithms, however, are proposed for minimizing the primal formulation. It is still an open issue to explore duality theory and algorithms for such a non-convex and NP-hard combinatorial optimization problem. To address this issue, we develop in this article a novel duality theory for -regularized empirical risk minimization under cardinality constraint, along with an IHT-style algorithm for dual optimization. Our sparse duality theory establishes a set of sufficient and/or necessary conditions under which the original non-convex problem can be equivalently or approximately solved in a concave dual formulation. In view of this theory, we propose the Dual IHT (DIHT) algorithm  as a super-gradient ascent method to solve the non-smooth dual problem with provable guarantees on primal-dual gap convergence and sparsity recovery. Numerical results confirm our theoretical predictions and demonstrate the superiority of DIHT to the state-of-the-art primal IHT-style algorithms in model estimation accuracy and computational efficiency.",
            "keywords": [
                "Iterative hard thresholding",
                "Duality theory",
                "Sparsity recovery"
            ],
            "author": [
                "Xiao-Tong Yuan",
                "Bo Liu",
                "Lezi Wang",
                "Qingshan Liu",
                "Dimitris N. Metaxas"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-487/18-487.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral Algorithms for Community Detection in Directed Networks",
            "abstract": "Community detection in large social networks is affected by degree heterogeneity of nodes. The D-SCORE algorithm for directed networks was introduced to reduce this effect by taking the element-wise ratios of the singular vectors of the adjacency matrix before clustering. Meaningful results were obtained for the statistician citation network, but rigorous analysis on its performance was missing. First, this paper  establishes theoretical guarantee for this algorithm and its variants for the directed degree-corrected block model (Directed-DCBM). Second, this paper provides significant improvements for the original D-SCORE algorithms  by attaching the nodes outside of the community cores using the information of the original network instead of the singular vectors.",
            "keywords": [
                "directed networks",
                "community detection",
                "clustering",
                "degree-corrected block     model",
                "k-means"
            ],
            "author": [
                "Zhe Wang",
                "Yingbin Liang",
                "Pengsheng Ji"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-581/18-581.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning from Binary Multiway Data: Probabilistic Tensor Decomposition and its Statistical Optimality",
            "abstract": "We consider the problem of decomposing a higher-order tensor with binary entries. Such data problems arise frequently in applications such as neuroimaging, recommendation system, topic modeling, and sensor network localization. We propose a multilinear Bernoulli model, develop a rank-constrained likelihood-based estimation method, and obtain the theoretical accuracy guarantees. In contrast to continuous-valued problems, the binary tensor problem exhibits an interesting phase transition phenomenon according to the signal-to-noise ratio. The error bound for the parameter tensor estimation is established, and we show that the obtained rate is minimax optimal under the considered model. Furthermore, we develop an alternating optimization algorithm with convergence guarantees. The efficacy of our approach is demonstrated through both simulations and analyses of multiple data sets on the tasks of tensor completion and clustering.",
            "keywords": [
                "binary tensor"
            ],
            "author": [
                "Miaoyan Wang",
                "Lexin Li"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-766/18-766.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimate Sequences for Stochastic Composite Optimization: Variance Reduction, Acceleration, and Robustness to Noise",
            "abstract": "In this paper, we propose a unified view of gradient-based algorithms for stochastic convex composite optimization by extending the concept of estimate sequence introduced by Nesterov. More precisely, we interpret a large class of stochastic optimization methods as procedures that iteratively minimize a surrogate of the objective, which covers the stochastic gradient descent method and variants of the incremental approaches SAGA, SVRG, and MISO/Finito/SDCA. This point of view has several advantages: (i) we provide a simple generic proof of convergence for all of the aforementioned methods; (ii) we naturally obtain new algorithms with the same guarantees; (iii) we derive generic strategies to make these algorithms robust to stochastic noise, which is useful when data is corrupted by small random perturbations. Finally, we propose a new accelerated stochastic gradient descent algorithm and a new accelerated SVRG algorithm that is robust to stochastic noise.",
            "keywords": [
                "convex optimization",
                "variance reduction"
            ],
            "author": [
                "Andrei Kulunchakov",
                "Julien Mairal"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-073/19-073.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotic Consistency of α-Rényi-Approximate Posteriors",
            "abstract": "We study the asymptotic consistency properties of -Rényi approximate posteriors, a class of variational Bayesian methods that approximate an intractable Bayesian posterior with a member of a tractable family of distributions, the member chosen to minimize the -Rényi divergence from the true posterior. Unique to our work is that we consider settings with , resulting in approximations that upperbound the log-likelihood, and consequently have wider spread than traditional variational approaches that minimize the Kullback-Liebler (KL) divergence from the posterior. Our primary result identifies sufficient conditions under which consistency holds, centering around the existence of a `good' sequence of distributions in the approximating family that possesses, among other properties, the right rate of convergence to a limit distribution. We further characterize the good sequence by demonstrating that a sequence of distributions that converges too quickly cannot be a good sequence. We also extend our analysis to the setting where  equals one, corresponding to the minimizer of the reverse KL divergence, and to models with local latent variables. We also illustrate the existence of a good sequence with a number of examples. Our results complement a growing body of work focused on the frequentist properties of variational Bayesian methods.",
            "keywords": [],
            "author": [
                "Prateek Jaiswal",
                "Vinayak Rao",
                "Harsha Honnappa"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-161/19-161.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Streamlined Variational Inference with Higher Level Random Effects",
            "abstract": "We derive and present explicit algorithms to facilitate streamlined computing for variational inference for models containing higher level random effects. Existing literature is such that streamlined variational inference is restricted to mean field variational Bayes algorithms for two-level random effects models. Here we provide the following extensions: (1) explicit Gaussian response mean field variational Bayes algorithms for three-level models, (2) explicit algorithms for the alternative variational message passing approach in the case of two-level and three-level models, and (3) an explanation of how arbitrarily high levels of nesting can be handled based on the recently published matrix algebraic results of the authors. A pay-off from (2) is simple extension to non-Gaussian response models. In summary, we remove barriers for streamlining variational inference algorithms based on either the mean field variational Bayes approach or the variational message passing approach when higher level random effects are present.",
            "keywords": [
                "Factor Graph Fragment",
                "Longitudinal Data Analysis",
                "Mixed Models",
                "Multi-     level Models"
            ],
            "author": [
                "Tui H. Nolan",
                "Marianne Menictas",
                "Matt P. Wand"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-222/19-222.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Big Gaussian Bayesian Networks: Partition, Estimation and Fusion",
            "abstract": "Structure learning of Bayesian networks has always been a challenging problem. Nowadays, massive-size networks with thousands or more of nodes but fewer samples frequently appear in many areas. We develop a divide-and-conquer framework, called partition-estimation-fusion (PEF), for structure learning of such big networks. The proposed method first partitions nodes into clusters, then learns a subgraph on each cluster of nodes, and finally fuses all learned subgraphs into one Bayesian network. The PEF method is designed in a flexible way so that any structure learning method may be used in the second step to learn a subgraph structure as either a DAG or a CPDAG. In the clustering step, we adapt hierarchical clustering to automatically choose a proper number of clusters. In the fusion step, we propose a novel hybrid method that sequentially adds edges between subgraphs. Extensive numerical experiments demonstrate the competitive performance of our PEF method, in terms of both speed and accuracy compared to existing methods. Our method can improve the accuracy of structure learning by 20% or more, while reducing running time up to two orders-of-magnitude.",
            "keywords": [
                "Bayesian network",
                "conditional independence",
                "directed acyclic graph",
                "divide-     and-conquer"
            ],
            "author": [
                "Jiaying Gu",
                "Qing Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-318/19-318.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generating Weighted MAX-2-SAT Instances with Frustrated Loops: an RBM Case Study",
            "abstract": "Many optimization problems can be cast into the maximum satisfiability (MAX-SAT) form, and many solvers have been developed for tackling such problems. To evaluate a MAXSAT solver, it is convenient to generate hard MAX-SAT instances with known solutions. Here, we propose a method of generating weighted MAX-2-SAT instances inspired by the frustrated-loop algorithm used by the quantum annealing community. We extend the algorithm for instances of general bipartite couplings, with the associated optimization problem being the minimization of the restricted Boltzmann machine (RBM) energy over the nodal values, which is useful for effectively pre-training the RBM. The hardness of the generated instances can be tuned through a central parameter known as the frustration index. Two versions of the algorithm are presented: the random- and structured-loop algorithms. For the random-loop algorithm, we provide a thorough theoretical and empirical analysis on its mathematical properties from the perspective of frustration, and observe empirically a double phase transition behavior in the hardness scaling behavior driven by the frustration index. For the structured-loop algorithm, we show that it offers an improvement in hardness over the random-loop algorithm in the regime of high loop density, with the variation of hardness tunable through the concentration of frustrated weights.",
            "keywords": [
                "Maximum Satisfiability",
                "Restricted Boltzmann Machine",
                "Frustration Index",
                "Loop Algorithm"
            ],
            "author": [
                "Yan Ru Pei",
                "Haik Manukian",
                "Massimiliano Di Ventra"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-368/19-368.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generative Adversarial Nets for Robust Scatter Estimation: A Proper Scoring Rule Perspective",
            "abstract": "Robust covariance matrix estimation is a fundamental task in statistics. The recent discovery on the connection between robust estimation and generative adversarial nets (GANs) suggests that it is possible to compute depth-like robust estimators using similar techniques that optimize GANs. In this paper, we introduce a general learning via classification framework based on the notion of proper scoring rules. This framework allows us to understand both matrix depth function, a technique of rate-optimal robust estimation, and various  GANs through the lens of variational approximations of -divergences induced by proper scoring rules. We then propose a new class of robust covariance matrix estimators in this framework by carefully constructing discriminators with appropriate neural network structures. These estimators are proved to achieve the minimax rate of covariance matrix estimation under Huber's contamination model. The results are also extended to robust scatter estimation for elliptical distributions. Our numerical results demonstrate the good performance of the proposed procedures under various settings against competitors in the literature.",
            "keywords": [
                "robust statistics",
                "neural networks",
                "minimax rate",
                "data depth",
                "contamination     model"
            ],
            "author": [
                "Chao Gao",
                "Yuan Yao",
                "Weizhi Zhu"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-462/19-462.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "apricot: Submodular selection for data summarization in Python",
            "abstract": "We present apricot, an open source Python package for selecting representative subsets from large data sets using submodular optimization. The package implements several efficient greedy selection algorithms that offer strong theoretical guarantees on the quality of the selected set. Additionally, several submodular set functions are implemented, including facility location, which is broadly applicable but requires memory quadratic in the number of examples in the data set, and a feature-based function that is less broadly applicable but can scale to millions of examples. Apricot is extremely efficient, using both algorithmic speedups such as the lazy greedy algorithm and memoization as well as code optimization using numba. We demonstrate the use of subset selection by training machine learning models to comparable accuracy using either the full data set or a representative subset thereof. This paper presents an explanation of submodular selection, an overview of the features in apricot, and applications to two data sets.",
            "keywords": [
                "submodular selection",
                "submodularity",
                "big data",
                "machine learning"
            ],
            "author": [
                "Jacob Schreiber",
                "Jeffrey Bilmes",
                "William Stafford Noble"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-467/19-467.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regression with Comparisons: Escaping the Curse of Dimensionality with Ordinal Information",
            "abstract": "In supervised learning, we typically leverage a fully labeled dataset to design methods for function estimation or prediction. In many practical situations, we are able to obtain alternative feedback, possibly at a low cost. A broad goal is to understand the usefulness of, and to design algorithms to exploit, this alternative feedback. In this paper, we consider a semi-supervised regression setting, where we obtain additional ordinal (or comparison) information for the unlabeled samples. We consider ordinal feedback of varying qualities where we have either a perfect ordering of the samples, a noisy ordering of the samples or noisy pairwise comparisons between the samples. We provide a precise quantification of the usefulness of these types of ordinal feedback in both nonparametric and linear regression, showing that in many cases it is possible to accurately estimate an underlying function with a very small labeled set, effectively escaping the curse of dimensionality. We also present lower bounds, that establish fundamental limits for the task and show that our algorithms are optimal in a variety of settings. Finally, we present extensive experiments on new datasets that demonstrate the efficacy and practicality of our algorithms and investigate their robustness to various sources of noise and model misspecification.",
            "keywords": [
                "Pairwise Comparison",
                "Ranking",
                "Regression"
            ],
            "author": [
                "Yichong Xu",
                "Sivaraman Balakrishnan",
                "Aarti Singh",
                "Artur Dubrawski"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-505/19-505.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Trust-Region Variational Inference with Gaussian Mixture Models",
            "abstract": "Many methods for machine learning rely on approximate inference from intractable probability distributions. Variational inference approximates such distributions by tractable models that can be subsequently used for approximate inference. Learning sufficiently accurate approximations requires a rich model family and careful exploration of the relevant modes of the target distribution. We propose a method for learning accurate GMM approximations of intractable probability distributions based on insights from policy search by using information-geometric trust regions for principled exploration. For efficient improvement of the GMM approximation, we derive a lower bound on the corresponding optimization objective enabling us to update the components independently. Our use of the lower bound ensures convergence to a stationary point of the original objective. The number of components is adapted online by adding new components in promising regions and by deleting components with negligible weight. We demonstrate on several domains that we can learn approximations of complex, multimodal distributions with a quality that is unmet by previous variational inference methods, and that the GMM approximation can be used for drawing samples that are on par with samples created by state-of-the-art MCMC samplers while requiring up to three orders of magnitude less computational resources.",
            "keywords": [],
            "author": [
                "Oleg Arenz",
                "Mingjun Zhong",
                "Gerhard Neumann"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-524/19-524.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cramer-Wold Auto-Encoder",
            "abstract": "The computation of the distance to the true distribution is a key component of most state-of-the-art generative models. Inspired by prior works on the Sliced-Wasserstein Auto-Encoders (SWAE) and the Wasserstein Auto-Encoders with MMD-based penalty (WAE-MMD), we propose a new generative model - a Cramer-Wold Auto-Encoder (CWAE). A fundamental component of CWAE is the characteristic kernel, the construction of which is one of the goals of this paper, from here on referred to as the Cramer-Wold kernel. Its main distinguishing feature is that it has a closed-form of the kernel product of radial Gaussians. Consequently, CWAE model has a~closed-form for the distance between the posterior and the normal prior, which simplifies the optimization procedure by removing the need to sample in order to compute the loss function. At the same time, CWAE performance often improves upon WAE-MMD and SWAE on standard benchmarks.",
            "keywords": [
                "Auto-Encoder",
                "Generative model",
                "Wasserstein Auto-Encoder",
                "Cramer-Wold    Theorem"
            ],
            "author": [
                "Szymon Knop",
                "Przemysław Spurek",
                "Jacek Tabor",
                "Igor Podolak",
                "Marcin Mazur",
                "Stanisław Jastrzębski"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-560/19-560.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Complete Dictionary Learning via L4-Norm Maximization over the Orthogonal Group",
            "abstract": "This paper considers the fundamental problem of learning a complete (orthogonal) dictionary from samples of sparsely generated signals. Most existing methods solve the dictionary (and sparse representations) based on heuristic algorithms, usually without theoretical guarantees for either optimality or complexity. The recent -minimization based methods do provide such guarantees but the associated algorithms recover the dictionary one column at a time. In this work, we propose a new formulation that maximizes the -norm over the orthogonal group, to learn the entire dictionary. We prove that under a random data model, with nearly minimum sample complexity, the global optima of the -norm are very close to signed permutations of the ground truth. Inspired by this observation, we give a conceptually simple and yet effective algorithm based on matching, stretching, and projection (MSP). The algorithm provably converges locally and cost per iteration is merely an SVD. In addition to strong theoretical guarantees, experiments show that the new algorithm is significantly more efficient and effective than existing methods, including KSVD and -based methods. Preliminary experimental results on mixed real imagery data clearly demonstrate advantages of so learned dictionary over classic PCA bases.",
            "keywords": [
                "sparse dictionary learning"
            ],
            "author": [
                "Yuexiang Zhai",
                "Zitong Yang",
                "Zhenyu Liao",
                "John Wright",
                "Yi Ma"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-755/19-755.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High Dimensional Forecasting via Interpretable Vector Autoregression",
            "abstract": "Vector autoregression (VAR) is a fundamental tool for modeling multivariate time series.  However, as the number of component series is increased, the VAR model becomes overparameterized.  Several authors have addressed this issue by incorporating regularized approaches, such as the lasso in VAR estimation. Traditional approaches address overparameterization by selecting a low lag order, based on the assumption of short range dependence, assuming that a universal lag order applies to all components.  Such an approach constrains the relationship between the components and impedes forecast performance.  The lasso-based approaches perform much better in high-dimensional situations but do not incorporate the notion of lag order selection. We propose a new class of hierarchical lag structures (HLag) that embed the notion of lag selection into a convex regularizer. The key modeling tool is a group lasso with nested groups which guarantees that the sparsity pattern of lag coefficients honors the VAR's ordered structure.  The proposed HLag framework offers three basic structures, which allow for varying levels of flexibility, with many possible generalizations.  A simulation study demonstrates improved performance in forecasting and lag order selection over previous approaches, and macroeconomic, financial, and energy applications further highlight forecasting improvements as well as HLag's convenient, interpretable output.",
            "keywords": [
                "forecasting",
                "group lasso",
                "multivariate time series",
                "variable selection"
            ],
            "author": [
                "William B. Nicholson",
                "Ines Wilms",
                "Jacob Bien",
                "David S. Matteson"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-777/19-777.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convex and Non-Convex Approaches for Statistical Inference with Class-Conditional Noisy Labels",
            "abstract": "We study the problem of estimation and testing in logistic regression with class-conditional noise in the observed labels, which has an important implication in the Positive-Unlabeled (PU) learning setting. With the key observation that the label noise problem belongs to a special sub-class of generalized linear models (GLM), we discuss convex and non-convex approaches that address this problem. A non-convex approach based on the maximum likelihood estimation produces an estimator with several optimal properties, but a convex approach has an obvious advantage in optimization. We demonstrate that in the low-dimensional setting, both estimators are consistent and asymptotically normal, where the asymptotic variance of the non-convex estimator is smaller than the convex counterpart. We also quantify the efficiency gap which provides insight into when the two methods are comparable. In the high-dimensional setting, we show that both estimation procedures achieve -consistency at the minimax optimal  rates under mild conditions. Finally, we propose an inference procedure using a de-biasing approach. We validate our theoretical findings through simulations and a real-data example.",
            "keywords": [
                "generalized linear model",
                "non-convexity",
                "class-conditional label noise",
                "PU-     learning"
            ],
            "author": [
                "Hyebin Song",
                "Ran Dai",
                "Garvesh Raskutti",
                "Rina Foygel Barber"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-833/19-833.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Optimal Ridge Penalty for Real-world High-dimensional Data Can Be Zero or Negative due to the Implicit Ridge Regularization",
            "abstract": "A conventional wisdom in statistical learning is that large models require strong regularization to prevent overfitting. Here we show that this rule can be violated by  linear regression in the underdetermined  situation under realistic conditions. Using simulations and real-life high-dimensional datasets, we demonstrate that an explicit positive ridge penalty can fail to provide any improvement over the minimum-norm least squares estimator. Moreover, the optimal value of ridge penalty in this situation can be negative. This happens when the high-variance directions in the predictor space can predict the response variable, which is often the case in the real-world high-dimensional data. In this regime, low-variance directions provide an implicit ridge regularization and can make any further positive ridge penalty detrimental. We prove that augmenting any linear model with random covariates and using minimum-norm estimator is asymptotically equivalent to adding the ridge penalty. We use a spiked covariance model as an analytically tractable example and prove that the optimal ridge penalty in this case is negative when .",
            "keywords": [
                "High-dimensional",
                "ridge regression"
            ],
            "author": [
                "Dmitry Kobak",
                "Jonathan Lomond",
                "Benoit Sanchez"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-844/19-844.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rationally Inattentive Inverse Reinforcement Learning Explains YouTube Commenting Behavior",
            "abstract": "We consider a novel application of inverse reinforcement learning with behavioral economics constraints to model, learn and predict the commenting behavior of YouTube viewers. Each group of users is modeled as a rationally inattentive Bayesian agent which solves a contextual bandit problem. Our methodology integrates three key components. First, to identify distinct commenting patterns, we use deep embedded clustering to estimate framing information (essential extrinsic  features) that clusters users into distinct groups. Second, we present an inverse reinforcement learning algorithm that uses Bayesian revealed preferences to test for rationality: does there exist a utility function that rationalizes the given data, and if yes, can it be used to predict commenting behavior? Finally, we impose behavioral economics constraints stemming from rational inattention to characterize the attention span of groups of users. The test imposes a Renyi mutual information cost constraint which impacts how the agent can select attention strategies to maximize their expected utility. After a careful analysis of a massive YouTube dataset, our surprising result is that in most YouTube user groups, the commenting behavior is consistent with optimizing a Bayesian utility with rationally inattentive constraints. The paper also highlights how the rational inattention model can accurately predict commenting behavior. The massive YouTube dataset and analysis used in this paper are available on GitHub and completely reproducible.",
            "keywords": [
                "Inverse Reinforcement Learning",
                "Bayesian Revealed Preference",
                "YouTube",
                "Rational     Inattention",
                "Rényi Mutual Information",
                "Framing",
                "Behavioral Economics",
                "Deep Embedded Clustering"
            ],
            "author": [
                "William Hoiles",
                "Vikram Krishnamurthy",
                "Kunal Pattanayak"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-872/19-872.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Randomization as Regularization:  A Degrees of Freedom Explanation for Random Forest Success",
            "abstract": "Random forests remain among the most popular off-the-shelf supervised machine learning tools with a well-established track record of predictive accuracy in both regression and classification settings.  Despite their empirical success as well as a bevy of recent work investigating their statistical properties, a full and satisfying explanation for their success has yet to be put forth. Here we aim to take a step forward in this direction by demonstrating that the additional randomness injected into individual trees serves as a form of implicit regularization, making random forests an ideal model in low signal-to-noise ratio (SNR) settings. Specifically, from a model-complexity perspective, we show that the mtry parameter in random forests serves much the same purpose as the shrinkage penalty in explicitly regularized regression procedures like lasso and ridge regression.  To highlight this point, we design a randomized linear-model-based forward selection procedure intended as an analogue to tree-based random forests and demonstrate its surprisingly strong empirical performance.  Numerous demonstrations on both real and synthetic data are provided.",
            "keywords": [
                "Regularization",
                "Bagging",
                "Degrees of Freedom",
                "Model Selection"
            ],
            "author": [
                "Lucas Mentch",
                "Siyu Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-905/19-905.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Krylov Subspace Method for Nonlinear Dynamical Systems with Random Noise",
            "abstract": "Operator-theoretic analysis of nonlinear dynamical systems has attracted much attention in a variety of engineering and scientific fields, endowed with practical estimation methods using data such as dynamic mode decomposition. In this paper, we address a lifted representation of nonlinear dynamical systems with random noise based on transfer operators, and develop a novel Krylov subspace method for estimating the operators using finite data, with consideration of the unboundedness of operators. For this purpose, we first consider Perron-Frobenius operators with kernel-mean embeddings for such systems. We then extend the Arnoldi method, which is the most classical type of Kryov subspace methods, so that it can be applied to the current case. Meanwhile, the Arnoldi method requires the assumption that the operator is bounded, which is not necessarily satisfied for transfer operators on nonlinear systems. We accordingly develop the shift-invert Arnoldi method for Perron-Frobenius operators to avoid this problem. Also, we describe an approach of evaluating predictive accuracy by estimated operators on the basis of the maximum mean discrepancy, which is applicable, for example, to anomaly detection in complex systems. The empirical performance of our methods is investigated using synthetic and real-world healthcare data.",
            "keywords": [],
            "author": [
                "Yuka Hashimoto",
                "Isao Ishikawa",
                "Masahiro Ikeda",
                "Yoichi Matsuo",
                "Yoshinobu Kawahara"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-993/19-993.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Doubly Distributed Supervised Learning and Inference with High-Dimensional Correlated Outcomes",
            "abstract": "This paper presents a unified framework for supervised learning and inference procedures using the divide-and-conquer approach for high-dimensional correlated outcomes. We propose a general class of estimators that can be implemented in a fully distributed and parallelized computational scheme. Modeling, computational and theoretical challenges related to high-dimensional correlated outcomes are overcome by dividing data at both outcome and subject levels, estimating the parameter of interest from blocks of data using a broad class of supervised learning procedures, and combining block estimators in a closed-form meta-estimator asymptotically equivalent to estimates obtained by Hansen (1982)'s generalized method of moments (GMM) that does not require the entire data to be reloaded on a common server. We provide rigorous theoretical justifications for the use of distributed estimators with correlated outcomes by studying the asymptotic behaviour of the combined estimator with fixed and diverging number of data divisions. Simulations illustrate the finite sample performance of the proposed method, and we provide an R package for ease of implementation.",
            "keywords": [
                "Divide-and-conquer",
                "Generalized method of moments",
                "Estimating functions",
                "Parallel computing"
            ],
            "author": [
                "Emily C. Hector",
                "Peter X.-K. Song"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-996/19-996.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Approximation and Generalization of Deep Neural Network with Intrinsic Dimensionality",
            "abstract": "In this study, we prove that an intrinsic low dimensionality of covariates is the main factor that determines the performance of deep neural networks (DNNs). DNNs generally provide outstanding empirical performance. Hence, numerous studies have actively investigated the theoretical properties of DNNs to understand their underlying mechanisms. In particular, the behavior of DNNs in terms of high-dimensional data is one of the most critical questions. However, this issue has not been sufficiently investigated from the aspect of covariates, although high-dimensional data have practically low intrinsic dimensionality. In this study, we derive bounds for an approximation error and a generalization error regarding DNNs with intrinsically low dimensional covariates. We apply the notion of the Minkowski dimension and develop a novel proof technique. Consequently, we show that convergence rates of the errors by DNNs do not depend on the nominal high dimensionality of data, but on its lower intrinsic dimension. We further prove that the rate is optimal in the minimax sense. We identify an advantage of DNNs by showing that DNNs can handle a broader class of intrinsic low dimensional data than other adaptive estimators. Finally, we conduct a numerical simulation to validate the theoretical results.",
            "keywords": [
                "Deep Learning",
                "Deep Neural Network",
                "Generalization Analysis",
                "Intrinsic     Dimension"
            ],
            "author": [
                "Ryumei Nakada",
                "Masaaki Imaizumi"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-002/20-002.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Wide Neural Networks with Bottlenecks are Deep Gaussian Processes",
            "abstract": "There has recently been much work on the \"wide limit\" of neural networks, where Bayesian neural networks (BNNs) are shown to converge to a Gaussian process (GP) as all hidden layers are sent to infinite width. However, these results do not apply to architectures that require one or more of the hidden layers to remain narrow. In this paper, we consider the wide limit of BNNs where some hidden layers, called \"bottlenecks\", are held at finite width. The result is a composition of GPs that we term a \"bottleneck neural network Gaussian process\" (bottleneck NNGP). Although intuitive, the subtlety of the proof is in showing that the wide limit of a composition of networks is in fact the composition of the limiting GPs. We also analyze theoretically a single-bottleneck NNGP, finding  that the bottleneck induces dependence between the outputs of a multi-output network that persists through extreme post-bottleneck depths, and prevents the kernel of the network from losing discriminative power at extreme post-bottleneck depths.",
            "keywords": [
                "Bayesian neural networks",
                "deep learning",
                "Gaussian processes",
                "kernels"
            ],
            "author": [
                "Devanshu Agrawal",
                "Theodore Papamarkou",
                "Jacob Hinkle"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-017/20-017.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Breaking the Curse of Nonregularity with Subagging --- Inference of the Mean Outcome under Optimal Treatment Regimes",
            "abstract": "Precision medicine is an emerging medical approach that allows physicians to select the treatment options based on individual patient information. The goal of precision medicine is to identify the optimal treatment regime (OTR) that yields the most favorable clinical outcome. Prior to adopting any OTR in clinical practice, it is crucial to know the impact of implementing such a policy. Although considerable research has been devoted to estimating the OTR in the literature, less attention has been paid to statistical inference of the OTR. Challenges arise in the nonregular cases where the OTR is not uniquely defined. To deal with nonregularity, we develop a novel inference method for the mean outcome under an OTR (the optimal value function) based on subsample aggregating (subagging). The proposed method can be applied to multi-stage studies where treatments are sequentially assigned over time. Bootstrap aggregating (bagging) and subagging have been recognized as effective vari- ance reduction techniques to improve unstable estimators or classifiers (Buhlmann and Yu, 2002). However, it remains unknown whether these approaches can yield valid inference results. We show the proposed confidence interval (CI) for the optimal value function achieves nominal coverage. In addition, due to the variance reduction effect of subagging, our method enjoys certain statistical optimality. Specifically, we show that the mean squared error of the proposed value estimator is strictly smaller than that based on the simple sample-splitting estimator in the nonregular cases. Moreover, under certain conditions, the length of our proposed CI is shown to be on average shorter than CIs constructed based on the existing state-of-the-art method (Luedtke and van der Laan, 2016) and the 'oracle' method which works as well as if an OTR were known. Extensive numerical studies are conducted to back up our theoretical findings.",
            "keywords": [],
            "author": [
                "Chengchun Shi",
                "Wenbin Lu",
                "Rui Song"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-066/20-066.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
            "abstract": "In many real-world settings, a team of agents must coordinate its behaviour while acting in a decentralised fashion. At the same time, it is often possible to train the agents in a centralised fashion where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion.  QMIX employs a mixing network that estimates joint action-values as a monotonic combination of per-agent values. We structurally enforce that the joint-action value is monotonic in the per-agent values, through the use of non-negative weights in the mixing network, which  guarantees consistency between the centralised and decentralised policies. To evaluate the performance of QMIX, we propose the StarCraft Multi-Agent Challenge (SMAC) as a new benchmark for deep multi-agent reinforcement learning. We evaluate QMIX on a challenging set of SMAC scenarios and show that it significantly outperforms existing multi-agent reinforcement learning methods.",
            "keywords": [
                "Reinforcement Learning",
                "Multi-Agent Learning"
            ],
            "author": [
                "Tabish Rashid",
                "Mikayel Samvelyan",
                "Christian Schroeder de Witt",
                "Gregory Farquhar",
                "Jakob Foerster",
                "Shimon Whiteson"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-081/20-081.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Inference for Computational Imaging Inverse Problems",
            "abstract": "Machine learning methods for computational imaging require uncertainty estimation to be reliable in real settings. While Bayesian models offer a computationally tractable way of recovering uncertainty, they need large data volumes to be trained, which in imaging applications implicates prohibitively expensive collections with specific imaging instruments. This paper introduces a novel framework to train variational inference for inverse problems exploiting in combination few experimentally collected data, domain expertise and existing image data sets. In such a way, Bayesian machine learning models can solve imaging inverse problems with minimal data collection efforts. Extensive simulated experiments show the advantages of the proposed framework. The approach is then applied to two real experimental optics settings: holographic image reconstruction and imaging through highly scattering media. In both settings, state of the art reconstructions are achieved with little collection of training data.",
            "keywords": [
                "Inverse Problems",
                "Approximate Inference",
                "Bayesian Inference"
            ],
            "author": [
                "Francesco Tonolini",
                "Jack Radford",
                "Alex Turpin",
                "Daniele Faccio",
                "Roderick Murray-Smith"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-151/20-151.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Communication-Efficient Distributed Optimization in Networks with Gradient Tracking and Variance Reduction",
            "abstract": "There is growing interest in large-scale machine learning and optimization over decentralized networks, e.g. in the context of multi-agent learning and federated learning. Due to the imminent need to alleviate the communication burden, the investigation of communication-efficient distributed optimization algorithms --- particularly for empirical risk minimization --- has flourished in recent years. A large fraction of these algorithms have been developed for the master/slave setting, relying on the presence of a central parameter server that can communicate with all agents. This paper focuses on distributed optimization over networks, or decentralized optimization, where each agent is only allowed to aggregate information from its neighbors over a network (namely, no centralized coordination is present). By properly adjusting the global gradient estimate via local averaging in conjunction with proper correction, we develop a communication-efficient approximate Newton-type method, called Network-DANE, which generalizes DANE to accommodate decentralized scenarios. Our key ideas can be applied, in a systematic manner, to obtain decentralized versions of other master/slave distributed algorithms. A notable development is Network-SVRG/SARAH, which employs variance reduction at each agent to further accelerate local computation. We establish linear convergence of Network-DANE and Network-SVRG for strongly  convex losses, and Network-SARAH for quadratic losses, which shed light on the impacts of data homogeneity, network connectivity, and local averaging upon the rate of convergence. We further extend Network-DANE to composite optimization by allowing a nonsmooth penalty term. Numerical evidence is provided to demonstrate the appealing performance of our algorithms over competitive baselines, in terms of both communication and computation efficiency. Our work suggests that by performing a judiciously chosen amount of local communication and computation per iteration, the overall efficiency can be substantially improved.",
            "keywords": [
                "decentralized optimization",
                "federated learning",
                "communication efficiency",
                "gradient tracking",
                "variance reductionc 2020 Boyue Li",
                "Shicong Cen",
                "Yuxin Chen"
            ],
            "author": [
                "Boyue Li",
                "Shicong Cen",
                "Yuxin Chen",
                "Yuejie Chi"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-210/20-210.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey",
            "abstract": "Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.",
            "keywords": [
                "curriculum learning",
                "reinforcement learning",
                "transfer learningc 2020 Sanmit Narvekar",
                "Bei Peng",
                "Matteo Leonetti",
                "Jivko Sinapov"
            ],
            "author": [
                "Sanmit Narvekar",
                "Bei Peng",
                "Matteo Leonetti",
                "Jivko Sinapov",
                "Matthew E. Taylor",
                "Peter Stone"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-212/20-212.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed High-dimensional  Regression Under a Quantile Loss Function",
            "abstract": "This paper studies distributed estimation and support recovery for high-dimensional linear regression model with heavy-tailed noise. To deal with heavy-tailed noise whose variance can be infinite, we adopt the quantile regression loss function instead of the commonly used squared loss. However, the non-smooth quantile loss  poses new challenges to high-dimensional distributed estimation in both computation and theoretical development. To address the challenge, we transform the response variable and establish a new connection between quantile regression and ordinary linear regression. Then, we provide a distributed estimator that is both computationally and communicationally efficient, where  only the gradient information is communicated at each iteration. Theoretically, we show that, after a constant number of iterations, the proposed estimator achieves a near-oracle convergence  rate without any restriction on the number of machines. Moreover, we establish the theoretical guarantee for the support recovery. The simulation analysis is provided to demonstrate the effectiveness of our method.",
            "keywords": [
                "Distributed estimation",
                "high-dimensional linear model",
                "quantile loss",
                "robust     estimator"
            ],
            "author": [
                "Xi Chen",
                "Weidong Liu",
                "Xiaojun Mao",
                "Zhuoyi Yang"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-297/20-297.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Topology of Deep Neural Networks",
            "abstract": "We study how the topology of a data set , representing two classes  and  in a binary classification problem, changes as it passes through the layers of a well-trained neural network, i.e., one with perfect accuracy on training set and near-zero generalization error (). The goal is to shed light on two mysteries in deep neural networks: (i) a nonsmooth activation function like ReLU outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, even though a shallow network can approximate any function arbitrarily well. We performed extensive experiments on the persistent homology of a wide range of point cloud data sets, both real and simulated. The results consistently demonstrate the following: (1) Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of  we begin with, when passed through a well-trained neural network , there is a vast reduction in the Betti numbers of both components  and ; in fact they nearly always reduce to their lowest possible values:  for  and , . (2) The reduction in Betti numbers is significantly faster for ReLU activation than for hyperbolic tangent activation as the former defines nonhomeomorphic maps that change topology, whereas the latter defines homeomorphic maps that preserve topology. (3) Shallow and deep networks transform data sets differently --- a shallow network operates mainly through changing geometry and changes topology only in its final layers, a deep one spreads topological changes more evenly across all layers.",
            "keywords": [
                "neural networks",
                "topology change",
                "Betti numbers",
                "topological complexity"
            ],
            "author": [
                "Gregory Naitzat",
                "Andrey Zhitnikov",
                "Lek-Heng Lim"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-345/20-345.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency of Semi-Supervised Learning Algorithms on Graphs: Probit and One-Hot Methods",
            "abstract": "Graph-based semi-supervised learning is the problem of propagating labels from a small number of labelled data points to a larger set of unlabelled data.  This paper is concerned with the consistency of optimization-based techniques for such problems, in the limit where the labels have small noise and the underlying unlabelled data is well clustered.  We study graph-based probit for binary classification,  and a natural generalization of this method to multi-class classification using one-hot encoding.  The resulting objective function to be optimized comprises the sum of a quadratic form defined through a rational function of the graph Laplacian, involving only the unlabelled data, and a fidelity term involving only the labelled data.  The consistency analysis sheds light on the choice of the rational function defining the optimization.",
            "keywords": [
                "Semi-supervised learning",
                "classification",
                "consistency",
                "graph Laplacian",
                "probit"
            ],
            "author": [
                "Franca Hoffmann",
                "Bamdad Hosseini",
                "Zhi Ren",
                "Andrew M Stuart"
            ],
            "ref": "http://jmlr.org/papers/volume21/15-900/15-900.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kriging Prediction with Isotropic Matern Correlations: Robustness and Experimental Designs",
            "abstract": "This work investigates the prediction performance of the kriging predictors. We derive some error bounds for the prediction error in terms of non-asymptotic probability under the uniform metric and  metrics when the spectral densities of both the true and the imposed correlation functions decay algebraically. The Matern family is a prominent class of correlation functions of this kind. Our analysis shows that, when the smoothness of the imposed correlation function exceeds that of the true correlation function, the prediction error becomes more sensitive to the space-filling property of the design points. In particular, the kriging predictor can still reach the optimal rate of convergence, if the experimental design scheme is quasi-uniform. Lower bounds of the kriging prediction error are also derived under the uniform metric and  metrics. An accurate characterization of this error is obtained, when an oversmoothed correlation function and a space-filling design is used.",
            "keywords": [
                "Computer Experiments",
                "Uncertainty Quantification",
                "Scattered Data Approx-     imation",
                "Space-filling Designs"
            ],
            "author": [
                "Rui Tuo",
                "Wenjia Wang"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-1045/19-1045.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Adjustment Sets for Population Average Causal Treatment Effect Estimation in Graphical Models",
            "abstract": "The method of covariate adjustment is often used for estimation of total treatment effects from observational studies. Restricting attention to causal linear models, a recent article (Henckel et al., 2019) derived two novel graphical criteria: one to compare the asymptotic variance of linear regression treatment effect estimators that control for certain distinct adjustment sets and another to identify the optimal adjustment set that yields the least squares estimator with the smallest asymptotic variance. In this paper we show that the same graphical criteria can be used in non-parametric causal graphical models when treatment effects are estimated using non-parametrically adjusted estimators of the interventional means. We also provide a new graphical criterion for determining the optimal adjustment set among the minimal adjustment sets and another novel graphical criterion for comparing  time dependent adjustment sets. We show that uniformly optimal time dependent adjustment sets do not always exist. For point interventions, we provide a sound and complete graphical criterion for determining when a non-parametric optimally adjusted estimator of an interventional mean, or of a contrast of interventional means, is semiparametric efficient under the non-parametric causal graphical model. In addition, when the criterion is not met, we provide a sound algorithm that checks for possible simplifications of the efficient influence function of the parameter. Finally, we find an interesting connection between identification and efficient covariate adjustment estimation. Specifically, we show that if there exists an identifying formula for an interventional mean that depends only on treatment, outcome and mediators, then the non-parametric optimally adjusted estimator can never be globally efficient under the causal graphical model.",
            "keywords": [
                "adjustment sets",
                "back-door formula",
                "Bayesian networks",
                "causal inference"
            ],
            "author": [
                "Andrea Rotnitzky",
                "Ezequiel Smucler"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-1026/19-1026.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Beyond Trees: Classification with Sparse Pairwise Dependencies",
            "abstract": "Several classification methods assume that the underlying distributions follow tree-structured graphical models. Indeed, trees capture statistical dependencies between pairs of variables, which may be crucial to attaining low classification errors. In this setting, the optimal classifier is linear in the log-transformed univariate and bivariate densities that correspond to the tree edges. In practice, observed data may not be well approximated by trees. Yet, motivated by the importance of pairwise dependencies for accurate classification, here we propose to approximate the optimal decision boundary by a sparse linear combination of the univariate and bivariate log-transformed densities. Our proposed approach is semi-parametric in nature: we non-parametrically estimate the univariate and bivariate densities, remove pairs of variables that are nearly independent using the Hilbert-Schmidt independence criterion, and finally construct a linear SVM using the retained log-transformed densities. We demonstrate on synthetic and real data sets, that our classifier, named SLB (sparse log-bivariate density), is competitive with other popular classification methods.",
            "keywords": [
                "binary classification",
                "graphical model",
                "Bayesian network",
                "sparsity",
                "semi-parametricc 2020 Yaniv Tenzer",
                "Amit Moscovich",
                "Mary Frances Dorn"
            ],
            "author": [
                "Yaniv Tenzer",
                "Amit Moscovich",
                "Mary Frances Dorn",
                "Boaz Nadler",
                "Clifford Spiegelman"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-348/18-348.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unified q-Memorization  Framework for   Asynchronous Stochastic Optimization",
            "abstract": "Asynchronous stochastic algorithms with various variance reduction techniques (such as SVRG, S2GD, SAGA and q-SAGA) are popular in solving large scale learning problems. Recently, Reddi et al. (2015) proposed an unified variance reduction framework (i.e., HSAG) to analyze the asynchronous stochastic gradient optimization. However, the HSAG framework cannot incorporate the S2GD technique, the analysis of the HSAG framework is limited to the SVRG and SAGA techniques on the smooth convex optimization. They did not analyze other important various variance techniques (e.g., S2GD and q-SAGA) and other important optimization problems (e.g., convex optimization with non-smooth regularization and non-convex optimization with cardinality constraint). In this paper, we bridge this gap by using an unified q-memorization framework for various variance reduction techniques (including SVRG, S2GD, SAGA, q-SAGA) to analyze asynchronous stochastic algorithms for three important optimization problems. Specifically, based on the q-memorization framework, 1) we propose an asynchronous stochastic gradient hard thresholding algorithm with q-memorization (AsySGHT-qM) for the non-convex optimization with cardinality constraint, and prove that the convergence rate of AsySGHT-qM before reaching the inherent error induced by gradient hard thresholding methods is geometric. 2) We propose an asynchronous stochastic proximal gradient algorithm (AsySPG-qM) for the convex optimization with non-smooth regularization, and prove that AsySPG-qM can achieve a linear convergence rate. 3) We propose an asynchronous stochastic gradient descent algorithm (AsySGD-qM) for the general non-convex optimization problem, and prove that AsySGD-qM can achieve a sublinear convergence rate to stationary points. The experimental results on various large-scale datasets confirm the fast convergence of our AsySGHT-qM, AsySPG-qM and AsySGD-qM through concrete realizations of SVRG and SAGA.",
            "keywords": [],
            "author": [
                "Bin Gu",
                "Wenhan Xian",
                "Zhouyuan Huo",
                "Cheng Deng",
                "Heng Huang"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-434/18-434.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning and Interpreting Multi-Multi-Instance Learning Networks",
            "abstract": "We introduce an extension of the multi-instance learning problem where examples are organized as nested bags of instances (e.g., a document could be represented as a bag of sentences, which in turn are bags of words). This framework can be useful in various scenarios, such as text and image classification, but also supervised learning over graphs. As a further advantage, multi-multi instance learning enables a particular way of interpreting predictions and the decision function. Our approach is based on a special neural network layer, called bag-layer, whose units aggregate bags of inputs of arbitrary size. We prove theoretically that the associated class of functions contains all Boolean functions over sets of sets of instances and we provide empirical evidence that functions of this kind can be actually learned on semi-synthetic datasets. We finally present experiments on text classification, on citation graphs, and social graph data, which show that our model obtains competitive results with respect to accuracy when compared to other approaches such as convolutional networks on graphs, while at the same time it supports a general approach to interpret the learnt model, as well as explain individual predictions.",
            "keywords": [
                "Multi-multi instance learning",
                "relational learning"
            ],
            "author": [
                "Alessandro Tibo",
                "Manfred Jaeger",
                "Paolo Frasconi"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-811/18-811.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Contextual Explanation Networks",
            "abstract": "Modern learning algorithms excel at producing accurate but complex models of the data.  However, deploying such models in the real-world requires extra care: we must ensure their reliability, robustness, and absence of undesired biases.  This motivates the development of models that are equally accurate but can be also easily inspected and assessed beyond their predictive performance.  To this end, we introduce contextual explanation networks (CENs)---a class of architectures that learn to predict by generating and utilizing intermediate, simplified probabilistic models.  Specifically, CENs generate parameters for intermediate graphical models which are further used for prediction and play the role of explanations.  Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain simultaneously.  Our approach offers two major advantages: (i) for each prediction, valid, instance-specific explanation is generated with no computational overhead and (ii) prediction via explanation acts as a regularizer and boosts performance in data-scarce settings.  We analyze the proposed framework theoretically and experimentally.  Our results on image and text classification and survival analysis tasks demonstrate that CENs are not only competitive with the state-of-the-art methods but also offer additional insights behind each prediction, that can be valuable for decision support.  We also show that while post-hoc methods may produce misleading explanations in certain cases, CENs are consistent and allow to detect such cases systematically.",
            "keywords": [],
            "author": [
                "Maruan Al-Shedivat",
                "Avinava Dubey",
                "Eric Xing"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-856/18-856.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Conic Optimization for Quadratic Regression Under Sparse Noise",
            "abstract": "This paper is concerned with the quadratic regression problem, where the goal is to find the unknown state (numerical parameters) of a system modeled by a set of equations that are quadratic in the state. We focus on the setting when a subset of equations of fixed cardinality is subject to errors of arbitrary magnitudes (potentially adversarial). We develop two methods to address this problem, which are both based on conic optimization and are able to accept any available prior knowledge on the solution as an input. We derive sufficient conditions for guaranteeing the correct recovery of the unknown state for each method and show that one method provides a better accuracy while the other one scales better to large-scale systems. The obtained conditions consist in bounds on the number of bad measurements each method can tolerate without producing a nonzero estimation error. In the case when no prior knowledge is available, we develop an iterative-based conic optimization technique. It is proved that the proposed methods allow up to half of the total number of measurements to be grossly erroneous.The efficacy of the developed methods is demonstrated in different case studies, including data analytics for a European power grid.",
            "keywords": [
                "nonlinear regression",
                "conic programming"
            ],
            "author": [
                "Igor Molybog",
                "Ramtin Madani",
                "Javad Lavaei"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-881/18-881.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Successor Features Combine Elements of Model-Free and Model-based Reinforcement Learning",
            "abstract": "A key question in reinforcement learning is how an intelligent agent can generalize knowledge across different inputs. By generalizing across different inputs, information learned for one input can be immediately reused for improving predictions for another input. Reusing information allows an agent to compute an optimal decision-making strategy using less data. State representation is a key element of the generalization process, compressing a high-dimensional input space into a low-dimensional latent state space. This article analyzes properties of different latent state spaces, leading to new connections between model-based and model-free reinforcement learning. Successor features, which predict frequencies of future observations, form a link between model-based and model-free learning: Learning to predict future expected reward outcomes, a key characteristic of model-based agents, is equivalent to learning successor features. Learning successor features is a form of temporal difference learning and is equivalent to learning to predict a single policy's utility, which is a characteristic of model-free agents. Drawing on the connection between model-based reinforcement learning and successor features, we demonstrate that representations that are predictive of future reward outcomes generalize across variations in both transitions and rewards. This result extends previous work on successor features, which is constrained to fixed transitions and assumes re-learning of the transferred state representation.",
            "keywords": [
                "Successor Features",
                "Model-Based Reinforcement Learning",
                "State Represen-     tations"
            ],
            "author": [
                "Lucas Lehnert",
                "Michael L. Littman"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-060/19-060.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A determinantal point process for column subset selection",
            "abstract": "Two popular approaches to dimensionality reduction are principal component analysis, which projects onto a small number of well-chosen but non-interpretable directions, and feature selection, which selects a small number of the original features. Feature selection can be abstracted as selecting the subset of columns of a matrix  which minimize the approximation error, i.e., the norm of the residual after projecting  onto the space spanned by the selected columns. Such a combinatorial optimization is usually impractical, and there has been interest in polynomial-cost, random subset selection algorithms that favour small values of this approximation error. We propose sampling from a projection determinantal point process, a repulsive distribution over column indices that favours diversity among the selected columns. We bound the ratio of the expected approximation error over the optimal error of PCA. These bounds improve over the state-of-the-art bounds of volume sampling when some realistic structural assumptions are satisfied for . Numerical experiments suggest that our bounds are tight, and that our algorithms have comparable performance with the double phase algorithm, often considered the practical state-of-the-art.",
            "keywords": [
                "column subset selection",
                "determinantal point process",
                "volume sampling",
                "leverage score sampling"
            ],
            "author": [
                "Ayoub Belhadji",
                "Rémi Bardenet",
                "Pierre Chainais"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-080/19-080.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Reinforcement Learning in Continuous Time and Space: A Stochastic Control Approach",
            "abstract": "We consider reinforcement learning (RL) in continuous time with continuous feature and action spaces. We motivate and devise an exploratory formulation for the feature dynamics that captures  learning under exploration, with the resulting optimization problem being a revitalization of the classical relaxed stochastic control. We then study the problem of achieving the best trade-off between exploration and exploitation by considering an entropy-regularized  reward function. We carry out a complete analysis of the problem in the linear--quadratic (LQ) setting and deduce that the optimal feedback control distribution for balancing exploitation and exploration is Gaussian. This in turn interprets the widely adopted Gaussian exploration in RL, beyond its simplicity for sampling. Moreover, the exploitation and exploration are captured respectively  by the mean and variance of the Gaussian distribution. We characterize the cost of exploration, which, for the LQ case, is shown to be proportional to the entropy regularization weight and inversely proportional to the discount rate. Finally, as the weight of exploration decays to zero, we prove the convergence of the solution of the entropy-regularized  LQ problem to the one of the classical LQ problem.",
            "keywords": [
                "Reinforcement learning",
                "entropy regularization",
                "stochastic control",
                "relaxed     control"
            ],
            "author": [
                "Haoran Wang",
                "Thaleia Zariphopoulou",
                "Xun Yu Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-144/19-144.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotic Analysis via Stochastic Differential Equations of Gradient Descent Algorithms in Statistical and Computational Paradigms",
            "abstract": "This paper investigates the asymptotic behaviors of gradient descent algorithms (particularly accelerated gradient descent and stochastic gradient descent) in the context of stochastic optimization arising in statistics and machine learning, where objective functions are estimated from available data.  We show that these algorithms can be computationally modeled by continuous-time ordinary or stochastic differential equations. We establish gradient flow central limit theorems to describe the limiting dynamic behaviors of these computational algorithms and the large-sample performances of the related statistical procedures, as the number of algorithm iterations and data size both go to infinity, where the gradient flow central limit theorems are governed by some linear ordinary or stochastic differential equations, like time-dependent Ornstein-Uhlenbeck processes.  We illustrate that our study can provide a novel unified framework for a joint computational and statistical asymptotic analysis, where the computational asymptotic analysis studies the dynamic behaviors of these algorithms with time (or the number of iterations in the algorithms), the statistical asymptotic analysis investigates the large-sample behaviors of the statistical procedures (like estimators and classifiers) that are computed by applying the algorithms; in fact, the statistical procedures are equal to the limits of the random sequences generated from these iterative algorithms, as the  number of iterations goes to infinity. The joint analysis results based on the obtained gradient flow central limit theorems lead to the identification of  four factors---learning rate, batch size, gradient covariance, and Hessian---to derive new theories regarding the local minima  found by stochastic gradient descent for solving non-convex optimization problems.",
            "keywords": [
                "acceleration",
                "gradient descent",
                "gradient flow central limit theorem",
                "joint     asymptotic analysis",
                "joint computational and statistical analysis",
                "Lagrangian flow central     limit theorem",
                "mini-batch",
                "optimization",
                "ordinary differential equation",
                "stochastic differen-     tial equation",
                "stochastic gradient descent"
            ],
            "author": [
                "Yazhen Wang",
                "Shang Wu"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-245/19-245.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Empirical Risk Minimization in the Non-interactive Local Model of Differential Privacy",
            "abstract": "In this paper, we study the Empirical Risk Minimization (ERM) problem in the non-interactive Local Differential Privacy (LDP) model. Previous research on this problem (Smith et al., 2017) indicates that the sample complexity, to achieve error , needs to be exponentially depending on the dimensionality  for general loss functions. In this paper, we make two attempts to resolve this issue by investigating conditions on the loss functions that allow us to remove such a limit. In our first attempt, we show that if the loss function is -smooth, by using the Bernstein polynomial approximation we can avoid the exponential dependency in the term of . We then propose player-efficient algorithms with -bit communication complexity and  computation cost for each player. The error bound of these algorithms is asymptotically the same as the original one. With some additional assumptions, we also give an algorithm which is more efficient for the server. In our second attempt,  we show that for any -Lipschitz generalized linear convex loss function, there is an -LDP algorithm whose sample complexity for achieving error  is only linear in the dimensionality . Our results use a polynomial of inner product approximation technique. Finally, motivated by the idea of using polynomial approximation and based on different types of polynomial approximations, we propose (efficient) non-interactive locally differentially private algorithms for learning the set of k-way marginal queries and the set of smooth queries.",
            "keywords": [
                "Differential Privacy",
                "Empirical Risk Minimization",
                "Local Differential Privacy",
                "Round Complexity",
                "Convex Learningc 2020 Di Wang",
                "Marco Gaboardi"
            ],
            "author": [
                "Di Wang",
                "Marco Gaboardi",
                "Adam Smith",
                "Jinhui Xu"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-253/19-253.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Continuous-Time Birth-Death MCMC for Bayesian Regression Tree Models",
            "abstract": "Decision trees are flexible models that are well suited for many statistical regression problems. In the Bayesian framework for regression trees, Markov Chain Monte Carlo (MCMC) search algorithms are required to generate samples of tree models according to their posterior probabilities. The critical component of such MCMC algorithms is to construct \"good\" Metropolis-Hastings steps to update the tree topology. Such algorithms frequently suffer from poor mixing and local mode stickiness; therefore, the algorithms are slow to converge. Hitherto, authors have primarily used discrete-time birth/death mechanisms for Bayesian (sums of) regression tree models to explore the tree-model space. These algorithms are efficient, in terms of computation and convergence, only if the rejection rate is low which is not always the case. We overcome this issue by developing a novel search algorithm which is based on a continuous-time birth-death Markov process. The search algorithm explores the tree-model space by jumping between parameter spaces corresponding to different tree structures. The jumps occur in continuous time corresponding to the birth-death events which are modeled as independent Poisson processes. In the proposed algorithm, the moves between models are always accepted which can dramatically improve the convergence and mixing properties of the search algorithm. We provide theoretical support of the algorithm for Bayesian regression tree models and demonstrate its performance in a simulated example.",
            "keywords": [
                "Bayesian Regression trees",
                "Decision trees",
                "Continuous-time MCMC",
                "Bayesian     structure learning",
                "Birth-death process",
                "Bayesian model averaging"
            ],
            "author": [
                "Reza Mohammadi",
                "Matthew Pratola",
                "Maurits Kaptein"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-307/19-307.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Control of Stochastic Evolution: A Deep Reinforcement Learning Approach to Adaptively Targeting Emergent Drug Resistance",
            "abstract": "The challenge in controlling stochastic systems in which low-probability events can set the system on catastrophic trajectories is to develop a robust ability to respond to such events without significantly compromising the optimality of the baseline control policy. This paper presents CelluDose, a stochastic simulation-trained deep reinforcement learning adaptive feedback control prototype for automated precision drug dosing targeting stochastic and heterogeneous cell proliferation. Drug resistance can emerge from random and variable mutations in targeted cell populations; in the absence of an appropriate dosing policy, emergent resistant subpopulations can proliferate and lead to treatment failure. Dynamic feedback dosage control holds promise in combatting this phenomenon, but the application of traditional control approaches to such systems is fraught with challenges due to the complexity of cell dynamics, uncertainty in model parameters, and the need in medical applications for a robust controller that can be trusted to properly handle unexpected outcomes. Here, training on a sample biological scenario identified single-drug and combination therapy policies that exhibit a  success rate at suppressing cell proliferation and responding to diverse system perturbations while establishing low-dose no-event baselines. These policies were found to be highly robust to variations in a key model parameter subject to significant uncertainty and unpredictable dynamical changes.",
            "keywords": [
                "reinforcement learning",
                "deep learning",
                "control",
                "adaptive dosing"
            ],
            "author": [
                "Dalit Engelhardt"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-546/19-546.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Two-Stage Approach to Multivariate Linear Regression with Sparsely Mismatched Data",
            "abstract": "A tacit assumption in linear regression is that (response, predictor)-pairs correspond  to identical observational units. A series of recent works have studied scenarios in which this assumption is violated under terms such as “Unlabeled Sensing and “Regression with Unknown Permutation”. In this paper, we study the setup of multiple response variables and a notion of mismatches that generalizes permutations in order to allow for missing matches as well as for one-to-many matches. A two-stage method is proposed under the assumption that most pairs are correctly matched. In the first stage, the regression parameter is estimated by handling mismatches as contaminations, and subsequently the generalized permutation is estimated by a basic variant of matching. The approach is both computationally convenient and equipped with favorable statistical guarantees. Specifically, it is shown that the conditions for permutation recovery become considerably less stringent as the number of responses  per observation increase. Particularly, for , the required signal-to-noise ratio no longer depends on the sample size . Numerical results on synthetic and real data are presented to support the main findings of our analysis.",
            "keywords": [],
            "author": [
                "Martin Slawski",
                "Emanuel Ben-David",
                "Ping Li"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-645/19-645.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sobolev Norm Learning Rates for Regularized Least-Squares Algorithms",
            "abstract": "Learning rates for least-squares regression are typically expressed in terms of -norms. In this paper we extend these rates to norms stronger than the -norm without requiring the regression function to be contained in the hypothesis space. In the special case of Sobolev reproducing kernel Hilbert spaces used as hypotheses spaces, these stronger norms coincide with fractional Sobolev norms between the used Sobolev space and . As a consequence, not only the target function but also some of its derivatives can be estimated without changing the algorithm. From a technical point of view, we combine the well-known integral operator techniques with an embedding property, which so far has only been used in combination with empirical process arguments. This combination results in new finite sample bounds with respect to the stronger norms. From these finite sample bounds our rates easily follow. Finally, we prove the asymptotic optimality of our results in many cases.",
            "keywords": [
                "statistical learning theory",
                "regularized kernel methods",
                "least-squares regression",
                "interpolation norms",
                "uniform convergence"
            ],
            "author": [
                "Simon Fischer",
                "Ingo Steinwart"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-734/19-734.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Functional Martingale Residual Process for High-Dimensional Cox Regression with Model Averaging",
            "abstract": "Regularization methods for the Cox proportional hazards regression with high-dimensional survival data have been studied extensively in the literature. However, if the model is misspecified, this would result in misleading statistical inference and prediction. To enhance the prediction accuracy for the relative risk and the survival probability, we propose three model averaging approaches for the high-dimensional Cox proportional hazards regression. Based on the martingale residual process, we define the delete-one cross-validation (CV) process, and further propose three novel CV functionals, including the end-time CV, integrated CV, and supremum CV, to achieve more accurate prediction for the risk quantities of clinical interest. The optimal weights for candidate models, without the constraint of summing up to one, can be obtained by minimizing these functionals, respectively. The proposed model averaging approach can attain the lowest possible prediction loss asymptotically. Furthermore, we develop a greedy model averaging algorithm to overcome the computational obstacle when the dimension is high. The performances of the proposed model averaging procedures are evaluated via extensive simulation studies, demonstrating that our methods achieve superior prediction accuracy over the existing regularization methods. As an illustration, we apply the proposed methods to the mantle cell lymphoma study.",
            "keywords": [
                "Asymptotic Optimality",
                "Censored Data",
                "Cross Validation",
                "Greedy Algorithm",
                "Martingale Residual Process",
                "Prediction",
                "Survival Analysisc 2020 Baihua He",
                "Yanyan Liu",
                "Yuanshan Wu",
                "Guosheng",
                "Yin and Xingqiu"
            ],
            "author": [
                "Baihua He",
                "Yanyan Liu",
                "Yuanshan Wu",
                "Guosheng Yin",
                "Xingqiu Zhao"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-829/19-829.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Data-adaptive Non-parametric Kernels",
            "abstract": "In this paper, we propose a data-adaptive non-parametric kernel learning framework in margin based kernel methods. In model formulation, given an initial kernel matrix, a data-adaptive matrix with two constraints is imposed in an entry-wise scheme. Learning this data-adaptive matrix in a formulation-free strategy enlarges the margin between classes and thus improves the model flexibility. The introduced two constraints are imposed either exactly (on small data sets) or approximately (on large data sets) in our model, which provides a controllable trade-off between model flexibility and complexity with theoretical demonstration. In algorithm optimization, the objective function of our learning framework is proven to be gradient-Lipschitz continuous. Thereby, kernel and classifier/regressor learning can be efficiently optimized in a unified framework via Nesterov's acceleration. For the scalability issue, we study a decomposition-based approach to our model in the large sample case. The effectiveness of this approximation is illustrated by both empirical studies and theoretical guarantees. Experimental results on various classification and regression benchmark data sets demonstrate that our non-parametric kernel learning framework achieves good performance when compared with other representative kernel learning based algorithms.",
            "keywords": [
                "support vector machines",
                "non-parametric kernel learning"
            ],
            "author": [
                "Fanghui Liu",
                "Xiaolin Huang",
                "Chen Gong",
                "Jie Yang",
                "Li Li"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-900/19-900.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Sober Look at the Unsupervised Learning of Disentangled Representations and their Evaluation",
            "abstract": "The idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train over  models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on eight data sets. We observe that while the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, different evaluation metrics do not always agree on what should be considered “disentangled” and exhibit systematic differences in the estimation.  Finally, increased disentanglement does not seem to necessarily lead to a decreased sample complexity of learning for downstream tasks.  Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.",
            "keywords": [],
            "author": [
                "Francesco Locatello",
                "Stefan Bauer",
                "Mario Lucic",
                "Gunnar Raetsch",
                "Sylvain Gelly",
                "Bernhard Schoelkopf",
                "Olivier Bachem"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-976/19-976.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Smoothing Might be Unable to Certify L∞ Robustness for High-Dimensional Images",
            "abstract": "We show a hardness result for random smoothing to achieve certified adversarial robustness against attacks in the  ball of radius  when . Although random smoothing has been well understood for the  case using the Gaussian distribution, much remains unknown concerning the existence of a noise distribution that works for the case of . This has been posed as an open problem by Cohen et al. (2019) and includes many significant paradigms such as the  threat model. In this work, we show that any noise distribution  over  that provides  robustness for all base classifiers with  must satisfy  for 99% of the features (pixels) of vector , where  is the robust radius and  is the score gap between the highest-scored class and the runner-up. Therefore, for high-dimensional images with pixel values bounded in , the required noise will eventually dominate the useful information in the images, leading to trivial smoothed classifiers.",
            "keywords": [
                "random smoothing",
                "certified adversarial robustness",
                "hardness results",
                "high-dimensional     data"
            ],
            "author": [
                "Avrim Blum",
                "Travis Dick",
                "Naren Manoj",
                "Hongyang Zhang"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-209/20-209.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikit-learn",
            "abstract": "scikit-survival is an open-source Python package for time-to-event analysis fully compatible with scikit-learn. It provides implementations of many popular machine learning techniques for time-to-event analysis, including penalized Cox model, Random Survival Forest, and Survival Support Vector Machine. In addition, the library includes tools to evaluate model performance on censored time-to-event data. The documentation contains installation instructions, interactive notebooks, and a full description of the API. scikit-survival is distributed under the GPL-3 license with the source code and detailed instructions available at https://github.com/sebp/scikit-survival",
            "keywords": [
                "Time-to-event Analysis",
                "Survival Analysis",
                "Censored Data"
            ],
            "author": [
                "Sebastian Pölsterl"
            ],
            "ref": "http://jmlr.org/papers/volume21/20-729/20-729.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Provable Convex Co-clustering of Tensors",
            "abstract": "Cluster analysis is a fundamental tool for pattern discovery of complex heterogeneous data. Prevalent clustering methods mainly focus on vector or matrix-variate data and are not applicable to general-order tensors, which arise frequently in modern scientific and business applications.  Moreover, there is a gap between statistical guarantees and computational efficiency for existing tensor clustering solutions due to the nature of their non-convex formulations. In this work, we bridge this gap by developing a provable convex formulation of tensor co-clustering. Our convex co-clustering (CoCo) estimator enjoys stability guarantees and its computational and storage costs are polynomial in the size of the data. We further establish a non-asymptotic error bound for the CoCo estimator, which reveals a surprising “blessing of dimensionality” phenomenon that does not exist in vector or matrix-variate cluster analysis. Our theoretical findings are supported by extensive simulated studies. Finally, we apply the CoCo estimator to the cluster analysis of advertisement click tensor data from a major online company. Our clustering results provide meaningful business insights to improve advertising effectiveness.",
            "keywords": [
                "Clustering",
                "Fused lasso",
                "High-dimensional Statistical Learning",
                "Multiway     Data"
            ],
            "author": [
                "Eric C. Chi",
                "Brian J. Gaines",
                "Will Wei Sun",
                "Hua Zhou",
                "Jian Yang"
            ],
            "ref": "http://jmlr.org/papers/volume21/18-155/18-155.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Mining Topological Structure in Graphs through Forest Representations",
            "abstract": "We consider the problem of inferring simplified topological substructures—which we term backbones—in metric and non-metric graphs. Intuitively, these are subgraphs with ‘few’ nodes, multifurcations, and cycles, that model the topology of the original graph well. We present a multistep procedure for inferring these backbones. First, we encode local (geometric) information of each vertex in the original graph by means of the boundary coefficient (BC) to identify ‘core’ nodes in the graph. Next, we construct a forest representation of the graph, termed an f-pine, that connects every node of the graph to a local ‘core’ node. The final backbone is then inferred from the f-pine through CLOF (Constrained Leaves Optimal subForest), a novel graph optimization problem we introduce in this paper. On a theoretical level, we show that CLOF is NP-hard for general graphs. However, we prove that CLOF can be efficiently solved for forest graphs, a surprising fact given that CLOF induces a nontrivial monotone submodular set function maximization problem on tree graphs. This result is the basis of our method for mining backbones in graphs through forest representation. We qualitatively and quantitatively confirm the applicability, effectiveness, and scalability of our method for discovering backbones in a variety of graph-structured data, such as social networks, earthquake locations scattered across the Earth, and high-dimensional cell trajectory data",
            "keywords": [
                "topological data analysis",
                "graph mining",
                "metric spaces",
                "visualization",
                "topo-     logical skeletonization",
                "cluster coefficient"
            ],
            "author": [
                "Robin Vandaele",
                "Yvan Saeys",
                "Tijl De Bie"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-1032/19-1032.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Assortment Optimization with Changing Contextual Information",
            "abstract": "In this paper, we study the dynamic assortment optimization problem over a finite selling season of length . At each time period, the seller offers an arriving customer an assortment of substitutable products under a cardinality constraint, and the customer makes the purchase among offered products according to a discrete choice model. Most existing work associates each product with a real-valued fixed mean utility and assumes a multinomial logit choice (MNL) model. In many practical applications,  feature/contextual information of products is readily available. In this paper, we incorporate the feature information by assuming a linear relationship between the mean utility and the feature. In addition, we allow the feature information of products to change over time so that the underlying choice model can also be non-stationary. To solve the dynamic assortment optimization under this changing contextual MNL model, we need to simultaneously learn the underlying unknown coefficient and make the decision on the assortment. To this end, we develop an upper confidence bound (UCB) based  policy and establish the regret bound on the order of , where  is the dimension of the feature and  suppresses logarithmic dependence. We further establish a lower bound , where  is the cardinality constraint of an offered assortment, which is usually small. When  is a constant, our policy is optimal up to logarithmic factors. In the exploitation phase of the UCB algorithm, we need to solve a combinatorial optimization problem for assortment optimization based on the learned information.  We further develop an approximation algorithm and an efficient greedy heuristic. The effectiveness of the proposed policy is further demonstrated by our numerical studies.",
            "keywords": [
                "Dynamic assortment optimization",
                "regret analysis",
                "contextual information",
                "bandit learning"
            ],
            "author": [
                "Xi Chen",
                "Yining Wang",
                "Yuan Zhou"
            ],
            "ref": "http://jmlr.org/papers/volume21/19-1054/19-1054.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        }
    ]
}