{
    "papers": [
        {
            "title": "Multiple-Instance Learning from Distributions",
            "abstract": [
                "We propose a new theoretical framework for analyzing the multiple-instance learning (MIL) setting. In MIL, training examples are provided to a learning algorithm in the form of labeled sets, or \"bags,\" of instances. Applications of MIL include 3-D quantitative structureactivity relationship prediction for drug discovery and content-based image retrieval for web search. The goal of an algorithm is to learn a function that correctly labels new bags or a function that correctly labels new instances. We propose that bags should be treated as latent distributions from which samples are observed. We show that it is possible to learn accurate instance-and bag-labeling functions in this setting as well as functions that correctly rank bags or instances under weak assumptions. Additionally, our theoretical results suggest that it is possible to learn to rank efficiently using traditional, well-studied \"supervised\" learning approaches. We perform an extensive empirical evaluation that supports the theoretical predictions entailed by the new framework. The proposed theoretical framework leads to a better understanding of the relationship between the MI and standard supervised learning settings, and it provides new methods for learning from MI data that are more accurate, more efficient, and have better understood theoretical properties than existing MI-specific algorithms."
            ],
            "keywords": [
                "multiple-instance learning",
                "learning theory",
                "ranking",
                "classification"
            ],
            "author": [
                "Gary Doran"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-171/15-171.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-dimensional Varying Index Coefficient Models via Stein's Identity",
            "abstract": [
                "We study the parameter estimation problem for a varying index coefficient model in high dimensions. Unlike the most existing works that iteratively estimate the parameters and link functions, based on the generalized Stein's identity, we propose computationally efficient estimators for the high-dimensional parameters without estimating the link functions. We consider two different setups where we either estimate each sparse parameter vector individually or estimate the parameters simultaneously as a sparse or low-rank matrix. For all these cases, our estimators are shown to achieve optimal statistical rates of convergence (up to logarithmic terms in the low-rank setting). Moreover, throughout our analysis, we only require the covariate to satisfy certain moment conditions, which is significantly weaker than the Gaussian or elliptically symmetric assumptions that are commonly made in the existing literature. Finally, we conduct extensive numerical experiments to corroborate the theoretical results."
            ],
            "keywords": [
                "high-dimensional estimation",
                "semiparametric modeling",
                "Stein's identity",
                "varying index coefficient model"
            ],
            "author": [
                "Sen Na",
                "Zhuoran Yang",
                "Zhaoran Wang",
                "Mladen Kolar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-705/18-705.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning the Structure and Parameters of Large-Population Graphical Games from Behavioral Data",
            "abstract": [
                "We consider learning, from strictly behavioral data, the structure and parameters of linear influence games (LIGs), a class of parametric graphical games introduced by Irfan and Ortiz (2014). LIGs facilitate causal strategic inference (CSI): Making inferences from causal interventions on stable behavior in strategic settings. Applications include the identification of the most influential individuals in large (social) networks. Such tasks can also support policy-making analysis. Motivated by the computational work on LIGs, we cast the learning problem as maximum-likelihood estimation (MLE) of a generative model defined by pure-strategy Nash equilibria (PSNE). Our simple formulation uncovers the fundamental interplay between goodness-of-fit and model complexity: good models capture equilibrium behavior within the data while controlling the true number of equilibria, including those unobserved. We provide a generalization bound establishing the sample complexity for MLE in our framework. We propose several algorithms including convex loss minimization (CLM) and sigmoidal approximations. We prove that the number of exact PSNE in LIGs is small, with high probability; thus, CLM is sound. We illustrate our approach on synthetic data and real-world U.S. congressional voting records. We briefly discuss our learning framework's generality and potential applicability to general graphical games."
            ],
            "keywords": [
                "linear influence games",
                "graphical games",
                "structure and parameter learning",
                "behavioral data in strategic settings"
            ],
            "author": [
                "Jean Honorio",
                "Luis Ortiz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/honorio15a/honorio15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SPMF: A Java Open-Source Pattern Mining Library",
            "abstract": [
                "We present SPMF, an open-source data mining library offering implementations of more than 55 data mining algorithms. SPMF is a cross-platform library implemented in Java, specialized for discovering patterns in transaction and sequence databases such as frequent itemsets, association rules and sequential patterns. The source code can be integrated in other Java programs. Moreover, SPMF offers a command line interface and a simple graphical interface for quick testing. The source code is available under the GNU General Public License, version 3. The website of the project offers several resources such as documentation with examples of how to run each algorithm, a developer's guide, performance comparisons of algorithms, data sets, an active forum, a FAQ and a mailing list."
            ],
            "keywords": [
                "data mining",
                "library",
                "frequent pattern mining",
                "sequence database",
                "transaction database",
                "open-source"
            ],
            "author": [
                "Philippe Fournier-Viger",
                "Antonio Gomariz",
                "Ted Gueniche",
                "Cheng-Wei Wu",
                "Vincent S Tseng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/fournierviger14a/fournierviger14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Relational Reinforcement Learning for Planning with Exogenous Effects",
            "abstract": [
                "Probabilistic planners have improved recently to the point that they can solve difficult tasks with complex and expressive models. In contrast, learners cannot tackle yet the expressive models that planners do, which forces complex models to be mostly handcrafted. We propose a new learning approach that can learn relational probabilistic models with both action effects and exogenous effects. The proposed learning approach combines a multivalued variant of inductive logic programming for the generation of candidate models, with an optimization method to select the best set of planning operators to model a problem. We also show how to combine this learner with reinforcement learning algorithms to solve complete problems. Finally, experimental validation is provided that shows improvements over previous work in both simulation and a robotic task. The robotic task involves a dynamic scenario with several agents where a manipulator robot has to clear the tableware on a table. We show that the exogenous effects learned by our approach allowed the robot to clear the table in a more efficient way."
            ],
            "keywords": [
                "Learning Models for Planning",
                "Model-Based RL",
                "Probabilistic Planning",
                "Active Learning",
                "Robot Learning"
            ],
            "author": [
                "David Martínez",
                "Tony Ribeiro"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-326/16-326.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parallel Symmetric Class Expression Learning",
            "abstract": [
                "In machine learning, one often encounters data sets where a general pattern is violated by a relatively small number of exceptions (for example, a rule that says that all birds can fly is violated by examples such as penguins). This complicates the concept learning process and may lead to the rejection of some simple and expressive rules that cover many cases. In this paper we present an approach to this problem in description logic learning by computing partial descriptions (which are not necessarily entirely complete) of both positive and negative examples and combining them. Our Symmetric Parallel Class Expression Learning approach enables the generation of general rules with exception patterns included. We demonstrate that this algorithm provides significantly better results (in terms of metrics such as accuracy, search space covered, and learning time) than standard approaches on some typical data sets. Further, the approach has the added benefit that it can be parallelised relatively simply, leading to much faster exploration of the search tree on modern computers."
            ],
            "keywords": [
                "description logic learning",
                "parallel",
                "symmetric",
                "exception"
            ],
            "author": [
                "An C Tran",
                "Jens Dietrich",
                "Hans W Guesgen",
                "Stephen Marsland"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-317/14-317.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structural Learning of Chain Graphs via Decomposition",
            "abstract": [
                "Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse."
            ],
            "keywords": [
                "chain graph",
                "conditional independence",
                "decomposition",
                "graphical model",
                "structural learning"
            ],
            "author": [
                "Zongming Ma",
                "Xianchao Xie"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/ma08a/ma08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Principle Based Algorithms for Deep Learning",
            "abstract": [
                "The continuous dynamical system approach to deep learning is explored in order to devise alternative frameworks for training algorithms. Training is recast as a control problem and this allows us to formulate necessary optimality conditions in continuous time using the Pontryagin's maximum principle (PMP). A modification of the method of successive approximations is then used to solve the PMP, giving rise to an alternative training algorithm for deep learning. This approach has the advantage that rigorous error estimates and convergence results can be established. We also show that it may avoid some pitfalls of gradient-based methods, such as slow convergence on flat landscapes near saddle points. Furthermore, we demonstrate that it obtains favorable initial convergence rate periteration, provided Hamiltonian maximization can be efficiently carried out-a step which is still in need of improvement. Overall, the approach opens up new avenues to attack problems associated with deep learning, such as trapping in slow manifolds and inapplicability of gradient-based methods for discrete trainable variables."
            ],
            "keywords": [
                "deep learning",
                "optimal control",
                "Pontryagin's maximum principle",
                "method of successive approximations"
            ],
            "author": [
                "Qianxiao Li",
                "Long Chen",
                "Cheng Tai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-653/17-653.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Information-Theoretic Analysis of Thompson Sampling",
            "abstract": [
                "We provide an information-theoretic analysis of Thompson sampling that applies across a broad range of online optimization problems in which a decision-maker must learn from partial feedback. This analysis inherits the simplicity and elegance of information theory and leads to regret bounds that scale with the entropy of the optimal-action distribution. This strengthens preexisting results and yields new insight into how information improves performance."
            ],
            "keywords": [
                "Thompson sampling",
                "online optimization",
                "mutli-armed bandit",
                "information theory",
                "regret bounds"
            ],
            "author": [
                "Daniel Russo"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-087/14-087.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Mixed Latent Tree Models",
            "abstract": [
                "Latent structural learning has attracted more attention in recent years. But most related works only focuses on pure continuous or pure discrete data. In this paper, we consider mixed latent tree models for mixed data mining. We address the latent structural learning and parameter estimation for those mixed models. For structural learning, we propose a consistent bottom-up algorithm, and give a finite sample bound guarantee for the exact structural recovery. For parameter estimation, we suggest a moment estimator by exploiting matrix decomposition, and prove asymptotic normality of the estimator. Experiments on the simulated and real data support that our method is valid for mining the hierarchical structure and latent information."
            ],
            "keywords": [
                "Information distance",
                "Latent variables",
                "Mixed latent tree",
                "Parameter estimation",
                "Structural learning"
            ],
            "author": [
                "Can Zhou",
                "Xiaofei Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-365/20-365.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency of Trace Norm Minimization",
            "abstract": [
                "Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufficient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulfilled."
            ],
            "keywords": [
                "convex optimization",
                "singular value decomposition",
                "trace norm",
                "consistency"
            ],
            "author": [
                "Francis R Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/bach08a/bach08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Deep Optimal Stopping",
            "abstract": [
                "In this paper we develop a deep learning method for optimal stopping problems which directly learns the optimal stopping rule from Monte Carlo samples. As such, it is broadly applicable in situations where the underlying randomness can efficiently be simulated. We test the approach on three problems: the pricing of a Bermudan max-call option, the pricing of a callable multi barrier reverse convertible and the problem of optimally stopping a fractional Brownian motion. In all three cases it produces very accurate results in highdimensional situations with short computing times."
            ],
            "keywords": [
                "optimal stopping",
                "deep learning",
                "Bermudan option",
                "callable multi barrier reverse convertible",
                "fractional Brownian motion"
            ],
            "author": [
                "Sebastian Becker",
                "Patrick Cheridito",
                "Arnulf Jentzen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-232/18-232.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Efficient Explanation of Individual Classifications using Game Theory",
            "abstract": [
                "We present a general method for explaining individual predictions of classification models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method's initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efficient and that the explanations are intuitive and useful."
            ],
            "keywords": [
                "data postprocessing",
                "classification",
                "explanation",
                "visualization"
            ],
            "author": [
                "Igor Kononenko"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/strumbelj10a/strumbelj10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Beyond the Hazard Rate: More Perturbation Algorithms for Adversarial Multi-armed Bandits",
            "abstract": [
                "Recent work on follow the perturbed leader (FTPL) algorithms for the adversarial multiarmed bandit problem has highlighted the role of the hazard rate of the distribution generating the perturbations. Assuming that the hazard rate is bounded, it is possible to provide regret analyses for a variety of FTPL algorithms for the multi-armed bandit problem. This paper pushes the inquiry into regret bounds for FTPL algorithms beyond the bounded hazard rate condition. There are good reasons to do so: natural distributions such as the uniform and Gaussian violate the condition. We give regret bounds for both bounded support and unbounded support distributions without assuming the hazard rate condition. We also disprove a conjecture that the Gaussian distribution cannot lead to a low-regret algorithm. In fact, it turns out that it leads to near optimal regret, up to logarithmic factors. A key ingredient in our approach is the introduction of a new notion called the generalized hazard rate."
            ],
            "keywords": [
                "online learning",
                "regret",
                "multi-armed bandits",
                "follow the perturbed leader",
                "gradient based algorithms"
            ],
            "author": [
                "Zifan Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-364/17-364.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Equivariant Functions with Matrix Valued Kernels",
            "abstract": [
                "This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant filter for 2D-images and propose an invariant object detector based on the generalized Hough transform."
            ],
            "keywords": [
                "kernel methods",
                "matrix kernels",
                "equivariance",
                "group integration",
                "representation theory",
                "Hough transform",
                "signal processing",
                "Volterra series"
            ],
            "author": [
                "Marco Reisert",
                "Hans Burkhardt",
                "Georges-Koehler-Allee Lmb"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/reisert07a/reisert07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Machine Learning Methods for Predicting Failures in Hard Drives: A Multiple-Instance Application",
            "abstract": [
                "We compare machine learning methods applied to a difficult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classifier (mi-NB) which is specifically designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum significance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures."
            ],
            "keywords": [
                "hard drive failure prediction",
                "rank-sum test",
                "support vector machines (SVM)",
                "exact nonparametric statistics",
                "multiple instance naive-Bayes"
            ],
            "author": [
                "Joseph F Murray",
                "Gordon F Hughes"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/murray05a/murray05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Truncated EM Approach for Spike-and-Slab Sparse Coding",
            "abstract": [
                "We study inference and learning based on a sparse coding model with 'spike-and-slab' prior. As in standard sparse coding, the model used assumes independent latent sources that linearly combine to generate data points. However, instead of using a standard sparse prior such as a Laplace distribution, we study the application of a more flexible 'spike-and-slab' distribution which models the absence or presence of a source's contribution independently of its strength if it contributes. We investigate two approaches to optimize the parameters of spike-and-slab sparse coding: a novel truncated EM approach and, for comparison, an approach based on standard factored variational distributions. The truncated approach can be regarded as a variational approach with truncated posteriors as variational distributions. In applications to source separation we find that both approaches improve the state-of-theart in a number of standard benchmarks, which argues for the use of 'spike-and-slab' priors for the corresponding data domains. Furthermore, we find that the truncated EM approach improves on the standard factored approach in source separation tasks-which hints to biases introduced by assuming posterior independence in the factored variational approach. Likewise, on a standard benchmark for image denoising, we find that the truncated EM approach improves on the factored variational approach. While the performance of the factored approach saturates with increasing numbers of hidden dimensions, the performance of the truncated approach improves the state-of-the-art for higher noise levels."
            ],
            "keywords": [
                "sparse coding",
                "spike-and-slab distributions",
                "approximate EM",
                "variational Bayes",
                "unsupervised learning",
                "source separation",
                "denoising"
            ],
            "author": [
                "Abdul-Saboor Sheikh",
                "Jacquelyn A Shelton",
                "Jörg Lücke",
                "Jörg Shelton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/sheikh14a/sheikh14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Positive Semidefinite Metric Learning Using Boosting-like Algorithms",
            "abstract": [
                "The success of many machine learning and pattern recognition methods relies heavily upon the identification of an appropriate distance metric on the input data. It is often beneficial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed BOOSTMETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semidefinite. Semidefinite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. BOOSTMETRIC is instead based on the observation that any positive semidefinite matrix can be decomposed into a linear combination of trace-one rank-one matrices. BOOSTMETRIC thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process. The resulting methods are easy to implement, efficient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semidefinite matrix with trace and rank being one rather than a classifier or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classification accuracy and running time."
            ],
            "keywords": [
                "Mahalanobis distance",
                "semidefinite programming",
                "column generation",
                "boosting",
                "Lagrange duality",
                "large margin nearest neighbor"
            ],
            "author": [
                "Chunhua Shen",
                "Junae Kim",
                "Sören Sonnenburg",
                "Francis Bach",
                "Soon Cheng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/shen12a/shen12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Enhancing Identification of Causal Effects by Pruning",
            "abstract": [
                "Causal models communicate our assumptions about causes and effects in real-world phenomena. Often the interest lies in the identification of the effect of an action which means deriving an expression from the observed probability distribution for the interventional distribution resulting from the action. In many cases an identifiability algorithm may return a complicated expression that contains variables that are in fact unnecessary. In practice this can lead to additional computational burden and increased bias or inefficiency of estimates when dealing with measurement error or missing data. We present graphical criteria to detect variables which are redundant in identifying causal effects. We also provide an improved version of a well-known identifiability algorithm that implements these criteria."
            ],
            "keywords": [
                "causal inference",
                "identifiability",
                "causal model",
                "pruning",
                "algorithm"
            ],
            "author": [
                "Santtu Tikka"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-563/17-563.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit",
            "abstract": [
                "The performance of orthogonal matching pursuit (OMP) for variable selection is analyzed for random designs. When contrasted with the deterministic case, since the performance is here measured after averaging over the distribution of the design matrix, one can have far less stringent sparsity constraints on the coefficient vector. We demonstrate that for exact sparse vectors, the performance of the OMP is similar to known results on the Lasso algorithm (Wainwright, 2009). Moreover, variable selection under a more relaxed sparsity assumption on the coefficient vector, whereby one has only control on the ℓ 1 norm of the smaller coefficients, is also analyzed. As consequence of these results, we also show that the coefficient estimate satisfies strong oracle type inequalities."
            ],
            "keywords": [
                "high dimensional regression",
                "greedy algorithms",
                "Lasso",
                "compressed sensing"
            ],
            "author": [
                "Antony Joseph"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/joseph13a/joseph13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation",
            "abstract": [
                "We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classification. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-defined sufficient conditions. In a first set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance. Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distribuc 2010 Constantin F. Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani and Xenofon D. Koutsoukos. ALIFERIS, STATNIKOV, TSAMARDINOS, MANI AND KOUTSOUKOS tions, types of classifiers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we find that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show how local techniques can be used for scalable and accurate global causal graph learning."
            ],
            "keywords": [
                "local causal discovery",
                "Markov blanket induction",
                "feature selection",
                "classification",
                "causal structure learning",
                "learning of Bayesian networks"
            ],
            "author": [
                "Constantin F Aliferis",
                "Alexander Statnikov",
                "Xenofon D Koutsoukos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/aliferis10a/aliferis10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Reinforcement Learning in Continuous Time and Space: A Stochastic Control Approach",
            "abstract": [
                "We consider reinforcement learning (RL) in continuous time with continuous feature and action spaces. We motivate and devise an exploratory formulation for the feature dynamics that captures learning under exploration, with the resulting optimization problem being a revitalization of the classical relaxed stochastic control. We then study the problem of achieving the best trade-off between exploration and exploitation by considering an entropy-regularized reward function. We carry out a complete analysis of the problem in the linear-quadratic (LQ) setting and deduce that the optimal feedback control distribution for balancing exploitation and exploration is Gaussian. This in turn interprets the widely adopted Gaussian exploration in RL, beyond its simplicity for sampling. Moreover, the exploitation and exploration are captured respectively by the mean and variance of the Gaussian distribution. We characterize the cost of exploration, which, for the LQ case, is shown to be proportional to the entropy regularization weight and inversely proportional to the discount rate. Finally, as the weight of exploration decays to zero, we prove the convergence of the solution of the entropy-regularized LQ problem to the one of the classical LQ problem."
            ],
            "keywords": [
                "Reinforcement learning",
                "entropy regularization",
                "stochastic control",
                "relaxed control",
                "linear-quadratic",
                "Gaussian distribution"
            ],
            "author": [
                "Haoran Wang",
                "Thaleia Zariphopoulou",
                "Xun Yu Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-144/19-144.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion",
            "abstract": [
                "We consider the problem of high-dimensional Gaussian graphical model selection. We identify a set of graphs for which an efficient estimation algorithm exists, and this algorithm is based on thresholding of empirical conditional covariances. Under a set of transparent conditions, we establish structural consistency (or sparsistency) for the proposed algorithm, when the number of samples n = Ω(J −2 min log p), where p is the number of variables and J min is the minimum (absolute) edge potential of the graphical model. The sufficient conditions for sparsistency are based on the notion of walk-summability of the model and the presence of sparse local vertex separators in the underlying graph. We also derive novel non-asymptotic necessary conditions on the number of samples required for sparsistency."
            ],
            "keywords": [
                "Gaussian graphical model selection",
                "high-dimensional learning",
                "local-separation property",
                "walk-summability",
                "necessary conditions for model selection"
            ],
            "author": [
                "Animashree Anandkumar",
                "Vincent Y F Tan",
                "Furong Huang",
                "Alan S Willsky"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/anandkumar12a/anandkumar12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MULAN: A Java Library for Multi-Label Learning",
            "abstract": [
                "MULAN is a Java library for learning from multi-label data. It offers a variety of classification, ranking, thresholding and dimensionality reduction algorithms, as well as algorithms for learning from hierarchically structured labels. In addition, it contains an evaluation framework that calculates a rich variety of performance measures."
            ],
            "keywords": [
                "multi-label data",
                "classification",
                "ranking",
                "thresholding",
                "dimensionality reduction",
                "hierarchical classification",
                "evaluation"
            ],
            "author": [
                "Grigorios Tsoumakas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/tsoumakas11a/tsoumakas11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fused Lasso Approach in Regression Coefficients Clustering -Learning Parameter Heterogeneity in Data Integration",
            "abstract": [
                "As data sets of related studies become more easily accessible, combining data sets of similar studies is often undertaken in practice to achieve a larger sample size and higher power. A major challenge arising from data integration pertains to data heterogeneity in terms of study population, study design, or study coordination. Ignoring such heterogeneity in data analysis may result in biased estimation and misleading inference. Traditional techniques of remedy to data heterogeneity include the use of interactions and random effects, which are inferior to achieving desirable statistical power or providing a meaningful interpretation, especially when a large number of smaller data sets are combined. In this paper, we propose a regularized fusion method that allows us to identify and merge inter-study homogeneous parameter clusters in regression analysis, without the use of hypothesis testing approach. Using the fused lasso, we establish a computationally efficient procedure to deal with large-scale integrated data. Incorporating the estimated parameter ordering in the fused lasso facilitates computing speed with no loss of statistical power. We conduct extensive simulation studies and provide an application example to demonstrate the performance of the new method with a comparison to the conventional methods."
            ],
            "keywords": [
                "Fused lasso",
                "Data integration",
                "Extended BIC",
                "Generalized Linear Models"
            ],
            "author": [
                "Lu Tang",
                "Peter X K Song"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-598/15-598.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Spectrum Gaussian Process Regression",
            "abstract": [
                "We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class."
            ],
            "keywords": [
                "Gaussian process",
                "probabilistic regression",
                "sparse approximation",
                "power spectrum",
                "computational efficiency"
            ],
            "author": [
                "Miguel Lázaro-Gredilla",
                "Carl Edward Rasmussen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/lazaro-gredilla10a/lazaro-gredilla10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption",
            "abstract": [
                "We consider semi-supervised classification when part of the available data is unlabeled. These unlabeled data can be useful for the classification problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples."
            ],
            "keywords": [
                "semi-supervised learning",
                "statistical learning theory",
                "classification",
                "cluster assumption",
                "generalization bounds"
            ],
            "author": [
                "Philippe Rigollet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/rigollet07a/rigollet07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cells in Multidimensional Recurrent Neural Networks",
            "abstract": [
                "The transcription of handwritten text on images is one task in machine learning and one solution to solve it is using multi-dimensional recurrent neural networks (MDRNN) with connectionist temporal classification (CTC). The RNNs can contain special units, the long short-term memory (LSTM) cells. They are able to learn long term dependencies but they get unstable when the dimension is chosen greater than one. We defined some useful and necessary properties for the one-dimensional LSTM cell and extend them in the multidimensional case. Thereby we introduce several new cells with better stability. We present a method to design cells using the theory of linear shift invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT and Rimes database, where we can improve the recognition rate compared to the LSTM cell. So each application where the LSTM cells in MDRNNs are used could be improved by substituting them by the new developed cells."
            ],
            "keywords": [
                "LSTM",
                "MDRNN",
                "CTC",
                "handwriting recognition",
                "neural network"
            ],
            "author": [
                "Gundram Leifert",
                "Tobias Strauß",
                "Tobias Grüning",
                "Welf Wustlich",
                "Roger Labahn"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-203/14-203.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Breaking the Curse of Nonregularity with Subagging -Inference of the Mean Outcome under Optimal Treatment Regimes",
            "abstract": [
                "Precision medicine is an emerging medical approach that allows physicians to select the treatment options based on individual patient information. The goal of precision medicine is to identify the optimal treatment regime (OTR) that yields the most favorable clinical outcome. Prior to adopting any OTR in clinical practice, it is crucial to know the impact of implementing such a policy. Although considerable research has been devoted to estimating the OTR in the literature, less attention has been paid to statistical inference of the OTR. Challenges arise in the nonregular cases where the OTR is not uniquely defined. To deal with nonregularity, we develop a novel inference method for the mean outcome under an OTR (the optimal value function) based on subsample aggregating (subagging). The proposed method can be applied to multi-stage studies where treatments are sequentially assigned over time. Bootstrap aggregating (bagging) and subagging have been recognized as effective variance reduction techniques to improve unstable estimators or classifiers (Bühlmann and Yu, 2002). However, it remains unknown whether these approaches can yield valid inference results. We show the proposed confidence interval (CI) for the optimal value function achieves nominal coverage. In addition, due to the variance reduction effect of subagging, our method enjoys certain statistical optimality. Specifically, we show that the mean squared error of the proposed value estimator is strictly smaller than that based on the simple sample-splitting estimator in the nonregular cases. Moreover, under certain conditions, the length of our proposed CI is shown to be on average shorter than CIs constructed based on the existing state-of-the-art method (Luedtke and van der Laan, 2016) and the \"oracle\" method which works as well as if an OTR were known. Extensive numerical studies are conducted to back up our theoretical findings."
            ],
            "keywords": [
                "optimal (dynamic) treatment regime",
                "optimal value function",
                "precision medicine",
                "subsample aggregating",
                "nonregularity"
            ],
            "author": [
                "Chengchun Shi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-066/20-066.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Joint Label Inference in Networks",
            "abstract": [
                "We consider the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Our primary example, and the focus of this paper, is the joint inference of label types such as hometown, current city, and employers for people connected by a social network; by predicting these user profile fields, the network can provide a better experience to its users. Existing approaches such as Label Propagation (Zhu et al., 2003) fail to consider interactions between the label types. Our proposed method, called Edge-Explain, explicitly models these interactions, while still allowing scalable inference under a distributed message-passing architecture. On a large subset of the Facebook social network, collected in a previous study (Chakrabarti et al., 2014), EdgeExplain outperforms label propagation for several label types, with lifts of up to 120% for recall@1 and 60% for recall@3."
            ],
            "keywords": [
                "label inference",
                "graphs",
                "social networks",
                "variational methods",
                "label propagation"
            ],
            "author": [
                "Deepayan Chakrabarti",
                "Health Coda",
                "Sofus A Macskassy",
                "Branch Metrics"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-214/16-214.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DL-Learner: Learning Concepts in Description Logics",
            "abstract": [
                "In this paper, we introduce DL-Learner, a framework for learning in description logics and OWL. OWL is the official W3C standard ontology language for the Semantic Web. Concepts in this language can be learned for constructing and maintaining OWL ontologies or for solving problems similar to those in Inductive Logic Programming. DL-Learner includes several learning algorithms, support for different OWL formats, reasoner interfaces, and learning problems. It is a cross-platform framework implemented in Java. The framework allows easy programmatic access and provides a command line interface, a graphical interface as well as a WSDL-based web service."
            ],
            "keywords": [
                "concept learning",
                "description logics",
                "OWL",
                "classification",
                "open-source"
            ],
            "author": [
                "Jens Lehmann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/lehmann09a/lehmann09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks",
            "abstract": [
                "Log-linear and maximum-margin models are two commonly-used methods in supervised machine learning, and are frequently used in structured prediction problems. Efficient learning of parameters in these models is therefore an important problem, and becomes a key factor when learning from very large data sets. This paper describes exponentiated gradient (EG) algorithms for training such models, where EG updates are applied to the convex dual of either the log-linear or maxmargin objective function; the dual in both the log-linear and max-margin cases corresponds to minimizing a convex function with simplex constraints. We study both batch and online variants of the algorithm, and provide rates of convergence for both cases. In the max-margin case, O(1 ε) EG updates are required to reach a given accuracy ε in the dual; in contrast, for log-linear models only O(log(1 ε)) updates are required. For both the max-margin and log-linear cases, our bounds suggest that the online EG algorithm requires a factor of n less computation to reach a desired accuracy than the batch EG algorithm, where n is the number of training examples. Our experiments confirm that the online algorithms are much faster than the batch algorithms in practice. We describe how the EG updates factor in a convenient way for structured prediction problems, allowing the algorithms to be efficiently applied to problems such as sequence learning or natural language parsing. We perform extensive evaluation of the algorithms, comparing them to L-BFGS and stochastic gradient descent for log-linear models, and to SVM-Struct for max-margin models. The algorithms are applied to a multi-class problem as well as to a more complex large-scale parsing task. In all these settings, the EG algorithms presented here outperform the other methods."
            ],
            "keywords": [
                "exponentiated gradient",
                "log-linear models",
                "maximum-margin models",
                "structured prediction",
                "conditional random fields"
            ],
            "author": [
                "Michael Collins",
                "Amir Globerson",
                "Terry Koo",
                "Xavier Carreras",
                "Peter L Bartlett",
                "GLOBERSON, KOO Peter L Bartlett Collins"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/collins08a/collins08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Numerical Analysis near Singularities in RBF Networks",
            "abstract": [
                "The existence of singularities often affects the learning dynamics in feedforward neural networks. In this paper, based on theoretical analysis results, we numerically analyze the learning dynamics of radial basis function (RBF) networks near singularities to understand to what extent singularities influence the learning dynamics. First, we show the explicit expression of the Fisher information matrix for RBF networks. Second, we demonstrate through numerical simulations that the singularities have a significant impact on the learning dynamics of RBF networks. Our results show that overlap singularities mainly have influence on the low dimensional RBF networks and elimination singularities have a more significant impact to the learning processes than overlap singularities in both low and high dimensional RBF networks, whereas the plateau phenomena are mainly caused by the elimination singularities. The results can also be the foundation to investigate the singular learning dynamics in deep feedforward neural networks."
            ],
            "keywords": [
                "RBF networks",
                "Singularity",
                "Learning dynamics",
                "Numerical analysis",
                "Deep learning"
            ],
            "author": [
                "Weili Guo",
                "Haikun Wei",
                "Jaime Rubio Hervas",
                "Junsheng Zhao",
                "Hai Wang",
                "Kanjian Zhang",
                "C 2018",
                "Yew-Soon Ong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-210/16-210.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Combination of Feature Engineering and Ranking Models for Paper-Author Identification in KDD Cup 2013",
            "abstract": [
                "This paper describes the winning solution of team National Taiwan University for track 1 of KDD Cup 2013. The track 1 in KDD Cup 2013 considers the paper-author identification problem, which is to identify whether a paper is truly written by an author. First, we conduct feature engineering to transform the various types of provided text information into 97 features. Second, we train classification and ranking models using these features. Last, we combine our individual models to boost the performance by using results on the internal validation set and the official Valid set. Some effective post-processing techniques have also been proposed. Our solution achieves 0.98259 MAP score and ranks the first place on the private leaderboard of the Test set."
            ],
            "keywords": [],
            "author": [
                "Chun-Liang Li",
                "Yu-Chuan Su",
                "Ting-Wei Lin",
                "Cheng-Hao Tsai",
                "Wei-Cheng Chang",
                "Kuan-Hao Huang",
                "Tzu-Ming Kuo",
                "Shan-Wei Lin",
                "Yu-Chen Lu",
                "Chun-Pai Yang",
                "Cheng-Xia Chang",
                "Cheng-Kuang Wei",
                "Felix Wu",
                "Tu-Chun Yin",
                "Tong Yu",
                "Yong Zhuang",
                "Hsuan-Tien Lin",
                "Chih-Jen Lin",
                "Senjuti Basu Roy",
                "Vani Mandava",
                "Martine De Cock"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/li15b/li15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "CoCoA: A General Framework for Communication-Efficient Distributed Optimization",
            "abstract": [
                "The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for distributed computing environments, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly-convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly-convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets."
            ],
            "keywords": [
                "Convex optimization",
                "distributed systems",
                "large-scale machine learning",
                "parallel and distributed algorithms"
            ],
            "author": [
                "Virginia Smith",
                "Simone Forte",
                "Chenxin Ma",
                "Martin Takáč",
                "Michael I Jordan",
                "Martin Jaggi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-512/16-512.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sign Language Recognition using Sub-Units",
            "abstract": [
                "This paper discusses sign language recognition using linguistic sub-units. It presents three types of sub-units for consideration; those learnt from appearance data as well as those inferred from both 2D or 3D tracking data. These sub-units are then combined using a sign level classifier; here, two options are presented. The first uses Markov Models to encode the temporal changes between sub-units. The second makes use of Sequential Pattern Boosting to apply discriminative feature selection at the same time as encoding temporal information. This approach is more robust to noise and performs well in signer independent tests, improving results from the 54% achieved by the Markov Chains to 76%."
            ],
            "keywords": [
                "sign language recognition",
                "sequential pattern boosting",
                "depth cameras",
                "sub-units",
                "signer independence",
                "data set"
            ],
            "author": [
                "Helen Cooper",
                "Eng-Jon Ong",
                "Nicolas Pugeault",
                "Richard Bowden",
                "Isabelle Guyon",
                "Vassilis Athitsos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/cooper12a/cooper12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bounded p-norm Approximation of Max-Convolution for Sub-Quadratic Bayesian Inference on Additive Factors",
            "abstract": [
                "Max-convolution is an important problem closely resembling standard convolution; as such, maxconvolution occurs frequently across many fields. Here we extend the method with fastest known worst-case runtime, which can be applied to nonnegative vectors by numerically approximating the Chebyshev norm • ∞, and use this approach to derive two numerically stable methods based on the idea of computing p-norms via fast convolution: The first method proposed, with runtime in O(k log(k) log(log(k))) (which is less than 18k log(k) for any vectors that can be practically realized), uses the p-norm as a direct approximation of the Chebyshev norm. The second approach proposed, with runtime in O(k log(k)) (although in practice both perform similarly), uses a novel null space projection method, which extracts information from a sequence of p-norms to estimate the maximum value in the vector (this is equivalent to querying a small number of moments from a distribution of bounded support in order to estimate the maximum). The p-norm approaches are compared to one another and are shown to compute an approximation of the Viterbi path in a hidden Markov model where the transition matrix is a Toeplitz matrix; the runtime of approximating the Viterbi path is thus reduced from O(nk 2) steps to O(nk log(k)) steps in practice, and is demonstrated by inferring the U.S. unemployment rate from the S&P 500 stock index."
            ],
            "keywords": [
                "Bayesian inference",
                "maximum a posteriori",
                "fast Fourier transform",
                "max-convolution",
                "p-norm",
                "Lp space",
                "hidden Markov model",
                "null space projection",
                "polynomial matrix"
            ],
            "author": [
                "Julianus Pfeuffer",
                "Oliver Serang",
                "The Leibniz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-319/15-319.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Compressed Gaussian Process for Manifold Regression",
            "abstract": [
                "Nonparametric regression for large numbers of features (p) is an increasingly important problem. If the sample size n is massive, a common strategy is to partition the feature space, and then separately apply simple models to each partition set. This is not ideal when n is modest relative to p, and we propose an alternative approach relying on random compression of the feature vector combined with Gaussian process regression. The proposed approach is particularly motivated by the setting in which the response is conditionally independent of the features given the projection to a low dimensional manifold. Conditionally on the random compression matrix and a smoothness parameter, the posterior distribution for the regression surface and posterior predictive distributions are available analytically. Running the analysis in parallel for many random compression matrices and smoothness parameters, model averaging is used to combine the results. The algorithm can be implemented rapidly even in very large p and moderately large n nonparametric regression, has strong theoretical justification, and is found to yield state of the art predictive performance."
            ],
            "keywords": [
                "Compressed regression",
                "Gaussian process",
                "Gaussian random projection",
                "Large p",
                "Manifold regression"
            ],
            "author": [
                "Rajarshi Guhaniyogi",
                "David B Dunson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-230/14-230.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Monte Carlo Gradient Estimation in Machine Learning",
            "abstract": [
                "This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies-the pathwise, score function, and measure-valued gradient estimatorsexploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support."
            ],
            "keywords": [
                "gradient estimation",
                "Monte Carlo",
                "sensitivity analysis",
                "score-function estimator",
                "pathwise estimator",
                "measure-valued estimator",
                "variance reduction"
            ],
            "author": [
                "Shakir Mohamed",
                "Michael Figurnov",
                "©2020 S Mohamed",
                "M Rosca"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-346/19-346.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Contextual Bandits with Similarity Information",
            "abstract": [
                "In a multi-armed bandit (MAB) problem, an online algorithm makes a sequence of choices. In each round it chooses from a time-invariant set of alternatives and receives the payoff associated with this alternative. While the case of small strategy sets is by now wellunderstood, a lot of recent work has focused on MAB problems with exponentially or infinitely large strategy sets, where one needs to assume extra structure in order to make the problem tractable. In particular, recent literature considered information on similarity between arms. We consider similarity information in the setting of contextual bandits, a natural extension of the basic MAB problem where before each round an algorithm is given the context-a hint about the payoffs in this round. Contextual bandits are directly motivated by placing advertisements on web pages, one of the crucial problems in sponsored search. A particularly simple way to represent similarity information in the contextual bandit setting is via a similarity distance between the context-arm pairs which bounds from above the difference between the respective expected payoffs. Prior work on contextual bandits with similarity uses \"uniform\" partitions of the similarity space, so that each context-arm pair is approximated by the closest pair in the partition. Algorithms based on \"uniform\" partitions disregard the structure of the payoffs and the context arrivals, which is potentially wasteful. We present algorithms that are based on adaptive partitions, and take advantage of \"benign\" payoffs and context arrivals without sacrificing the worst-case performance. The central idea is to maintain a finer partition in high-payoff regions of the similarity space and in popular regions of the context space. Our results apply to several other settings, e.g., MAB with constrained temporal change (Slivkins and Upfal, 2008) and sleeping bandits (Kleinberg et al., 2008a)."
            ],
            "keywords": [
                "multi-armed bandits",
                "contextual bandits",
                "regret",
                "Lipschitz-continuity",
                "metric space"
            ],
            "author": [
                "Aleksandrs Slivkins"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/slivkins14a/slivkins14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Mutual Nearest Neighbors Estimate in Regression",
            "abstract": [
                "Motivated by promising experimental results, this paper investigates the theoretical properties of a recently proposed nonparametric estimator, called the Mutual Nearest Neighbors rule, which estimates the regression function m(x) = E[Y |X = x] as follows: first identify the k nearest neighbors of x in the sample D n , then keep only those for which x is itself one of the k nearest neighbors, and finally take the average over the corresponding response variables. We prove that this estimator is consistent and that its rate of convergence is optimal. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, we also present adaptation results by data-splitting."
            ],
            "keywords": [
                "nonparametric estimation",
                "nearest neighbor methods",
                "mathematical statistics"
            ],
            "author": [
                "Arnaud Guyader",
                "Nick Hengartner"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/guyader13a/guyader13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Search for Hyper-Parameter Optimization",
            "abstract": [
                "Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent \"High Throughput\" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms."
            ],
            "keywords": [
                "global optimization",
                "model selection",
                "neural networks",
                "deep learning",
                "response surface modeling"
            ],
            "author": [
                "James Bergstra",
                "Yoshua Bengio"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/bergstra12a/bergstra12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Clustering is semidefinitely not that hard: Nonnegative SDP for manifold disentangling",
            "abstract": [
                "In solving hard computational problems, semidefinite program (SDP) relaxations often play an important role because they come with a guarantee of optimality. Here, we focus on a popular semidefinite relaxation of K-means clustering which yields the same solution as the non-convex original formulation for well segregated datasets. We report an unexpected finding: when data contains (greater than zero-dimensional) manifolds, the SDP solution captures such geometrical structures. Unlike traditional manifold embedding techniques, our approach does not rely on manually defining a kernel but rather enforces locality via a nonnegativity constraint. We thus call our approach NOnnegative MAnifold Disentangling, or NOMAD. To build an intuitive understanding of its manifold learning capabilities, we develop a theoretical analysis of NOMAD on idealized datasets. While NOMAD is convex and the globally optimal solution can be found by generic SDP solvers with polynomial time complexity, they are too slow for modern datasets. To address this problem, we analyze a non-convex heuristic and present a new, convex and yet efficient, algorithm, based on the conditional gradient method. Our results render NOMAD a versatile, understandable, and powerful tool for manifold learning."
            ],
            "keywords": [
                "K-means",
                "semidefinite programming",
                "manifolds",
                "conditional gradient method"
            ],
            "author": [
                "Mariano Tepper",
                "Anirvan M Sengupta",
                "Dmitri Chklovskii"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-088/18-088.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "AI-Toolbox: A C ++ library for Reinforcement Learning and Planning (with Python Bindings)",
            "abstract": [
                "This paper describes AI-Toolbox, a C ++ software library that contains reinforcement learning and planning algorithms, and supports both single and multi agent problems, as well as partial observability. It is designed for simplicity and clarity, and contains extensive documentation of its API and code. It supports Python to enable users not comfortable with C ++ to take advantage of the library's speed and functionality. AI-Toolbox is free software, and is hosted online at https://github.com/Svalorzen/AI-Toolbox."
            ],
            "keywords": [
                "MDP",
                "POMDP",
                "multiagent",
                "reinforcement learning",
                "software",
                "open-source"
            ],
            "author": [
                "Eugenio Bargiacchi",
                "Diederik M Roijers",
                "Ann Nowé",
                "Eugenio ©2020",
                "Diederik M Bargiacchi",
                "Ann Roijers"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-402/18-402.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DirectLiNGAM: A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model",
            "abstract": [
                "Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. In such frameworks, linear acyclic models are typically used to model the data-generating process of variables. Recently, it was shown that use of non-Gaussianity identifies the full structure of a linear acyclic model, that is, a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a finite number of steps. In this paper, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity. In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model, that is, if all the model assumptions are met and the sample size is infinite."
            ],
            "keywords": [
                "structural equation models",
                "Bayesian networks",
                "independent component analysis",
                "non-Gaussianity",
                "causal discovery"
            ],
            "author": [
                "Shohei Shimizu",
                "Takashi Washio",
                "Patrik O Hoyer",
                "Takanori Shimizu",
                "Yasuhiro Inazumi",
                "Aapo Sogawa",
                "Yoshinobu Hyvärinen",
                "Takashi Kawahara",
                "Patrik O Washio",
                "Kenneth Hoyer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/shimizu11a/shimizu11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Introduction to Variable and Feature Selection",
            "abstract": [
                "Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is threefold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods."
            ],
            "keywords": [
                "Variable selection",
                "feature selection",
                "space dimensionality reduction",
                "pattern discovery",
                "filters",
                "wrappers",
                "clustering",
                "information theory",
                "support vector machines",
                "model selection",
                "statistical testing",
                "bioinformatics",
                "computational biology",
                "gene expression",
                "microarray",
                "genomics",
                "proteomics",
                "QSAR",
                "text classification",
                "information retrieval"
            ],
            "author": [
                "Isabelle Guyon"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tutorial on Practical Prediction Theory for Classification",
            "abstract": [
                "We discuss basic prediction theory and its impact on classification success evaluation, implications for learning algorithm design, and uses in learning algorithm execution. This tutorial is meant to be a comprehensive compilation of results which are both theoretically rigorous and quantitatively useful. There are two important implications of the results presented here. The first is that common practices for reporting results in classification should change to use the test set bound. The second is that train set bounds can sometimes be used to directly motivate learning algorithms."
            ],
            "keywords": [
                "sample complexity bounds",
                "classification",
                "quantitative bounds"
            ],
            "author": [
                "John Langford"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/langford05a/langford05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Practical Approaches to Principal Component Analysis in the Presence of Missing Values",
            "abstract": [
                "Principal component analysis (PCA) is a classical data analysis technique that finds linear transformations of data that retain the maximal amount of variance. We study a case where some of the data values are missing, and show that this problem has many features which are usually associated with nonlinear models, such as overfitting and bad locally optimal solutions. A probabilistic formulation of PCA provides a good foundation for handling missing values, and we provide formulas for doing that. In case of high dimensional and very sparse data, overfitting becomes a severe problem and traditional algorithms for PCA are very slow. We introduce a novel fast algorithm and extend it to variational Bayesian learning. Different versions of PCA are compared in artificial experiments, demonstrating the effects of regularization and modeling of posterior variance. The scalability of the proposed algorithm is demonstrated by applying it to the Netflix problem."
            ],
            "keywords": [
                "principal component analysis",
                "missing values",
                "overfitting",
                "regularization",
                "variational Bayes"
            ],
            "author": [
                "Alexander Ilin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/ilin10a/ilin10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Forward-Backward Selection with Early Dropping",
            "abstract": [
                "Forward-backward selection is one of the most basic and commonly-used feature selection algorithms available. It is also general and conceptually applicable to many different types of data. In this paper, we propose a heuristic that significantly improves its running time, while preserving predictive performance. The idea is to temporarily discard the variables that are conditionally independent with the outcome given the selected variable set. Depending on how those variables are reconsidered and reintroduced, this heuristic gives rise to a family of algorithms with increasingly stronger theoretical guarantees. In distributions that can be faithfully represented by Bayesian networks or maximal ancestral graphs, members of this algorithmic family are able to correctly identify the Markov blanket in the sample limit. In experiments we show that the proposed heuristic increases computational efficiency by about 1-2 orders of magnitude, while selecting fewer or the same number of variables and retaining predictive performance. Furthermore, we show that the proposed algorithm and feature selection with LASSO perform similarly when restricted to select the same number of variables, making the proposed algorithm an attractive alternative for problems where no (efficient) algorithm for LASSO exists."
            ],
            "keywords": [
                "Feature Selection",
                "Forward Selection",
                "Markov Blanket Discovery",
                "Bayesian Networks",
                "Maximal Ancestral Graphs"
            ],
            "author": [
                "Giorgos Borboudakis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-334/17-334.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximation Vector Machines for Large-scale Online Learning",
            "abstract": [
                "One of the most challenging problems in kernel online learning is to bound the model size and to promote model sparsity. Sparse models not only improve computation and memory usage, but also enhance the generalization capacity-a principle that concurs with the law of parsimony. However, inappropriate sparsity modeling may also significantly degrade the performance. In this paper, we propose Approximation Vector Machine (AVM), a model that can simultaneously encourage sparsity and safeguard its risk in compromising the performance. In an online setting context, when an incoming instance arrives, we approximate this instance by one of its neighbors whose distance to it is less than a predefined threshold. Our key intuition is that since the newly seen instance is expressed by its nearby neighbor the optimal performance can be analytically formulated and maintained. We develop theoretical foundations to support this intuition and further establish an analysis for the common loss functions including Hinge, smooth Hinge, and Logistic (i.e., for the classification task) and 1 , 2 , and ε-insensitive (i.e., for the regression task) to characterize the gap between the approximation and optimal solutions. This gap crucially depends on two key factors including the frequency of approximation (i.e., how frequent the approximation operation takes place) and the predefined threshold. We conducted extensive experiments for classification and regression tasks in batch and online modes using several benchmark datasets. The quantitative results show that our proposed AVM obtained comparable predictive performances with current state-of-the-art methods while simultaneously achieving significant computational speed-up due to the ability of the proposed AVM in maintaining the model size."
            ],
            "keywords": [
                "kernel",
                "online learning",
                "large-scale machine learning",
                "sparsity",
                "big data",
                "core set",
                "stochastic gradient descent",
                "convergence analysis"
            ],
            "author": [
                "Trung Le",
                "Tu Dinh Nguyen",
                "Vu Nguyen",
                "Dinh Phung"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-191/16-191.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Node-Based Learning of Multiple Gaussian Graphical Models",
            "abstract": [
                "We consider the problem of estimating high-dimensional Gaussian graphical models corresponding to a single set of variables under several distinct conditions. This problem is motivated by the task of recovering transcriptional regulatory networks on the basis of gene expression data containing heterogeneous samples, such as different disease states, multiple species, or different developmental stages. We assume that most aspects of the conditional dependence networks are shared, but that there are some structured differences between them. Rather than assuming that similarities and differences between networks are driven by individual edges, we take a node-based approach, which in many cases provides a more intuitive interpretation of the network differences. We consider estimation under two distinct assumptions: (1) differences between the K networks are due to individual nodes that are perturbed across conditions, or (2) similarities among the K networks are due to the presence of common hub nodes that are shared across all K networks. Using a rowcolumn overlap norm penalty function, we formulate two convex optimization problems that correspond to these two assumptions. We solve these problems using an alternating direction method of multipliers algorithm, and we derive a set of necessary and sufficient conditions that allows us to decompose the problem into independent subproblems so that our algorithm can be scaled to high-dimensional settings. Our proposal is illustrated on synthetic data, a webpage data set, and a brain cancer gene expression data set."
            ],
            "keywords": [
                "graphical model",
                "structured sparsity",
                "alternating direction method of multipliers",
                "gene regulatory network",
                "lasso",
                "multivariate normal"
            ],
            "author": [
                "Karthik Mohan",
                "Palma London",
                "Maryam Fazel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/mohan14a/mohan14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "NEVAE: A Deep Generative Model for Molecular Graphs *",
            "abstract": [
                "Deep generative models have been praised for their ability to learn smooth latent representations of images, text, and audio, which can then be used to generate new, plausible data. Motivated by these success stories, there has been a surge of interest in developing deep generative models for automated molecule design. However, these models face several difficulties due to the unique characteristics of molecular graphs-their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes' labels, and they come with a different number of nodes and edges. In this paper, we first propose a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. Moreover, in contrast with the state of the art, our decoder is able to provide the spatial coordinates of the atoms of the molecules it generates. Then, we develop a gradient-based algorithm to optimize the decoder of our model so that it learns to generate molecules that maximize the value of certain property of interest and, given any arbitrary molecule, it is able to optimize the spatial configuration of its atoms for greater stability. Experiments reveal that our variational autoencoder can discover plausible, diverse and novel molecules more effectively than several state of the art models. Moreover, for several properties of interest, our optimized decoder is able to identify molecules with property values 121% higher than those identified by several state of the art methods based on Bayesian optimization and reinforcement learning."
            ],
            "keywords": [
                "Drug design",
                "Molecule discovery",
                "Deep generative models",
                "Variational autoencoders",
                "Geometric deep learning"
            ],
            "author": [
                "Bidisha Samanta",
                "Abir De",
                "Jana Gourhari",
                "Pratim Vicenç Gomez",
                "Pratim Kumar Chattaraj",
                "Niloy Ganguly",
                "Manuel Gomez-Rodriguez",
                "Gourhari Jana",
                "Kumar Chattaraj"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-671/19-671.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Computationally Efficient Convolved Multiple Output Gaussian Processes",
            "abstract": [
                "Recently there has been an increasing interest in regression methods that deal with multiple outputs. This has been motivated partly by frameworks like multitask learning, multisensor networks or structured output data. From a Gaussian processes perspective, the problem reduces to specifying an appropriate covariance function that, whilst being positive semi-definite, captures the dependencies between all the data points and across all the outputs. One approach to account for non-trivial correlations between outputs employs convolution processes. Under a latent function interpretation of the convolution transform we establish dependencies between output variables. The main drawbacks of this approach are the associated computational and storage demands. In this paper we address these issues. We present different efficient approximations for dependent output Gaussian processes constructed through the convolution formalism. We exploit the conditional independencies present naturally in the model. This leads to a form of the covariance similar in spirit to the so called PITC and FITC approximations for a single output. We show experimental results with synthetic and real data, in particular, we show results in school exams score prediction, pollution prediction and gene expression data."
            ],
            "keywords": [
                "Gaussian processes",
                "convolution processes",
                "efficient approximations",
                "multitask learning",
                "structured outputs",
                "multivariate processes"
            ],
            "author": [
                "Mauricio A Álvarez",
                "Neil D Lawrence"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/alvarez11a/alvarez11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model Selection via the VC Dimension",
            "abstract": [
                "We derive an objective function that can be optimized to give an estimator for the Vapnik-Chervonenkis dimension for use in model selection in regression problems. We verify our estimator is consistent. Then, we verify it performs well compared to seven other model selection techniques. We do this for a variety of types of data sets."
            ],
            "keywords": [
                "Vapnik-Chervonenkis dimension",
                "model selection",
                "Bayesian information criterion",
                "sparsity methods",
                "empirical risk minimization",
                "multi-type data"
            ],
            "author": [
                "Merlin Mpoudeu",
                "Bertrand Clarke"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-669/17-669.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tensor Regression Networks",
            "abstract": [
                "Convolutional neural networks typically consist of many convolutional layers followed by one or more fully connected layers. While convolutional layers map between high-order activation tensors, the fully connected layers operate on flattened activation vectors. Despite empirical success, this approach has notable drawbacks. Flattening followed by fully connected layers discards multilinear structure in the activations and requires many parameters. We address these problems by incorporating tensor algebraic operations that preserve multilinear structure at every layer. First, we introduce Tensor Contraction Layers (TCLs) that reduce the dimensionality of their input while preserving their multilinear structure using tensor contraction. Next, we introduce Tensor Regression Layers (TRLs), which express outputs through a low-rank multilinear mapping from a high-order activation tensor to an output tensor of arbitrary order. We learn the contraction and regression factors end-to-end, and produce accurate nets with fewer parameters. Additionally, our layers regularize networks by imposing low-rank constraints on the activations (TCL) and regression weights (TRL). Experiments on ImageNet show that, applied to VGG and ResNet architectures, TCLs and TRLs reduce the number of parameters compared to fully connected layers by more than 65% while maintaining or increasing accuracy. In addition to the space savings, our approach's ability to leverage topological structure can be crucial for structured data such as MRI. In particular, we demonstrate significant performance improvements over comparable architectures on three tasks associated with the UK Biobank dataset."
            ],
            "keywords": [
                "Machine Learning",
                "Tensor Methods",
                "Tensor Regression Networks",
                "Low-Rank Regression",
                "Tensor Regression Layers",
                "Deep Learning",
                "Tensor Contraction"
            ],
            "author": [
                "Jean Kossaifi",
                "Arinbjörn Kolbeinsson",
                "Aran Khanna",
                "Tommaso Furlanello"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-503/18-503.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Submodular Maximization",
            "abstract": [
                "Many large-scale machine learning problems-clustering, non-parametric learning, kernel machines, etc.-require selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Classical approaches to submodular optimization require centralized access to the full dataset, which is impractical for truly large-scale problems. In this paper, we consider the problem of submodular function maximization in a distributed fashion. We develop a simple, two-stage protocol GreeDi, that is easily implemented using MapReduce style computations. We theoretically analyze our approach, and show that under certain natural conditions, performance close to the centralized approach can be achieved. We begin with monotone submodular maximization subject to a cardinality constraint, and then extend this approach to obtain approximation guarantees for (not necessarily monotone) submodular maximization subject to more general constraints including matroid or knapsack constraints. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including sparse Gaussian process inference and exemplar based clustering on tens of millions of examples using Hadoop."
            ],
            "keywords": [
                "distributed computing",
                "submodular functions",
                "approximation algorithms",
                "greedy algorithms",
                "map-reduce"
            ],
            "author": [
                "Baharan Mirzasoleiman",
                "Amin Karbasi",
                "Rik Sarkar",
                "Andreas Krause"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/mirzasoleiman16a/mirzasoleiman16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DEAP: Evolutionary Algorithms Made Easy",
            "abstract": [
                "DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license."
            ],
            "keywords": [],
            "author": [
                "Félix-Antoine Fortin",
                "François-Michel De Rainville",
                "Marc-André Gardner",
                "Marc Parizeau"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/fortin12a/fortin12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Learning by Spherical Subdivision",
            "abstract": [
                "We introduce a computationally feasible, \"constructive\" active learning method for binary classification. The learning algorithm is initially formulated for separable classification problems, for a hyperspherical data space with constant data density, and for great spheres as classifiers. In order to reduce computational complexity the version space is restricted to spherical simplices and learning procedes by subdividing the edges of maximal length. We show that this procedure optimally reduces a tight upper bound on the generalization error. The method is then extended to other separable classification problems using products of spheres as data spaces and isometries induced by charts of the sphere. An upper bound is provided for the probability of disagreement between classifiers (hence the generalization error) for non-constant data densities on the sphere. The emphasis of this work lies on providing mathematically exact performance estimates for active learning strategies."
            ],
            "keywords": [
                "active learning",
                "spherical subdivision",
                "error bounds",
                "simplex halving"
            ],
            "author": [
                "Falk-Florian Henrich",
                "Klaus Obermayer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/henrich08a/henrich08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Independent Component Analysis",
            "abstract": [
                "We present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria and their derivatives can be computed efficiently. Minimizing these criteria leads to flexible and robust algorithms for ICA. We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms."
            ],
            "keywords": [
                "kernel methods",
                "independent component analysis",
                "blind source separation",
                "mutual information",
                "Gram matrices",
                "canonical correlations",
                "semiparametric models",
                "integral equations",
                "Stiefel manifold",
                "incomplete Cholesky decomposition"
            ],
            "author": [
                "Francis R Bach",
                "Michael I Jordan"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/bach02a/bach02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantile Graphical Models: a Bayesian Approach",
            "abstract": [
                "Graphical models are ubiquitous tools to describe the interdependence between variables measured simultaneously such as large-scale gene or protein expression data. Gaussian graphical models (GGMs) are well-established tools for probabilistic exploration of dependence structures using precision matrices and they are generated under a multivariate normal joint distribution. However, they suffer from several shortcomings since they are based on Gaussian distribution assumptions. In this article, we propose a Bayesian quantile based approach for sparse estimation of graphs. We demonstrate that the resulting graph estimation is robust to outliers and applicable under general distributional assumptions. Furthermore, we develop efficient variational Bayes approximations to scale the methods for large data sets. Our methods are applied to a novel cancer proteomics data dataset where-in multiple proteomic antibodies are simultaneously assessed on tumor samples using reverse-phase protein arrays (RPPA) technology."
            ],
            "keywords": [
                "Graphical model",
                "Quantile regression",
                "Variational Bayes"
            ],
            "author": [
                "Nilabja Guha",
                "Veera Baladandayuthapani",
                "Bani K Mallick"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/17-231/17-231.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Algorithms and Hardness Results for Parallel Large Margin Learning",
            "abstract": [
                "We consider the problem of learning an unknown large-margin halfspace in the context of parallel computation, giving both positive and negative results. As our main positive result, we give a parallel algorithm for learning a large-margin halfspace, based on an algorithm of Nesterov's that performs gradient descent with a momentum term. We show that this algorithm can learn an unknown γ-margin halfspace over n dimensions using n • poly(1/γ) processors and running in timeÕ(1/γ) + O(log n). In contrast, naive parallel algorithms that learn a γ-margin halfspace in time that depends polylogarithmically on n have an inverse quadratic running time dependence on the margin parameter γ. Our negative result deals with boosting, which is a standard approach to learning large-margin halfspaces. We prove that in the original PAC framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized. More precisely, we show that, if the algorithm is allowed to call the weak learner multiple times in parallel within a single boosting stage, this ability does not reduce the overall number of successive stages of boosting needed for learning by even a single stage. Our proof is information-theoretic and does not rely on unproven assumptions."
            ],
            "keywords": [
                "PAC learning",
                "parallel learning algorithms",
                "halfspace learning",
                "linear classifiers"
            ],
            "author": [
                "Philip M Long",
                "Rocco A Servedio"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/long13a/long13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs",
            "abstract": [
                "We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is first transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences."
            ],
            "keywords": [
                "pattern extraction",
                "sign language recognition",
                "signeme extraction",
                "sign modeling",
                "iterated conditional modes"
            ],
            "author": [
                "Sunita Nayak",
                "Kester Duncan",
                "Barbara Loeding"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/nayak12a/nayak12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks",
            "abstract": [
                "In the machine learning community, the Bayesian scoring criterion is widely used for model selection problems. One of the fundamental theoretical properties justifying the usage of the Bayesian scoring criterion is its consistency. In this paper we refine this property for the case of binomial Bayesian network models. As a by-product of our derivations we establish strong consistency and obtain the law of iterated logarithm for the Bayesian scoring criterion."
            ],
            "keywords": [
                "Bayesian networks",
                "consistency",
                "scoring criterion",
                "model selection",
                "BIC"
            ],
            "author": [
                "Nikolai Slobodianik",
                "MADRAS@MATHSTAT Neal Madras"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/slobodianik09a/slobodianik09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Theory of Learning with Corrupted Labels",
            "abstract": [
                "It is usual in machine learning theory to assume that the training and testing sets comprise of draws from the same distribution. This is rarely, if ever, true and one must admit the presence of corruption. There are many different types of corruption that can arise and as of yet there is no general means to compare the relative ease of learning in these settings. Such results are necessary if we are to make informed economic decisions regarding the acquisition of data. Here we begin to develop an abstract framework for tackling these problems. We present a generic method for learning from a fixed, known, reconstructible corruption, along with an analyses of its statistical properties. We demonstrate the utility of our framework via concrete novel results in solving supervised learning problems wherein the labels are corrupted, such as learning with noisy labels, semi-supervised learning and learning with partial labels."
            ],
            "keywords": [
                "Supervised Learning",
                "Generalized Supervision",
                "Decision Theory",
                "Minimax Bounds",
                "Data Processing",
                "Noise"
            ],
            "author": [
                "Brendan Van Rooyen",
                "Robert C Williamson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-315/16-315.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "JKernelMachines: A Simple Framework for Kernel Machines",
            "abstract": [
                "JKernelMachines is a Java library for learning with kernels. It is primarily designed to deal with custom kernels that are not easily found in standard libraries, such as kernels on structured data. These types of kernels are often used in computer vision or bioinformatics applications. We provide several kernels leading to state of the art classification performances in computer vision, as well as various kernels on sets. The main focus of the library is to be easily extended with new kernels. Standard SVM optimization algorithms are available, but also more sophisticated learning-based kernel combination methods such as Multiple Kernel Learning (MKL), and a recently published algorithm to learn powered products of similarities (Product Kernel Learning)."
            ],
            "keywords": [
                "classification",
                "support vector machines",
                "kernel",
                "computer vision"
            ],
            "author": [
                "David Picard",
                "Nicolas Thome"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/picard13a/picard13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Text Classification using String Kernels",
            "abstract": [
                "We propose a novel approach for categorizing text documents based on the use of a special kernel. The kernel is an inner product in the feature space generated by all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. Experimental comparisons of the performance of the kernel compared with a standard word feature space kernel (Joachims, 1998) show positive results on modestly sized datasets. The case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors. For larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets."
            ],
            "keywords": [
                "Kernels and Support Vector Machines",
                "String Subsequence Kernel",
                "Approximating Kernels",
                "Text Classification"
            ],
            "author": [
                "Huma Lodhi",
                "Craig Saunders",
                "John Shawe-Taylor",
                "Nello Cristianini",
                "Chris Watkins"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/lodhi02a/lodhi02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Hidden Variable Networks: The Information Bottleneck Approach",
            "abstract": [
                "A central challenge in learning probabilistic graphical models is dealing with domains that involve hidden variables. The common approach for learning model parameters in such domains is the expectation maximization (EM) algorithm. This algorithm, however, can easily get trapped in suboptimal local maxima. Learning the model structure is even more challenging. The structural EM algorithm can adapt the structure in the presence of hidden variables, but usually performs poorly without prior knowledge about the cardinality and location of the hidden variables. In this work, we present a general approach for learning Bayesian networks with hidden variables that overcomes these problems. The approach builds on the information bottleneck framework of Tishby et al. (1999). We start by proving formal correspondence between the information bottleneck objective and the standard parametric EM functional. We then use this correspondence to construct a learning algorithm that combines an information-theoretic smoothing term with a continuation procedure. Intuitively, the algorithm bypasses local maxima and achieves superior solutions by following a continuous path from a solution of, an easy and smooth, target function, to a solution of the desired likelihood function. As we show, our algorithmic framework allows learning of the parameters as well as the structure of a network. In addition, it also allows us to introduce new hidden variables during model selection and learn their cardinality. We demonstrate the performance of our procedure on several challenging real-life data sets."
            ],
            "keywords": [
                "Bayesian networks",
                "hidden variables",
                "information bottleneck",
                "continuation",
                "variational methods"
            ],
            "author": [
                "Gal Elidan",
                "Nir Friedman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/elidan05a/elidan05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning a Mahalanobis Metric from Equivalence Constraints",
            "abstract": [
                "Many learning algorithms use a metric defined over the input space as a principal tool, and their performance critically depends on the quality of this metric. We address the problem of learning metrics using side-information in the form of equivalence constraints. Unlike labels, we demonstrate that this type of side-information can sometimes be automatically obtained without the need of human intervention. We show how such side-information can be used to modify the representation of the data, leading to improved clustering and classification. Specifically, we present the Relevant Component Analysis (RCA) algorithm, which is a simple and efficient algorithm for learning a Mahalanobis metric. We show that RCA is the solution of an interesting optimization problem, founded on an information theoretic basis. If dimensionality reduction is allowed within RCA, we show that it is optimally accomplished by a version of Fisher's linear discriminant that uses constraints. Moreover, under certain Gaussian assumptions, RCA can be viewed as a Maximum Likelihood estimation of the within class covariance matrix. We conclude with extensive empirical evaluations of RCA, showing its advantage over alternative methods."
            ],
            "keywords": [
                "clustering",
                "metric learning",
                "dimensionality reduction",
                "equivalence constraints",
                "side information"
            ],
            "author": [
                "Aharon Bar-Hillel",
                "Tomer Hertz",
                "Noam Shental",
                "Daphna Weinshall"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/bar-hillel05a/bar-hillel05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics",
            "abstract": [
                "Kernel-based methods exhibit well-documented performance in various nonlinear learning tasks. Most of them rely on a preselected kernel, whose prudent choice presumes task-specific prior information. Especially when the latter is not available, multi-kernel learning has gained popularity thanks to its flexibility in choosing kernels from a prescribed kernel dictionary. Leveraging the random feature approximation and its recent orthogonality-promoting variant, the present contribution develops a scalable multi-kernel learning scheme (termed Raker) to obtain the sought nonlinear learning function 'on the fly,' first for static environments. To further boost performance in dynamic environments, an adaptive multi-kernel learning scheme (termed AdaRaker) is developed. AdaRaker accounts not only for data-driven learning of kernel combination, but also for the unknown dynamics. Performance is analyzed in terms of both static and dynamic regrets. AdaRaker is uniquely capable of tracking nonlinear learning functions in environments with unknown dynamics, and with with analytic performance guarantees. Tests with synthetic and real datasets are carried out to showcase the effectiveness of the novel algorithms."
            ],
            "keywords": [
                "Online learning",
                "reproducing kernel Hilbert space",
                "multi-kernel learning",
                "random features",
                "dynamic and adversarial environments"
            ],
            "author": [
                "Yanning Shen",
                "Tianyi Chen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-030/18-030.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning to Classify Ordinal Data: The Data Replication Method",
            "abstract": [
                "Classification of ordinal data is one of the most important tasks of relation learning. This paper introduces a new machine learning paradigm specifically intended for classification problems where the classes have a natural order. The technique reduces the problem of classifying ordered classes to the standard two-class problem. The introduced method is then mapped into support vector machines and neural networks. Generalization bounds of the proposed ordinal classifier are also provided. An experimental study with artificial and real data sets, including an application to gene expression analysis, verifies the usefulness of the proposed approach."
            ],
            "keywords": [
                "classification",
                "ordinal data",
                "support vector machines",
                "neural networks"
            ],
            "author": [
                "Jaime S Cardoso",
                "Joaquim F Pinto Da Costa"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/cardoso07a/cardoso07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods",
            "abstract": [
                "We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of εand ν-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples."
            ],
            "keywords": [
                "sampling inequality",
                "radial basis functions",
                "approximation theory",
                "reproducing kernel Hilbert space",
                "Sobolev space"
            ],
            "author": [
                "Christian Rieger",
                "Barbara Zwicknagl"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/rieger09a/rieger09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Latent Space Inference of Internet-Scale Networks",
            "abstract": [
                "The rise of Internet-scale networks, such as web graphs and social media with hundreds of millions to billions of nodes, presents new scientific opportunities, such as overlapping community detection to discover the structure of the Internet, or to analyze trends in online social behavior. However, many existing probabilistic network models are difficult or impossible to deploy at these massive scales. We propose a scalable approach for modeling and inferring latent spaces in Internet-scale networks, with an eye towards overlapping community detection as a key application. By applying a succinct representation of networks as a bag of triangular motifs, developing a parsimonious statistical model, deriving an efficient stochastic variational inference algorithm, and implementing it as a distributed cluster program via the Petuum parameter server system, we demonstrate overlapping community detection on real networks with up to 100 million nodes and 1000 communities on 5 machines in under 40 hours. Compared to other state-of-the-art probabilistic network approaches, our method is several orders of magnitude faster, with competitive or improved accuracy at overlapping community detection."
            ],
            "keywords": [
                "probabilistic network models",
                "triangular modeling",
                "stochastic variational inference",
                "distributed computation",
                "big data"
            ],
            "author": [
                "Qirong Ho",
                "Eric P Xing"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-142/15-142.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Group Sparse Optimization via p,q Regularization",
            "abstract": [
                "In this paper, we investigate a group sparse optimization problem via p,q regularization in three aspects: theory, algorithm and application. In the theoretical aspect, by introducing a notion of group restricted eigenvalue condition, we establish an oracle property and a global recovery bound of order O(λ 2 2−q) for any point in a level set of the p,q regularization problem, and by virtue of modern variational analysis techniques, we also provide a local analysis of recovery bound of order O(λ 2) for a path of local minima. In the algorithmic aspect, we apply the well-known proximal gradient method to solve the p,q regularization problems, either by analytically solving some specific p,q regularization subproblems, or by using the Newton method to solve general p,q regularization subproblems. In particular, we establish a local linear convergence rate of the proximal gradient method for solving the 1,q regularization problem under some mild conditions and by first proving a second-order growth condition. As a consequence, the local linear convergence rate of proximal gradient method for solving the usual q regularization problem (0 < q < 1) is obtained. Finally in *. Corresponding author."
            ],
            "keywords": [
                "group sparse optimization",
                "lower-order regularization",
                "nonconvex optimization",
                "restricted eigenvalue condition",
                "proximal gradient method",
                "iterative thresholding algorithm",
                "gene regulation network"
            ],
            "author": [
                "Yaohua Hu",
                "Chong Li",
                "Jing Qin",
                "Xiaoqi Yang",
                "Meng Qin Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-651/15-651.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ε-MDPs: Learning in Varying Environments",
            "abstract": [
                "In this paper ε-MDP-models are introduced and convergence theorems are proven using the generalized MDP framework of Szepesvári and Littman. Using this model family, we show that Q-learning is capable of finding near-optimal policies in varying environments. The potential of this new family of MDP models is illustrated via a reinforcement learning algorithm called event-learning which separates the optimization of decision making from the controller. We show that event-learning augmented by a particular controller, which gives rise to an ε-MDP, enables near optimal performance even if considerable and sudden changes may occur in the environment. Illustrations are provided on the two-segment pendulum problem."
            ],
            "keywords": [
                "reinforcement learning",
                "convergence",
                "event-learning",
                "SARSA",
                "MDP",
                "generalized MDP",
                "ε-MDP",
                "SDS controller"
            ],
            "author": [
                "István Szita"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/szita02a/szita02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cautious Collective Classification",
            "abstract": [
                "Many collective classification (CC) algorithms have been shown to increase accuracy when instances are interrelated. However, CC algorithms must be carefully applied because their use of estimated labels can in some cases decrease accuracy. In this article, we show that managing this label uncertainty through cautious algorithmic behavior is essential to achieving maximal, robust performance. First, we describe cautious inference and explain how four well-known families of CC algorithms can be parameterized to use varying degrees of such caution. Second, we introduce cautious learning and show how it can be used to improve the performance of almost any CC algorithm, with or without cautious inference. We then evaluate cautious inference and learning for the four collective inference families, with three local classifiers and a range of both synthetic and real-world data. We find that cautious learning and cautious inference typically outperform less cautious approaches. In addition, we identify the data characteristics that predict more substantial performance differences. Our results reveal that the degree of caution used usually has a larger impact on performance than the choice of the underlying inference algorithm. Together, these results identify the most appropriate CC algorithms to use for particular task characteristics and explain multiple conflicting findings from prior CC research."
            ],
            "keywords": [
                "collective inference",
                "statistical relational learning",
                "approximate probabilistic inference",
                "networked data",
                "cautious inference"
            ],
            "author": [
                "Luke K Mcdowell",
                "Kalyan Moy",
                "Gupta Kalyan",
                "Knexusresearch Com",
                "David W Aha"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/mcdowell09a/mcdowell09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Sparse Semismooth Newton Based Proximal Majorization-Minimization Algorithm for Nonconvex Square-Root-Loss Regression Problems",
            "abstract": [
                "In this paper, we consider high-dimensional nonconvex square-root-loss regression problems and introduce a proximal majorization-minimization (PMM) algorithm for solving these problems. Our key idea for making the proposed PMM to be efficient is to develop a sparse semismooth Newton method to solve the corresponding subproblems. By using the Kurdyka-Lojasiewicz property exhibited in the underlining problems, we prove that the PMM algorithm converges to a d-stationary point. We also analyze the oracle property of the initial subproblem used in our algorithm. Extensive numerical experiments are presented to demonstrate the high efficiency of the proposed PMM algorithm."
            ],
            "keywords": [
                "nonconvex square-root regression problems",
                "proximal majorization-minimization",
                "semismooth Newton method"
            ],
            "author": [
                "Peipei Tang",
                "Chengjing Wang",
                "Kim-Chuan Toh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-247/19-247.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalization Bounds for the Area Under the ROC Curve *",
            "abstract": [
                "We study generalization properties of the area under the ROC curve (AUC), a quantity that has been advocated as an evaluation criterion for the bipartite ranking problem. The AUC is a different term than the error rate used for evaluation in classification problems; consequently, existing generalization bounds for the classification error rate cannot be used to draw conclusions about the AUC. In this paper, we define the expected accuracy of a ranking function (analogous to the expected error rate of a classification function), and derive distribution-free probabilistic bounds on the deviation of the empirical AUC of a ranking function (observed on a finite data sequence) from its expected accuracy. We derive both a large deviation bound, which serves to bound the expected accuracy of a ranking function in terms of its empirical AUC on a test sequence, and a uniform convergence bound, which serves to bound the expected accuracy of a learned ranking function in terms of its empirical AUC on a training sequence. Our uniform convergence bound is expressed in terms of a new set of combinatorial parameters that we term the bipartite rank-shatter coefficients; these play the same role in our result as do the standard VC-dimension related shatter coefficients (also known as the growth function) in uniform convergence results for the classification error rate. A comparison of our result with a recent uniform convergence result derived by Freund et al. (2003) for a quantity closely related to the AUC shows that the bound provided by our result can be considerably tighter."
            ],
            "keywords": [
                "generalization bounds",
                "area under the ROC curve",
                "ranking",
                "large deviations",
                "uniform convergence"
            ],
            "author": [
                "Shivani Agarwal",
                "Ralf Herbrich",
                "Dan Roth",
                "Shivani ©2005",
                "Thore Agarwal",
                "Ralf Graepel",
                "Sariel Herbrich",
                "Dan Roth Har-Peled"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/agarwal05a/agarwal05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Faster Algorithms for Max-Product Message-Passing *",
            "abstract": [
                "Maximum A Posteriori inference in graphical models is often solved via message-passing algorithms, such as the junction-tree algorithm or loopy belief-propagation. The exact solution to this problem is well-known to be exponential in the size of the maximal cliques of the triangulated model, while approximate inference is typically exponential in the size of the model's factors. In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist only of latent variables (i.e., they do not depend on an observation). This is a common case in a wide variety of applications that deal with grid-, tree-, and ring-structured models. In such cases, we are able to decrease the exponent of complexity for message-passing by 0.5 for both exact and approximate inference. We demonstrate that message-passing operations in such models are equivalent to some variant of matrix multiplication in the tropical semiring, for which we offer an O(N 2.5) expected-case solution."
            ],
            "keywords": [
                "graphical models",
                "belief-propagation",
                "tropical matrix multiplication"
            ],
            "author": [
                "Julian J Mcauley",
                "Tibério S Caetano"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/mcauley11a/mcauley11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Using Confidence Bounds for Exploitation-Exploration Trade-offs",
            "abstract": [
                "We show how a standard tool from statistics-namely confidence bounds-can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We apply our technique to two models with such an exploitation-exploration trade-off. For the adversarial bandit problem with shifting our new algorithm suffers onlyÕ (ST) 1/2 regret with high probability over T trials with S shifts. Such a regret bound was previously known only in expectation. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O T 3/4 toÕ T 1/2 ."
            ],
            "keywords": [
                "Online Learning",
                "Exploitation-Exploration",
                "Bandit Problem",
                "Reinforcement Learning",
                "Linear Value Function"
            ],
            "author": [
                "Peter Auer"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/auer02a/auer02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Sums of Independent Random Variables with Sparse Collective Support",
            "abstract": [
                "We study the learnability of sums of independent integer random variables given a bound on the size of the union of their supports. For A ⊂ Z + , a sum of independent random variables with collective support A (called an A-sum in this paper) is a distribution S = X 1 + • • • + X N where the X i 's are mutually independent (but not necessarily identically distributed) integer random variables with ∪ i supp(X i) ⊆ A. We give two main algorithmic results for learning such distributions. First, for the case |A| = 3, we give an algorithm for learning an unknown A-sum to accuracy ε using poly(1/ε) samples and running in time poly(1/ε), independent of N and of the elements of A. Second, for an arbitrary constant k ≥ 4, if A = {a 1 , ..., a k } with 0 ≤ a 1 < ... < a k , we give an algorithm that uses poly(1/ε) • log log a k samples (independent of N) and runs in time poly(1/ε, log a k). We prove an essentially matching lower bound: if |A| = 4, then any algorithm must use * ."
            ],
            "keywords": [
                "Central limit theorem",
                "sample complexity",
                "sums of independent random variables",
                "equidistribution",
                "semi-agnostic learning"
            ],
            "author": [
                "Anindya De",
                "Philip M Long",
                "Rocco A Servedio"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-531/18-531.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bayesian Model for Supervised Clustering with the Dirichlet Process Prior",
            "abstract": [
                "We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to define distributions over the countably infinite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these \"reference types\") that are generic across all clusters. Inference in our framework, which requires integrating over infinitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple-but general-parameterization of our model based on a Gaussian assumption. We evaluate this model on one artificial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics."
            ],
            "keywords": [
                "supervised clustering",
                "record linkage",
                "citation matching",
                "coreference",
                "Dirichlet process",
                "non-parametric Bayesian"
            ],
            "author": [
                "Hal Daumé III",
                "Daniel Marcu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/daume05a/daume05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dlib-ml: A Machine Learning Toolkit",
            "abstract": [
                "There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools."
            ],
            "keywords": [
                "kernel-methods",
                "svm",
                "rvm",
                "kernel clustering",
                "C++",
                "Bayesian networks"
            ],
            "author": [
                "Davis E King"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/king09a/king09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Propagation in Conditional Gaussian Bayesian Networks",
            "abstract": [
                "This paper describes a scheme for local computation in conditional Gaussian Bayesian networks that combines the approach of Lauritzen and Jensen (2001) with some elements of Shachter and Kenley (1989). Message passing takes place on an elimination tree structure rather than the more compact (and usual) junction tree of cliques. This yields a local computation scheme in which all calculations involving the continuous variables are performed by manipulating univariate regressions, and hence matrix operations are avoided."
            ],
            "keywords": [
                "Bayesian networks",
                "conditional Gaussian distributions",
                "propagation algorithm",
                "elimination tree"
            ],
            "author": [
                "Robert G Cowell"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/cowell05a/cowell05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Selective Sampling and Active Learning from Single and Multiple Teachers",
            "abstract": [
                "We present a new online learning algorithm in the selective sampling framework, where labels must be actively queried before they are revealed. We prove bounds on the regret of our algorithm and on the number of labels it queries when faced with an adaptive adversarial strategy of generating the instances. Our bounds both generalize and strictly improve over previous bounds in similar settings. Additionally, our selective sampling algorithm can be converted into an efficient statistical active learning algorithm. We extend our algorithm and analysis to the multiple-teacher setting, where the algorithm can choose which subset of teachers to query for each label. Finally, we demonstrate the effectiveness of our techniques on a real-world Internet search problem."
            ],
            "keywords": [
                "online learning",
                "regret",
                "label-efficient",
                "crowdsourcing"
            ],
            "author": [
                "Ofer Dekel",
                "Claudio Gentile"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/dekel12b/dekel12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Simulation-Based Algorithm for Ergodic Control of Markov Chains Conditioned on Rare Events",
            "abstract": [
                "We study the problem of long-run average cost control of Markov chains conditioned on a rare event. In a related recent work, a simulation based algorithm for estimating performance measures associated with a Markov chain conditioned on a rare event has been developed. We extend ideas from this work and develop an adaptive algorithm for obtaining, online, optimal control policies conditioned on a rare event. Our algorithm uses three timescales or step-size schedules. On the slowest timescale, a gradient search algorithm for policy updates that is based on one-simulation simultaneous perturbation stochastic approximation (SPSA) type estimates is used. Deterministic perturbation sequences obtained from appropriate normalized Hadamard matrices are used here. The fast timescale recursions compute the conditional transition probabilities of an associated chain by obtaining solutions to the multiplicative Poisson equation (for a given policy estimate). Further, the risk parameter associated with the value function for a given policy estimate is updated on a timescale that lies in between the two scales above. We briefly sketch the convergence analysis of our algorithm and present a numerical application in the setting of routing multiple flows in communication networks."
            ],
            "keywords": [
                "Markov decision processes",
                "optimal control conditioned on a rare event",
                "simulation based algorithms",
                "SPSA with deterministic perturbations",
                "reinforcement learning"
            ],
            "author": [
                "Shalabh Bhatnagar",
                "Vivek S Borkar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/bhatnagar06a/bhatnagar06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Optimal Transport and the Rot Mover's Distance",
            "abstract": [
                "This paper presents a unified framework for smooth convex regularization of discrete optimal transport problems. In this context, the regularized optimal transport turns out to be equivalent to a matrix nearness problem with respect to Bregman divergences. Our framework thus naturally generalizes a previously proposed regularization based on the Boltzmann-Shannon entropy related to the Kullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We call the regularized optimal transport distance the rot mover's distance in reference to the classical earth mover's distance. By exploiting alternate Bregman projections, we develop the alternate scaling algorithm and non-negative alternate scaling algorithm, to compute efficiently the regularized optimal plans depending on whether the domain of the regularizer lies within the non-negative orthant or not. We further enhance the separable case with a sparse extension to deal with high data dimensions. We also instantiate our framework and discuss the inherent specificities for well-known regularizers and statistical divergences in the machine learning and information geometry communities. Finally, we demonstrate the merits of our methods with experiments using synthetic data to illustrate the effect of different regularizers, penalties and dimensions, as well as real-world data for a pattern recognition application to audio scene classification."
            ],
            "keywords": [
                "alternate projections",
                "convex analysis",
                "regularized optimal transport",
                "rot mover's distance",
                "statistical divergences"
            ],
            "author": [
                "Arnaud Dessein",
                "Nicolas Papadakis",
                "Jean-Luc Rouas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-361/17-361.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graphical Lasso and Thresholding: Equivalence and Closed-form Solutions",
            "abstract": [
                "Graphical Lasso (GL) is a popular method for learning the structure of an undirected graphical model, which is based on an l 1 regularization technique. The objective of this paper is to compare the computationally-heavy GL technique with a numerically-cheap heuristic method that is based on simply thresholding the sample covariance matrix. To this end, two notions of sign-consistent and inverse-consistent matrices are developed, and then it is shown that the thresholding and GL methods are equivalent if: (i) the thresholded sample covariance matrix is both sign-consistent and inverse-consistent, and (ii) the gap between the largest thresholded and the smallest un-thresholded entries of the sample covariance matrix is not too small. By building upon this result, it is proved that the GL methodas a conic optimization problem-has an explicit closed-form solution if the thresholded sample covariance matrix has an acyclic structure. This result is then generalized to arbitrary sparse support graphs, where a formula is found to obtain an approximate solution of GL. Furthermore, it is shown that the approximation error of the derived explicit formula decreases exponentially fast with respect to the length of the minimum-length cycle of the sparsity graph. The developed results are demonstrated on synthetic data, functional MRI data, traffic flows for transportation networks, and massive randomly generated data sets. We show that the proposed method can obtain an accurate approximation of the GL for instances with the sizes as large as 80, 000 × 80, 000 (more than 3.2 billion variables) in less than 30 minutes on a standard laptop computer running MATLAB, while other state-of-the-art methods do not converge within 4 hours."
            ],
            "keywords": [
                "Graphical Lasso",
                "Graphical Model",
                "Sparse Graphs",
                "Optimization"
            ],
            "author": [
                "Salar Fattahi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-501/17-501.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Sample Complexity of Dictionary Learning",
            "abstract": [
                "A large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary. Algorithms for various signal processing applications, including classification, denoising and signal separation, learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples? We assume signals are generated from a fixed distribution, and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefficient selection, as measured by the expected L 2 error in representation when the dictionary is used. For the case of l 1 regularized coefficient selection we provide a generalization bound of the order of O np ln(mλ)/m , where n is the dimension, p is the number of elements in the dictionary, λ is a bound on the l 1 norm of the coefficient vector and m is the number of samples, which complements existing results. For the case of representing a new signal as a combination of at most k dictionary elements, we provide a bound of the order O(np ln(mk)/m) under an assumption on the closeness to orthogonality of the dictionary (low Babel function). We further show that this assumption holds for most dictionaries in high dimensions in a strong probabilistic sense. Our results also include bounds that converge as 1/m, not previously known for this problem. We provide similar results in a general setting using kernels with weak smoothness requirements."
            ],
            "keywords": [
                "dictionary learning",
                "generalization bound",
                "sparse representation"
            ],
            "author": [
                "Daniel Vainsencher",
                "Alfred M Bruckstein"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/vainsencher11a/vainsencher11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Characteristic Kernels and Infinitely Divisible Distributions",
            "abstract": [
                "We connect shift-invariant characteristic kernels to infinitely divisible distributions on R d. Characteristic kernels play an important role in machine learning applications with their kernel means to distinguish any two probability measures. The contribution of this paper is twofold. First, we show, using the Lévy-Khintchine formula, that any shift-invariant kernel given by a bounded, continuous, and symmetric probability density function (pdf) of an infinitely divisible distribution on R d is characteristic. We mention some closure properties of such characteristic kernels under addition, pointwise product, and convolution. Second, in developing various kernel mean algorithms, it is fundamental to compute the following values: (i) kernel mean values m P (x), x ∈ X , and (ii) kernel mean RKHS inner products m P , m Q H , for probability measures P, Q. If P, Q, and kernel k are Gaussians, then the computation of (i) and (ii) results in Gaussian pdfs that are tractable. We generalize this Gaussian combination to more general cases in the class of infinitely divisible distributions. We then introduce a conjugate kernel and a convolution trick, so that the above (i) and (ii) have the same pdf form, expecting tractable computation at least in some cases. As specific instances, we explore α-stable distributions and a rich class of generalized hyperbolic distributions, where the Laplace, Cauchy, and Student's t distributions are included."
            ],
            "keywords": [
                "Characteristic Kernel",
                "Kernel Mean",
                "Infinitely Divisible Distribution",
                "Conjugate Kernel",
                "Convolution Trick"
            ],
            "author": [
                "Yu Nishiyama"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-132/14-132.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Aggregation of SVM Classifiers Using Sobolev Spaces",
            "abstract": [
                "This paper investigates statistical performances of Support Vector Machines (SVM) and considers the problem of adaptation to the margin parameter and to complexity. In particular we provide a classifier with no tuning parameter. It is a combination of SVM classifiers. Our contribution is twofold: (1) we propose learning rates for SVM using Sobolev spaces and build a numerically realizable aggregate that converges with same rate; (2) we present practical experiments of this method of aggregation for SVM using both Sobolev spaces and Gaussian kernels."
            ],
            "keywords": [
                "classification",
                "support vector machines",
                "learning rates",
                "approximation",
                "aggregation of classifiers"
            ],
            "author": [
                "Sébastien Loustau"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/loustau08a/loustau08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Universal Kernel-Based Learning with Applications to Regular Languages",
            "abstract": [
                "We propose a novel framework for supervised learning of discrete concepts. Since the 1970's, the standard computational primitive has been to find the most consistent hypothesis in a given complexity class. In contrast, in this paper we propose a new basic operation: for each pair of input instances, count how many concepts of bounded complexity contain both of them. Our approach maps instances to a Hilbert space, whose metric is induced by a universal kernel coinciding with our computational primitive, and identifies concepts with half-spaces. We prove that all concepts are linearly separable under this mapping. Hence, given a labeled sample and an oracle for evaluating the universal kernel, we can efficiently compute a linear classifier (via SVM, for example) and use margin bounds to control its generalization error. Even though exact evaluation of the universal kernel may be infeasible, in various natural situations it is efficiently approximable. Though our approach is general, our main application is to regular languages. Our approach presents a substantial departure from current learning paradigms and in particular yields a novel method for learning this fundamental concept class. Unlike existing techniques, we make no structural assumptions on the corresponding unknown automata, the string distribution or the completeness of the training set. Instead, given a labeled sample our algorithm outputs a classifier with guaranteed distribution-free generalization bounds; to our knowledge, the proposed framework is the only one capable of achieving the latter. Along the way, we touch upon several fundamental questions in complexity, automata, and machine learning."
            ],
            "keywords": [
                "grammar induction",
                "regular language",
                "finite state automaton",
                "maximum margin hyperplane",
                "kernel approximation"
            ],
            "author": [
                "Aryeh Kontorovich",
                "Paolo Frasconi",
                "Kristian Kersting",
                "Hannu Toivonen",
                "Koji Tsuda"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/kontorovich09a/kontorovich09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structure and Majority Classes in Decision Tree Learning",
            "abstract": [
                "To provide good classification accuracy on unseen examples, a decision tree, learned by an algorithm such as ID3, must have sufficient structure and also identify the correct majority class in each of its leaves. If there are inadequacies in respect of either of these, the tree will have a percentage classification rate below that of the maximum possible for the domain, namely (100-Bayes error rate). An error decomposition is introduced which enables the relative contributions of deficiencies in structure and in incorrect determination of majority class to be isolated and quantified. A sub-decomposition of majority class error permits separation of the sampling error at the leaves from the possible bias introduced by the attribute selection method of the induction algorithm. It is shown that sampling error can extend to 25% when there are more than two classes. Decompositions are obtained from experiments on several data sets. For ID3, the effect of selection bias is shown to vary from being statistically non-significant to being quite substantial, with the latter appearing to be associated with a simple underlying model."
            ],
            "keywords": [
                "decision tree learning",
                "error decomposition",
                "majority classes",
                "sampling error",
                "attribute selection bias"
            ],
            "author": [
                "Ray J Hickey"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/hickey07a/hickey07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improving CUR Matrix Decomposition and the Nyström Approximation via Adaptive Sampling",
            "abstract": [
                "The CUR matrix decomposition and the Nyström approximation are two important low-rank matrix approximation techniques. The Nyström method approximates a symmetric positive semidefinite matrix in terms of a small number of its columns, while CUR approximates an arbitrary data matrix by a small number of its columns and rows. Thus, CUR decomposition can be regarded as an extension of the Nyström approximation. In this paper we establish a more general error bound for the adaptive column/row sampling algorithm, based on which we propose more accurate CUR and Nyström algorithms with expected relative-error bounds. The proposed CUR and Nyström algorithms also have low time complexity and can avoid maintaining the whole data matrix in RAM. In addition, we give theoretical analysis for the lower error bounds of the standard Nyström method and the ensemble Nyström method. The main theoretical results established in this paper are novel, and our analysis makes no special assumption on the data matrices."
            ],
            "keywords": [
                "large-scale matrix computation",
                "CUR matrix decomposition",
                "the Nyström method",
                "randomized algorithms",
                "adaptive sampling"
            ],
            "author": [
                "Shusen Wang",
                "Zhihua Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/wang13c/wang13c.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes",
            "abstract": [
                "Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at fixed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach."
            ],
            "keywords": [
                "PCA",
                "Bayesian model selection",
                "random matrix theory",
                "high dimensional inference"
            ],
            "author": [
                "David C Hoyle"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/hoyle08a/hoyle08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Equivalence Classes of Bayesian-Network Structures",
            "abstract": [
                "Two Bayesian-network structures are said to be equivalent if the set of distributions that can be represented with one of those structures is identical to the set of distributions that can be represented with the other. Many scoring criteria that are used to learn Bayesiannetwork structures from data are score equivalent; that is, these criteria do not distinguish among networks that are equivalent. In this paper, we consider using a score equivalent criterion in conjunction with a heuristic search algorithm to perform model selection or model averaging. We argue that it is often appropriate to search among equivalence classes of network structures as opposed to the more common approach of searching among individual Bayesian-network structures. We describe a convenient graphical representation for an equivalence class of structures, and introduce a set of operators that can be applied to that representation by a search algorithm to move among equivalence classes. We show that our equivalence-class operators can be scored locally, and thus share the computational efficiency of traditional operators defined for individual structures. We show experimentally that a greedy model-selection algorithm using our representation yields slightly higherscoring structures than the traditional approach without any additional time overhead, and we argue that more sophisticated search algorithms are likely to benefit much more."
            ],
            "keywords": [],
            "author": [
                "David Maxwell Chickering"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/chickering02a/chickering02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Exchangeable Graphs and Their Limits via Graphon Processes",
            "abstract": [
                "In a recent paper, Caron and Fox suggest a probabilistic model for sparse graphs which are exchangeable when associating each vertex with a time parameter in R +. Here we show that by generalizing the classical definition of graphons as functions over probability spaces to functions over σ-finite measure spaces, we can model a large family of exchangeable graphs, including the Caron-Fox graphs and the traditional exchangeable dense graphs as special cases. Explicitly, modelling the underlying space of features by a σ-finite measure space (S, S, µ) and the connection probabilities by an integrable function W : S × S → [0, 1], we construct a random family (G t) t≥0 of growing graphs such that the vertices of G t are given by a Poisson point process on S with intensity tµ, with two points x, y of the point process connected with probability W (x, y). We call such a random family a graphon process. We prove that a graphon process has convergent subgraph frequencies (with possibly infinite limits) and that, in the natural extension of the cut metric to our setting, the sequence converges to the generating graphon. We also show that the underlying graphon is identifiable only as an equivalence class over graphons with cut distance zero. More generally, we study metric convergence for arbitrary (not necessarily random) sequences of graphs, and show that a sequence of graphs has a convergent subsequence if and only if it has a subsequence satisfying a property we call uniform regularity of tails. Finally, we prove that every graphon is equivalent to a graphon on R + equipped with Lebesgue measure."
            ],
            "keywords": [
                "graphons",
                "graph convergence",
                "sparse graph convergence",
                "modelling of sparse networks",
                "exchangeable graph models"
            ],
            "author": [
                "Christian Borgs",
                "Jennifer T Chayes",
                "Henry Cohn",
                "Nina Holden",
                "Edoardo M Airoldi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-421/16-421.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "What Regularized Auto-Encoders Learn from the Data-Generating Distribution",
            "abstract": [
                "What do auto-encoders learn about the underlying data-generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data-generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parameterization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments."
            ],
            "keywords": [
                "auto-encoders",
                "denoising auto-encoders",
                "score matching",
                "unsupervised representation learning",
                "manifold learning",
                "Markov chains",
                "generative models"
            ],
            "author": [
                "Guillaume Alain",
                "Yoshua Bengio",
                "Aaron Courville",
                "Rob Fergus",
                "Christopher Manning"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/alain14a/alain14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Guarding against Spurious Discoveries in High Dimensions",
            "abstract": [
                "Many data mining and statistical machine learning algorithms have been developed to select a subset of covariates to associate with a response variable. Spurious discoveries can easily arise in high-dimensional data analysis due to enormous possibilities of such selections. How can we know statistically our discoveries better than those by chance? In this paper, we define a measure of goodness of spurious fit, which shows how good a response variable can be fitted by an optimally selected subset of covariates under the null model, and propose a simple and effective LAMM algorithm to compute it. It coincides with the maximum spurious correlation for linear models and can be regarded as a generalized maximum spurious correlation. We derive the asymptotic distribution of such goodness of spurious fit for generalized linear models and L 1 regression. Such an asymptotic distribution depends on the sample size, ambient dimension, the number of variables used in the fit, and the covariance information. It can be consistently estimated by multiplier bootstrapping and used as a benchmark to guard against spurious discoveries. It can also be applied to model selection, which considers only candidate models with goodness of fits better than those by spurious fits. The theory and method are convincingly illustrated by simulated examples and an application to the binary outcomes from German Neuroblastoma Trials."
            ],
            "keywords": [
                "Bootstrap",
                "Gaussian approximation",
                "generalized linear models",
                "L 1 regression",
                "model selection",
                "sparsity",
                "spurious correlation",
                "spurious fit"
            ],
            "author": [
                "Jianqing Fan",
                "Wen-Xin Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-068/16-068.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Representations of Persistence Barcodes",
            "abstract": [
                "We consider the problem of supervised learning with summary representations of topological features in data. In particular, we focus on persistent homology, the prevalent tool used in topological data analysis. As the summary representations, referred to as barcodes or persistence diagrams, come in the unusual format of multi sets, equipped with computationally expensive metrics, they can not readily be processed with conventional learning techniques. While different approaches to address this problem have been proposed, either in the context of kernel-based learning, or via carefully designed vectorization techniques, it remains an open problem how to leverage advances in representation learning via deep neural networks. Appropriately handling topological summaries as input to neural networks would address the disadvantage of previous strategies which handle this type of data in a task-agnostic manner. In particular, we propose an approach that is designed to learn a task-specific representation of barcodes. In other words, we aim to learn a representation that adapts to the learning problem while, at the same time, preserving theoretical properties (such as stability). This is done by projecting barcodes into a finite dimensional vector space using a collection of parametrized functionals, so called structure elements, for which we provide a generic construction scheme. A theoretical analysis of this approach reveals sufficient conditions to preserve stability, and also shows that different choices of structure elements lead to great differences with respect to their suitability for numerical optimization. When implemented as a neural network input layer, our approach demonstrates compelling performance on various types of problems, including graph classification and eigenvalue prediction, the classification of 2D/3D object shapes and recognizing activities from EEG signals."
            ],
            "keywords": [
                "Topological data analysis",
                "persistent homology",
                "topological summary",
                "supervised learning",
                "deep learning"
            ],
            "author": [
                "Christoph D Hofer",
                "Roland Kwitt",
                "Marc Niethammer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-358/18-358.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parallelizing Stochastic Gradient Descent for Least Squares Regression: Mini-batching, Averaging, and Model Misspecification",
            "abstract": [
                "This work characterizes the benefits of averaging techniques widely used in conjunction with stochastic gradient descent (SGD). In particular, this work presents a sharp analysis of: (1) minibatching, a method of averaging many samples of a stochastic gradient to both reduce the variance of a stochastic gradient estimate and for parallelizing SGD and (2) tail-averaging, a method involving averaging the final few iterates of SGD in order to decrease the variance in SGD's final iterate. This work presents sharp finite sample generalization error bounds for these schemes for the stochastic approximation problem of least squares regression. Furthermore, this work establishes a precise problem-dependent extent to which mini-batching can be used to yield provable near-linear parallelization speedups over SGD with batch size one. This characterization is used to understand the relationship between learning rate versus batch size when considering the excess risk of the final iterate of an SGD procedure. Next, this mini-batching characterization is utilized in providing a highly parallelizable SGD method that achieves the minimax risk with nearly the same number of serial updates as batch gradient descent, improving significantly over existing SGD-style methods. Following this, a non-asymptotic excess risk bound for model averaging (which is a communication efficient parallelization scheme) is provided. Finally, this work sheds light on fundamental differences in SGD's behavior when dealing with mis-specified models in the non-realizable least squares problem. This paper shows that maximal stepsizes ensuring minimax risk for the mis-specified case must depend on the noise properties. The analysis tools used by this paper generalize the operator view of averaged SGD (Défossez and Bach, 2015) followed by developing a novel analysis in bounding these operators to characterize the generalization error. These techniques are of broader interest in analyzing various computational aspects of stochastic approximation."
            ],
            "keywords": [
                "Stochastic Gradient Descent",
                "Stochastic Approximation",
                "Least Squares Regression",
                "Parallelization",
                "Mini Batch SGD",
                "Iterate Averaging",
                "Suffix Averaging",
                "Batchsize Doubling",
                "Model Averaging",
                "Parameter Mixing",
                "Mis-specified models",
                "Heteroscedastic Noise",
                "Agnostic Learning"
            ],
            "author": [
                "Praneeth Prateek Jain",
                "PRANEETH}@MICROSOFT Netrapalli {prajain",
                "Sham M Kakade",
                "Paul G Allen",
                "Rahul Kidambi",
                "SIDFORD@STANFORD Aaron Sidford"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-595/16-595.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise",
            "abstract": [
                "We consider the matrix completion problem under a form of row/column weighted entrywise sampling, including the case of uniform entrywise sampling as a special case. We analyze the associated random observation operator, and prove that with high probability, it satisfies a form of restricted strong convexity with respect to weighted Frobenius norm. Using this property, we obtain as corollaries a number of error bounds on matrix completion in the weighted Frobenius norm under noisy sampling and for both exact and near low-rank matrices. Our results are based on measures of the \"spikiness\" and \"low-rankness\" of matrices that are less restrictive than the incoherence conditions imposed in previous work. Our technique involves an M-estimator that includes controls on both the rank and spikiness of the solution, and we establish non-asymptotic error bounds in weighted Frobenius norm for recovering matrices lying with ℓ q-\"balls\" of bounded spikiness. Using information-theoretic methods, we show that no algorithm can achieve better estimates (up to a logarithmic factor) over these same sets, showing that our conditions on matrices and associated rates are essentially optimal."
            ],
            "keywords": [
                "matrix completion",
                "collaborative filtering",
                "convex optimization"
            ],
            "author": [
                "Sahand Negahban",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/negahban12a/negahban12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Modelling Interactions in High-dimensional Data with Backtracking",
            "abstract": [
                "We study the problem of high-dimensional regression when there may be interacting variables. Approaches using sparsity-inducing penalty functions such as the Lasso can be useful for producing interpretable models. However, when the number variables runs into the thousands, and so even two-way interactions number in the millions, these methods may become computationally infeasible. Typically variable screening based on model fits using only main effects must be performed first. One problem with screening is that important variables may be missed if they are only useful for prediction when certain interaction terms are also present in the model. To tackle this issue, we introduce a new method we call Backtracking. It can be incorporated into many existing high-dimensional methods based on penalty functions, and works by building increasing sets of candidate interactions iteratively. Models fitted on the main effects and interactions selected early on in this process guide the selection of future interactions. By also making use of previous fits for computation, as well as performing calculations is parallel, the overall run-time of the algorithm can be greatly reduced. The effectiveness of our method when applied to regression and classification problems is demonstrated on simulated and real data sets. In the case of using Backtracking with the Lasso, we also give some theoretical support for our procedure."
            ],
            "keywords": [
                "high-dimensional data",
                "interactions",
                "Lasso",
                "path algorithm"
            ],
            "author": [
                "Rajen D Shah"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/13-515/13-515.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Introduction to Causal Inference",
            "abstract": [
                "The goal of many sciences is to understand the mechanisms by which variables came to take on the values they have (that is, to find a generative model), and to predict what the values of those variables would be if the naturally occurring mechanisms were subject to outside manipulations. The past 30 years has seen a number of conceptual developments that are partial solutions to the problem of causal inference from observational sample data or a mixture of observational sample and experimental data, particularly in the area of graphical causal modeling. However, in many domains, problems such as the large numbers of variables, small samples sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of these developments. The articles in the Special Topic on Causality address these and other problems in applying graphical causal modeling algorithms. This introduction to the Special Topic on Causality provides a brief introduction to graphical causal modeling, places the articles in a broader context, and describes the differences between causal inference and ordinary machine learning classification and prediction problems."
            ],
            "keywords": [
                "Bayesian networks",
                "causation",
                "causal inference"
            ],
            "author": [
                "Peter Spirtes"
            ],
            "ref": "http://www.jmlr.org/papers/volume11/spirtes10a/spirtes10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Why Does Unsupervised Pre-training Help Deep Learning?",
            "abstract": [
                "Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training."
            ],
            "keywords": [
                "deep architectures",
                "unsupervised pre-training",
                "deep belief networks",
                "stacked denoising auto-encoders",
                "non-convex optimization"
            ],
            "author": [
                "Dumitru Erhan",
                "Yoshua Bengio",
                "Aaron Courville",
                "Pierre-Antoine Manzagol",
                "Pascal Vincent",
                "Samy Bengio"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/erhan10a/erhan10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "True Online Temporal-Difference Learning Harm van Seijen † ‡",
            "abstract": [
                "The temporal-difference methods TD(λ) and Sarsa(λ) form a core part of modern reinforcement learning. Their appeal comes from their good performance, low computational cost, and their simple interpretation, given by their forward view. Recently, new versions of these methods were introduced, called true online TD(λ) and true online Sarsa(λ), respectively (van Seijen & Sutton, 2014). Algorithmically, these true online methods only make two small changes to the update rules of the regular methods, and the extra computational cost is negligible in most cases. However, they follow the ideas underlying the forward view much more closely. In particular, they maintain an exact equivalence with the forward view at all times, whereas the traditional versions only approximate it for small step-sizes. We hypothesize that these true online methods not only have better theoretical properties, but also dominate the regular methods empirically. In this article, we put this hypothesis to the test by performing an extensive empirical comparison. Specifically, we compare the performance of true online TD(λ)/Sarsa(λ) with regular TD(λ)/Sarsa(λ) on random MRPs, a real-world myoelectric prosthetic arm, and a domain from the Arcade Learning Environment. We use linear function approximation with tabular, binary, and non-binary features. Our results suggest that the true online methods indeed dominate the regular methods. Across all domains/representations the learning speed of the true online methods are often better, but never worse than that of the regular methods. An additional advantage is that no choice between traces has to be made for the true online methods. Besides the empirical results, we provide an in-dept analysis of the theory behind true online temporal-difference learning. In addition, we show that new true online temporal-difference methods can be derived by making changes to the online forward view and then rewriting the update equations."
            ],
            "keywords": [
                "temporal-difference learning",
                "eligibility traces",
                "forward-view equivalence"
            ],
            "author": [
                "Patrick M Pilarski",
                "Marlos C Machado",
                "Richard S Sutton",
                "Patrick M Rupam Mahmood",
                "Marlos C Pilarski",
                "Richard S Sutton Machado",
                "Mahmood, Pilarski, Machado Sutton Van Seijen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-599/15-599.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On p -Support Vector Machines and Multidimensional Kernels",
            "abstract": [
                "In this paper, we extend the methodology developed for Support Vector Machines (SVM) using the 2-norm (2-SVM) to the more general case of p-norms with p > 1 (p-SVM). We derive second order cone formulations for the resulting dual and primal problems. The concept of kernel function, widely applied in 2-SVM, is extended to the more general case of p-norms with p > 1 by defining a new operator called multidimensional kernel. This object gives rise to reformulations of dual problems, in a transformed space of the original data, where the dependence on the original data always appear as homogeneous polynomials. We adapt known solution algorithms to efficiently solve the primal and dual resulting problems and some computational experiments on real-world datasets are presented showing rather good behavior in terms of the accuracy of p-SVM with p > 1."
            ],
            "keywords": [
                "Support Vector Machines",
                "Kernel functions",
                "p -norms",
                "Mathematical Optimization"
            ],
            "author": [
                "Víctor Blanco",
                "Antonio M Rodríguez-Chía"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-601/18-601.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression",
            "abstract": [
                "To model categorical response variables given their covariates, we propose a permuted and augmented stick-breaking (paSB) construction that one-to-one maps the observed categories to randomly permuted latent sticks. This new construction transforms multinomial regression into regression analysis of stick-specific binary random variables that are mutually independent given their covariate-dependent stick success probabilities, which are parameterized by the regression coefficients of their corresponding categories. The paSB construction allows transforming an arbitrary cross-entropy-loss binary classifier into a Bayesian multinomial one. Specifically, we parameterize the negative logarithms of the stick failure probabilities with a family of covariate-dependent softplus functions to construct nonparametric Bayesian multinomial softplus regression, and transform Bayesian support vector machine (SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression models are not only capable of providing probability estimates, quantifying uncertainty, increasing robustness, and producing nonlinear classification decision boundaries, but also amenable to posterior simulation. Example results demonstrate their attractive properties and performance."
            ],
            "keywords": [
                "Discrete choice models",
                "logistic regression",
                "nonlinear classification",
                "softplus regression",
                "support vector machines"
            ],
            "author": [
                "Quan Zhang",
                "Mingyuan Zhou",
                "David M Blei"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-409/17-409.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": null,
            "abstract": [
                "Multiple kernel learning (MKL) aims at simultaneously learning a kernel and the associated predictor in supervised learning settings. For the support vector machine, an efficient and general multiple kernel learning algorithm, based on semi-infinite linear programming, has been recently proposed. This approach has opened new perspectives since it makes MKL tractable for large-scale problems, by iteratively using existing support vector machine code. However, it turns out that this iterative algorithm needs numerous iterations for converging towards a reasonable solution. In this paper, we address the MKL problem through a weighted 2-norm regularization formulation with an additional constraint on the weights that encourages sparse kernel combinations. Apart from learning the combination, we solve a standard SVM optimization problem, where the kernel is defined as a linear combination of multiple kernels. We propose an algorithm, named SimpleMKL, for solving this MKL problem and provide a new insight on MKL algorithms based on mixed-norm regularization by showing that the two approaches are equivalent. We show how SimpleMKL can be applied beyond binary classification, for problems like regression, clustering (one-class classification) or multiclass classification. Experimental results show that the proposed algorithm converges rapidly and that its efficiency compares favorably to other MKL algorithms. Finally, we illustrate the usefulness of MKL for some regressors based on wavelet kernels and on some model selection problems related to multiclass classification problems."
            ],
            "keywords": [
                "multiple kernel learning",
                "support vector machines",
                "support vector regression",
                "multiclass SVM",
                "gradient descent"
            ],
            "author": [
                "Francis R Bach",
                "Yves Grandvalet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/rakotomamonjy08a/rakotomamonjy08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Delay and Cooperation in Nonstochastic Bandits",
            "abstract": [
                "We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than d hops to arrive, where d is a delay parameter. We introduce Exp3-Coop, a cooperative version of the Exp3 algorithm and prove that with K actions and N agents the average per-agent regret after T rounds is at most of order d + 1 + K N α ≤d (T ln K), where α ≤d is the independence number of the d-th power of the communication graph G. We then show that for any connected graph, for d = √ K the regret bound is K 1/4 √ T , strictly better than the minimax regret √ KT for noncooperating agents. More informed choices of d lead to bounds which are arbitrarily close to the full information minimax regret √ T ln K when G is dense. When G has sparse components, we show that a variant of Exp3-Coop, allowing agents to choose their parameters according to their centrality in G, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay."
            ],
            "keywords": [
                "Multi-armed bandits",
                "distributed learning",
                "cooperative multi-agent systems",
                "regret minimization",
                "LOCAL communication"
            ],
            "author": [
                "Nicolò Cesa-Bianchi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-631/17-631.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Extension of Slow Feature Analysis for Nonlinear Blind Source Separation",
            "abstract": [
                "We present and test an extension of slow feature analysis as a novel approach to nonlinear blind source separation. The algorithm relies on temporal correlations and iteratively reconstructs a set of statistically independent sources from arbitrary nonlinear instantaneous mixtures. Simulations show that it is able to invert a complicated nonlinear mixture of two audio signals with a high reliability. The algorithm is based on a mathematical analysis of slow feature analysis for the case of input data that are generated from statistically independent sources."
            ],
            "keywords": [
                "slow feature analysis",
                "nonlinear blind source separation",
                "statistical independence",
                "independent component analysis",
                "slowness principle"
            ],
            "author": [
                "Henning Sprekeler",
                "Tiziano Zito",
                "Laurenz Wiskott"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/sprekeler14a/sprekeler14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Parts-Based Representations of Data",
            "abstract": [
                "Many perceptual models and theories hinge on treating objects as a collection of constituent parts. When applying these approaches to data, a fundamental problem arises: how can we determine what are the parts? We attack this problem using learning, proposing a form of generative latent factor model, in which each data dimension is allowed to select a different factor or part as its explanation. This approach permits a range of variations that posit different models for the appearance of a part. Here we provide the details for two such models: a discrete and a continuous one. Further, we show that this latent factor model can be extended hierarchically to account for correlations between the appearances of different parts. This permits modeling of data consisting of multiple categories, and learning these categories simultaneously with the parts when they are unobserved. Experiments demonstrate the ability to learn parts-based representations, and categories, of facial images and user-preference data."
            ],
            "keywords": [
                "parts",
                "unsupervised learning",
                "latent factor models",
                "collaborative filtering",
                "hierarchical learning"
            ],
            "author": [
                "David A Ross",
                "Richard S Zemel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/ross06a/ross06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Estimation and Testing for High-Dimensional Multi-Block Vector-Autoregressive Models",
            "abstract": [
                "Dynamical systems comprising of multiple components that can be partitioned into distinct blocks originate in many scientific areas. A pertinent example is the interactions between financial assets and selected macroeconomic indicators, which has been studied at aggregate level-e.g. a stock index and an employment index-extensively in the macroeconomics literature. A key shortcoming of this approach is that it ignores potential influences from other related components (e.g. Gross Domestic Product) that may impact the system's dynamics and structure and thus produces incorrect results. To mitigate this issue, we consider a multi-block linear dynamical system with Granger-causal ordering between blocks, wherein the blocks' temporal dynamics are described by vector autoregressive processes and are influenced by blocks higher in the system hierarchy. We derive the maximum likelihood estimator for the posited model for Gaussian data in the high-dimensional setting based on appropriate regularization schemes for the parameters of the block components. To optimize the underlying non-convex likelihood function, we develop an iterative algorithm with convergence guarantees. We establish theoretical properties of the maximum likelihood estimates, leveraging the decomposability of the regularizers and a careful analysis of the iterates. Finally, we develop testing procedures for the null hypothesis of whether a block \"Granger-causes\" another block of variables. The performance of the model and the testing procedures are evaluated on synthetic data, and illustrated on a data set involving log-returns of the US S&P100 component stocks and key macroeconomic variables for the 2001-16 period."
            ],
            "keywords": [
                "Vector-autoregression",
                "Stability",
                "Block-coordinate descent",
                "Consistency",
                "Global testing"
            ],
            "author": [
                "Jiahe Lin",
                "George Michailidis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-055/17-055.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiclass Boosting: Margins, Codewords, Losses, and Algorithms",
            "abstract": [
                "The problem of multiclass boosting is considered. A new formulation is presented, combining multi-dimensional predictors, multi-dimensional real-valued codewords, and proper multiclass margin loss functions. This leads to a number of contributions, such as maximum capacity codeword sets, a family of proper and margin enforcing losses, denoted as γ − φ losses, and two new multiclass boosting algorithms. These are descent procedures on the functional space spanned by a set of weak learners. The first, CD-MCBoost, is a coordinate descent procedure that updates one predictor component at a time. The second, GD-MCBoost, a gradient descent procedure that updates all components jointly. Both MCBoost algorithms are defined with respect to a γ − φ loss and can reduce to classical boosting procedures (such as AdaBoost and LogitBoost) for binary problems. Beyond the algorithms themselves, the proposed formulation enables a unified treatment of many previous multiclass boosting algorithms. This is used to show that the latter implement different combinations of optimization strategy, codewords, weak learners, and loss function, highlighting some of their deficiencies. It is shown that no previous method matches the support of MCBoost for real codewords of maximum capacity, a proper margin-enforcing loss function, and any family of multidimensional predictors and weak learners. Experimental results confirm the superiority of MCBoost, showing that the two proposed MCBoost algorithms outperform comparable prior methods on a number of datasets."
            ],
            "keywords": [
                "Boosting",
                "Multiclass Boosting",
                "Multiclass Classification",
                "Margin Maximization",
                "Loss Function"
            ],
            "author": [
                "Mohammad Saberian",
                "Nuno Vasconcelos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-137/17-137.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Document Neural Autoregressive Distribution Estimation",
            "abstract": [
                "We present an approach based on feed-forward neural networks for learning the distribution over textual documents. This approach is inspired by the Neural Autoregressive Distribution Estimator (NADE) model which has been shown to be a good estimator of the distribution over discretevalued high-dimensional vectors. In this paper, we present how NADE can successfully be adapted to textual data, retaining the property that sampling or computing the probability of an observation can be done exactly and efficiently. The approach can also be used to learn deep representations of documents that are competitive to those learned by alternative topic modeling approaches. Finally, we describe how the approach can be combined with a regular neural network N-gram model and substantially improve its performance, by making its learned representation sensitive to the larger, document-level context."
            ],
            "keywords": [
                "Neural networks",
                "Deep learning",
                "Topic models",
                "Language models",
                "Autoregressive models"
            ],
            "author": [
                "Stanislas Lauly",
                "Hugo Larochelle"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-017/16-017.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning When Concepts Abound",
            "abstract": [
                "Many learning tasks, such as large-scale text categorization and word prediction, can benefit from efficient training and classification when the number of classes, in addition to instances and features, is large, that is, in the thousands and beyond. We investigate the learning of sparse class indices to address this challenge. An index is a mapping from features to classes. We compare the index-learning methods against other techniques, including one-versus-rest and top-down classification using perceptrons and support vector machines. We find that index learning is highly advantageous for space and time efficiency, at both training and classification times. Moreover, this approach yields similar and at times better accuracies. On problems with hundreds of thousands of instances and thousands of classes, the index is learned in minutes, while other methods can take hours or days. As we explain, the design of the learning update enables conveniently constraining each feature to connect to a small subset of the classes in the index. This constraint is crucial for scalability. Given an instance with l active (positive-valued) features, each feature on average connecting to d classes in the index (in the order of 10s in our experiments), update and classification take O(dl log(dl))."
            ],
            "keywords": [
                "index learning",
                "many-class learning",
                "multiclass learning",
                "online learning",
                "text categorization"
            ],
            "author": [
                "Omid Madani",
                "Michael Connor",
                "Wiley Greiner"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/madani09a/madani09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Density Estimation for Dynamical Systems",
            "abstract": [
                "We study the density estimation problem with observations generated by certain dynamical systems that admit a unique underlying invariant Lebesgue density. Observations drawn from dynamical systems are not independent and moreover, usual mixing concepts may not be appropriate for measuring the dependence among these observations. By employing the C-mixing concept to measure the dependence, we conduct statistical analysis on the consistency and convergence of the kernel density estimator. Our main results are as follows: First, we show that with properly chosen bandwidth, the kernel density estimator is universally consistent under L 1-norm; Second, we establish convergence rates for the estimator with respect to several classes of dynamical systems under L 1-norm. In the analysis, the density function f is only assumed to be Hölder continuous or pointwise Hölder controllable which is a weak assumption in the literature of nonparametric density estimation and also more realistic in the dynamical system context. Last but not least, we prove that the same convergence rates of the estimator under L ∞-norm and L 1-norm can be achieved when the density function is Hölder continuous, compactly supported, and bounded. The bandwidth selection problem of the kernel density estimator for dynamical system is also discussed in our study via numerical simulations."
            ],
            "keywords": [
                "Kernel density estimation",
                "dynamical system",
                "dependent observations",
                "Cmixing process",
                "universal consistency",
                "convergence rates",
                "covering number",
                "learning theory Hang",
                "Steinwart",
                "Feng",
                "Suykens"
            ],
            "author": [
                "Hanyuan Hang",
                "Ingo Steinwart",
                "Johan A K Suykens"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-349/16-349.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graph Kernels",
            "abstract": [
                "We present a unified framework to study graph kernels, special cases of which include the random walk (Gärtner et al., 2003; Borgwardt et al., 2005) and marginalized (Kashima et al., 2003, 2004; Mahé et al., 2004) graph kernels. Through reduction to a Sylvester equation we improve the time complexity of kernel computation between unlabeled graphs with n vertices from O(n 6) to O(n 3). We find a spectral decomposition approach even more efficient when computing entire kernel matrices. For labeled graphs we develop conjugate gradient and fixed-point methods that take O(dn 3) time per iteration, where d is the size of the label set. By extending the necessary linear algebra to Reproducing Kernel Hilbert Spaces (RKHS) we obtain the same result for d-dimensional edge kernels, and O(n 4) in the infinite-dimensional case; on sparse graphs these algorithms only take O(n 2) time per iteration in all cases. Experiments on graphs from bioinformatics and other application domains show that these techniques can speed up computation of the kernel by an order of magnitude or more. We also show that certain rational kernels (Cortes et al., 2002, 2003, 2004) when specialized to graphs reduce to our random walk graph kernel. Finally, we relate our framework to R-convolution kernels (Haussler, 1999) and provide a kernel that is close to the optimal assignment kernel of Fröhlich et al. (2006) yet provably positive semi-definite."
            ],
            "keywords": [
                "linear algebra in RKHS",
                "Sylvester equations",
                "spectral decomposition",
                "bioinformatics",
                "rational kernels",
                "transducers",
                "semirings",
                "random walks"
            ],
            "author": [
                "S V N Vishwanathan",
                "Nicol N Schraudolph",
                "Karsten M Borgwardt"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/vishwanathan10a/vishwanathan10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Second-Order Non-Stationary Online Learning for Regression",
            "abstract": [
                "The goal of a learner in standard online learning, is to have the cumulative loss not much larger compared with the best-performing function from some fixed class. Numerous algorithms were shown to have this gap arbitrarily close to zero, compared with the best function that is chosen off-line. Nevertheless, many real-world applications, such as adaptive filtering, are non-stationary in nature, and the best prediction function may drift over time. We introduce two novel algorithms for online regression, designed to work well in non-stationary environment. Our first algorithm performs adaptive resets to forget the history, while the second is last-step min-max optimal in context of a drift. We analyze both algorithms in the worst-case regret framework and show that they maintain an average loss close to that of the best slowly changing sequence of linear functions, as long as the cumulative drift is sublinear. In addition, in the stationary case, when no drift occurs, our algorithms suffer logarithmic regret, as for previous algorithms. Our bounds improve over existing ones, and simulations demonstrate the usefulness of these algorithms compared with other state-of-the-art approaches."
            ],
            "keywords": [
                "online learning",
                "regret bounds",
                "non-stationary input"
            ],
            "author": [
                "Edward Moroshko",
                "Nina Vaits"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/moroshko15a/moroshko15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Probabilistic Low-Rank Matrix Completion from Quantized Measurements",
            "abstract": [
                "We consider the recovery of a low rank real-valued matrix M given a subset of noisy discrete (or quantized) measurements. Such problems arise in several applications such as collaborative filtering, learning and content analytics, and sensor network localization. We consider constrained maximum likelihood estimation of M , under a constraint on the entrywise infinity-norm of M and an exact rank constraint. We provide upper bounds on the Frobenius norm of matrix estimation error under this model. Previous theoretical investigations have focused on binary (1-bit) quantizers, and been based on convex relaxation of the rank. Compared to the existing binary results, our performance upper bound has faster convergence rate with matrix dimensions when the fraction of revealed observations is fixed. We also propose a globally convergent optimization algorithm based on low rank factorization of M and validate the method on synthetic and real data, with improved performance over previous methods."
            ],
            "keywords": [
                "constrained maximum likelihood",
                "quantization",
                "matrix completion",
                "collaborative filtering",
                "convex optimization"
            ],
            "author": [
                "Sonia A Bhaskar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-273/15-273.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Round Robin Classification",
            "abstract": [
                "In this paper, we discuss round robin classification (aka pairwise classification), a technique for handling multi-class problems with binary classifiers by learning one classifier for each pair of classes. We present an empirical evaluation of the method, implemented as a wrapper around the Ripper rule learning algorithm, on 20 multi-class datasets from the UCI database repository. Our results show that the technique is very likely to improve Ripper's classification accuracy without having a high risk of decreasing it. More importantly, we give a general theoretical analysis of the complexity of the approach and show that its run-time complexity is below that of the commonly used one-against-all technique. These theoretical results are not restricted to rule learning but are also of interest to other communities where pairwise classification has recently received some attention. Furthermore, we investigate its properties as a general ensemble technique and show that round robin classification with C5.0 may improve C5.0's performance on multi-class problems. However, this improvement does not reach the performance increase of boosting, and a combination of boosting and round robin classification does not produce any gain over conventional boosting. Finally, we show that the performance of round robin classification can be further improved by a straightforward integration with bagging."
            ],
            "keywords": [
                "pairwise classification",
                "inductive rule learning",
                "multi-class problems",
                "class binarization",
                "ensemble techniques"
            ],
            "author": [
                "Johannes Fürnkranz"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/fuernkranz02a/fuernkranz02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graph Laplacians and their Convergence on Random Neighborhood Graphs",
            "abstract": [
                "Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator."
            ],
            "keywords": [
                "graphs",
                "graph Laplacians",
                "semi-supervised learning",
                "spectral clustering",
                "dimensionality reduction"
            ],
            "author": [
                "Matthias Hein",
                "Jean-Yves Audibert",
                "Ulrike Von Luxburg"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/hein07a/hein07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Learning with Sample Path Constraints",
            "abstract": [
                "We study online learning where a decision maker interacts with Nature with the objective of maximizing her long-term average reward subject to some sample path average constraints. We define the reward-in-hindsight as the highest reward the decision maker could have achieved, while satisfying the constraints, had she known Nature's choices in advance. We show that in general the reward-in-hindsight is not attainable. The convex hull of the reward-in-hindsight function is, however, attainable. For the important case of a single constraint, the convex hull turns out to be the highest attainable function. Using a calibrated forecasting rule, we provide an explicit strategy that attains this convex hull. We also measure the performance of heuristic methods based on non-calibrated forecasters in experiments involving a CPU power management problem."
            ],
            "keywords": [
                "online learning",
                "calibration",
                "regret minimization",
                "approachability"
            ],
            "author": [
                "Shie Mannor",
                "@ Mcgill",
                "John N Tsitsiklis",
                "Jia Yuan",
                "Yu Jia Yu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/mannor09a/mannor09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Bayesian Learning with Stochastic Natural Gradient Expectation Propagation and the Posterior Server",
            "abstract": [
                "This paper makes two contributions to Bayesian machine learning algorithms. Firstly, we propose stochastic natural gradient expectation propagation (SNEP), a novel alternative to expectation propagation (EP), a popular variational inference algorithm. SNEP is a black box variational algorithm, in that it does not require any simplifying assumptions on the distribution of interest, beyond the existence of some Monte Carlo sampler for estimating the moments of the EP tilted distributions. Further, as opposed to EP which has no guarantee of convergence, SNEP can be shown to be convergent, even when using Monte Carlo moment estimates. Secondly, we propose a novel architecture for distributed Bayesian learning which we call the posterior server. The posterior server allows scalable and robust Bayesian learning in cases where a data set is stored in a distributed manner across a cluster, with each compute node containing a disjoint subset of data. An independent Monte Carlo sampler is run on each compute node, with direct access only to the local data subset, but which targets an approximation to the global posterior distribution given all data across the whole cluster. This is achieved by using a distributed asynchronous implementation of SNEP to pass messages across the cluster. We demonstrate SNEP and the posterior server on distributed Bayesian learning of logistic regression and neural networks."
            ],
            "keywords": [
                "Distributed Learning",
                "Large Scale Learning",
                "Deep Learning",
                "Bayesian Learning",
                "Variational Inference",
                "Expectation Propagation",
                "Stochastic Approximation",
                "Natural Gradient",
                "Markov chain Monte Carlo",
                "Parameter Server",
                "Posterior Server"
            ],
            "author": [
                "Leonard Hasenclever",
                "Stefan Webb",
                "Thibaut Lienart",
                "Sebastian Vollmer",
                "Balaji Lakshminarayanan",
                "Charles Blundell",
                "Yee Whye Teh",
                "Yee Whye"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-478/16-478.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stabilized Sparse Online Learning for Sparse Data",
            "abstract": [
                "Stochastic gradient descent (SGD) is commonly used for optimization in large-scale machine learning problems. Langford et al. (2009) introduce a sparse online learning method to induce sparsity via truncated gradient. With high-dimensional sparse data, however, this method suffers from slow convergence and high variance due to heterogeneity in feature sparsity. To mitigate this issue, we introduce a stabilized truncated stochastic gradient descent algorithm. We employ a soft-thresholding scheme on the weight vector where the imposed shrinkage is adaptive to the amount of information available in each feature. The variability in the resulted sparse weight vector is further controlled by stability selection integrated with the informative truncation. To facilitate better convergence, we adopt an annealing strategy on the truncation rate, which leads to a balanced trade-off between exploration and exploitation in learning a sparse weight vector. Numerical experiments show that our algorithm compares favorably with the original truncated gradient SGD in terms of prediction accuracy, achieving both better sparsity and stability."
            ],
            "keywords": [
                "sparse online learning",
                "sparse features",
                "truncated gradient",
                "stability selection",
                "adaptive shrinkage"
            ],
            "author": [
                "Yuting Ma"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-190/16-190.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-Parametric Estimation of Topic Hierarchies from Texts with Hierarchical Dirichlet Processes",
            "abstract": [
                "This paper presents hHDP, a hierarchical algorithm for representing a document collection as a hierarchy of latent topics, based on Dirichlet process priors. The hierarchical nature of the algorithm refers to the Bayesian hierarchy that it comprises, as well as to the hierarchy of the latent topics. hHDP relies on nonparametric Bayesian priors and it is able to infer a hierarchy of topics, without making any assumption about the depth of the learned hierarchy and the branching factor at each level. We evaluate the proposed method on real-world data sets in document modeling, as well as in ontology learning, and provide qualitative and quantitative evaluation results, showing that the model is robust, it models accurately the training data set and is able to generalize on held-out data."
            ],
            "keywords": [
                "hierarchical Dirichlet processes",
                "probabilistic topic models",
                "topic distributions",
                "ontology learning from text",
                "topic hierarchy"
            ],
            "author": [
                "Elias Zavitsanos",
                "George A Vouros",
                "Georgev @ Aegean"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/zavitsanos11a/zavitsanos11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Nonparametric Covariance Regression",
            "abstract": [
                "Capturing predictor-dependent correlations amongst the elements of a multivariate response vector is fundamental to numerous applied domains, including neuroscience, epidemiology, and finance. Although there is a rich literature on methods for allowing the variance in a univariate regression model to vary with predictors, relatively little has been done in the multivariate case. As a motivating example, we consider the Google Flu Trends data set, which provides indirect measurements of influenza incidence at a large set of locations over time (our predictor). To accurately characterize temporally evolving influenza incidence across regions, it is important to develop statistical methods for a time-varying covariance matrix. Importantly, the locations provide a redundant set of measurements and do not yield a sparse nor static spatial dependence structure. We propose to reduce dimensionality and induce a flexible Bayesian nonparametric covariance regression model by relating these location-specific trajectories to a lower-dimensional subspace through a latent factor model with predictor-dependent factor loadings. These loadings are in terms of a collection of basis functions that vary nonparametrically over the predictor space. Such low-rank approximations are in contrast to sparse precision assumptions, and are appropriate in a wide range of applications. Our formulation aims to address three challenges: scaling to large p domains, coping with missing values, and allowing an irregular grid of observations. The model is shown to be highly flexible, while leading to a computationally feasible implementation via Gibbs sampling. The ability to scale to large p domains and cope with missing values is fundamental in analyzing the Google Flu Trends data."
            ],
            "keywords": [
                "covariance regression",
                "dictionary learning",
                "Gaussian process",
                "latent factor model",
                "nonparametric Bayes",
                "time series"
            ],
            "author": [
                "Emily B Fox",
                "David B Dunson",
                "Edoardo M Airoldi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/fox15a/fox15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Complexity of Equivalence and Learning for Multiplicity Tree Automata",
            "abstract": [
                "We consider the query and computational complexity of learning multiplicity tree automata in Angluin's exact learning model. In this model, there is an oracle, called the Teacher, that can answer membership and equivalence queries posed by the Learner. Motivated by this feature, we first characterise the complexity of the equivalence problem for multiplicity tree automata, showing that it is logspace equivalent to polynomial identity testing. We then move to query complexity, deriving lower bounds on the number of queries needed to learn multiplicity tree automata over both fixed and arbitrary fields. In the latter case, the bound is linear in the size of the target automaton. The best known upper bound on the query complexity over arbitrary fields derives from an algorithm of Habrard and Oncina (2006), in which the number of queries is proportional to the size of the target automaton and the size of a largest counterexample, represented as a tree, that is returned by the Teacher. However, a smallest counterexample tree may already be exponential in the size of the target automaton. Thus the above algorithm has query complexity exponentially larger than our lower bound, and does not run in time polynomial in the size of the target automaton. We give a new learning algorithm for multiplicity tree automata in which counterexamples to equivalence queries are represented as DAGs. The query complexity of this algorithm is quadratic in the target automaton size and linear in the size of a largest counterexample. In particular, if the Teacher always returns DAG counterexamples of minimal size then the query complexity is quadratic in the target automaton size-almost matching the lower bound, and improving the best previously-known algorithm by an exponential factor."
            ],
            "keywords": [
                "exact learning",
                "query complexity",
                "multiplicity tree automata",
                "Hankel matrices",
                "DAG representations of trees",
                "polynomial identity testing"
            ],
            "author": [
                "Ines Marušić",
                "James Worrell"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/marusic15a/marusic15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Bayes Risk Lower Bounds",
            "abstract": [
                "This paper provides a general technique for lower bounding the Bayes risk of statistical estimation, applicable to arbitrary loss functions and arbitrary prior distributions. A lower bound on the Bayes risk not only serves as a lower bound on the minimax risk, but also characterizes the fundamental limit of any estimator given the prior knowledge. Our bounds are based on the notion of f-informativity (Csiszár, 1972), which is a function of the underlying class of probability measures and the prior. Application of our bounds requires upper bounds on the f-informativity, thus we derive new upper bounds on f-informativity which often lead to tight Bayes risk lower bounds. Our technique leads to generalizations of a variety of classical minimax bounds (e.g., generalized Fano's inequality). Our Bayes risk lower bounds can be directly applied to several concrete estimation problems, including Gaussian location models, generalized linear models, and principal component analysis for spiked covariance models. To further demonstrate the applications of our Bayes risk lower bounds to machine learning problems, we present two new theoretical results: (1) a precise characterization of the minimax risk of learning spherical Gaussian mixture models under the smoothed analysis framework, and (2) lower bounds for the Bayes risk under a natural prior for both the prediction and estimation errors for high-dimensional sparse linear regression under an improper learning setting."
            ],
            "keywords": [
                "Bayes risk",
                "Minimax risk",
                "f -divergence",
                "f -informativity",
                "Fano's inequality",
                "Smoothed analysis"
            ],
            "author": [
                "Xi Chen",
                "Yuchen Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-185/16-185.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2",
            "abstract": [
                "In this paper, the naive credal classifier, which is a set-valued counterpart of naive Bayes, is extended to a general and flexible treatment of incomplete data, yielding a new classifier called naive credal classifier 2 (NCC2). The new classifier delivers classifications that are reliable even in the presence of small sample sizes and missing values. Extensive empirical evaluations show that, by issuing set-valued classifications, NCC2 is able to isolate and properly deal with instances that are hard to classify (on which naive Bayes accuracy drops considerably), and to perform as well as naive Bayes on the other instances. The experiments point to a general problem: they show that with missing values, empirical evaluations may not reliably estimate the accuracy of a traditional classifier, such as naive Bayes. This phenomenon adds even more value to the robust approach to classification implemented by NCC2."
            ],
            "keywords": [
                "naive Bayes",
                "naive credal classifier",
                "imprecise probabilities",
                "missing values",
                "conservative inference rule",
                "missing at random"
            ],
            "author": [
                "Giorgio Corani",
                "Marco Zaffalon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/corani08a/corani08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Transfer Learning via Inter-Task Mappings for Temporal Difference Learning",
            "abstract": [
                "Temporal difference (TD) learning (Sutton and Barto, 1998) has become a popular reinforcement learning technique in recent years. TD methods, relying on function approximators to generalize learning to novel situations, have had some experimental successes and have been shown to exhibit some desirable properties in theory, but the most basic algorithms have often been found slow in practice. This empirical result has motivated the development of many methods that speed up reinforcement learning by modifying a task for the learner or helping the learner better generalize to novel situations. This article focuses on generalizing across tasks, thereby speeding up learning, via a novel form of transfer using handcoded task relationships. We compare learning on a complex task with three function approximators, a cerebellar model arithmetic computer (CMAC), an artificial neural network (ANN), and a radial basis function (RBF), and empirically demonstrate that directly transferring the action-value function can lead to a dramatic speedup in learning with all three. Using transfer via inter-task mapping (TVITM), agents are able to learn one task and then markedly reduce the time it takes to learn a more complex task. Our algorithms are fully implemented and tested in the RoboCup soccer Keepaway domain. This article contains and extends material published in two conference papers (Taylor and Stone, 2005; Taylor et al., 2005)."
            ],
            "keywords": [
                "transfer learning",
                "reinforcement learning",
                "temporal difference methods",
                "value function approximation",
                "inter-task mapping"
            ],
            "author": [
                "Matthew E Taylor",
                "Peter Stone",
                "Yaxin Liu",
                "Michael L Littman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/taylor07a/taylor07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classes of Kernels for Machine Learning: A Statistics Perspective",
            "abstract": [
                "In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernelbased methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity."
            ],
            "keywords": [
                "Anisotropic",
                "Compactly Supported",
                "Covariance",
                "Isotropic",
                "Locally Stationary",
                "Nonstationary",
                "Reducible",
                "Separable",
                "Stationary"
            ],
            "author": [
                "Marc G Genton",
                "Nello Cristianini",
                "John Shawe-Taylor",
                "Robert Williamson"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/genton01a/genton01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Nondeterministic Classifiers",
            "abstract": [
                "Nondeterministic classifiers are defined as those allowed to predict more than one class for some entries from an input space. Given that the true class should be included in predictions and the number of classes predicted should be as small as possible, these kind of classifiers can be considered as Information Retrieval (IR) procedures. In this paper, we propose a family of IR loss functions to measure the performance of nondeterministic learners. After discussing such measures, we derive an algorithm for learning optimal nondeterministic hypotheses. Given an entry from the input space, the algorithm requires the posterior probabilities to compute the subset of classes with the lowest expected loss. From a general point of view, nondeterministic classifiers provide an improvement in the proportion of predictions that include the true class compared to their deterministic counterparts; the price to be paid for this increase is usually a tiny proportion of predictions with more than one class. The paper includes an extensive experimental study using three deterministic learners to estimate posterior probabilities: a multiclass Support Vector Machine (SVM), a Logistic Regression, and a Naïve Bayes. The data sets considered comprise both UCI multi-class learning tasks and microarray expressions of different kinds of cancer. We successfully compare nondeterministic classifiers with other alternative approaches. Additionally, we shall see how the quality of posterior probabilities (measured by the Brier score) determines the goodness of nondeterministic predictions."
            ],
            "keywords": [
                "nondeterministic",
                "multiclassification",
                "reject option",
                "multi-label classification",
                "posterior probabilities"
            ],
            "author": [
                "Juan José Del Coz",
                "Jorge Díez",
                "Antonio Bahamonde"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/delcoz09a/delcoz09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Union of Low-Rank Tensor Spaces: Clustering and Completion",
            "abstract": [
                "We consider the problem of clustering and completing a set of tensors with missing data that are drawn from a union of low-rank tensor spaces. In the clustering problem, given a partially sampled tensor data that is composed of a number of subtensors, each chosen from one of a certain number of unknown tensor spaces, we need to group the subtensors that belong to the same tensor space. We provide a geometrical analysis on the sampling pattern and subsequently derive the sampling rate that guarantees the correct clustering under some assumptions with high probability. Moreover, we investigate the fundamental conditions for finite/unique completability for the union of tensor spaces completion problem. Both deterministic and probabilistic conditions on the sampling pattern to ensure finite/unique completability are obtained. For both the clustering and completion problems, our tensor analysis provides significantly better bound than the bound given by the matrix analysis applied to any unfolding of the tensor data."
            ],
            "keywords": [
                "Low-rank tensor completion",
                "canonical polyadic (CP) decomposition",
                "union of tensor spaces",
                "clustering tensor spaces",
                "finite completability",
                "unique completability"
            ],
            "author": [
                "Morteza Ashraphijuo",
                "Xiaodong Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-347/19-347.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Practical Locally Private Heavy Hitters",
            "abstract": [
                "We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error and running time-TreeHist and Bitstogram. In both algorithms, server running time isÕ(n) and user running time isÕ(1), hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring O(n 5/2) server time and O(n 3/2) user time. With a typically large number of participants in local algorithms (n in the millions), this reduction in time complexity, in particular at the user side, is crucial for making locally private heavy hitters algorithms usable in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code."
            ],
            "keywords": [
                "Differential privacy",
                "local differential privacy",
                "heavy hitters",
                "histograms",
                "sketching"
            ],
            "author": [
                "Raef Bassily",
                "Uri Stemmer",
                "Santa Cruz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-786/18-786.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimal Sample Subspace Learning: Theory and Algorithms",
            "abstract": [
                "Subspace segmentation, or subspace learning, is a challenging and complicated task in machine learning. This paper builds a primary frame and solid theoretical bases for the minimal subspace segmentation (MSS) of finite samples. The existence and conditional uniqueness of MSS are discussed with conditions generally satisfied in applications. Utilizing weak prior information of MSS, the minimality inspection of segments is further simplified to the prior detection of partitions. The MSS problem is then modeled as a computable optimization problem via the self-expressiveness of samples. A closed form of the representation matrices is first given for the self-expressiveness, and the connection of diagonal blocks is addressed. The MSS model uses a rank restriction on the sum of segment ranks. Theoretically, it can retrieve the minimal sample subspaces that could be heavily intersected. The optimization problem is solved via a basic manifold conjugate gradient algorithm, alternative optimization and hybrid optimization, therein considering solutions to both the primal MSS problem and its pseudo-dual problem. The MSS model is further modified for handling noisy data and solved by an ADMM algorithm. The reported experiments show the strong ability of the MSS method to retrieve minimal sample subspaces that are heavily intersected."
            ],
            "keywords": [
                "Subspace learning",
                "Clustering",
                "Rank restriction",
                "Sparse optimization",
                "Selfexpressiveness"
            ],
            "author": [
                "Zhenyue Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-084/18-084.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Universal Multi-Task Kernels",
            "abstract": [
                "In this paper we are concerned with reproducing kernel Hilbert spaces H K of functions from an input space into a Hilbert space Y , an environment appropriate for multi-task learning. The reproducing kernel K associated to H K has its values as operators on Y. Our primary goal here is to derive conditions which ensure that the kernel K is universal. This means that on every compact subset of the input space, every continuous function with values in Y can be uniformly approximated by sections of the kernel. We provide various characterizations of universal kernels and highlight them with several concrete examples of some practical importance. Our analysis uses basic principles of functional analysis and especially the useful notion of vector measures which we describe in sufficient detail to clarify our results."
            ],
            "keywords": [
                "multi-task learning",
                "multi-task kernels",
                "universal approximation",
                "vector-valued reproducing kernel Hilbert spaces"
            ],
            "author": [
                "Andrea Caponnetto",
                "Charles A Micchelli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/caponnetto08a/caponnetto08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Consistent Information Criterion for Support Vector Machines in Diverging Model Spaces",
            "abstract": [
                "Information criteria have been popularly used in model selection and proved to possess nice theoretical properties. For classification, Claeskens et al. (2008) proposed support vector machine information criterion for feature selection and provided encouraging numerical evidence. Yet no theoretical justification was given there. This work aims to fill the gap and to provide some theoretical justifications for support vector machine information criterion in both fixed and diverging model spaces. We first derive a uniform convergence rate for the support vector machine solution and then show that a modification of the support vector machine information criterion achieves model selection consistency even when the number of features diverges at an exponential rate of the sample size. This consistency result can be further applied to selecting the optimal tuning parameter for various penalized support vector machine methods. Finite-sample performance of the proposed information criterion is investigated using Monte Carlo studies and one real-world gene selection problem."
            ],
            "keywords": [
                "Bayesian Information Criterion",
                "Diverging Model Spaces",
                "Feature Selection",
                "Support Vector Machines"
            ],
            "author": [
                "Xiang Zhang",
                "Yichao Wu",
                "Lan Wang",
                "Runze Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-231/14-231.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence of Sparse Variational Inference in Gaussian Processes Regression",
            "abstract": [
                "Gaussian processes are distributions over functions that are versatile and mathematically convenient priors in Bayesian modelling. However, their use is often impeded for data with large numbers of observations, N , due to the cubic (in N) cost of matrix operations used in exact inference. Many solutions have been proposed that rely on M N inducing variables to form an approximation at a cost of O(N M 2). While the computational cost appears linear in N , the true complexity depends on how M must scale with N to ensure a certain quality of the approximation. In this work, we investigate upper and lower bounds on how M needs to grow with N to ensure high quality approximations. We show that we can make the KL-divergence between the approximate model and the exact posterior arbitrarily small for a Gaussian-noise regression model with M N. Specifically, for the popular squared exponential kernel and D-dimensional Gaussian distributed covariates, M = O((log N) D) suffice and a method with an overall computational cost of O N (log N) 2D (log log N) 2 can be used to perform inference."
            ],
            "keywords": [
                "Gaussian processes",
                "approximate inference",
                "variational methods",
                "Bayesian non-parameterics",
                "kernel methods * . Previous affiliation where significant portion of work was completed"
            ],
            "author": [
                "David R Burt",
                "Carl Edward Rasmussen",
                "UK Cambridge"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-1015/19-1015.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast mixing of Metropolized Hamiltonian Monte Carlo: Benefits of multi-step gradients",
            "abstract": [
                "Hamiltonian Monte Carlo (HMC) is a state-of-the-art Markov chain Monte Carlo sampling algorithm for drawing samples from smooth probability densities over continuous spaces. We study the variant most widely used in practice, Metropolized HMC with the Störmer-Verlet or leapfrog integrator, and make two primary contributions. First, we provide a non-asymptotic upper bound on the mixing time of the Metropolized HMC with explicit choices of step-size and number of leapfrog steps. This bound gives a precise quantification of the faster convergence of Metropolized HMC relative to simpler MCMC algorithms such as the Metropolized random walk, or Metropolized Langevin algorithm. Second, we provide a general framework for sharpening mixing time bounds of Markov chains initialized at a substantial distance from the target distribution over continuous spaces. We apply this sharpening device to the Metropolized random walk and Langevin algorithms, thereby obtaining improved mixing time bounds from a non-warm initial distribution."
            ],
            "keywords": [],
            "author": [
                "Yuansi Chen",
                "Raaz Dwivedi",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-441/19-441.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Identifying Unreliable and Adversarial Workers in Crowdsourced Labeling Tasks",
            "abstract": [
                "We study the problem of identifying unreliable and adversarial workers in crowdsourcing systems where workers (or users) provide labels for tasks (or items). Most existing studies assume that worker responses follow specific probabilistic models; however, recent evidence shows the presence of workers adopting non-random or even malicious strategies. To account for such workers, we suppose that workers comprise a mixture of honest and adversarial workers. Honest workers may be reliable or unreliable, and they provide labels according to an unknown but explicit probabilistic model. Adversaries adopt labeling strategies different from those of honest workers, whether probabilistic or not. We propose two reputation algorithms to identify unreliable honest workers and adversarial workers from only their responses. Our algorithms assume that honest workers are in the majority, and they classify workers with outlier label patterns as adversaries. Theoretically, we show that our algorithms successfully identify unreliable honest workers, workers adopting deterministic strategies, and worst-case sophisticated adversaries who can adopt arbitrary labeling strategies to degrade the accuracy of the inferred task labels. Empirically, we show that filtering out outliers using our algorithms can significantly improve the accuracy of several state-of-the-art label aggregation algorithms in real-world crowdsourcing datasets."
            ],
            "keywords": [
                "crowdsourcing",
                "reputation",
                "adversary",
                "outliers"
            ],
            "author": [
                "Srikanth Jagabathula",
                "Lakshminarayanan Subramanian",
                "Ashwin Venkataraman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-650/15-650.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi Kernel Learning with Online-Batch Optimization *",
            "abstract": [
                "In recent years there has been a lot of interest in designing principled classification algorithms over multiple cues, based on the intuitive notion that using more features should lead to better performance. In the domain of kernel methods, a principled way to use multiple features is the Multi Kernel Learning (MKL) approach. Here we present a MKL optimization algorithm based on stochastic gradient descent that has a guaranteed convergence rate. We directly solve the MKL problem in the primal formulation. By having a p-norm formulation of MKL, we introduce a parameter that controls the level of sparsity of the solution, while leading to an easier optimization problem. We prove theoretically and experimentally that 1) our algorithm has a faster convergence rate as the number of kernels grows; 2) the training complexity is linear in the number of training examples; 3) very few iterations are sufficient to reach good solutions. Experiments on standard benchmark databases support our claims."
            ],
            "keywords": [
                "multiple kernel learning",
                "learning kernels",
                "online optimization",
                "stochastic subgradient descent",
                "convergence bounds",
                "large scale"
            ],
            "author": [
                "Francesco @ Orabona",
                "Barbara Caputo"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/orabona12a/orabona12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Reliable Effective Terascale Linear Learning System",
            "abstract": [
                "We present a system and a set of techniques for learning linear predictors with convex losses on terascale data sets, with trillions of features, 1 billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature. 2 We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices."
            ],
            "keywords": [
                "distributed machine learning",
                "Hadoop",
                "AllReduce",
                "repeated online averaging",
                "distributed L-BFGS"
            ],
            "author": [
                "Alekh Agarwal",
                "Olivier Chapelle",
                "John Langford"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/agarwal14a/agarwal14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary β-Mixing Processes",
            "abstract": [
                "PAC-Bayes bounds are among the most accurate generalization bounds for classifiers learned from independently and identically distributed (IID) data, and it is particularly so for margin classifiers: there have been recent contributions showing how practical these bounds can be either to perform model selection (Ambroladze et al., 2007) or even to directly guide the learning of linear classifiers (Germain et al., 2009). However, there are many practical situations where the training data show some dependencies and where the traditional IID assumption does not hold. Stating generalization bounds for such frameworks is therefore of the utmost interest, both from theoretical and practical standpoints. In this work, we propose the first-to the best of our knowledge-PAC-Bayes generalization bounds for classifiers trained on data exhibiting interdependencies. The approach undertaken to establish our results is based on the decomposition of a so-called dependency graph that encodes the dependencies within the data, in sets of independent data, thanks to graph fractional covers. Our bounds are very general, since being able to find an upper bound on the fractional chromatic number of the dependency graph is sufficient to get new PAC-Bayes bounds for specific settings. We show how our results can be used to derive bounds for ranking statistics (such as AUC) and classifiers trained on data distributed according to a stationary β-mixing process. In the way, we show how our approach seamlessly allows us to deal with U-processes. As a side note, we also provide a PAC-Bayes generalization bound for classifiers learned on data from stationary ϕ-mixing distributions."
            ],
            "keywords": [
                "PAC-Bayes bounds",
                "non IID data",
                "ranking",
                "U-statistics",
                "mixing processes"
            ],
            "author": [
                "Liva Ralaivola",
                "Marie Szafranski",
                "Guillaume Stempfel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/ralaivola10a/ralaivola10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiple Output Regression with Latent Noise",
            "abstract": [
                "In high-dimensional data, structured noise caused by observed and unobserved factors affecting multiple target variables simultaneously, imposes a serious challenge for modeling, by masking the often weak signal. Therefore, (1) explaining away the structured noise in multiple-output regression is of paramount importance. Additionally, (2) assumptions about the correlation structure of the regression weights are needed. We note that both can be formulated in a natural way in a latent variable model, in which both the interesting signal and the noise are mediated through the same latent factors. Under this assumption, the signal model then borrows strength from the noise model by encouraging similar effects on correlated targets. We introduce a hyperparameter for the latent signal-to-noise ratio which turns out to be important for modelling weak signals, and an ordered infinitedimensional shrinkage prior that resolves the rotational unidentifiability in reduced-rank regression models. Simulations and prediction experiments with metabolite, gene expression, FMRI measurement, and macroeconomic time series data show that our model equals or exceeds the state-of-the-art performance and, in particular, outperforms the standard approach of assuming independent noise and signal models."
            ],
            "keywords": [
                "Bayesian reduced-rank regression",
                "latent variable models",
                "latent signal-tonoise ratio",
                "multiple-output regression",
                "nonparametric Bayes",
                "shrinkage priors",
                "structured noise",
                "weak effects"
            ],
            "author": [
                "Jussi Gillberg",
                "Pekka Marttinen",
                "Matti Pirinen",
                "Pasi Soininen",
                "Mehreen Ali",
                "Aki S Havulinna",
                "Marjo-Riitta Järvelin",
                "Mika Ala-Korpela",
                "Samuel Kaski"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-436/14-436.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Smoothed Nonparametric Derivative Estimation using Weighted Difference Quotients",
            "abstract": [
                "Derivatives play an important role in bandwidth selection methods (e.g., plug-ins), data analysis and bias-corrected confidence intervals. Therefore, obtaining accurate derivative information is crucial. Although many derivative estimation methods exist, the majority require a fixed design assumption. In this paper, we propose an effective and fully datadriven framework to estimate the first and second order derivative in random design. We establish the asymptotic properties of the proposed derivative estimator, and also propose a fast selection method for the tuning parameters. The performance and flexibility of the method is illustrated via an extensive simulation study."
            ],
            "keywords": [
                "derivative estimation",
                "asymptotic properties",
                "random design"
            ],
            "author": [
                "Yu Liu",
                "Kris De Brabanter"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-246/19-246.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Loop Corrections for Approximate Inference on Factor Graphs",
            "abstract": [
                "We propose a method to improve approximate inference methods by correcting for the influence of loops in the graphical model. The method is a generalization and alternative implementation of a recent idea from Montanari and Rizzo (2005). It is applicable to arbitrary factor graphs, provided that the size of the Markov blankets is not too large. It consists of two steps: (i) an approximate inference method, for example, belief propagation, is used to approximate cavity distributions for each variable (i.e., probability distributions on the Markov blanket of a variable for a modified graphical model in which the factors involving that variable have been removed); (ii) all cavity distributions are improved by a message-passing algorithm that cancels out approximation errors by imposing certain consistency constraints. This loop correction (LC) method usually gives significantly better results than the original, uncorrected, approximate inference algorithm that is used to estimate the effect of loops. Indeed, we often observe that the loop-corrected error is approximately the square of the error of the uncorrected approximate inference method. In this article, we compare different variants of the loop correction method with other approximate inference methods on a variety of graphical models, including \"real world\" networks, and conclude that the LC method generally obtains the most accurate results."
            ],
            "keywords": [
                "loop corrections",
                "approximate inference",
                "graphical models",
                "factor graphs",
                "belief propagation"
            ],
            "author": [
                "Joris M Mooij",
                "Hilbert J Kappen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/mooij07a/mooij07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical-Computational Tradeoffs in Planted Problems and Submatrix Localization with a Growing Number of Clusters and Submatrices *",
            "abstract": [
                "We consider two closely related problems: planted clustering and submatrix localization. In the planted clustering problem, a random graph is generated based on an underlying cluster structure of the nodes; the task is to recover these clusters given the graph. The submatrix localization problem concerns locating hidden submatrices with elevated means inside a large real-valued random matrix. Of particular interest is the setting where the number of clusters/submatrices is allowed to grow unbounded with the problem size. These formulations cover several classical models such as planted clique, planted densest subgraph, planted partition, planted coloring, and the stochastic block model, which are widely used for studying community detection, graph clustering and bi-clustering. For both problems, we show that the space of the model parameters (cluster/submatrix size, edge probabilities and the mean of the submatrices) can be partitioned into four disjoint regions corresponding to decreasing statistical and computational complexities: (1) the impossible regime, where all algorithms fail; (2) the hard regime, where the computationally expensive Maximum Likelihood Estimator (MLE) succeeds; (3) the easy regime, where the polynomial-time convexified MLE succeeds; (4) the simple regime, where a local counting/thresholding procedure succeeds. Moreover, we show that each of these algorithms provably fails in the harder regimes. Our results establish the minimax recovery limits, which are tight up to universal constants and hold even with a growing number of clusters/submatrices, and provide order-wise stronger performance guarantees for polynomial-time algorithms than previously known. Our study demonstrates the tradeoffs between statistical and computational considerations, and suggests that the minimax limits may not be achievable by polynomial-time algorithms."
            ],
            "keywords": [
                "planted partition",
                "planted clique",
                "planted coloring",
                "submatrix localization",
                "graph clustering",
                "bi-clustering",
                "minimax recovery",
                "computational hardness",
                "convex relaxation"
            ],
            "author": [
                "Yudong Chen",
                "Jiaming Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-330/14-330.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Numerical Measure of the Instability of Mapper-Type Algorithms",
            "abstract": [
                "Mapper is an unsupervised machine learning algorithm generalising the notion of clustering to obtain a geometric description of a dataset. The procedure splits the data into possibly overlapping bins which are then clustered. The output of the algorithm is a graph where nodes represent clusters and edges represent the sharing of data points between two clusters. However, several parameters must be selected before applying Mapper and the resulting graph may vary dramatically with the choice of parameters. We define an intrinsic notion of Mapper instability that measures the variability of the output as a function of the choice of parameters required to construct a Mapper output. Our results and discussion are general and apply to all Mapper-type algorithms. We derive theoretical results that provide estimates for the instability and suggest practical ways to control it. We provide also experiments to illustrate our results and in particular we demonstrate that a reliable candidate Mapper output can be identified as a local minimum of instability regarded as a function of Mapper input parameters."
            ],
            "keywords": [
                "topological data analysis",
                "Mapper",
                "clustering stability",
                "parameter selection",
                "sub-sampling"
            ],
            "author": [
                "Francisco Belchí",
                "Matthew Burfitt"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-540/19-540.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized maximum entropy estimation",
            "abstract": [
                "We consider the problem of estimating a probability distribution that maximizes the entropy while satisfying a finite number of moment constraints, possibly corrupted by noise. Based on duality of convex programming, we present a novel approximation scheme using a smoothed fast gradient method that is equipped with explicit bounds on the approximation error. We further demonstrate how the presented scheme can be used for approximating the chemical master equation through the zero-information moment closure method, and for an approximate dynamic programming approach in the context of constrained Markov decision processes with uncountable state and action spaces."
            ],
            "keywords": [
                "Entropy maximization",
                "convex optimization",
                "relative entropy minimization",
                "fast gradient method",
                "approximate dynamic programming"
            ],
            "author": [
                "Tobias Sutter",
                "David Sutter",
                "Peyman Mohajerin Esfahani",
                "John Lygeros"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-486/17-486.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": null,
            "abstract": [
                "PyBrain is a versatile machine learning library for Python. Its goal is to provide flexible, easyto-use yet still powerful algorithms for machine learning tasks, including a variety of predefined environments and benchmarks to test and compare algorithms. Implemented algorithms include Long Short-Term Memory (LSTM), policy gradient methods, (multidimensional) recurrent neural networks and deep belief networks."
            ],
            "keywords": [
                "Python",
                "neural networks",
                "reinforcement learning",
                "optimization"
            ],
            "author": [
                "Justin Bayer",
                "Daan Wierstra",
                "Yi Sun",
                "Martin Felder",
                "Frank Sehnke",
                "Thomas Rückstieß",
                "Jürgen Schmidhuber"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/schaul10a/schaul10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "General Polynomial Time Decomposition Algorithms Nikolas List",
            "abstract": [
                "We present a general decomposition algorithm that is uniformly applicable to every (suitably normalized) instance of Convex Quadratic Optimization and efficiently approaches an optimal solution. The number of iterations required to be within ε of optimality grows linearly with 1/ε and quadratically with the number m of variables. The working set selection can be performed in polynomial time. If we restrict our considerations to instances of Convex Quadratic Optimization with at most k 0 equality constraints for some fixed constant k 0 plus some so-called box-constraints (conditions that hold for most variants of SVM-optimization), the working set is found in linear time. Our analysis builds on a generalization of the concept of rate certifying pairs that was introduced by Hush and Scovel. In order to extend their results to arbitrary instances of Convex Quadratic Optimization, we introduce the general notion of a rate certifying q-set. We improve on the results by Hush and Scovel (2003) in several ways. First our result holds for Convex Quadratic Optimization whereas the results by Hush and Scovel are specialized to SVM-optimization. Second, we achieve a higher rate of convergence even for the special case of SVM-optimization (despite the generality of our approach). Third, our analysis is technically simpler. We prove furthermore that the strategy for working set selection which is based on rate certifying sets coincides with a strategy which is based on a so-called \"sparse witness of sub-optimality\". Viewed from this perspective, our main result improves on convergence results by List and Simon (2004) and Simon (2004) by providing convergence rates (and by holding under more general conditions)."
            ],
            "keywords": [
                "convex quadratic optimization",
                "decomposition algorithms",
                "support vector machines"
            ],
            "author": [
                "Hans Ulrich Simon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/list07a/list07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Implicit Bias of Gradient Descent on Separable Data",
            "abstract": [
                "We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods."
            ],
            "keywords": [
                "gradient descent",
                "implicit regularization",
                "generalization",
                "margin",
                "logistic regression"
            ],
            "author": [
                "Daniel Soudry",
                "Nathan Srebro"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-188/18-188.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Interpretable Multi-Response Regression via SEED",
            "abstract": [
                "Sparse reduced-rank regression is an important tool for uncovering meaningful dependence structure between large numbers of predictors and responses in many big data applications such as genome-wide association studies and social media analysis. Despite the recent theoretical and algorithmic advances, scalable estimation of sparse reduced-rank regression remains largely unexplored. In this paper, we suggest a scalable procedure called sequential estimation with eigen-decomposition (SEED) which needs only a single top-r sparse singular value decomposition from a generalized eigenvalue problem to find the optimal low-rank and sparse matrix estimate. Our suggested method is not only scalable but also performs simultaneous dimensionality reduction and variable selection. Under some mild regularity conditions, we show that SEED enjoys nice sampling properties including consistency in estimation, rank selection, prediction, and model selection. Moreover, SEED employs only basic matrix operations that can be efficiently parallelized in high performance computing devices. Numerical studies on synthetic and real data sets show that SEED outperforms the state-of-the-art approaches for large-scale matrix estimation problem."
            ],
            "keywords": [
                "reduced-rank regression",
                "scalability",
                "high dimensionality",
                "greedy algorithm",
                "sparse eigenvector estimation"
            ],
            "author": [
                "Zemin Zheng",
                "M Taha Bahadori",
                "Yan Liu",
                "Jinchi Lv"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-200/18-200.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Fourier Features for Gaussian Processes",
            "abstract": [
                "This work brings together two powerful concepts in Gaussian processes: the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for Matérn kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the data set, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non-conjugate likelihoods, our MCMC scheme reduces the cost of computation from O(N M 2) (for a sparse Gaussian process) to O(N M) per iteration, where N is the number of data and M is the number of features."
            ],
            "keywords": [
                "Gaussian processes",
                "Fourier features",
                "variational inference"
            ],
            "author": [
                "James Hensman",
                "Nicolas Durrande"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-579/16-579.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lower Bounds for Parallel and Randomized Convex Optimization *",
            "abstract": [
                "We study the question of whether parallelization in the exploration of the feasible set can be used to speed up convex optimization, in the local oracle model of computation and in the high-dimensional regime. We show that the answer is negative for both deterministic and randomized algorithms applied to essentially any of the interesting geometries and nonsmooth, weakly-smooth, or smooth objective functions. In particular, we show that it is not possible to obtain a polylogarithmic (in the sequential complexity of the problem) number of parallel rounds with a polynomial (in the dimension) number of queries per round. In the majority of these settings and when the dimension of the space is polynomial in the inverse target accuracy, our lower bounds match the oracle complexity of sequential convex optimization, up to at most a logarithmic factor in the dimension, which makes them (nearly) tight. Another conceptual contribution of our work is in providing a general and streamlined framework for proving lower bounds in the setting of parallel convex optimization. Prior to our work, lower bounds for parallel convex optimization algorithms were only known in a small fraction of the settings considered in this paper, mainly applying to Euclidean (2) and ∞ spaces."
            ],
            "keywords": [
                "lower bounds",
                "convex optimization",
                "parallel algorithms",
                "randomized algorithms",
                "non-Euclidean optimization"
            ],
            "author": [
                "Jelena Diakonikolas",
                "Cristóbal Guzmán"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-771/19-771.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Link Prediction in Graphs with Autoregressive Features",
            "abstract": [
                "In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices. On the adjacency matrix it takes into account both sparsity and low rank properties and on the VAR it encodes the sparsity. The analysis involves oracle inequalities that illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank. The estimate is computed efficiently using proximal methods, and evaluated through numerical experiments."
            ],
            "keywords": [
                "graphs",
                "link prediction",
                "low-rank",
                "sparsity",
                "autoregression"
            ],
            "author": [
                "Emile Richard",
                "Nicolas Vayatis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/richard14a/richard14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Bipartite Network Clustering",
            "abstract": [
                "We study bipartite community detection in networks, or more generally the network biclustering problem. We present a fast two-stage procedure based on spectral initialization followed by the application of a pseudo-likelihood classifier twice. Under mild regularity conditions, we establish the weak consistency of the procedure (i.e., the convergence of the misclassification rate to zero) under a general bipartite stochastic block model. We show that the procedure is optimal in the sense that it achieves the optimal convergence rate that is achievable by a biclustering oracle, adaptively over the whole class, up to constants. This is further formalized by deriving a minimax lower bound over a class of biclustering problems. The optimal rate we obtain sharpens some of the existing results and generalizes others to a wide regime of average degree growth, from sparse networks with average degrees growing arbitrarily slowly to fairly dense networks with average degrees of order √ n. As a special case, we recover the known exact recovery threshold in the log n regime of sparsity. To obtain the consistency result, as part of the provable version of the algorithm, we introduce a sub-block partitioning scheme that is also computationally attractive, allowing for distributed implementation of the algorithm without sacrificing optimality. The provable algorithm is derived from a general class of pseudo-likelihood biclustering algorithms that employ simple EM type updates. We show the effectiveness of this general class by numerical simulations."
            ],
            "keywords": [
                "Bipartite networks",
                "stochastic block model",
                "community detection",
                "biclustering",
                "network analysis",
                "pseudo-likelihood",
                "spectral clustering"
            ],
            "author": [
                "Zhixin Zhou",
                "Arash A Amini"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-299/19-299.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Generalized Subset Scan for Anomalous Pattern Detection",
            "abstract": [
                "We propose Fast Generalized Subset Scan (FGSS), a new method for detecting anomalous patterns in general categorical data sets. We frame the pattern detection problem as a search over subsets of data records and attributes, maximizing a nonparametric scan statistic over all such subsets. We prove that the nonparametric scan statistics possess a novel property that allows for efficient optimization over the exponentially many subsets of the data without an exhaustive search, enabling FGSS to scale to massive and high-dimensional data sets. We evaluate the performance of FGSS in three real-world application domains (customs monitoring, disease surveillance, and network intrusion detection), and demonstrate that FGSS can successfully detect and characterize relevant patterns in each domain. As compared to three other recently proposed detection algorithms, FGSS substantially decreased run time and improved detection power for massive multivariate data sets."
            ],
            "keywords": [
                "pattern detection",
                "anomaly detection",
                "knowledge discovery",
                "Bayesian networks",
                "scan statistics"
            ],
            "author": [
                "Edward Mcfowland III",
                "Skyler Speakman",
                "Daniel B Neill"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/mcfowland13a/mcfowland13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Operator Norm Convergence of Spectral Clustering on Level Sets",
            "abstract": [
                "Following Hartigan (1975), a cluster is defined as a connected component of the t-level set of the underlying density, that is, the set of points for which the density is greater than t. A clustering algorithm which combines a density estimate with spectral clustering techniques is proposed. Our algorithm is composed of two steps. First, a nonparametric density estimate is used to extract the data points for which the estimated density takes a value greater than t. Next, the extracted points are clustered based on the eigenvectors of a graph Laplacian matrix. Under mild assumptions, we prove the almost sure convergence in operator norm of the empirical graph Laplacian operator associated with the algorithm. Furthermore, we give the typical behavior of the representation of the data set into the feature space, which establishes the strong consistency of our proposed algorithm."
            ],
            "keywords": [
                "spectral clustering",
                "graph",
                "unsupervised classification",
                "level sets",
                "connected components"
            ],
            "author": [
                "Bruno Pelletier"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/pelletier11a/pelletier11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "\"Ideal Parent\" Structure Learning for Continuous Variable Bayesian Networks",
            "abstract": [
                "Bayesian networks in general, and continuous variable networks in particular, have become increasingly popular in recent years, largely due to advances in methods that facilitate automatic learning from data. Yet, despite these advances, the key task of learning the structure of such models remains a computationally intensive procedure, which limits most applications to parameter learning. This problem is even more acute when learning networks in the presence of missing values or hidden variables, a scenario that is part of many real-life problems. In this work we present a general method for speeding structure search for continuous variable networks with common parametric distributions. We efficiently evaluate the approximate merit of candidate structure modifications and apply time consuming (exact) computations only to the most promising ones, thereby achieving significant improvement in the running time of the search algorithm. Our method also naturally and efficiently facilitates the addition of useful new hidden variables into the network structure, a task that is typically considered both conceptually difficult and computationally prohibitive. We demonstrate our method on synthetic and real-life data sets, both for learning structure on fully and partially observable data, and for introducing new hidden variables during structure search."
            ],
            "keywords": [
                "Bayesian networks",
                "structure learning",
                "continuous variables",
                "hidden variables"
            ],
            "author": [
                "Gal Elidan",
                "Iftach Nachman",
                "Nir Friedman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/elidan07a/elidan07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Parameter-Free Classification Method for Large Scale Learning",
            "abstract": [
                "With the rapid growth of computer storage capacities, available data and demand for scoring models both follow an increasing trend, sharper than that of the processing power. However, the main limitation to a wide spread of data mining solutions is the non-increasing availability of skilled data analysts, which play a key role in data preparation and model selection. In this paper, we present a parameter-free scalable classification method, which is a step towards fully automatic data mining. The method is based on Bayes optimal univariate conditional density estimators, naive Bayes classification enhanced with a Bayesian variable selection scheme, and averaging of models using a logarithmic smoothing of the posterior distribution. We focus on the complexity of the algorithms and show how they can cope with data sets that are far larger than the available central memory. We finally report results on the Large Scale Learning challenge, where our method obtains state of the art performance within practicable computation time."
            ],
            "keywords": [
                "large scale learning",
                "naive Bayes",
                "Bayesianism",
                "model selection",
                "model averaging"
            ],
            "author": [
                "Marc Boullé"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/boulle09a/boulle09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model Selection: Beyond the Bayesian/Frequentist Divide",
            "abstract": [
                "The principle of parsimony also known as \"Ockham's razor\" has inspired many theories of model selection. Yet such theories, all making arguments in favor of parsimony, are based on very different premises and have developed distinct methodologies to derive algorithms. We have organized challenges and edited a special issue of JMLR and several conference proceedings around the theme of model selection. In this editorial, we revisit the problem of avoiding overfitting in light of the latest results. We note the remarkable convergence of theories as different as Bayesian theory, Minimum Description Length, bias/variance tradeoff, Structural Risk Minimization, and regularization, in some approaches. We also present new and interesting examples of the complementarity of theories leading to hybrid algorithms, neither frequentist, nor Bayesian, or perhaps both frequentist and Bayesian!"
            ],
            "keywords": [
                "model selection",
                "ensemble methods",
                "multilevel inference",
                "multilevel optimization",
                "performance prediction",
                "bias-variance tradeoff",
                "Bayesian priors",
                "structural risk minimization",
                "guaranteed risk minimization",
                "over-fitting",
                "regularization",
                "minimum description length"
            ],
            "author": [
                "Isabelle Guyon",
                "Amir Saffari",
                "Gideon Dror",
                "Gavin Cawley"
            ],
            "ref": "http://www.jmlr.org/papers/volume11/guyon10a/guyon10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Laplacian Support Vector Machines Trained in the Primal",
            "abstract": [
                "In the last few years, due to the growing ubiquity of unlabeled data, much effort has been spent by the machine learning community to develop better understanding and improve the quality of classifiers exploiting unlabeled data. Following the manifold regularization approach, Laplacian Support Vector Machines (LapSVMs) have shown the state of the art performance in semi-supervised classification. In this paper we present two strategies to solve the primal LapSVM problem, in order to overcome some issues of the original dual formulation. In particular, training a LapSVM in the primal can be efficiently performed with preconditioned conjugate gradient. We speed up training by using an early stopping strategy based on the prediction on unlabeled data or, if available, on labeled validation examples. This allows the algorithm to quickly compute approximate solutions with roughly the same classification accuracy as the optimal ones, considerably reducing the training time. The computational complexity of the training algorithm is reduced from O(n 3) to O(kn 2), where n is the combined number of labeled and unlabeled examples and k is empirically evaluated to be significantly smaller than n. Due to its simplicity, training LapSVM in the primal can be the starting point for additional enhancements of the original LapSVM formulation, such as those for dealing with large data sets. We present an extensive experimental evaluation on real world data showing the benefits of the proposed approach."
            ],
            "keywords": [
                "Laplacian support vector machines",
                "manifold regularization",
                "semi-supervised learning",
                "classification",
                "optimization"
            ],
            "author": [
                "Stefano Melacci",
                "Mikhail Belkin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/melacci11a/melacci11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalization Bounds and Complexities Based on Sparsity and Clustering for Convex Combinations of Functions from Random Classes",
            "abstract": [
                "A unified approach is taken for deriving new generalization data dependent bounds for several classes of algorithms explored in the existing literature by different approaches. This unified approach is based on an extension of Vapnik's inequality for VC classes of sets to random classes of sets-that is, classes depending on the random data, invariant under permutation of the data and possessing the increasing property. Generalization bounds are derived for convex combinations of functions from random classes with certain properties. Algorithms, such as SVMs (support vector machines), boosting with decision stumps, radial basis function networks, some hierarchies of kernel machines or convex combinations of indicator functions over sets with finite VC dimension, generate classifier functions that fall into the above category. We also explore the individual complexities of the classifiers, such as sparsity of weights and weighted variance over clusters from the convex combination introduced by Koltchinskii and Panchenko (2004), and show sparsity-type and cluster-variance-type generalization bounds for random classes."
            ],
            "keywords": [
                "complexities of classifiers",
                "generalization bounds",
                "SVM",
                "voting classifiers",
                "random classes"
            ],
            "author": [
                "Savina Andonova Jaeger"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/jaeger05a/jaeger05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Learning of Multiple Tasks with a Shared Loss",
            "abstract": [
                "We study the problem of learning multiple tasks in parallel within the online learning framework. On each online round, the algorithm receives an instance for each of the parallel tasks and responds by predicting the label of each instance. We consider the case where the predictions made on each round all contribute toward a common goal. The relationship between the various tasks is defined by a global loss function, which evaluates the overall quality of the multiple predictions made on each round. Specifically, each individual prediction is associated with its own loss value, and then these multiple loss values are combined into a single number using the global loss function. We focus on the case where the global loss function belongs to the family of absolute norms, and present several online learning algorithms for the induced problem. We prove worst-case relative loss bounds for all of our algorithms, and demonstrate the effectiveness of our approach on a largescale multiclass-multilabel text categorization problem."
            ],
            "keywords": [
                "online learning",
                "multitask learning",
                "multiclass multilabel classiifcation",
                "perceptron"
            ],
            "author": [
                "Ofer Dekel",
                "Philip M Long"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/dekel07a/dekel07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Monotonic Calibrated Interpolated Look-Up Tables",
            "abstract": [
                "Real-world machine learning applications may have requirements beyond accuracy, such as fast evaluation times and interpretability. In particular, guaranteed monotonicity of the learned function with respect to some of the inputs can be critical for user confidence. We propose meeting these goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables. We extend the structural risk minimization framework of lattice regression to monotonic functions by adding linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, at the cost of making the objective non-convex. We address large-scale learning through parallelization, mini-batching, and random sampling of additive regularizer terms. Case studies on real-world problems with up to sixteen features and up to hundreds of millions of training samples demonstrate the proposed monotonic functions can achieve state-of-the-art accuracy in practice while providing greater transparency to users."
            ],
            "keywords": [
                "interpretability",
                "interpolation",
                "look-up tables",
                "monotonicity"
            ],
            "author": [
                "Maya Gupta",
                "Andrew Cotter",
                "Jan Pfeifer",
                "Konstantin Voevodski",
                "Alexander Mangylov",
                "Wojciech Moczydlowski"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-243/15-243.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lagrangian Support Vector Machines",
            "abstract": [
                "An implicit Lagrangian for the dual of a simple reformulation of the standard quadratic program of a linear support vector machine is proposed. This leads to the minimization of an unconstrained differentiable convex function in a space of dimensionality equal to the number of classified points. This problem is solvable by an extremely simple linearly convergent Lagrangian support vector machine (LSVM) algorithm. LSVM requires the inversion at the outset of a single matrix of the order of the much smaller dimensionality of the original input space plus one. The full algorithm is given in this paper in 11 lines of MATLAB code without any special optimization tools such as linear or quadratic programming solvers. This LSVM code can be used \"as is\" to solve classification problems with millions of points. For example, 2 million points in 10 dimensional input space were classified by a linear surface in 82 minutes on a Pentium III 500 MHz notebook with 384 megabytes of memory (and additional swap space), and in 7 minutes on a 250 MHz UltraSPARC II processor with 2 gigabytes of memory. Other standard classification test problems were also solved. Nonlinear kernel classification can also be solved by LSVM. Although it does not scale up to very large problems, it can handle any positive semidefinite kernel and is guaranteed to converge. A short MATLAB code is also given for nonlinear kernels and tested on a number of problems."
            ],
            "keywords": [],
            "author": [
                "O L Mangasarian",
                "David R Musicant"
            ],
            "ref": "http://www.jmlr.org/papers/volume1/mangasarian01a/mangasarian01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Finding Optimal Bayesian Network Given a Super-Structure",
            "abstract": [
                "Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. However, a recent empirical study has shown that a hybrid algorithm improves sensitively accuracy and speed: it learns a skeleton with an independency test (IT) approach and constrains on the directed acyclic graphs (DAG) considered during the search-and-score phase. Subsequently, we theorize the structural constraint by introducing the concept of superstructure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a superstructure constrained optimal search (COS): its time complexity is upper bounded by O(γ m n), where γ m < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degreem and sparse structures allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even finds more accurate results when given a sound superstructure. Practically, S can be approximated by IT approaches; significance level of the tests controls its sparseness, enabling to control the trade-off between speed and accuracy. For incomplete superstructures , a greedily post-processed version (COS+) still enables to significantly outperform other heuristic searches."
            ],
            "keywords": [
                "Bayesian networks",
                "structure learning",
                "optimal search",
                "super-structure",
                "connected subset"
            ],
            "author": [
                "Eric Perrier",
                "Satoru Miyano"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/perrier08a/perrier08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Propagation of Low-Rate Measurement Error to Subgraph Counts in Large Networks",
            "abstract": [
                "Our work in this paper is inspired by a statistical observation that is both elementary and broadly relevant to network analysis in practice-that the uncertainty in approximating some true graph G = (V, E) by some estimated graphĜ = (V,Ê) manifests as errors in our knowledge of the presence/absence of edges between vertex pairs, which must necessarily propagate to any estimates of network summaries η(G) we seek. Motivated by the common practice of using plug-in estimates η(Ĝ) as proxies for η(G), our focus is on the problem of characterizing the distribution of the discrepancy D = η(Ĝ) − η(G), in the case where η(•) is a subgraph count. Specifically, we study the fundamental case where the statistic of interest is |E|, the number of edges in G. Our primary contribution in this paper is to show that in the empirically relevant setting of large graphs with low-rate measurement errors, the distribution of D E = |Ê| − |E| is well-characterized by a Skellam distribution, when the errors are independent or weakly dependent. Under an assumption of independent errors, we are able to further show conditions under which this characterization is strictly better than that of an appropriate normal distribution. These results derive from our formulation of a general result, quantifying the accuracy with which the difference of two sums of dependent Bernoulli random variables may be approximated by the difference of two independent Poisson random variables, i.e., by a Skellam distribution. This general result is developed through the use of Stein's method, and may be of some general interest. We finish with a discussion of possible extension of our work to subgraph counts η(G) of higher order."
            ],
            "keywords": [
                "Limit distribution",
                "network analysis",
                "Skellam distribution",
                "Stein's method"
            ],
            "author": [
                "Prakash Balachandran",
                "Eric D Kolaczyk",
                "Weston D Viles",
                "Dartmouth Edu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-497/16-497.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bouligand Derivatives and Robustness of Support Vector Machines for Regression",
            "abstract": [
                "We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in infinite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand influence function (BIF) a modification of F.R. Hampel's influence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel's influence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on influence functions."
            ],
            "keywords": [
                "Bouligand derivatives",
                "empirical risk minimization",
                "influence function",
                "robustness",
                "support vector machines"
            ],
            "author": [
                "Andreas Christmann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/christmann08a/christmann08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Exact Matrix Completion: A Unified Optimization Framework for Matrix Completion",
            "abstract": [
                "We formulate the problem of matrix completion with and without side information as a non-convex optimization problem. We design fastImpute based on non-convex gradient descent and show it converges to a global minimum that is guaranteed to recover closely the underlying matrix while it scales to matrices of sizes beyond 10 5 × 10 5. We report experiments on both synthetic and real-world datasets that show fastImpute is competitive in both the accuracy of the matrix recovered and the time needed across all cases. Furthermore, when a high number of entries are missing, fastImpute is over 75% lower in MAPE and 15 times faster than current state-of-the-art matrix completion methods in both the case with side information and without."
            ],
            "keywords": [
                "Matrix Completion",
                "Projected Gradient Descent",
                "Stochastic Approximation"
            ],
            "author": [
                "Dimitris Bertsimas",
                "Michael Lingzhi Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-471/19-471.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Learning of Dynamic Multilayer Networks",
            "abstract": [
                "A plethora of networks is being collected in a growing number of fields, including disease transmission, international relations, social interactions, and others. As data streams continue to grow, the complexity associated with these highly multidimensional connectivity data presents novel challenges. In this paper, we focus on the time-varying interconnections among a set of actors in multiple contexts, called layers. Current literature lacks flexible statistical models for dynamic multilayer networks, which can enhance quality in inference and prediction by efficiently borrowing information within each network, across time, and between layers. Motivated by this gap, we develop a Bayesian nonparametric model leveraging latent space representations. Our formulation characterizes the edge probabilities as a function of shared and layer-specific actors positions in a latent space, with these positions changing in time via Gaussian processes. This representation facilitates dimensionality reduction and incorporates different sources of information in the observed data. In addition, we obtain tractable procedures for posterior computation, inference, and prediction. We provide theoretical results on the flexibility of our model. Our methods are tested on simulations and infection studies monitoring dynamic face-to-face contacts among individuals in multiple days, where we perform better than current methods in inference and prediction."
            ],
            "keywords": [
                "Dynamic multilayer network",
                "edge prediction",
                "face-to-face contact network",
                "Gaussian process",
                "latent space model"
            ],
            "author": [
                "Daniele Durante",
                "Nabanita Mukherjee",
                "Rebecca C Steorts"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-391/16-391.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sampling Methods for the Nyström Method",
            "abstract": [
                "The Nyström method is an efficient technique to generate low-rank matrix approximations and is used in several large-scale learning applications. A key aspect of this method is the procedure according to which columns are sampled from the original matrix. In this work, we explore the efficacy of a variety of fixed and adaptive sampling schemes. We also propose a family of ensemble-based sampling algorithms for the Nyström method. We report results of extensive experiments that provide a detailed comparison of various fixed and adaptive sampling techniques, and demonstrate the performance improvement associated with the ensemble Nyström method when used in conjunction with either fixed or adaptive sampling schemes. Corroborating these empirical findings, we present a theoretical analysis of the Nyström method, providing novel error bounds guaranteeing a better convergence rate of the ensemble Nyström method in comparison to the standard Nyström method."
            ],
            "keywords": [
                "low-rank approximation",
                "nyström method",
                "ensemble methods",
                "large-scale learning"
            ],
            "author": [
                "Sanjiv Kumar",
                "Mehryar Mohri"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/kumar12a/kumar12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Estimation and Model Combination in A Bandit Problem with Covariates",
            "abstract": [
                "Multi-armed bandit problem is an important optimization game that requires an explorationexploitation tradeoff to achieve optimal total reward. Motivated from industrial applications such as online advertising and clinical research, we consider a setting where the rewards of bandit machines are associated with covariates, and the accurate estimation of the corresponding mean reward functions plays an important role in the performance of allocation rules. Under a flexible problem setup, we establish asymptotic strong consistency and perform a finite-time regret analysis for a sequential randomized allocation strategy based on kernel estimation. In addition, since many nonparametric and parametric methods in supervised learning may be applied to estimating the mean reward functions but guidance on how to choose among them is generally unavailable, we propose a model combining allocation strategy for adaptive performance. Simulations and a real data evaluation are conducted to illustrate the performance of the proposed allocation strategy."
            ],
            "keywords": [
                "contextual bandit problem",
                "exploration-exploitation tradeoff",
                "nonparametric regression",
                "regret bound",
                "upper confidence bound"
            ],
            "author": [
                "Wei Qian",
                "Yuhong Yang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/13-210/13-210.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiscale Adaptive Representation of Signals: I. The Basic Framework",
            "abstract": [
                "We introduce a framework for designing multi-scale, adaptive, shift-invariant frames and bi-frames for representing signals. The new framework, called AdaFrame, improves over dictionary learning-based techniques in terms of computational efficiency at inference time. It improves classical multi-scale basis such as wavelet frames in terms of coding efficiency. It provides an attractive alternative to dictionary learning-based techniques for low level signal processing tasks, such as compression and denoising, as well as high level tasks, such as feature extraction for object recognition. Connections with deep convolutional networks are also discussed. In particular, the proposed framework reveals a drawback in the commonly used approach for visualizing the activations of the intermediate layers in convolutional networks, and suggests a natural alternative."
            ],
            "keywords": [
                "AdaFrame",
                "Dictionary Learning",
                "Wavelet Frames/Bi-frames"
            ],
            "author": [
                "Cheng Tai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-072/15-072.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Effective String Processing and Matching for Author Disambiguation",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "Wei-Sheng Chin",
                "Yong Zhuang",
                "Felix Wu",
                "Tong Yu",
                "Cheng-Xia Chang",
                "Chun-Pai Yang",
                "Wei-Cheng Chang",
                "Kuan-Hao Huang",
                "Tzu-Ming Kuo",
                "Shan-Wei Lin",
                "Yu-Chen Lu",
                "Yu-Chuan Su",
                "Cheng-Kuang Wei",
                "Tu-Chun Yin",
                "Chun-Liang Li",
                "Ting-Wei Lin",
                "Cheng-Hao Tsai",
                "Hsuan-Tien Lin",
                "Chih-Jen Lin",
                "Senjuti Basu Roy",
                "Vani Mandava",
                "Martine De Cock"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/chin14a/chin14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "V -Matrix Method of Solving Statistical Inference Problems",
            "abstract": [
                "This paper presents direct settings and rigorous solutions of the main Statistical Inference problems. It shows that rigorous solutions require solving multidimensional Fredholm integral equations of the first kind in the situation where not only the right-hand side of the equation is an approximation, but the operator in the equation is also defined approximately. Using Stefanuyk-Vapnik theory for solving such ill-posed operator equations, constructive methods of empirical inference are introduced. These methods are based on a new concept called V-matrix. This matrix captures geometric properties of the observation data that are ignored by classical statistical methods."
            ],
            "keywords": [
                "conditional probability",
                "regression",
                "density ratio",
                "ill-posed problem",
                "mutual information",
                "reproducing kernel Hilbert space • function estimation",
                "interpolation function",
                "support vector machines",
                "data adaptation",
                "data balancing",
                "conditional density"
            ],
            "author": [
                "Vladimir Vapnik",
                "Rauf Izmailov",
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/vapnik15a/vapnik15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Truncating the Loop Series Expansion for Belief Propagation",
            "abstract": [
                "Recently, Chertkov and Chernyak (2006b) derived an exact expression for the partition sum (normalization constant) corresponding to a graphical model, which is an expansion around the belief propagation (BP) solution. By adding correction terms to the BP free energy, one for each \"generalized loop\" in the factor graph, the exact partition sum is obtained. However, the usually enormous number of generalized loops generally prohibits summation over all correction terms. In this article we introduce truncated loop series BP (TLSBP), a particular way of truncating the loop series of Chertkov & Chernyak by considering generalized loops as compositions of simple loops. We analyze the performance of TLSBP in different scenarios, including the Ising model on square grids and regular random graphs, and on PROMEDAS, a large probabilistic medical diagnostic system. We show that TLSBP often improves upon the accuracy of the BP solution, at the expense of increased computation time. We also show that the performance of TLSBP strongly depends on the degree of interaction between the variables. For weak interactions, truncating the series leads to significant improvements, whereas for strong interactions it can be ineffective, even if a high number of terms is considered."
            ],
            "keywords": [
                "belief propagation",
                "loop calculus",
                "approximate inference",
                "partition function",
                "Ising grid",
                "random regular graphs",
                "medical diagnosis"
            ],
            "author": [
                "Vicenç Gómez",
                "Joris M Mooij",
                "Hilbert J Kappen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/gomez07a/gomez07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unified Framework for Structured Graph Learning via Spectral Constraints",
            "abstract": [
                "Graph learning from data is a canonical problem that has received substantial attention in the literature. Learning a structured graph is essential for interpretability and identification of the relationships among data. In general, learning a graph with a specific structure is an NP-hard combinatorial problem and thus designing a general tractable algorithm is challenging. Some useful structured graphs include connected, sparse, multi-component, bipartite, and regular graphs. In this paper, we introduce a unified framework for structured graph learning that combines Gaussian graphical model and spectral graph theory. We propose to convert combinatorial structural constraints into spectral constraints on graph matrices and develop an optimization framework based on block majorization-minimization to solve structured graph learning problem. The proposed algorithms are provably convergent and practically amenable for a number of graph based applications such as data clustering. Extensive numerical experiments with both synthetic and real data sets illustrate the effectiveness of the proposed algorithms."
            ],
            "keywords": [
                "Structured graph learning",
                "spectral graph theory",
                "Markov random field",
                "Gaussian graphical model",
                "Laplacian matrix",
                "clustering",
                "adjacency matrix",
                "bipartite structure",
                "spectral similarity"
            ],
            "author": [
                "Sandeep Kumar",
                "Jiaxi Ying",
                "Daniel P Palomar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-276/19-276.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lovász ϑ function, SVMs and Finding Dense Subgraphs",
            "abstract": [
                "In this paper we establish that the Lovász ϑ function on a graph can be restated as a kernel learning problem. We introduce the notion of SVM − ϑ graphs, on which Lovász ϑ function can be approximated well by a Support vector machine (SVM). We show that Erdös-Rényi random G(n, p) graphs are SVM − ϑ graphs for log 4 n n ≤ p < 1. Even if we embed a large clique of size Θ np 1−p in a G(n, p) graph the resultant graph still remains a SVM − ϑ graph. This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. Associated with the ϑ function is the notion of orthogonal labellings. We introduce common orthogonal labellings which extends the idea of orthogonal labellings to multiple graphs. This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude."
            ],
            "keywords": [
                "orthogonal labellings of graphs",
                "planted cliques",
                "random graphs",
                "common dense subgraph"
            ],
            "author": [
                "Vinay Jethava",
                "Anders Martinsson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/jethava13a/jethava13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Confidence Sets with Expected Sizes for Multiclass Classification",
            "abstract": [
                "Multiclass classification problems such as image annotation can involve a large number of classes. In this context, confusion between classes can occur, and single label classification may be misleading. We provide in the present paper a general device that, given an unlabeled dataset and a score function defined as the minimizer of some empirical and convex risk, outputs a set of class labels, instead of a single one. Interestingly, this procedure does not require that the unlabeled dataset explores the whole classes. Even more, the method is calibrated to control the expected size of the output set while minimizing the classification risk. We show the statistical optimality of the procedure and establish rates of convergence under the Tsybakov margin condition. It turns out that these rates are linear on the number of labels. We apply our methodology to convex aggregation of confidence sets based on the V-fold cross validation principle also known as the superlearning principle (van der Laan et al., 2007). We illustrate the numerical performance of the procedure on real data and demonstrate in particular that with moderate expected size, w.r.t. the number of labels, the procedure provides significant improvement of the classification risk."
            ],
            "keywords": [
                "Multiclass Classification",
                "Confidence Sets",
                "Empirical Risk Minimization",
                "Convex Loss",
                "Superlearning"
            ],
            "author": [
                "Christophe Denis",
                "Mohamed Hebiri"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-596/16-596.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Faster Convergence of Cyclic Block Coordinate Descent-type Methods for Strongly Convex Minimization *",
            "abstract": [
                "*. Some preliminary results in this paper were presented at the 19th International Conference on Artificial Intelligence and Statistics (Li et al., 2016)."
            ],
            "keywords": [
                "cyclic block coordinate descent",
                "gradient descent",
                "strongly convex minimization",
                "quadratic minimization",
                "improved iteration complexity"
            ],
            "author": [
                "Xingguo Li",
                "Tuo Zhao",
                "Raman Arora",
                "Han Liu",
                "Mingyi Hong",
                "Tuo Li",
                "Raman Zhao",
                "Han Arora",
                "Hong Liu",
                ", Li",
                "Arora, Liu Hong Zhao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-157/17-157.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dimension Independent Similarity Computation",
            "abstract": [
                "We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high-dimensional sparse vectors. All of our results are provably independent of dimension, meaning that apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension; thus the dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard similarity measures. For Jaccard similarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems with large scale experiments using data from the social networking site Twitter. At time of writing, our algorithms are live in production at twitter.com."
            ],
            "keywords": [
                "cosine",
                "Jaccard",
                "overlap",
                "dice",
                "similarity",
                "MapReduce",
                "dimension independent"
            ],
            "author": [
                "Reza Bosagh Zadeh",
                "Ashish Goel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/bosagh-zadeh13a/bosagh-zadeh13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Greedy Feature Selection for Subspace Clustering",
            "abstract": [
                "Unions of subspaces provide a powerful generalization of single subspace models for collections of high-dimensional data; however, learning multiple subspaces from data is challenging due to the fact that segmentation-the identification of points that live in the same subspace-and subspace estimation must be performed simultaneously. Recently, sparse recovery methods were shown to provide a provable and robust strategy for exact feature selection (EFS)-recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with ℓ 1-minimization, in this paper, we develop sufficient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide significant advantages over NN methods and that the gap between the two approaches is particularly pronounced when the sampling of subspaces in the data set is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble."
            ],
            "keywords": [
                "subspace clustering",
                "unions of subspaces",
                "hybrid linear models",
                "sparse approximation",
                "structured sparsity",
                "nearest neighbors",
                "low-rank approximation"
            ],
            "author": [
                "Eva L Dyer",
                "Aswin C Sankaranarayanan",
                "Richard G Baraniuk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/dyer13a/dyer13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning to Combine Motor Primitives Via Greedy Additive Regression",
            "abstract": [
                "The computational complexities arising in motor control can be ameliorated through the use of a library of motor synergies. We present a new model, referred to as the Greedy Additive Regression (GAR) model, for learning a library of torque sequences, and for learning the coefficients of a linear combination of sequences minimizing a cost function. From the perspective of numerical optimization, the GAR model is interesting because it creates a library of \"local features\"-each sequence in the library is a solution to a single training task-and learns to combine these sequences using a local optimization procedure, namely, additive regression. We speculate that learners with local representational primitives and local optimization procedures will show good performance on nonlinear tasks. The GAR model is also interesting from the perspective of motor control because it outperforms several competing models. Results using a simulated two-joint arm suggest that the GAR model consistently shows excellent performance in the sense that it rapidly learns to perform novel, complex motor tasks. Moreover, its library is overcomplete and sparse, meaning that only a small fraction of the stored torque sequences are used when learning a new movement. The library is also robust in the sense that, after an initial training period, nearly all novel movements can be learned as additive combinations of sequences in the library, and in the sense that it shows good generalization when an arm's dynamics are altered between training and test conditions, such as when a payload is added to the arm. Lastly, the GAR model works well regardless of whether motor tasks are specified in joint space or Cartesian space. We conclude that learning techniques using local primitives and optimization procedures are viable and potentially important methods for motor control and possibly other domains, and that these techniques deserve further examination by the artificial intelligence and cognitive science communities."
            ],
            "keywords": [
                "additive regression",
                "motor primitives",
                "sparse representations"
            ],
            "author": [
                "Manu Chhabra",
                "Robert A Jacobs"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/chhabra08a/chhabra08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Semi-Supervised Novelty Detection *",
            "abstract": [
                "A common setting for novelty detection assumes that labeled examples from the nominal class are available, but that labeled examples of novelties are unavailable. The standard (inductive) approach is to declare novelties where the nominal density is low, which reduces the problem to density level set estimation. In this paper, we consider the setting where an unlabeled and possibly contaminated sample is also available at learning time. We argue that novelty detection in this semi-supervised setting is naturally solved by a general reduction to a binary classification problem. In particular, a detector with a desired false positive rate can be achieved through a reduction to Neyman-Pearson classification. Unlike the inductive approach, semi-supervised novelty detection (SSND) yields detectors that are optimal (e.g., statistically consistent) regardless of the distribution on novelties. Therefore, in novelty detection, unlabeled data have a substantial impact on the theoretical properties of the decision rule. We validate the practical utility of SSND with an extensive experimental study. We also show that SSND provides distribution-free, learning-theoretic solutions to two well known problems in hypothesis testing. First, our results provide a general solution to the general two-sample problem, that is, the problem of determining whether two random samples arise from the same distribution. Second, a specialization of SSND coincides with the standard p-value approach to multiple testing under the so-called random effects model. Unlike standard rejection regions based on thresholded p-values, the general SSND framework allows for adaptation to arbitrary alternative distributions in multiple dimensions."
            ],
            "keywords": [
                "semi-supervised learning",
                "novelty detection",
                "Neyman-Pearson classification",
                "learning reduction",
                "two-sample problem",
                "multiple testing"
            ],
            "author": [
                "Gilles Blanchard",
                "Clayton Scott"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/blanchard10a/blanchard10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularization and the small-ball method II: complexity dependent error rates",
            "abstract": [
                "We study estimation properties of regularized procedures of the form f ∈ argmin f ∈F 1 N N i=1 Y i − f (X i) 2 + λΨ(f) for a convex class of functions F , regularization function Ψ(•) and some well chosen regularization parameter λ, where the given data is an independent sample (X i , Y i) N i=1. We obtain bounds on the L 2 estimation error rate that depend on the complexity of the true model F * := {f ∈ F : Ψ(f) ≤ Ψ(f *)}, where f * ∈ argmin f ∈F E(Y − f (X)) 2 and the (X i , Y i)'s are independent and distributed as (X, Y). Our estimate holds under weak stochastic assumptions-one of which being a small-ball condition satisfied by F-and for rather flexible choices of regularization functions Ψ(•). Moreover, the result holds in the learning theory framework: we do not assume any a-priori connection between the output Y and the input X. As a proof of concept, we apply our general estimation bound to various choices of Ψ, for example, the p and S p-norms (for p ≥ 1), weakp , atomic norms, max-norm and SLOPE. In many cases, the estimation rate almost coincides with the minimax rate in the class F * ."
            ],
            "keywords": [
                "Empirical processes theory",
                "high-dimensional Statistics",
                "regularization",
                "learning theory",
                "minimax rates"
            ],
            "author": [
                "Guillaume Lecué",
                "Shahar Mendelson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-422/16-422.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Proximal Distance Algorithms: Theory and Practice",
            "abstract": [
                "Proximal distance algorithms combine the classical penalty method of constrained minimization with distance majorization. If f (x) is the loss function, and C is the constraint set in a constrained minimization problem, then the proximal distance principle mandates minimizing the penalized loss f (x) + ρ dist(x, C) 2 and following the solution x ρ to its limit as ρ tends to ∞. At each iteration the squared Euclidean distance dist(x, C) 2 is majorized by the spherical quadratic x − P C (x k) 2 , where P C (x k) denotes the projection of the current iterate x k onto C. The minimum of the surrogate function f (x) + ρ x − P C (x k) 2 is given by the proximal map prox ρ −1 f [P C (x k)]. The next iterate x k+1 automatically decreases the original penalized loss for fixed ρ. Since many explicit projections and proximal maps are known, it is straightforward to derive and implement novel optimization algorithms in this setting. These algorithms can take hundreds if not thousands of iterations to converge, but the simple nature of each iteration makes proximal distance algorithms competitive with traditional algorithms. For convex problems, proximal distance algorithms reduce to proximal gradient algorithms and therefore enjoy well understood convergence properties. For nonconvex problems, one can attack convergence by invoking Zangwill's theorem. Our numerical examples demonstrate the utility of proximal distance algorithms in various high-dimensional settings, including a) linear programming, b) constrained least squares, c) projection to the closest kinship matrix, d) projection onto a second-order cone constraint, e) calculation of Horn's copositive matrix index, f) linear complementarity programming, and g) sparse principal components analysis. The proximal distance algorithm in each case is competitive or superior in speed to traditional methods such as the interior point method and the alternating direction method of multipliers (ADMM). Source code for the numerical examples can be found at https://github.com/klkeys/proxdist."
            ],
            "keywords": [
                "constrained optimization",
                "EM algorithm",
                "majorization",
                "projection",
                "proximal operator"
            ],
            "author": [
                "Kevin L Keys",
                "Hua Zhou",
                "Kenneth Lange"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-687/17-687.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Incremental Algorithms for Hierarchical Classification",
            "abstract": [
                "We study the problem of classifying data in a given taxonomy when classifications associated with multiple and/or partial paths are allowed. We introduce a new algorithm that incrementally learns a linear-threshold classifier for each node of the taxonomy. A hierarchical classification is obtained by evaluating the trained node classifiers in a top-down fashion. To evaluate classifiers in our multipath framework, we define a new hierarchical loss function, the H-loss, capturing the intuition that whenever a classification mistake is made on a node of the taxonomy, then no loss should be charged for any additional mistake occurring in the subtree of that node. Making no assumptions on the mechanism generating the data instances, and assuming a linear noise model for the labels, we bound the H-loss of our on-line algorithm in terms of the H-loss of a reference classifier knowing the true parameters of the label-generating process. We show that, in expectation, the excess cumulative H-loss grows at most logarithmically in the length of the data sequence. Furthermore, our analysis reveals the precise dependence of the rate of convergence on the eigenstructure of the data each node observes. Our theoretical results are complemented by a number of experiments on texual corpora. In these experiments we show that, after only one epoch of training, our algorithm performs much better than Perceptron-based hierarchical classifiers, and reasonably close to a hierarchical support vector machine."
            ],
            "keywords": [
                "incremental algorithms",
                "online learning",
                "hierarchical classification",
                "second order perceptron",
                "support vector machines",
                "regret bound",
                "loss function"
            ],
            "author": [
                "Nicolò Cesa-Bianchi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/cesa-bianchi06a/cesa-bianchi06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model-Preserving Sensitivity Analysis for Families of Gaussian Distributions",
            "abstract": [
                "The accuracy of probability distributions inferred using machine-learning algorithms heavily depends on data availability and quality. In practical applications it is therefore fundamental to investigate the robustness of a statistical model to misspecification of some of its underlying probabilities. In the context of graphical models, investigations of robustness fall under the notion of sensitivity analyses. These analyses consist in varying some of the model's probabilities or parameters and then assessing how far apart the original and the varied distributions are. However, for Gaussian graphical models, such variations usually make the original graph an incoherent representation of the model's conditional independence structure. Here we develop an approach to sensitivity analysis which guarantees the original graph remains valid after any probability variation and we quantify the effect of such variations using different measures. To achieve this we take advantage of algebraic techniques to both concisely represent conditional independence and to provide a straightforward way of checking the validity of such relationships. Our methods are demonstrated to be robust and comparable to standard ones, which can break the conditional independence structure of the model, using an artificial example and a medical real-world application."
            ],
            "keywords": [
                "Conditional independence",
                "Gaussian models",
                "Graphical models",
                "Kullback-Leibler divergence",
                "Sensitivity analysis"
            ],
            "author": [
                "Christiane Görgen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-668/18-668.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Acyclic Probabilistic Circuits Using Test Paths",
            "abstract": [
                "We define a model of learning probabilistic acyclic circuits using value injection queries, in which fixed values are assigned to an arbitrary subset of the wires and the value on the single output wire is observed. We adapt the approach of using test paths from the Circuit Builder algorithm (Angluin et al., 2009) to show that there is a polynomial time algorithm that uses value injection queries to learn acyclic Boolean probabilistic circuits of constant fan-in and log depth. We establish upper and lower bounds on the attenuation factor for general and transitively reduced Boolean probabilistic circuits of test paths versus general experiments. We give computational evidence that a polynomial time learning algorithm using general value injection experiments may not do much better than one using test paths. For probabilistic circuits with alphabets of size three or greater, we show that the test path lemmas (Angluin et al., 2009, 2008b) fail utterly. To overcome this obstacle, we introduce function injection queries, in which the values on a wire may be mapped to other values rather than just to themselves or constants, and prove a generalized test path lemma for this case."
            ],
            "keywords": [
                "nonadaptive learning algorithms",
                "probabilistic circuits",
                "causal Bayesian networks",
                "value injection queries",
                "test paths"
            ],
            "author": [
                "Dana Angluin",
                "James Aspnes",
                "Jiang Chen",
                "David Eisenstat",
                "Lev Reyzin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/angluin09a/angluin09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Rates for Persistence Diagram Estimation in Topological Data Analysis",
            "abstract": [
                "Computational topology has recently seen an important development toward data analysis, giving birth to the field of topological data analysis. Topological persistence, or persistent homology, appears as a fundamental tool in this field. In this paper, we study topological persistence in general metric spaces, with a statistical approach. We show that the use of persistent homology can be naturally considered in general statistical frameworks and that persistence diagrams can be used as statistics with interesting convergence properties. Some numerical experiments are performed in various contexts to illustrate our results."
            ],
            "keywords": [
                "persistent homology",
                "convergence rates",
                "topological data analysis"
            ],
            "author": [
                "Frédéric Chazal",
                "Marc Glisse",
                "Catherine Labruère",
                "Bertrand Michel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/chazal15a/chazal15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Surprising properties of dropout in deep networks",
            "abstract": [
                "We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress."
            ],
            "keywords": [
                "Dropout",
                "deep neural networks",
                "regularization",
                "learning theory"
            ],
            "author": [
                "David P Helmbold",
                "Philip M Long"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-549/16-549.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kymatio: Scattering Transforms in Python",
            "abstract": [
                "The wavelet scattering transform is an invariant and stable signal representation suitable for many signal processing and machine learning applications. We present the Kymatio software package, an easy-to-use, high-performance Python implementation of the scattering transform in 1D, 2D, and 3D that is compatible with modern deep learning frameworks, including PyTorch and TensorFlow /Keras. The transforms are implemented on both CPUs"
            ],
            "keywords": [
                "Scattering Transform",
                "GPUs",
                "Wavelets",
                "Convolutional Networks",
                "Invariance"
            ],
            "author": [
                "Mathieu Andreux",
                "Roberto Leonarduzzi",
                "Gaspar Rochette",
                "Louis Thiry",
                "John Zarka",
                "Stéphane Mallat",
                "Joakim Andén",
                "Eugene Belilovsky",
                "Joan Bruna",
                "Vincent Lostanlen",
                "Muawiz Chaudhary",
                "Matthew J Hirn",
                "Edouard Oyallon",
                "Sixin Zhang",
                "Carmine Cella",
                "Michael Eickenberg",
                "Tomás Angles",
                "Georgios Exarchakis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-047/19-047.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Relevant Dimensions in Kernel Feature Spaces",
            "abstract": [
                "We show that the relevant information of a supervised learning problem is contained up to negligible error in a finite number of leading kernel PCA components if the kernel matches the underlying learning problem in the sense that it can asymptotically represent the function to be learned and is sufficiently smooth. Thus, kernels do not only transform data sets such that good generalization can be achieved using only linear discriminant functions, but this transformation is also performed in a manner which makes economical use of feature space dimensions. In the best case, kernels provide efficient implicit representations of the data for supervised learning problems. Practically, we propose an algorithm which enables us to recover the number of leading kernel PCA components relevant for good classification. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to aid in model selection, and (3) to denoise in feature space in order to yield better classification results."
            ],
            "keywords": [
                "kernel methods",
                "feature space",
                "dimension reduction",
                "effective dimensionality"
            ],
            "author": [
                "Mikio L Braun",
                "Joachim M Buhmann",
                "Klaus-Robert Müller",
                "Joachim M Braun",
                "Klaus-Robert Buhmann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/braun08a/braun08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analysis of a Random Forests Model",
            "abstract": [
                "Random forests are a scheme proposed by Leo Breiman in the 2000's for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm. In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman (2004), which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present."
            ],
            "keywords": [
                "random forests",
                "randomization",
                "sparsity",
                "dimension reduction",
                "consistency",
                "rate of convergence"
            ],
            "author": [
                "Gérard Biau"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/biau12a/biau12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structure-Leveraged Methods in Breast Cancer Risk Prediction",
            "abstract": [
                "Predicting breast cancer risk has long been a goal of medical research in the pursuit of precision medicine. The goal of this study is to develop novel penalized methods to improve breast cancer risk prediction by leveraging structure information in electronic health"
            ],
            "keywords": [
                "structure information",
                "breast cancer risk prediction",
                "mammography descriptors",
                "genetic variants",
                "personalized medicine"
            ],
            "author": [
                "Jun Fan",
                "Yirong Wu",
                "Ming Yuan",
                "Jie Liu",
                "Irene M Ong",
                "Peggy Peissig",
                "Elizabeth Burnside",
                "Benjamin M Marlin",
                "Suchi Saria",
                "David Page"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-444/15-444.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Objective Reinforcement Learning using Sets of Pareto Dominating Policies",
            "abstract": [
                "Many real-world problems involve the optimization of multiple, possibly conflicting objectives. Multi-objective reinforcement learning (MORL) is a generalization of standard reinforcement learning where the scalar reward signal is extended to multiple feedback signals, in essence, one for each objective. MORL is the process of learning policies that optimize multiple criteria simultaneously. In this paper, we present a novel temporal difference learning algorithm that integrates the Pareto dominance relation into a reinforcement learning approach. This algorithm is a multi-policy algorithm that learns a set of Pareto dominating policies in a single run. We name this algorithm Pareto Q-learning and it is applicable in episodic environments with deterministic as well as stochastic transition functions. A crucial aspect of Pareto Q-learning is the updating mechanism that bootstraps sets of Q-vectors. One of our main contributions in this paper is a mechanism that separates the expected immediate reward vector from the set of expected future discounted reward vectors. This decomposition allows us to update the sets and to exploit the learned policies consistently throughout the state space. To balance exploration and exploitation during learning, we also propose three set evaluation mechanisms. These three mechanisms evaluate the sets of vectors to accommodate for standard action selection strategies, such as-greedy. More precisely, these mechanisms use multi-objective evaluation principles such as the hypervolume measure, the cardinality indicator and the Pareto dominance relation to select the most promising actions. We experimentally validate the algorithm on multiple environments with two and three objectives and we demonstrate that Pareto Q-learning outperforms current state-of-the-art MORL algorithms with respect to the hypervolume of the obtained policies. We note that (1) Pareto Q-learning is able to learn the entire Pareto front under the usual assumption that each state-action pair is sufficiently sampled, while (2) not being biased by the shape of the Pareto front. Furthermore, (3) the set evaluation mechanisms provide indicative measures for local action selection and (4) the learned policies can be retrieved throughout the state and action space."
            ],
            "keywords": [
                "multiple criteria analysis",
                "multi-objective",
                "reinforcement learning",
                "Pareto sets",
                "hypervolume"
            ],
            "author": [
                "Kristof Van Moffaert",
                "Ann Nowé",
                "Peter Auer",
                "Marcus Hutter",
                "Laurent Orseau"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/vanmoffaert14a/vanmoffaert14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Representer Theorem and Equivalent Degrees of Freedom of SVR",
            "abstract": [
                "Support Vector Regression (SVR) for discrete data is considered. An alternative formulation of the representer theorem is derived. This result is based on the newly introduced notion of pseudoresidual and the use of subdifferential calculus. The representer theorem is exploited to analyze the sensitivity properties of ε-insensitive SVR and introduce the notion of approximate degrees of freedom. The degrees of freedom are shown to play a key role in the evaluation of the optimism, that is the difference between the expected in-sample error and the expected empirical risk. In this way, it is possible to define a C p-like statistic that can be used for tuning the parameters of SVR. The proposed tuning procedure is tested on a simulated benchmark problem and on a real world problem (Boston Housing data set)."
            ],
            "keywords": [
                "statistical learning",
                "reproducing kernel Hilbert spaces",
                "support vector machines",
                "representer theorem",
                "regularization theory"
            ],
            "author": [
                "Francesco Dinuzzo",
                "Marta Neve",
                "Giuseppe De Nicolao",
                "Ugo Pietro Gianazza"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/dinuzzo07a/dinuzzo07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-dimensional Linear Discriminant Analysis Classifier for Spiked Covariance Model *",
            "abstract": [
                "Linear discriminant analysis (LDA) is a popular classifier that is built on the assumption of common population covariance matrix across classes. The performance of LDA depends heavily on the quality of estimating the mean vectors and the population covariance matrix. This issue becomes more challenging in high-dimensional settings where the number of features is of the same order as the number of training samples. Several techniques for estimating the covariance matrix can be found in the literature. One of the most popular approaches are estimators based on using a regularized sample covariance matrix, giving the name regularized LDA (R-LDA) to the corresponding classifier. These estimators are known to be more resilient to the sample noise than the traditional sample covariance matrix estimator. However, the main challenge of the regularization approach is the choice of the optimal regularization parameter, as an arbitrary choice could lead to severe degradation of the classifier performance. In this work, we propose an improved LDA classifier based on the assumption that the covariance matrix follows a spiked covariance model. The main principle of our proposed technique is the design of a parametrized inverse covariance matrix estimator, the parameters of which are shown to be easily optimized. Numerical simulations, using both real and synthetic data, show that the proposed classifier yields better classification performance than the classical R-LDA while requiring lower computational complexity."
            ],
            "keywords": [
                "Linear Discriminant Analysis",
                "Spiked Covariance Models",
                "High-Dimensional Data",
                "Random Matrix Theory"
            ],
            "author": [
                "Houssem Sifaou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-428/19-428.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast SVM Training Using Approximate Extreme Points",
            "abstract": [
                "Applications of non-linear kernel support vector machines (SVMs) to large data sets is seriously hampered by its excessive training time. We propose a modification, called the approximate extreme points support vector machine (AESVM), that is aimed at overcoming this burden. Our approach relies on conducting the SVM optimization over a carefully selected subset, called the representative set, of the training data set. We present analytical results that indicate the similarity of AESVM and SVM solutions. A linear time algorithm based on convex hulls and extreme points is used to compute the representative set in kernel space. Extensive computational experiments on nine data sets compared AESVM to LIBSVM (Chang and Lin, 2011), CVM (Tsang et al., 2005) , BVM (Tsang et al., 2007), LASVM (Bordes et al., 2005), SVM perf (Joachims and Yu, 2009), and the random features method (Rahimi and Recht, 2007). Our AESVM implementation was found to train much faster than the other methods, while its classification accuracy was similar to that of LIBSVM in all cases. In particular, for a seizure detection data set, AESVM training was almost 500 times faster than LIBSVM and LASVM and 20 times faster than CVM and BVM. Additionally, AESVM also gave competitively fast classification times."
            ],
            "keywords": [
                "support vector machines",
                "convex hulls",
                "large scale classification",
                "non-linear kernels",
                "extreme points"
            ],
            "author": [
                "Manu Nandan",
                "Pramod P Khargonekar",
                "Sachin S Talathi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/nandan14a/nandan14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiscale Dictionary Learning: Non-Asymptotic Bounds and Robustness",
            "abstract": [
                "High-dimensional datasets are well-approximated by low-dimensional structures. Over the past decade, this empirical observation motivated the investigation of detection, measurement, and modeling techniques to exploit these low-dimensional intrinsic structures, yielding numerous implications for high-dimensional statistics, machine learning, and signal processing. Manifold learning (where the low-dimensional structure is a manifold) and dictionary learning (where the low-dimensional structure is the set of sparse linear combinations of vectors from a finite dictionary) are two prominent theoretical and computational frameworks in this area. Despite their ostensible distinction, the recently-introduced Geometric Multi-Resolution Analysis (GMRA) provides a robust, computationally efficient, multiscale procedure for simultaneously learning manifolds and dictionaries. In this work, we prove non-asymptotic probabilistic bounds on the approximation error of GMRA for a rich class of data-generating statistical models that includes \"noisy\" manifolds, thereby establishing the theoretical robustness of the procedure and confirming empirical observations. In particular, if a dataset aggregates near a low-dimensional manifold, our results show that the approximation error of the GMRA is completely independent of the ambient dimension. Our work therefore establishes GMRA as a provably fast algorithm for dictionary learning with approximation and sparsity guarantees. We include several numerical experiments confirming these theoretical results, and our theoretical framework provides new tools for assessing the behavior of manifold learning and dictionary learning procedures on a large class of interesting models."
            ],
            "keywords": [
                "dictionary learning",
                "multi-resolution analysis",
                "manifold learning",
                "robustness",
                "sparsity"
            ],
            "author": [
                "Mauro Maggioni",
                "Stanislav Minsker",
                "Nate Strawn"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/maggioni16a/maggioni16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PAC Optimal MDP Planning with Application to Invasive Species Management *",
            "abstract": [
                "In a simulator-defined MDP, the Markovian dynamics and rewards are provided in the form of a simulator from which samples can be drawn. This paper studies MDP planning algorithms that attempt to minimize the number of simulator calls before terminating and outputting a policy that is approximately optimal with high probability. The paper introduces two heuristics for efficient exploration and an improved confidence interval that enables earlier termination with probabilistic guarantees. We prove that the heuristics and the confidence interval are sound and produce with high probability an approximately optimal policy in polynomial time. Experiments on two benchmark problems and two instances of an invasive species management problem show that the improved confidence intervals and the new search heuristics yield reductions of between 8% and 47% in the number of simulator calls required to reach near-optimal policies."
            ],
            "keywords": [
                "invasive species management",
                "Markov decision processes",
                "MDP planning",
                "Good-Turing estimate",
                "reinforcement learning"
            ],
            "author": [
                "Alkaee Majid",
                "Thomas G Dietterich",
                "Mark Crowley",
                "Kim Hall",
                "H Jo Albers",
                "Peter Auer",
                "Marcus Hutter",
                "Laurent Orseau",
                "C Majid",
                "Alkaee Taleghan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/taleghan15a/taleghan15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graph Reduction with Spectral and Cut Guarantees",
            "abstract": [
                "Can one reduce the size of a graph without significantly altering its basic properties? The graph reduction problem is hereby approached from the perspective of restricted spectral approximation, a modification of the spectral similarity measure used for graph sparsification. This choice is motivated by the observation that restricted approximation carries strong spectral and cut guarantees, and that it implies approximation results for unsupervised learning problems relying on spectral embeddings. The article then focuses on coarsening-the most common type of graph reduction. Sufficient conditions are derived for a small graph to approximate a larger one in the sense of restricted approximation. These findings give rise to algorithms that, compared to both standard and advanced graph reduction methods, find coarse graphs of improved quality, often by a large margin, without sacrificing speed."
            ],
            "keywords": [
                "graph reduction and coarsening",
                "spectral methods",
                "unsupervised learning"
            ],
            "author": [
                "Andreas Loukas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-680/18-680.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Introduction to the Special Issue on the Fusion of Domain Knowledge with Data for Decision Support",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "Richard Dybowski",
                "Kathryn B Laskey",
                "James W Myers",
                "Northrop Grumman",
                "Simon Parsons"
            ],
            "ref": "http://www.jmlr.org/papers/volume4/dybowski03a/dybowski03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Considering Cost Asymmetry in Learning Classifiers",
            "abstract": [
                "Receiver Operating Characteristic (ROC) curves are a standard way to display the performance of a set of binary classifiers for all feasible ratios of the costs associated with false positives and false negatives. For linear classifiers, the set of classifiers is typically obtained by training once, holding constant the estimated slope and then varying the intercept to obtain a parameterized set of classifiers whose performances can be plotted in the ROC plane. We consider the alternative of varying the asymmetry of the cost function used for training. We show that the ROC curve obtained by varying both the intercept and the asymmetry, and hence the slope, always outperforms the ROC curve obtained by varying only the intercept. In addition, we present a path-following algorithm for the support vector machine (SVM) that can compute efficiently the entire ROC curve, and that has the same computational complexity as training a single classifier. Finally, we provide a theoretical analysis of the relationship between the asymmetric cost model assumed when training a classifier and the cost model assumed in applying the classifier. In particular, we show that the mismatch between the step function used for testing and its convex upper bounds, usually used for training, leads to a provable and quantifiable difference around extreme asymmetries."
            ],
            "keywords": [
                "support vector machines",
                "receiver operating characteristic (ROC) analysis",
                "linear classification"
            ],
            "author": [
                "Francis R Bach",
                "David Heckerman",
                "Eric Horvitz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/bach06a/bach06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotic Analysis via Stochastic Differential Equations of Gradient Descent Algorithms in Statistical and Computational Paradigms",
            "abstract": [
                "This paper investigates the asymptotic behaviors of gradient descent algorithms (particularly accelerated gradient descent and stochastic gradient descent) in the context of stochastic optimization arising in statistics and machine learning, where objective functions are estimated from available data. We show that these algorithms can be computationally modeled by continuous-time ordinary or stochastic differential equations. We establish gradient flow central limit theorems to describe the limiting dynamic behaviors of these computational algorithms and the large-sample performances of the related statistical procedures, as the number of algorithm iterations and data size both go to infinity, where the gradient flow central limit theorems are governed by some linear ordinary or stochastic differential equations, like time-dependent Ornstein-Uhlenbeck processes. We illustrate that our study can provide a novel unified framework for a joint computational and statistical asymptotic analysis, where the computational asymptotic analysis studies the dynamic behaviors of these algorithms with time (or the number of iterations in the algorithms), the statistical asymptotic analysis investigates the large-sample behaviors of the statistical procedures (like estimators and classifiers) that are computed by applying the algorithms; in fact, the statistical procedures are equal to the limits of the random sequences generated from these iterative algorithms, as the number of iterations goes to infinity. The joint analysis results based on the obtained gradient flow central limit theorems lead to the identification of four factors-learning rate, batch size, gradient covariance, and Hessian-to derive new theories regarding the local minima found by stochastic gradient descent for solving non-convex optimization problems."
            ],
            "keywords": [
                "acceleration",
                "gradient descent",
                "gradient flow central limit theorem",
                "joint asymptotic analysis",
                "joint computational and statistical analysis",
                "Lagrangian flow central limit theorem",
                "mini-batch",
                "optimization",
                "ordinary differential equation",
                "stochastic differential equation",
                "stochastic gradient descent",
                "weak convergence"
            ],
            "author": [
                "Yazhen Wang",
                "Shang Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-245/19-245.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Expectation Consistent Approximate Inference",
            "abstract": [
                "We propose a novel framework for approximations to intractable probabilistic models which is based on a free energy formulation. The approximation can be understood as replacing an average over the original intractable distribution with a tractable one. It requires two tractable probability distributions which are made consistent on a set of moments and encode different features of the original intractable distribution. In this way we are able to use Gaussian approximations for models with discrete or bounded variables which allow us to include non-trivial correlations. These are neglected in many other methods. We test the framework on toy benchmark problems for binary variables on fully connected graphs and 2D grids and compare with other methods, such as loopy belief propagation. Good performance is already achieved by using single nodes as tractable substructures. Significant improvements are obtained when a spanning tree is used instead."
            ],
            "keywords": [],
            "author": [
                "Manfred Opper",
                "Ole Winther"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/opper05a/opper05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structure Discovery in Bayesian Networks by Sampling Partial Orders *",
            "abstract": [
                "We present methods based on Metropolis-coupled Markov chain Monte Carlo (MC 3) and annealed importance sampling (AIS) for estimating the posterior distribution of Bayesian networks. The methods draw samples from an appropriate distribution of partial orders on the nodes, continued by sampling directed acyclic graphs (DAGs) conditionally on the sampled partial orders. We show that the computations needed for the sampling algorithms are feasible as long as the encountered partial orders have relatively few down-sets. While the algorithms assume suitable modularity properties of the priors, arbitrary priors can be handled by dividing the importance weight of each sampled DAG by the number of topological sorts it has-we give a practical dynamic programming algorithm to compute these numbers. Our empirical results demonstrate that the presented partial-order-based samplers are superior to previous Markov chain Monte Carlo methods, which sample DAGs either directly or via linear orders on the nodes. The results also suggest that the convergence rate of the estimators based on AIS are competitive to those of MC. Thus AIS is the preferred method, as it enables easier large-scale parallelization and, in addition, supplies good probabilistic lower bound guarantees for the marginal likelihood of the model."
            ],
            "keywords": [
                "annealed importance sampling",
                "directed acyclic graph",
                "fast zeta transform",
                "linear extension",
                "Markov chain Monte Carlo"
            ],
            "author": [
                "Teppo Niinimäki",
                "Pekka Parviainen",
                "Mikko Koivisto"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-140/15-140.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Feature Selection via Dependence Maximization",
            "abstract": [
                "We introduce a framework for feature selection based on dependence maximization between the selected features and the labels of an estimation problem, using the Hilbert-Schmidt Independence Criterion. The key idea is that good features should be highly dependent on the labels. Our approach leads to a greedy procedure for feature selection. We show that a number of existing feature selectors are special cases of this framework. Experiments on both artificial and real-world data show that our feature selector works well in practice."
            ],
            "keywords": [
                "kernel methods",
                "feature selection",
                "independence measure",
                "Hilbert-Schmidt independence criterion",
                "Hilbert space embedding of distribution"
            ],
            "author": [
                "Le Song",
                "Alex Smola",
                "Arthur Gretton",
                "Justin Bedo",
                "Karsten Borgwardt",
                "Tuebingen Mpg De"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/song12a/song12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Combined 1 and Greedy 0 Penalized Least Squares for Linear Model Selection *",
            "abstract": [
                "We introduce a computationally effective algorithm for a linear model selection consisting of three steps: screening-ordering-selection (SOS). Screening of predictors is based on the thresholded Lasso that is 1 penalized least squares. The screened predictors are then fitted using least squares (LS) and ordered with respect to their |t| statistics. Finally, a model is selected using greedy generalized information criterion (GIC) that is penalized LS in a nested family induced by the ordering. We give non-asymptotic upper bounds on error probability of each step of the SOS algorithm in terms of both penalties. Then we obtain selection consistency for different (n, p) scenarios under conditions which are needed for screening consistency of the Lasso. Our error bounds and numerical experiments show that SOS is worth considering alternative for multi-stage convex relaxation, the latest quasiconvex penalized LS. For the traditional setting (n > p) we give Sanov-type bounds on the error probabilities of the ordering-selection algorithm. It is surprising consequence of our bounds that the selection error of greedy GIC is asymptotically not larger than of exhaustive GIC."
            ],
            "keywords": [
                "linear model selection",
                "penalized least squares",
                "Lasso",
                "generalized information criterion",
                "greedy search",
                "multi-stage convex relaxation"
            ],
            "author": [
                "Piotr Pokarowski",
                "Jan Mielniczuk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/pokarowski15a/pokarowski15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exceptional Rotations of Random Graphs: A VC Theory",
            "abstract": [
                "In this paper we explore maximal deviations of large random structures from their typical behavior. We introduce a model for a high-dimensional random graph process and ask analogous questions to those of Vapnik and Chervonenkis for deviations of averages: how \"rich\" does the process have to be so that one sees atypical behavior. In particular, we study a natural process of Erdős-Rényi random graphs indexed by unit vectors in R d. We investigate the deviations of the process with respect to three fundamental properties: clique number, chromatic number, and connectivity. In all cases we establish upper and lower bounds for the minimal dimension d that guarantees the existence of \"exceptional directions\" in which the random graph behaves atypically with respect to the property. For each of the three properties, four theorems are established, to describe upper and lower bounds for the threshold dimension in the subcritical and supercritical regimes."
            ],
            "keywords": [
                "random graphs",
                "VC theory",
                "clique number",
                "chromatic number",
                "connectivity"
            ],
            "author": [
                "Louigi Addario-Berry",
                "Shankar Bhamidi",
                "Sébastien Bubeck",
                "Luc Devroye",
                "Roberto Imbuzeiro Oliveira",
                "Estrada Da",
                "Alex Gammerman",
                "Vladimir Vovk",
                "Gábor Lugosi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/addarioberry15a/addarioberry15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Reduced PC-Algorithm: Improved Causal Structure Learning in Large Random Networks",
            "abstract": [
                "We consider the task of estimating a high-dimensional directed acyclic graph, given observations from a linear structural equation model with arbitrary noise distribution. By exploiting properties of common random graphs, we develop a new algorithm that requires conditioning only on small sets of variables. The proposed algorithm, which is essentially a modified version of the PC-Algorithm, offers significant gains in both computational complexity and estimation accuracy. In particular, it results in more efficient and accurate estimation in large networks containing hub nodes, which are common in biological systems. We prove the consistency of the proposed algorithm, and show that it also requires a less stringent faithfulness assumption than the PC-Algorithm. Simulations in low and high-dimensional settings are used to illustrate these findings. An application to gene expression data suggests that the proposed algorithm can identify a greater number of clinically relevant genes than current methods."
            ],
            "keywords": [
                "causal discovery",
                "directed acyclic graphs",
                "faithfulness",
                "high dimensions",
                "random graphs"
            ],
            "author": [
                "Arjun Sondhi",
                "Ali Shojaie",
                "* Arjun Sondhi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-601/17-601.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Causal Network Learning for Finding Pairs of Total and Direct Effects",
            "abstract": [
                "In observational studies, it is important to evaluate not only the total effect but also the direct and indirect effects of a treatment variable on a response variable. In terms of local structural learning of causal networks, we try to find all possible pairs of total and direct causal effects, which can further be used to calculate indirect causal effects. An intuitive global learning approach is first to find an essential graph over all variables representing all Markov equivalent causal networks, and then enumerate all equivalent networks and estimate a pair of the total and direct effects for each of them. However, it could be inefficient to learn an essential graph and enumerate equivalent networks when the true causal graph is large. In this paper, we propose a local learning approach instead. In the local learning approach, we first learn locally a chain component containing the treatment. Then, if necessary, we learn locally a chain component containing the response. Next, we locally enumerate all possible pairs of the treatment's parents and the response's parents. Finally based on these pairs, we find all possible pairs of total and direct effects of the treatment on the response."
            ],
            "keywords": [
                "causal networks",
                "directed acyclic graphs",
                "total effects",
                "direct effects",
                "indirect effects"
            ],
            "author": [
                "Yue Liu",
                "Chunchen Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-083/18-083.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Characterizing the Sample Complexity of Pure Private Learners *",
            "abstract": [
                "Kasiviswanathan et al. (FOCS 2008) defined private learning as a combination of PAC learning and differential privacy. Informally, a private learner is applied to a collection of labeled individual information and outputs a hypothesis while preserving the privacy of each individual. Kasiviswanathan et al. left open the question of characterizing the sample complexity of private learners. We give a combinatorial characterization of the sample size sufficient and necessary to learn a class of concepts under pure differential privacy. This characterization is analogous to the well known characterization of the sample complexity of non-private learning in terms of the VC dimension of the concept class. We introduce the notion of probabilistic representation of a concept class, and our new complexity measure RepDim corresponds to the size of the smallest probabilistic representation of the concept class. We show that any private learning algorithm for a concept class C with sample complexity m implies RepDim(C) = O(m), and that there exists a private learning algorithm with sample complexity m = O(RepDim(C)). We further demonstrate that a similar characterization holds for the database size needed for computing a large class of optimization problems under pure differential privacy, and also for the well studied problem of private data release."
            ],
            "keywords": [
                "Differential privacy",
                "PAC learning",
                "Sample complexity"
            ],
            "author": [
                "Amos Beimel",
                "Uri Stemmer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-269/18-269.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models",
            "abstract": [
                "Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that can generically produce power laws, breaking generative models into two stages. The first stage, the generator, can be any standard probabilistic model, while the second stage, the adaptor, transforms the word frequencies of this model to provide a closer match to natural language. We show that two commonly used Bayesian models, the Dirichlet-multinomial model and the Dirichlet process, can be viewed as special cases of our framework. We discuss two stochastic processes-the Chinese restaurant process and its two-parameter generalization based on the Pitman-Yor process-that can be used as adaptors in our framework to produce power-law distributions over word frequencies. We show that these adaptors justify common estimation procedures based on logarithmic or inverse-power transformations of empirical frequencies. In addition, taking the Pitman-Yor Chinese restaurant process as an adaptor justifies the appearance of type frequencies in formal analyses of natural language and improves the performance of a model for unsupervised learning of morphology."
            ],
            "keywords": [
                "nonparametric Bayes",
                "Pitman-Yor process",
                "language model",
                "unsupervised"
            ],
            "author": [
                "Sharon Goldwater",
                "Thomas L Griffiths",
                "Tom Griffiths",
                "Berkeley Edu",
                "Mark Johnson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/goldwater11a/goldwater11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bootstrap-Based Regularization for Low-Rank Matrix Estimation",
            "abstract": [
                "We develop a flexible framework for low-rank matrix estimation that allows us to transform noise models into regularization schemes via a simple bootstrap algorithm. Effectively, our procedure seeks an autoencoding basis for the observed matrix that is stable with respect to the specified noise model; we call the resulting procedure a stable autoencoder. In the simplest case, with an isotropic noise model, our method is equivalent to a classical singular value shrinkage estimator. For non-isotropic noise models-e.g., Poisson noisethe method does not reduce to singular value shrinkage, and instead yields new estimators that perform well in experiments. Moreover, by iterating our stable autoencoding scheme, we can automatically generate low-rank estimates without specifying the target rank as a tuning parameter."
            ],
            "keywords": [
                "Correspondence analysis",
                "empirical Bayes",
                "Lévy bootstrap",
                "singular-value decomposition"
            ],
            "author": [
                "Julie Josse",
                "Stefan Wager"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-534/14-534.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "When Are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity",
            "abstract": [
                "Overcomplete latent representations have been very popular for unsupervised feature learning in recent years. In this paper, we specify which overcomplete models can be identified given observable moments of a certain order. We consider probabilistic admixture or topic models in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary. While general overcomplete topic models are not identifiable, we establish generic identifiability under a constraint, referred to as topic persistence. Our sufficient conditions for identifiability involve a novel set of \"higher order\" expansion conditions on the topic-word matrix or the population structure of the model. This set of higher-order expansion conditions allow for overcomplete models, and require the existence of a perfect matching from latent topics to higher order observed words. We establish that random structured topic models are identifiable w.h.p. in the overcomplete regime. Our identifiability results allows for general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework. Our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of Tucker decompositions, but is more general than the Candecomp/Parafac (CP) decomposition."
            ],
            "keywords": [
                "overcomplete representations",
                "topic models",
                "generic identifiability",
                "tensor decomposition"
            ],
            "author": [
                "Animashree Anandkumar",
                "Daniel Hsu",
                "Majid Janzamin",
                "Sham Kakade",
                "Hsu, Janzamin Kakade Anandkumar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/anandkumar15a/anandkumar15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Divisive Information-Theoretic Feature Clustering Algorithm for Text Classification",
            "abstract": [
                "High dimensionality of text can be a deterrent in applying complex learners such as Support Vector Machines to the task of text classification. Feature clustering is a powerful alternative to feature selection for reducing the dimensionality of text data. In this paper we propose a new informationtheoretic divisive algorithm for feature/word clustering and apply it to text classification. Existing techniques for such \"distributional clustering\" of words are agglomerative in nature and result in (i) sub-optimal word clusters and (ii) high computational cost. In order to explicitly capture the optimality of word clusters in an information theoretic framework, we first derive a global criterion for feature clustering. We then present a fast, divisive algorithm that monotonically decreases this objective function value. We show that our algorithm minimizes the \"within-cluster Jensen-Shannon divergence\" while simultaneously maximizing the \"between-cluster Jensen-Shannon divergence\". In comparison to the previously proposed agglomerative strategies our divisive algorithm is much faster and achieves comparable or higher classification accuracies. We further show that feature clustering is an effective technique for building smaller class models in hierarchical classification. We present detailed experimental results using Naive Bayes and Support Vector Machines on the 20Newsgroups data set and a 3-level hierarchy of HTML documents collected from the Open Directory project (www.dmoz.org)."
            ],
            "keywords": [
                "Information theory",
                "Feature Clustering",
                "Classification",
                "Entropy",
                "Kullback-Leibler Divergence",
                "Mutual Information",
                "Jensen-Shannon Divergence"
            ],
            "author": [
                "Inderjit S Dhillon",
                "Subramanyam Mallela",
                "Rahul Kumar",
                "Isabelle Guyon",
                "André Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/dhillon03a/dhillon03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing",
            "abstract": [
                "Nonnegative matrix factorization (NMF) has become a very popular technique in machine learning because it automatically extracts meaningful features through a sparse and part-based representation. However, NMF has the drawback of being highly ill-posed, that is, there typically exist many different but equivalent factorizations. In this paper, we introduce a completely new way to obtaining more well-posed NMF problems whose solutions are sparser. Our technique is based on the preprocessing of the nonnegative input data matrix, and relies on the theory of M-matrices and the geometric interpretation of NMF. This approach provably leads to optimal and sparse solutions under the separability assumption of Donoho and Stodden (2003), and, for rank-three matrices, makes the number of exact factorizations finite. We illustrate the effectiveness of our technique on several image data sets."
            ],
            "keywords": [
                "nonnegative matrix factorization",
                "data preprocessing",
                "uniqueness",
                "sparsity",
                "inversepositive matrices"
            ],
            "author": [
                "Nicolas Gillis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/gillis12a/gillis12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantifying Uncertainty in Random Forests via Confidence Intervals and Hypothesis Tests",
            "abstract": [
                "This work develops formal statistical inference procedures for predictions generated by supervised learning ensembles. Ensemble methods based on bootstrapping, such as bagging and random forests, have improved the predictive accuracy of individual trees, but fail to provide a framework in which distributional results can be easily determined. Instead of aggregating full bootstrap samples, we consider predicting by averaging over trees built on subsamples of the training set and demonstrate that the resulting estimator takes the form of a U-statistic. As such, predictions for individual feature vectors are asymptotically normal, allowing for confidence intervals to accompany predictions. In practice, a subset of subsamples is used for computational speed; here our estimators take the form of incomplete U-statistics and equivalent results are derived. We further demonstrate that this setup provides a framework for testing the significance of features. Moreover, the internal estimation method we develop allows us to estimate the variance parameters and perform these inference procedures at no additional computational cost. Simulations and illustrations on a real data set are provided."
            ],
            "keywords": [
                "trees",
                "u-statistics",
                "bagging",
                "subbagging",
                "random forests"
            ],
            "author": [
                "Lucas Mentch",
                "Giles Hooker"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-168/14-168.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Bradley-Terry Models and Multi-Class Probability Estimates",
            "abstract": [
                "The Bradley-Terry model for obtaining individual skill from paired comparisons has been popular in many areas. In machine learning, this model is related to multi-class probability estimates by coupling all pairwise classification results. Error correcting output codes (ECOC) are a general framework to decompose a multi-class problem to several binary problems. To obtain probability estimates under this framework, this paper introduces a generalized Bradley-Terry model in which paired individual comparisons are extended to paired team comparisons. We propose a simple algorithm with convergence proofs to solve the model and obtain individual skill. Experiments on synthetic and real data demonstrate that the algorithm is useful for obtaining multi-class probability estimates. Moreover, we discuss four extensions of the proposed model: 1) weighted individual skill, 2) home-field advantage, 3) ties, and 4) comparisons with more than two teams."
            ],
            "keywords": [
                "Bradley-Terry model",
                "probability estimates",
                "error correcting output codes",
                "support vector machines"
            ],
            "author": [
                "Tzu-Kuo Huang",
                "Chih-Jen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/huang06a/huang06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MOA: Massive Online Analysis",
            "abstract": [
                "Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA includes a collection of offline and online methods as well as tools for evaluation. In particular, it implements boosting, bagging, and Hoeffding Trees, all with and without Naïve Bayes classifiers at the leaves. MOA supports bi-directional interaction with WEKA, the Waikato Environment for Knowledge Analysis, and is released under the GNU GPL license."
            ],
            "keywords": [
                "data streams",
                "classification",
                "ensemble methods",
                "java",
                "machine learning software"
            ],
            "author": [
                "Albert Bifet",
                "Geoff Holmes",
                "Richard Kirkby",
                "Bernhard Pfahringer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/bifet10a/bifet10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GADMM: Fast and Communication Efficient Framework for Distributed Machine Learning",
            "abstract": [
                "When the data is distributed across multiple servers, lowering the communication cost between the servers (or workers) while solving the distributed learning problem is an important problem and is the focus of this paper. In particular, we propose a fast, and communication-efficient decentralized framework to solve the distributed machine learning (DML) problem. The proposed algorithm, Group Alternating Direction Method of Multipliers (GADMM) is based on the Alternating Direction Method of Multipliers (ADMM) framework. The key novelty in GADMM is that it solves the problem in a decentralized topology where at most half of the workers are competing for the limited communication resources at any given time. Moreover, each worker exchanges the locally trained model only with two neighboring workers, thereby training a global model with a lower amount of communication overhead in each exchange. We prove that GADMM converges to the optimal solution for convex loss functions, and numerically show that it converges faster and more communication-efficient than the state-of-the-art communication-efficient algorithms such as the Lazily Aggregated Gradient (LAG) and dual averaging, in linear and logistic regression tasks on synthetic and real datasets. Furthermore, we propose Dynamic GADMM (D-GADMM), a variant of GADMM, and prove its convergence under the time-varying network topology of the workers."
            ],
            "keywords": [],
            "author": [
                "Anis Elgabli",
                "Jihong Park",
                "Vaneet Aggarwal"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-718/19-718.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence of Gaussian Belief Propagation Under General Pairwise Factorization: Connecting Gaussian MRF with Pairwise Linear Gaussian Model",
            "abstract": [
                "Gaussian belief propagation (BP) is a low-complexity and distributed method for computing the marginal distributions of a high-dimensional joint Gaussian distribution. However, Gaussian BP is only guaranteed to converge in singly connected graphs and may fail to converge in loopy graphs. Therefore, convergence analysis is a core topic in Gaussian BP. Existing conditions for verifying the convergence of Gaussian BP are all tailored for one particular pairwise factorization of the distribution in Gaussian Markov random field (MRF) and may not be valid for another pairwise factorization. On the other hand, convergence conditions of Gaussian BP in pairwise linear Gaussian model are developed independently from those in Gaussian MRF, making the convergence results highly scattered with diverse settings. In this paper, the convergence condition of Gaussian BP is investigated under a general pairwise factorization, which includes Gaussian MRF and pairwise linear Gaussian model as special cases. Upon this, existing convergence conditions in Gaussian MRF are extended to any pairwise factorization. Moreover, the newly established link between Gaussian MRF and pairwise linear Gaussian model reveals an easily verifiable sufficient convergence condition in pairwise linear Gaussian model, which provides a unified criterion for assessing the convergence of Gaussian BP in multiple applications. Numerical examples are presented to corroborate the theoretical results of this paper."
            ],
            "keywords": [
                "Gaussian belief propagation",
                "convergence analysis",
                "Gaussian Markov random field",
                "pairwise linear Gaussian model",
                "pairwise factorization"
            ],
            "author": [
                "Bin Li",
                "Yik-Chung Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-133/18-133.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Composite Self-Concordant Minimization",
            "abstract": [
                "We propose a variable metric framework for minimizing the sum of a self-concordant function and a possibly non-smooth convex function, endowed with an easily computable proximal operator. We theoretically establish the convergence of our framework without relying on the usual Lipschitz gradient assumption on the smooth part. An important highlight of our work is a new set of analytic step-size selection and correction procedures based on the structure of the problem. We describe concrete algorithmic instances of our framework for several interesting applications and demonstrate them numerically on both synthetic and real data."
            ],
            "keywords": [
                "proximal-gradient/Newton method",
                "composite minimization",
                "self-concordance",
                "sparse convex optimization",
                "graph learning"
            ],
            "author": [
                "Quoc Tran-Dinh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/trandihn15a/trandihn15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Using Local Dependencies within Batches to Improve Large Margin Classifiers",
            "abstract": [
                "Most classification methods assume that the samples are drawn independently and identically from an unknown data generating distribution, yet this assumption is violated in several real life problems. In order to relax this assumption, we consider the case where batches or groups of samples may have internal correlations, whereas the samples from different batches may be considered to be uncorrelated. This paper introduces three algorithms for classifying all the samples in a batch jointly: one based on a probabilistic formulation, and two based on mathematical programming analysis. Experiments on three real-life computer aided diagnosis (CAD) problems demonstrate that the proposed algorithms are significantly more accurate than a naive support vector machine which ignores the correlations among the samples."
            ],
            "keywords": [
                "batch-wise classification",
                "support vector machine",
                "linear programming",
                "machine learning",
                "statistical methods",
                "unconstrained optimization"
            ],
            "author": [
                "Volkan Vural",
                "Glenn Fung",
                "Jennifer G Dy",
                "Bharat Rao",
                "@ Siemens"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/vural09a/vural09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Margin Trees for High-dimensional Classification",
            "abstract": [
                "We propose a method for the classification of more than two classes, from high-dimensional features. Our approach is to build a binary decision tree in a top-down manner, using the optimal margin classifier at each split. We implement an exact greedy algorithm for this task, and compare its performance to less greedy procedures based on clustering of the matrix of pairwise margins. We compare the performance of the \"margin tree\" to the closely related \"all-pairs\" (one versus one) support vector machine, and nearest centroids on a number of cancer microarray data sets. We also develop a simple method for feature selection. We find that the margin tree has accuracy that is competitive with other methods and offers additional interpretability in its putative grouping of the classes."
            ],
            "keywords": [
                "maximum margin classifier",
                "support vector machine",
                "decision tree",
                "CART"
            ],
            "author": [
                "Robert Tibshirani",
                "Trevor Hastie",
                "Hastie @ Stanford Stat"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/tibshirani07a/tibshirani07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Statistical Learning Approach to Modal Regression",
            "abstract": [
                "This paper studies the nonparametric modal regression problem systematically from a statistical learning viewpoint. Originally motivated by pursuing a theoretical understanding of the maximum correntropy criterion based regression (MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is essentially modal regression. We show that the nonparametric modal regression problem can be approached via the classical empirical risk minimization. Some efforts are then made to develop a framework for analyzing and implementing modal regression. For instance, the modal regression function is described, the modal regression risk is defined explicitly and its Bayes rule is characterized; for the sake of computational tractability, the surrogate modal regression risk, which is termed as the generalization risk in our study, is introduced. On the theoretical side, the excess modal regression risk, the excess generalization risk, the function estimation error, and the relations among the above three quantities are studied rigorously. It turns out that under mild conditions, function estimation consistency and convergence may be pursued in modal regression as in vanilla regression protocols such as mean regression, median regression, and quantile regression. On the practical side, the implementation issues of modal regression including the computational algorithm and the selection of the tuning parameters are discussed. Numerical validations on modal regression are also conducted to verify our findings."
            ],
            "keywords": [
                "Nonparametric modal regression",
                "empirical risk minimization",
                "generalization bounds",
                "kernel density estimation",
                "statistical learning theory"
            ],
            "author": [
                "Yunlong Feng",
                "Johan A K Suykens"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/17-068/17-068.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric Network Models for Link Prediction",
            "abstract": [
                "Many data sets can be represented as a sequence of interactions between entities-for example communications between individuals in a social network, protein-protein interactions or DNA-protein interactions in a biological context, or vehicles' journeys between cities. In these contexts, there is often interest in making predictions about future interactions, such as who will message whom. A popular approach to network modeling in a Bayesian context is to assume that the observed interactions can be explained in terms of some latent structure. For example, traffic patterns might be explained by the size and importance of cities, and social network interactions might be explained by the social groups and interests of individuals. Unfortunately, while elucidating this structure can be useful, it often does not directly translate into an effective predictive tool. Further, many existing approaches are not appropriate for sparse networks, a class that includes many interesting real-world situations. In this paper, we develop models for sparse networks that combine structure elucidation with predictive performance. We use a Bayesian nonparametric approach, which allows us to predict interactions with entities outside our training set, and allows the both the latent dimensionality of the model and the number of nodes in the network to grow in expectation as we see more data. We demonstrate that we can capture latent structure while maintaining predictive power, and discuss possible extensions."
            ],
            "keywords": [
                "Dirichlet process",
                "networks",
                "Bayesian nonparametrics",
                "Gibbs sampling",
                "hierarchical modeling"
            ],
            "author": [
                "Sinead A Williamson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-032/16-032.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Comparing Hard and Overlapping Clusterings",
            "abstract": [
                "Similarity measures for comparing clusterings is an important component, e.g., of evaluating clustering algorithms, for consensus clustering, and for clustering stability assessment. These measures have been studied for over 40 years in the domain of exclusive hard clusterings (exhaustive and mutually exclusive object sets). In the past years, the literature has proposed measures to handle more general clusterings (e.g., fuzzy/probabilistic clusterings). This paper provides an overview of these new measures and discusses their drawbacks. We ultimately develop a corrected-for-chance measure (13AGRI) capable of comparing exclusive hard, fuzzy/probabilistic, non-exclusive hard, and possibilistic clusterings. We prove that 13AGRI and the adjusted Rand index (ARI, by Hubert and Arabie) are equivalent in the exclusive hard domain. The reported experiments show that only 13AGRI could provide both a fine-grained evaluation across clusterings with different numbers of clusters and a constant evaluation between random clusterings, showing all the four desirable properties considered here. We identified a high correlation between 13AGRI applied to fuzzy clusterings and ARI applied to hard exclusive clusterings over 14 real data sets from the UCI repository, which corroborates the validity of 13AGRI fuzzy clustering evaluation. 13AGRI also showed good results as a clustering stability statistic for solutions produced by the expectation maximization algorithm for Gaussian mixture. Implementation and supplementary figures can be found at http://sn.im/25a9h8u."
            ],
            "keywords": [
                "overlapping",
                "fuzzy",
                "probabilistic",
                "clustering evaluation"
            ],
            "author": [
                "Danilo Horta",
                "Ricardo J G B Campello"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/horta15a/horta15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Refinement of Operator-valued Reproducing Kernels",
            "abstract": [
                "This paper studies the construction of a refinement kernel for a given operator-valued reproducing kernel such that the vector-valued reproducing kernel Hilbert space of the refinement kernel contains that of the given kernel as a subspace. The study is motivated from the need of updating the current operator-valued reproducing kernel in multi-task learning when underfitting or overfitting occurs. Numerical simulations confirm that the established refinement kernel method is able to meet this need. Various characterizations are provided based on feature maps and vector-valued integral representations of operator-valued reproducing kernels. Concrete examples of refining translation invariant and finite Hilbert-Schmidt operator-valued reproducing kernels are provided. Other examples include refinement of Hessian of scalar-valued translation-invariant kernels and transformation kernels. Existence and properties of operator-valued reproducing kernels preserved during the refinement process are also investigated."
            ],
            "keywords": [
                "vector-valued reproducing kernel Hilbert spaces",
                "operator-valued reproducing kernels",
                "refinement",
                "embedding",
                "translation invariant kernels",
                "Hessian of Gaussian kernels",
                "Hilbert-Schmidt kernels",
                "numerical experiments"
            ],
            "author": [
                "Haizhang Zhang",
                "Qinghui Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/zhang12a/zhang12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Bounds for Johnson-Lindenstrauss Transformations",
            "abstract": [
                "In 1984, Johnson and Lindenstrauss proved that any finite set of data in a high-dimensional space can be projected to a lower-dimensional space while preserving the pairwise Euclidean distances between points up to a bounded relative error. If the desired dimension of the image is too small, however, Kane, Meka, and Nelson (2011) and Jayram and Woodruff (2013) proved that such a projection does not exist. In this paper, we provide a precise asymptotic threshold for the dimension of the image, above which, there exists a projection preserving the Euclidean distance, but, below which, there does not exist such a projection."
            ],
            "keywords": [
                "Johnson-Lindenstrauss transformation",
                "Dimension reduction",
                "Phase transition",
                "Asymptotic threshold"
            ],
            "author": [
                "Michael Burr",
                "Fiona Knoll"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-264/18-264.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Derivative-Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems",
            "abstract": [
                "We study derivative-free methods for policy optimization over the class of linear policies. We focus on characterizing the convergence rate of these methods when applied to linearquadratic systems, and study various settings of driving noise and reward feedback. Our main theoretical result provides an explicit bound on the sample or evaluation complexity: we show that these methods are guaranteed to converge to within any pre-specified tolerance of the optimal policy with a number of zero-order evaluations that is an explicit polynomial of the error tolerance, dimension, and curvature properties of the problem. Our analysis reveals some interesting differences between the settings of additive driving noise and random initialization, as well as the settings of one-point and two-point reward feedback. Our theory is corroborated by simulations of derivative-free methods in application to these systems. Along the way, we derive convergence rates for stochastic zero-order optimization algorithms when applied to a certain class of non-convex problems."
            ],
            "keywords": [
                "Derivative-Free Optimization",
                "Linear Quadratic Control",
                "Non-Convex Optimization"
            ],
            "author": [
                "Dhruv Malik",
                "Kush Bhatia",
                "Koulik Khamaru",
                "Peter L Bartlett",
                "Martin J Wainwright",
                "Voleon Group",
                "Ashwin Pananjady"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-198/19-198.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularization via Mass Transportation",
            "abstract": [
                "The goal of regression and classification methods in supervised learning is to minimize the empirical risk, that is, the expectation of some loss function quantifying the prediction error under the empirical distribution. When facing scarce training data, overfitting is typically mitigated by adding regularization terms to the objective that penalize hypothesis complexity. In this paper we introduce new regularization techniques using ideas from distributionally robust optimization, and we give new probabilistic interpretations to existing techniques. Specifically, we propose to minimize the worst-case expected loss, where the worst case is taken over the ball of all (continuous or discrete) distributions that have a bounded transportation distance from the (discrete) empirical distribution. By choosing the radius of this ball judiciously, we can guarantee that the worst-case expected loss provides an upper confidence bound on the loss on test data, thus offering new generalization bounds. We prove that the resulting regularized learning problems are tractable and can be tractably kernelized for many popular loss functions. The proposed approach to regluarization is also extended to neural networks. We validate our theoretical out-of-sample guarantees through simulated and empirical experiments."
            ],
            "keywords": [
                "Distributionally robust optimization",
                "optimal transport",
                "Wasserstein distance",
                "robust optimization",
                "regularization"
            ],
            "author": [
                "Daniel Kuhn",
                "Peyman Mohajerin Esfahani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-633/17-633.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Probabilistic Learning on Graphs via Contextual Architectures",
            "abstract": [
                "We propose a novel methodology for representation learning on graph-structured data, in which a stack of Bayesian Networks learns different distributions of a vertex's neighbourhood. Through an incremental construction policy and layer-wise training, we can build deeper architectures with respect to typical graph convolutional neural networks, with benefits in terms of context spreading between vertices. First, the model learns from graphs via maximum likelihood estimation without using target labels. Then, a supervised readout is applied to the learned graph embeddings to deal with graph classification and vertex classification tasks, showing competitive results against neural models for graphs. The computational complexity is linear in the number of edges, facilitating learning on large scale data sets. By studying how depth affects the performances of our model, we discover that a broader context generally improves performances. In turn, this leads to a critical analysis of some benchmarks used in literature."
            ],
            "keywords": [
                "Structured domains",
                "deep graph networks",
                "graph neural networks",
                "deep learning",
                "maximum likelihood",
                "graph classification",
                "node classification"
            ],
            "author": [
                "Davide Bacciu",
                "Federico Errica",
                "Alessio Micheli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-470/19-470.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Best Practices for Scientific Research on Neural Architecture Search",
            "abstract": [
                "Finding a well-performing architecture is often tedious for both deep learning practitioners and researchers, leading to tremendous interest in the automation of this task by means of neural architecture search (NAS). Although the community has made major strides in developing better NAS methods, the quality of scientific empirical evaluations in the young field of NAS is still lacking behind that of other areas of machine learning. To address this issue, we describe a set of possible issues and ways to avoid them, leading to the NAS best practices checklist available at http://automl.org/nas_checklist.pdf."
            ],
            "keywords": [
                "Neural Architecture Search",
                "Scientific Best Practices",
                "Empirical Evaluation"
            ],
            "author": [
                "Marius Lindauer",
                "Frank Hutter"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-056/20-056.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-class Protein Classification Using Adaptive Codes",
            "abstract": [
                "Predicting a protein's structural class from its amino acid sequence is a fundamental problem in computational biology. Recent machine learning work in this domain has focused on developing new input space representations for protein sequences, that is, string kernels, some of which give state-of-the-art performance for the binary prediction task of discriminating between one class and all the others. However, the underlying protein classification problem is in fact a huge multiclass problem, with over 1000 protein folds and even more structural subcategories organized into a hierarchy. To handle this challenging many-class problem while taking advantage of progress on the binary problem, we introduce an adaptive code approach in the output space of one-vsthe-rest prediction scores. Specifically, we use a ranking perceptron algorithm to learn a weighting of binary classifiers that improves multi-class prediction with respect to a fixed set of output codes. We use a cross-validation setup to generate output vectors for training, and we define codes that capture information about the protein structural hierarchy. Our code weighting approach significantly improves on the standard one-vs-all method for two difficult multi-class protein classification problems: remote homology detection and fold recognition. Our algorithm also outperforms a previous code learning approach due to Crammer and Singer, trained here using a perceptron, when the dimension of the code vectors is high and the number of classes is large. Finally, we compare against PSI-BLAST, one of the most widely used methods in protein sequence analysis, and find that our method strongly outperforms it on every structure clas-*. The first two authors contributed equally to this work."
            ],
            "keywords": [
                "multi-class classification",
                "error-correcting output codes",
                "structured outputs"
            ],
            "author": [
                "Iain Melvin",
                "Eugene Ie",
                "Jason Weston",
                "William Stafford Noble",
                "Christina Leslie",
                "IE Christina Leslie Melvin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/melvin07a/melvin07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Text Chunking based on a Generalization of Winnow",
            "abstract": [
                "This paper describes a text chunking system based on a generalization of the Winnow algorithm. We propose a general statistical model for text chunking which we then convert into a classification problem. We argue that the Winnow family of algorithms is particularly suitable for solving classification problems arising from NLP applications, due to their robustness to irrelevant features. However in theory, Winnow may not converge for linearly non-separable data. To remedy this problem, we employ a generalization of the original Winnow method. An additional advantage of the new algorithm is that it provides reliable confidence estimates for its classification predictions. This property is required in our statistical modeling approach. We show that our system achieves state of the art performance in text chunking with less computational cost then previous systems."
            ],
            "keywords": [],
            "author": [
                "Tong Zhang",
                "Fred Damerau",
                "David Johnson",
                "T J Watson",
                "James Hammerton",
                "Miles Osborne",
                "Susan Armstrong",
                "Walter Daelemans"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/zhang02c/zhang02c.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Algorithms for Topic Models",
            "abstract": [
                "We describe distributed algorithms for two widely-used topic models, namely the Latent Dirichlet Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In our distributed algorithms the data is partitioned across separate processors and inference is done in a parallel, distributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straightforward mapping of LDA to a distributed processor setting. In this algorithm processors concurrently perform Gibbs sampling over local data followed by a global update of topic counts. The algorithm is simple to implement and can be viewed as an approximation to Gibbs-sampled LDA. The second version is a model that uses a hierarchical Bayesian extension of LDA to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world text corpora we show that distributed learning works well in practice. For both LDA and HDP, we show that the converged test-data log probability for distributed learning is indistinguishable from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer."
            ],
            "keywords": [
                "topic models",
                "latent Dirichlet allocation",
                "hierarchical Dirichlet processes",
                "distributed parallel computation"
            ],
            "author": [
                "David Newman",
                "Arthur Asuncion",
                "Padhraic Smyth",
                "Max Welling"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/newman09a/newman09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Alleviating Naive Bayes Attribute Independence Assumption by Attribute Weighting",
            "abstract": [
                "Despite the simplicity of the Naive Bayes classifier, it has continued to perform well against more sophisticated newcomers and has remained, therefore, of great interest to the machine learning community. Of numerous approaches to refining the naive Bayes classifier, attribute weighting has received less attention than it warrants. Most approaches, perhaps influenced by attribute weighting in other machine learning algorithms, use weighting to place more emphasis on highly predictive attributes than those that are less predictive. In this paper, we argue that for naive Bayes attribute weighting should instead be used to alleviate the conditional independence assumption. Based on this premise, we propose a weighted naive Bayes algorithm, called WANBIA, that selects weights to minimize either the negative conditional log likelihood or the mean squared error objective functions. We perform extensive evaluations and find that WANBIA is a competitive alternative to state of the art classifiers like Random Forest, Logistic Regression and A1DE."
            ],
            "keywords": [
                "classification",
                "naive Bayes",
                "attribute independence assumption",
                "weighted naive Bayes classification"
            ],
            "author": [
                "Nayyar A Zaidi",
                "Mark J Carman",
                "Geoffrey I Webb"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/zaidi13a/zaidi13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Theoretical Advantages of Lenient Learners: An Evolutionary Game Theoretic Perspective",
            "abstract": [
                "This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the benefits of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insignificant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning."
            ],
            "keywords": [
                "multiagent learning",
                "reinforcement learning",
                "cooperative coevolution",
                "evolutionary game theory",
                "formal models",
                "visualization",
                "basins of attraction"
            ],
            "author": [
                "Liviu Panait",
                "Karl Tuyls",
                "Sean Luke"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/panait08a/panait08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Benefit of Multitask Representation Learning",
            "abstract": [
                "We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks."
            ],
            "keywords": [
                "learning-to-learn",
                "multitask learning",
                "representation learning",
                "statistical learning theory",
                "transfer learning"
            ],
            "author": [
                "Andreas Maurer",
                "Massimiliano Pontil",
                "Urun Dogan",
                "Marius Kloft",
                "Francesco Orabona",
                "Tatiana Tommasi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-242/15-242.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Differential Privacy for Bayesian Inference through Posterior Sampling *",
            "abstract": [
                "Differential privacy formalises privacy-preserving mechanisms that provide access to a database. Can Bayesian inference be used directly to provide private access to data? The answer is yes: under certain conditions on the prior, sampling from the posterior distribution can lead to a desired level of privacy and utility. For a uniform treatment, we define differential privacy over arbitrary data set metrics, outcome spaces and distribution families. This allows us to also deal with non-i.i.d or non-tabular data sets. We then prove bounds on the sensitivity of the posterior to the data, which delivers a measure of robustness. We also show how to use posterior sampling to provide differentially private responses to queries, within a decision-theoretic framework. Finally, we provide bounds on the utility of answers to queries and on the ability of an adversary to distinguish between data sets. The latter are complemented by a novel use of Le Cam's method to obtain lower bounds on distinguishability. Our results hold for arbitrary metrics, including those for the common definition of differential privacy. For specific choices of the metric, we give a number of examples satisfying our assumptions."
            ],
            "keywords": [
                "Bayesian inference",
                "differential privacy",
                "robustness",
                "adversarial Learning"
            ],
            "author": [
                "Christos Dimitrakakis",
                "Blaine Nelson",
                "Zuhe Zhang",
                "Aikaterini Mitrokotsa",
                "Benjamin I P Rubinstein",
                "Zhang, Mitrokotsa Rubinstein Nelson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-257/15-257.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dual Iterative Hard Thresholding",
            "abstract": [
                "Iterative Hard Thresholding (IHT) is a popular class of first-order greedy selection methods for loss minimization under cardinality constraint. The existing IHT-style algorithms, however, are proposed for minimizing the primal formulation. It is still an open issue to explore duality theory and algorithms for such a non-convex and NP-hard combinatorial optimization problem. To address this issue, we develop in this article a novel duality theory for 2-regularized empirical risk minimization under cardinality constraint, along with an IHT-style algorithm for dual optimization. Our sparse duality theory establishes a set of sufficient and/or necessary conditions under which the original non-convex problem can be equivalently or approximately solved in a concave dual formulation. In view of this theory, we propose the Dual IHT (DIHT) algorithm as a super-gradient ascent method to solve the non-smooth dual problem with provable guarantees on primal-dual gap convergence and sparsity recovery. Numerical results confirm our theoretical predictions and demonstrate the superiority of DIHT to the state-of-the-art primal IHT-style algorithms in model estimation accuracy and computational efficiency."
            ],
            "keywords": [
                "Iterative hard thresholding",
                "Duality theory",
                "Sparsity recovery",
                "Non-convex optimization"
            ],
            "author": [
                "Xiao-Tong Yuan",
                "Bo Liu",
                "Lezi Wang",
                "Qingshan Liu",
                "Dimitris N Metaxas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-487/18-487.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fundamental Conditions for Low-CP-Rank Tensor Completion",
            "abstract": [
                "We consider the problem of low canonical polyadic (CP) rank tensor completion. A completion is a tensor whose entries agree with the observed entries and its rank matches the given CP rank. We analyze the manifold structure corresponding to the tensors with the given rank and define a set of polynomials based on the sampling pattern and CP decomposition. Then, we show that finite completability of the sampled tensor is equivalent to having a certain number of algebraically independent polynomials among the defined polynomials. Our proposed approach results in characterizing the maximum number of algebraically independent polynomials in terms of a simple geometric structure of the sampling pattern, and therefore we obtain the deterministic necessary and sufficient condition on the sampling pattern for finite completability of the sampled tensor. Moreover, assuming that the entries of the tensor are sampled independently with probability p and using the mentioned deterministic analysis, we propose a combinatorial method to derive a lower bound on the sampling probability p, or equivalently, the number of sampled entries that guarantees finite completability with high probability. We also show that the existing result for the matrix completion problem can be used to obtain a loose lower bound on the sampling probability p. In addition, we obtain deterministic and probabilistic conditions for unique completability. It is seen that the number of samples required for finite or unique completability obtained by the proposed analysis on the CP manifold is orders-of-magnitude lower than that is obtained by the existing analysis on the Grassmannian manifold."
            ],
            "keywords": [
                "Low-rank tensor completion",
                "canonical polyadic (CP) decomposition",
                "finite completability",
                "unique completability",
                "algebraic geometry",
                "Bernstein's theorem"
            ],
            "author": [
                "Morteza Ashraphijuo",
                "Xiaodong Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-189/17-189.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Collective Matrix Completion",
            "abstract": [
                "Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries. Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system. However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one. In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution. Then, we relax the assumption of exponential family distribution for the noise. In this setting, we do not assume any specific model for the observations. The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments."
            ],
            "keywords": [
                "High-dimensional prediction",
                "Exponential families",
                "Low-rank matrix estimation",
                "Nuclear norm minimization",
                "Low-rank optimization"
            ],
            "author": [
                "Mokhtar Z Alaya",
                "Olga Klopp"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-483/18-483.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximate Tree Kernels",
            "abstract": [
                "Convolution kernels for trees provide simple means for learning with tree-structured data. The computation time of tree kernels is quadratic in the size of the trees, since all pairs of nodes need to be compared. Thus, large parse trees, obtained from HTML documents or structured network data, render convolution kernels inapplicable. In this article, we propose an effective approximation technique for parse tree kernels. The approximate tree kernels (ATKs) limit kernel computation to a sparse subset of relevant subtrees and discard redundant structures, such that training and testing of kernel-based learning methods are significantly accelerated. We devise linear programming approaches for identifying such subsets for supervised and unsupervised learning tasks, respectively. Empirically, the approximate tree kernels attain run-time improvements up to three orders of magnitude while preserving the predictive accuracy of regular tree kernels. For unsupervised tasks, the approximate tree kernels even lead to more accurate predictions by identifying relevant dimensions in feature space."
            ],
            "keywords": [
                "tree kernels",
                "approximation",
                "kernel methods",
                "convolution kernels"
            ],
            "author": [
                "Konrad Rieck",
                "Tammo Krueger",
                "Ulf Brefeld",
                "Klaus- Robert Müller"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/rieck10a/rieck10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Diffusion Kernels on Statistical Manifolds",
            "abstract": [
                "A family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models. The kernels are based on the heat equation on the Riemannian manifold defined by the Fisher information metric associated with a statistical family, and generalize the Gaussian kernel of Euclidean space. As an important special case, kernels based on the geometry of multinomial families are derived, leading to kernel-based learning algorithms that apply naturally to discrete data. Bounds on covering numbers and Rademacher averages for the kernels are proved using bounds on the eigenvalues of the Laplacian on Riemannian manifolds. Experimental results are presented for document classification, for which the use of multinomial geometry is natural and well motivated, and improvements are obtained over the standard use of Gaussian or linear kernels, which have been the standard for text classification."
            ],
            "keywords": [
                "kernels",
                "heat equation",
                "diffusion",
                "information geometry",
                "text classification"
            ],
            "author": [
                "John Lafferty"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/lafferty05a/lafferty05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Semigroup Kernels on Measures",
            "abstract": [
                "We present a family of positive definite kernels on measures, characterized by the fact that the value of the kernel between two measures is a function of their sum. These kernels can be used to derive kernels on structured objects, such as images and texts, by representing these objects as sets of components, such as pixels or words, or more generally as measures on the space of components. Several kernels studied in this work make use of common quantities defined on measures such as entropy or generalized variance to detect similarities. Given an a priori kernel on the space of components itself, the approach is further extended by restating the previous results in a more efficient and flexible framework using the \"kernel trick\". Finally, a constructive approach to such positive definite kernels through an integral representation theorem is proved, before presenting experimental results on a benchmark experiment of handwritten digits classification to illustrate the validity of the approach."
            ],
            "keywords": [
                "kernels on measures",
                "semigroup theory",
                "Jensen divergence",
                "generalized variance",
                "reproducing kernel Hilbert space"
            ],
            "author": [
                "Marco Cuturi",
                "Jean-Philippe Vert"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/cuturi05a/cuturi05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Separating Models of Learning from Correlated and Uncorrelated Data",
            "abstract": [
                "We consider a natural framework of learning from correlated data, in which successive examples used for learning are generated according to a random walk over the space of possible examples. A recent paper by Bshouty et al. (2003) shows that the class of polynomial-size DNF formulas is efficiently learnable in this random walk model; this result suggests that the Random Walk model is more powerful than comparable standard models of learning from independent examples, in which similarly efficient DNF learning algorithms are not known. We give strong evidence that the Random Walk model is indeed more powerful than the standard model, by showing that if any cryptographic one-way function exists (a universally held belief in cryptography), then there is a class of functions that can be learned efficiently in the Random Walk setting but not in the standard setting where all examples are independent."
            ],
            "keywords": [
                "random walks",
                "uniform distribution learning",
                "cryptographic hardness",
                "correlated data",
                "PAC learning"
            ],
            "author": [
                "Ariel Elbaz",
                "Rocco A Servedio",
                "Andrew Wan",
                "Peter Auer",
                "Ron Meir"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/elbaz07a/elbaz07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices",
            "abstract": [
                "In this paper we study the problem of designing SVM classifiers when the kernel matrix, K, is affected by uncertainty. Specifically K is modeled as a positive affine combination of given positive semi definite kernels, with the coefficients ranging in a norm-bounded uncertainty set. We treat the problem using the Robust Optimization methodology. This reduces the uncertain SVM problem into a deterministic conic quadratic problem which can be solved in principle by a polynomial time Interior Point (IP) algorithm. However, for large-scale classification problems, IP methods become intractable and one has to resort to first-order gradient type methods. The strategy we use here is to reformulate the robust counterpart of the uncertain SVM problem as a saddle point problem and employ a special gradient scheme which works directly on the convex-concave saddle function. The algorithm is a simplified version of a general scheme due to Juditski and Nemirovski (2011). It achieves an O(1/T 2) reduction of the initial error after T iterations. A comprehensive empirical study on both synthetic data and real-world protein structure data sets show that the proposed formulations achieve the desired robustness, and the saddle point based algorithm outperforms the IP method significantly."
            ],
            "keywords": [
                "robust optimization",
                "uncertain classification",
                "kernel functions"
            ],
            "author": [
                "Aharon Ben-Tal",
                "William Davidson",
                "Arkadi Nemirovski",
                "H Milton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/ben-tal12a/ben-tal12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory",
            "abstract": [
                "In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2λ/n, where λ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion."
            ],
            "keywords": [
                "cross-validation",
                "information criterion",
                "singular learning machine",
                "birational invariant"
            ],
            "author": [
                "Sumio Watanabe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/watanabe10a/watanabe10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "LLORMA: Local Low-Rank Matrix Approximation",
            "abstract": [
                "Matrix approximation is a common tool in recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is low-rank. In this paper, we propose, analyze, and experiment with two procedures, one parallel and the other global, for constructing local matrix approximations. The two approaches approximate the observed matrix as a weighted sum of low-rank matrices. These matrices are limited to a local region of the observed matrix. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks."
            ],
            "keywords": [
                "Matrix approximation",
                "non-parametric methods",
                "kernel smoothing",
                "collaborative Filtering",
                "recommender systems"
            ],
            "author": [
                "Joonseok Lee",
                "Seungyeon Kim",
                "Guy Lebanon",
                "Yoram Singer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-301/14-301.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Double Reinforcement Learning for Efficient Off-Policy Evaluation in Markov Decision Processes",
            "abstract": [
                "Off-policy evaluation (OPE) in reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. We consider for the first time the semiparametric efficiency limits of OPE in Markov decision processes (MDPs), where actions, rewards, and states are memoryless. We show existing OPE estimators may fail to be efficient in this setting. We develop a new estimator based on cross-fold estimation of q-functions and marginalized density ratios, which we term double reinforcement learning (DRL). We show that DRL is efficient when both components are estimated at fourth-root rates and is also doubly robust when only one component is consistent. We investigate these properties empirically and demonstrate the performance benefits due to harnessing memorylessness."
            ],
            "keywords": [
                "Off-policy evaluation",
                "Markov decision processes",
                "Semiparametric efficiency",
                "Double machine learning"
            ],
            "author": [
                "Nathan Kallus",
                "Masatoshi Uehara"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-827/19-827.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The MADP Toolbox: An Open Source Library for Planning and Learning in (Multi-)Agent Systems",
            "abstract": [
                "This article describes the Multiagent Decision Process (MADP) Toolbox, a software library to support planning and learning for intelligent agents and multiagent systems in uncertain environments. Key features are that it supports partially observable environments and stochastic transition models; has unified support for single-and multiagent systems; provides a large number of models for decision-theoretic decision making, including one-shot and sequential decision making under various assumptions of observability and cooperation, such as Dec-POMDPs and POSGs; provides tools and parsers to quickly prototype new problems; provides an extensive range of planning and learning algorithms for single-and multiagent systems; is released under the GNU GPL v3 license; and is written in C++ and designed to be extensible via the object-oriented paradigm."
            ],
            "keywords": [
                "software",
                "decision-theoretic planning",
                "reinforcement learning",
                "multiagent systems"
            ],
            "author": [
                "Frans A Oliehoek",
                "Matthijs T J Spaan",
                "Bas Terwijn",
                "Philipp Robbel",
                "João V Messias"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-156/17-156.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "WONDER: Weighted One-shot Distributed Ridge Regression in High Dimensions",
            "abstract": [
                "In many areas, practitioners need to analyze large data sets that challenge conventional single-machine computing. To scale up data analysis, distributed and parallel computing approaches are increasingly needed. Here we study a fundamental and highly important problem in this area: How to do ridge regression in a distributed computing environment? Ridge regression is an extremely popular method for supervised learning, and has several optimality properties, thus it is important to study. We study one-shot methods that construct weighted combinations of ridge regression estimators computed on each machine. By analyzing the mean squared error in a high-dimensional random-effects model where each predictor has a small effect, we discover several new phenomena. Infinite-worker limit: The distributed estimator works well for very large numbers of machines, a phenomenon we call \"infinite-worker limit\". Optimal weights: The optimal weights for combining local estimators sum to more than unity, due to the downward bias of ridge. Thus, all averaging methods are suboptimal. We also propose a new Weighted ONe-shot DistributEd Ridge regression algorithm (WONDER). We test WONDER in simulation studies and using the Million Song Dataset as an example. There it can save at least 100x in computation time, while nearly preserving test accuracy."
            ],
            "keywords": [
                "distributed learning",
                "ridge regression",
                "high-dimensional statistics",
                "random matrix theory"
            ],
            "author": [
                "Edgar Dobriban",
                "Yue Sheng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-277/19-277.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Krylov Subspace Method for Nonlinear Dynamical Systems with Random Noise",
            "abstract": [
                "Operator-theoretic analysis of nonlinear dynamical systems has attracted much attention in a variety of engineering and scientific fields, endowed with practical estimation methods using data such as dynamic mode decomposition. In this paper, we address a lifted representation of nonlinear dynamical systems with random noise based on transfer operators, and develop a novel Krylov subspace method for estimating the operators using finite data, with consideration of the unboundedness of operators. For this purpose, we first consider Perron-Frobenius operators with kernel-mean embeddings for such systems. We then extend the Arnoldi method, which is the most classical type of Kryov subspace methods, so that it can be applied to the current case. Meanwhile, the Arnoldi method requires the assumption that the operator is bounded, which is not necessarily satisfied for transfer operators on nonlinear systems. We accordingly develop the shift-invert Arnoldi method for Perron-Frobenius operators to avoid this problem. Also, we describe an approach of evaluating predictive accuracy by estimated operators on the basis of the maximum mean discrepancy, which is applicable, for example, to anomaly detection in complex systems."
            ],
            "keywords": [
                "Nonlinear dynamical system",
                "Transfer operator",
                "Krylov subspace methods",
                "Operator theory",
                "Time-series data"
            ],
            "author": [
                "Yuka Hashimoto",
                "Masahiro Ikeda",
                "Yoichi Matsuo",
                "Ishikawa, Ikeda, Matsuo Kawahara Hashimoto"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-993/19-993.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications",
            "abstract": [
                "The ℓ 1-penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted ℓ 1-penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted ℓ 1-penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single-and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results."
            ],
            "keywords": [
                "variable selection",
                "penalized estimation",
                "oracle inequality",
                "generalized linear models",
                "selection consistency",
                "sparsity"
            ],
            "author": [
                "Jian Huang",
                "Cun-Hui Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/huang12b/huang12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parallel Software for Training Large Scale Support Vector Machines on Multiprocessor Systems *",
            "abstract": [
                "Parallel software for solving the quadratic program arising in training support vector machines for classification problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic subproblem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-ofthe-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes."
            ],
            "keywords": [
                "support vector machines",
                "large scale quadratic programs",
                "decomposition techniques",
                "gradient projection methods",
                "parallel computation"
            ],
            "author": [
                "Luca Zanni",
                "Thomas Serafini",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/zanni06a/zanni06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "What's Strange About Recent Events (WSARE): An Algorithm for the Early Detection of Disease Outbreaks",
            "abstract": [
                "Traditional biosurveillance algorithms detect disease outbreaks by looking for peaks in a univariate time series of health-care data. Current health-care surveillance data, however, are no longer simply univariate data streams. Instead, a wealth of spatial, temporal, demographic and symptomatic information is available. We present an early disease outbreak detection algorithm called What's Strange About Recent Events (WSARE), which uses a multivariate approach to improve its timeliness of detection. WSARE employs a rule-based technique that compares recent health-care data against data from a baseline distribution and finds subgroups of the recent data whose proportions have changed the most from the baseline data. In addition, health-care data also pose difficulties for surveillance algorithms because of inherent temporal trends such as seasonal effects and day of week variations. WSARE approaches this problem using a Bayesian network to produce a baseline distribution that accounts for these temporal trends. The algorithm itself incorporates a wide range of ideas, including association rules, Bayesian networks, hypothesis testing and permutation tests to produce a detection algorithm that is careful to evaluate the significance of the alarms that it raises."
            ],
            "keywords": [
                "anomaly detection",
                "syndromic surveillance",
                "biosurveillance",
                "Bayesian networks",
                "applications"
            ],
            "author": [
                "Weng-Keen Wong",
                "Andrew Moore",
                "Gregory Cooper",
                "Michael Wagner"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/wong05a/wong05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Python Environment for Bayesian Learning: Inferring the Structure of Bayesian Networks from Knowledge and Data",
            "abstract": [
                "In this paper, we introduce PEBL, a Python library and application for learning Bayesian network structure from data and prior knowledge that provides features unmatched by alternative software packages: the ability to use interventional data, flexible specification of structural priors, modeling with hidden variables and exploitation of parallel processing."
            ],
            "keywords": [
                "Bayesian networks",
                "python",
                "open source software"
            ],
            "author": [
                "Abhik Shah",
                "Peter Woolf"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/shah09a/shah09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hope and Fear for Discriminative Training of Statistical Translation Models",
            "abstract": [
                "In machine translation, discriminative models have almost entirely supplanted the classical noisychannel model, but are standardly trained using a method that is reliable only in low-dimensional spaces. Two strands of research have tried to adapt more scalable discriminative training methods to machine translation: the first uses log-linear probability models and either maximum likelihood or minimum risk, and the other uses linear models and large-margin methods. Here, we provide an overview of the latter. We compare several learning algorithms and describe in detail some novel extensions suited to properties of the translation task: no single correct output, a large space of structured outputs, and slow inference. We present experimental results on a large-scale Arabic-English translation task, demonstrating large gains in translation accuracy."
            ],
            "keywords": [
                "machine translation",
                "structured prediction",
                "large-margin methods",
                "online learning",
                "distributed computing"
            ],
            "author": [
                "David Chiang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/chiang12a/chiang12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Toward Attribute Efficient Learning of Decision Lists and Parities",
            "abstract": [
                "We consider two well-studied problems regarding attribute efficient learning: learning decision lists and learning parity functions. First, we give an algorithm for learning decision lists of length k over n variables using 2Õ (k 1/3) log n examples and time nÕ (k 1/3). This is the first algorithm for learning decision lists that has both subexponential sample complexity and subexponential running time in the relevant parameters. Our approach establishes a relationship between attribute efficient learning and polynomial threshold functions and is based on a new construction of low degree, low weight polynomial threshold functions for decision lists. For a wide range of parameters our construction matches a lower bound due to Beigel for decision lists and gives an essentially optimal tradeoff between polynomial threshold function degree and weight. Second, we give an algorithm for learning an unknown parity function on k out of n variables using O(n 1−1/k) examples in poly(n) time. For k = o(log n) this yields a polynomial time algorithm with sample complexity o(n); this is the first polynomial time algorithm for learning parity on a superconstant number of variables with sublinear sample complexity. We also give a simple algorithm for learning an unknown length-k parity using O(k log n) examples in n k/2 time, which improves on the naive n k time bound of exhaustive search."
            ],
            "keywords": [
                "PAC learning",
                "attribute efficiency",
                "learning parity",
                "decision lists",
                "Winnow"
            ],
            "author": [
                "Adam R Klivans",
                "Rocco A Servedio"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/klivans06a/klivans06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Theoretical Guarantees for Parameter Estimation of Gaussian Random Field Models: A Sparse Precision Matrix Approach",
            "abstract": [
                "Iterative methods for fitting a Gaussian Random Field (GRF) model via maximum likelihood (ML) estimation requires solving a nonconvex optimization problem. The problem is aggravated for anisotropic GRFs where the number of covariance function parameters increases with the dimension. Even evaluation of the likelihood function requires O(n 3) floating point operations, where n denotes the number of data locations. In this paper 1 , we propose a new two-stage procedure to estimate the parameters of second-order stationary GRFs. First, a convex likelihood problem regularized with a weighted 1-norm, utilizing the available distance information between observation locations, is solved to fit a sparse precision (inverse covariance) matrix to the observed data. Second, the parameters of the covariance function are estimated by solving a least squares problem. Theoretical error bounds for the solutions of stage I and II problems are provided, and their tightness are investigated."
            ],
            "keywords": [
                "nonconvex optimization",
                "Gaussian Markov random fields",
                "kernel methods",
                "hyperparameter optimization",
                "covariance selection",
                "spatial statistics"
            ],
            "author": [
                "Sam Davanloo Tajbakhsh",
                "Enrique Del Castillo"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/14-241/14-241.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods",
            "abstract": [
                "Nesterov's momentum trick is famously known for accelerating gradient descent, and has been proven useful in building fast iterative algorithms. However, in the stochastic setting, counterexamples exist and prevent Nesterov's momentum from providing similar acceleration, even if the underlying problem is convex and finite-sum. We introduce Katyusha, a direct, primal-only stochastic gradient method to fix this issue. In convex finite-sum stochastic optimization, Katyusha has an optimal accelerated convergence rate, and enjoys an optimal parallel linear speedup in the mini-batch setting. The main ingredient is Katyusha momentum, a novel \"negative momentum\" on top of Nesterov's momentum. It can be incorporated into a variance-reduction based algorithm and speed it up, both in terms of sequential and parallel performance. Since variance reduction has been successfully applied to a growing list of practical problems, our paper suggests that in each of such cases, one could potentially try to give Katyusha a hug."
            ],
            "keywords": [],
            "author": [
                "Zeyuan Allen-Zhu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-410/16-410.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric Risk Bounds for Time-Series Forecasting",
            "abstract": [
                "We derive generalization error bounds for traditional time-series forecasting models. Our results hold for many standard forecasting tools including autoregressive models, moving average models, and, more generally, linear state-space models. These non-asymptotic bounds need only weak assumptions on the data-generating process, yet allow forecasters to select among competing models and to guarantee, with high probability, that their chosen model will perform well. We motivate our techniques with and apply them to standard economic and financial forecasting tools-a GARCH model for predicting equity volatility and a dynamic stochastic general equilibrium model (DSGE), the standard tool in macroeconomic forecasting. We demonstrate in particular how our techniques can aid forecasters and policy makers in choosing models which behave well under uncertainty and mis-specification."
            ],
            "keywords": [
                "generalization error",
                "prediction risk",
                "model selection",
                "VC dimension",
                "statespace models",
                "linear time-invariant systems"
            ],
            "author": [
                "Daniel J Mcdonald",
                "Mark Schervish"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/13-336/13-336.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Normal Bandits of Unknown Means and Variances",
            "abstract": [
                "Consider the problem of sampling sequentially from a finite number of N 2 populations, specified by random variables X i k , i = 1,. .. , N, and k = 1, 2,. . .; where X i k denotes the outcome from population i the k th time it is sampled. It is assumed that for each fixed i, {X i k } k 1 is a sequence of i.i.d. normal random variables, with unknown mean µ i and unknown variance σ 2 i. The objective is to have a policy π for deciding from which of the N populations to sample from at any time t = 1, 2,. .. so as to maximize the expected sum of outcomes of n total samples or equivalently to minimize the regret due to lack on information of the parameters µ i and σ 2 i. In this paper, we present a simple inflated sample mean (ISM) index policy that is asymptotically optimal in the sense of Theorem 4 below. This resolves a standing open problem from Burnetas and Katehakis (1996b). Additionally, finite horizon regret bounds are given."
            ],
            "keywords": [
                "Inflated Sample Means",
                "UCB policies",
                "Multi-armed Bandits",
                "Sequential Allocation"
            ],
            "author": [
                "Wesley Cowan",
                "Junya Honda",
                "Michael N Katehakis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-154/15-154.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "algcomparison: Comparing the Performance of Graphical Structure Learning Algorithms with TETRAD",
            "abstract": [
                "In this report we describe a tool for comparing the performance of graphical causal structure learning algorithms implemented in the TETRAD freeware suite of causal analysis methods. Currently the tool is available as package in the TETRAD source code (written in Java). Simulations can be done varying the number of runs, sample sizes, and data modalities. Performance on this simulated data can then be compared for a number of algorithms, with parameters varied and with performance statistics as selected, producing a publishable report. The package presented here may also be used to compare structure learning methods across platforms and programming languages, i.e., to compare algorithms implemented in TETRAD with those implemented in MATLAB, Python, or R."
            ],
            "keywords": [
                "causal discovery",
                "graphical models",
                "structure learning",
                "evaluation"
            ],
            "author": [
                "Joseph D Ramsey",
                "Daniel Malinsky",
                "Kevin V Bui"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-773/19-773.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical and Computational Guarantees for the Baum-Welch Algorithm",
            "abstract": [
                "The Hidden Markov Model (HMM) is one of the mainstays of statistical modeling of discrete time series, with applications including speech recognition, computational biology, computer vision and econometrics. Estimating an HMM from its observation process is often addressed via the Baum-Welch algorithm, which is known to be susceptible to local optima. In this paper, we first give a general characterization of the basin of attraction associated with any global optimum of the population likelihood. By exploiting this characterization, we provide non-asymptotic finite sample guarantees on the Baum-Welch updates and show geometric convergence to a small ball of radius on the order of the minimax rate around a global optimum. As a concrete example, we prove a linear rate of convergence for a hidden Markov mixture of two isotropic Gaussians given a suitable mean separation and an initialization within a ball of large radius around (one of) the true parameters. To our knowledge, these are the first rigorous local convergence guarantees to global optima for the Baum-Welch algorithm in a setting where the likelihood function is nonconvex. We complement our theoretical results with thorough numerical simulations studying the convergence of the Baum-Welch algorithm and illustrating the accuracy of our predictions."
            ],
            "keywords": [
                "Hidden Markov Models",
                "Baum-Welch algorithm",
                "EM algorithm",
                "non-convex optimization",
                "graphical models"
            ],
            "author": [
                "Fanny Yang",
                "Sivaraman Balakrishnan",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-093/16-093.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Online Gesture Recognition with Crowdsourced Annotations",
            "abstract": [
                "Crowdsourcing is a promising way to reduce the effort of collecting annotations for training gesture recognition systems. Crowdsourced annotations suffer from \"noise\" such as mislabeling, or inaccurate identification of start and end time of gesture instances. In this paper we present SegmentedLCSS and WarpingLCSS, two template-matching methods offering robustness when trained with noisy crowdsourced annotations to spot gestures from wearable motion sensors. The methods quantize signals into strings of characters and then apply variations of the longest common subsequence algorithm (LCSS) to spot gestures. We compare the noise robustness of our methods against baselines which use dynamic time warping (DTW) and support vector machines (SVM). The experiments are performed on data sets with various gesture classes (10-17 classes) recorded from accelerometers on arms, with both real and synthetic crowdsourced annotations. WarpingLCSS has similar or better performance than baselines in absence of noisy annotations. In presence of 60% mislabeled instances, WarpingLCSS outperformed SVM by 22% F1-score and outperformed DTWbased methods by 36% F1-score on average. SegmentedLCSS yields similar performance as WarpingLCSS, however it performs one order of magnitude slower. Additionally, we show to use our methods to filter out the noise in the crowdsourced annotation before training a traditional classifier. The filtering increases the performance of SVM by 20% F1-score and of DTW-based methods by 8% F1-score on average in the noisy real crowdsourced annotations."
            ],
            "keywords": [
                "gesture spotting",
                "crowdsourced annotation",
                "longest common subsequence",
                "template matching methods",
                "accelerometer sensors"
            ],
            "author": [
                "Long-Van Nguyen-Dinh",
                "Alberto Calatroni",
                "Gerhard Tröster",
                "Sergio Escalera"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/nguyendinh14a/nguyendinh14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Condition for Perfect Dimensionality Recovery by Variational Bayesian PCA",
            "abstract": [
                "Having shown its good performance in many applications, variational Bayesian (VB) learning is known to be one of the best tractable approximations to Bayesian learning. However, its performance was not well understood theoretically. In this paper, we clarify the behavior of VB learning in probabilistic PCA (or fully-observed matrix factorization). More specifically, we establish a necessary and sufficient condition for perfect dimensionality (or rank) recovery in the large-scale limit when the matrix size goes to infinity. Our result theoretically guarantees the performance of VB-PCA. At the same time, it also reveals the conservative nature of VB learning-it offers a low false positive rate at the expense of low sensitivity. By contrasting with an alternative dimensionality selection method, we characterize VB learning in PCA. In our analysis, we obtain bounds of the noise variance estimator, and a new and simple analytic-form solution for the other parameters, which themselves are useful for implementation of VB-PCA."
            ],
            "keywords": [
                "variational Bayesian learning",
                "matrix factorization",
                "principal component analysis",
                "automatic relevance determination",
                "perfect dimensionality recovery"
            ],
            "author": [
                "Masashi Sugiyama",
                "S Derin Babacan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/nakajima15a/nakajima15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Contextual Explanation Networks",
            "abstract": [
                "Modern learning algorithms excel at producing accurate but complex models of the data. However, deploying such models in the real-world requires extra care: we must ensure their reliability, robustness, and absence of undesired biases. This motivates the development of models that are equally accurate but can be also easily inspected and assessed beyond their predictive performance. To this end, we introduce contextual explanation networks (CENs)a class of architectures that learn to predict by generating and utilizing intermediate, simplified probabilistic models. Specifically, CENs generate parameters for intermediate graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain simultaneously. Our approach offers two major advantages: (i) for each prediction, valid, instance-specific explanation is generated with no computational overhead and (ii) prediction via explanation acts as a regularizer and boosts performance in data-scarce settings. We analyze the proposed framework theoretically and experimentally. Our results on image and text classification and survival analysis tasks demonstrate that CENs are not only competitive with the state-of-the-art methods but also offer additional insights behind each prediction, that can be valuable for decision support. We also show that while post-hoc methods may produce misleading explanations in certain cases, CENs are consistent and allow to detect such cases systematically."
            ],
            "keywords": [],
            "author": [
                "Maruan Al-Shedivat",
                "Eric Xing",
                "Xing Dubey"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-856/18-856.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing",
            "abstract": [
                "We present GluonCV and GluonNLP, the deep learning toolkits for computer vision and natural language processing based on Apache MXNet (incubating). These toolkits provide state-of-the-art pre-trained models, training scripts, and training logs, to facilitate rapid prototyping and promote reproducible research. We also provide modular APIs with flexible building blocks to enable efficient customization. Leveraging the MXNet ecosystem, the deep learning models in GluonCV and GluonNLP can be deployed onto a variety of platforms with different programming languages. The Apache 2.0 license has been adopted by GluonCV and GluonNLP to allow for software distribution, modification, and usage."
            ],
            "keywords": [
                "Machine Learning",
                "Deep Learning",
                "Apache MXNet",
                "Computer Vision",
                "Natural Language Processing"
            ],
            "author": [
                "Jian Guo",
                "Leonard Lausen",
                "Haibin Lin",
                "Xingjian Shi",
                "Chenguang Wang",
                "Aston Zhang",
                "Hang Zhang",
                "Yi Zhu",
                "* Mu Li",
                "Mu Li",
                "Junyuan Xie",
                "Sheng Zha",
                "Zhi Zhang",
                "Zhongyue Zhang",
                "Shuai Zheng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-429/19-429.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Should We Really Use Post-Hoc Tests Based on Mean-Ranks?",
            "abstract": [
                "The statistical comparison of multiple algorithms over multiple data sets is fundamental in machine learning. This is typically carried out by the Friedman test. When the Friedman test rejects the null hypothesis, multiple comparisons are carried out to establish which are the significant differences among algorithms. The multiple comparisons are usually performed using the mean-ranks test. The aim of this technical note is to discuss the inconsistencies of the mean-ranks post-hoc test with the goal of discouraging its use in machine learning as well as in medicine, psychology, etc.. We show that the outcome of the mean-ranks test depends on the pool of algorithms originally included in the experiment. In other words, the outcome of the comparison between algorithms A and B depends also on the performance of the other algorithms included in the original experiment. This can lead to paradoxical situations. For instance the difference between A and B could be declared significant if the pool comprises algorithms C, D, E and not significant if the pool comprises algorithms F, G, H. To overcome these issues, we suggest instead to perform the multiple comparison using a test whose outcome only depends on the two algorithms being compared, such as the sign-test or the Wilcoxon signed-rank test."
            ],
            "keywords": [
                "statistical comparison",
                "Friedman test",
                "post-hoc test"
            ],
            "author": [
                "Alessio Benavoli",
                "Giorgio Corani",
                "Francesca Mangili"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/benavoli16a/benavoli16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes",
            "abstract": [
                "This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A specific instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a final parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A specific instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nyström extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are briefly summarized at the end."
            ],
            "keywords": [
                "Markov decision processes",
                "reinforcement learning",
                "value function approximation",
                "manifold learning",
                "spectral graph theory"
            ],
            "author": [
                "Sridhar Mahadevan",
                "Mauro Maggioni",
                "Mauro Maggioni@",
                "Duke Edu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/mahadevan07a/mahadevan07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-task Regression using Minimal Penalties",
            "abstract": [
                "In this paper we study the kernel multiple ridge regression framework, which we refer to as multitask regression, using penalization techniques. The theoretical analysis of this problem shows that the key element appearing for an optimal calibration is the covariance matrix of the noise between the different tasks. We present a new algorithm to estimate this covariance matrix, based on the concept of minimal penalty, which was previously used in the single-task regression framework to estimate the variance of the noise. We show, in a non-asymptotic setting and under mild assumptions on the target function, that this estimator converges towards the covariance matrix. Then plugging this estimator into the corresponding ideal penalty leads to an oracle inequality. We illustrate the behavior of our algorithm on synthetic examples."
            ],
            "keywords": [
                "multi-task",
                "oracle inequality",
                "learning theory"
            ],
            "author": [
                "Matthieu Solnon",
                "Sylvain Arlot",
                "Francis Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/solnon12a/solnon12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pattern Recognition for Conditionally Independent Data",
            "abstract": [
                "In this work we consider the task of relaxing the i.i.d. assumption in pattern recognition (or classification), aiming to make existing learning algorithms applicable to a wider range of tasks. Pattern recognition is guessing a discrete label of some object based on a set of given examples (pairs of objects and labels). We consider the case of deterministically defined labels. Traditionally, this task is studied under the assumption that examples are independent and identically distributed. However, it turns out that many results of pattern recognition theory carry over a weaker assumption. Namely, under the assumption of conditional independence and identical distribution of objects, while the only assumption on the distribution of labels is that the rate of occurrence of each label should be above some positive threshold. We find a broad class of learning algorithms for which estimations of the probability of the classification error achieved under the classical i.i.d. assumption can be generalized to the similar estimates for case of conditionally i.i.d. examples."
            ],
            "keywords": [],
            "author": [
                "Daniil Ryabko"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/ryabko06a/ryabko06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Noise Tolerant Variants of the Perceptron Algorithm",
            "abstract": [
                "A large number of variants of the Perceptron algorithm have been proposed and partially evaluated in recent work. One type of algorithm aims for noise tolerance by replacing the last hypothesis of the perceptron with another hypothesis or a vote among hypotheses. Another type simply adds a margin term to the perceptron in order to increase robustness and accuracy, as done in support vector machines. A third type borrows further from support vector machines and constrains the update function of the perceptron in ways that mimic soft-margin techniques. The performance of these algorithms, and the potential for combining different techniques, has not been studied in depth. This paper provides such an experimental study and reveals some interesting facts about the algorithms. In particular the perceptron with margin is an effective method for tolerating noise and stabilizing the algorithm. This is surprising since the margin in itself is not designed or used for noise tolerance, and there are no known guarantees for such performance. In most cases, similar performance is obtained by the voted-perceptron which has the advantage that it does not require parameter selection. Techniques using soft margin ideas are run-time intensive and do not give additional performance benefits. The results also highlight the difficulty with automatic parameter selection which is required with some of these variants."
            ],
            "keywords": [
                "perceptron algorithm",
                "on-line learning",
                "noise tolerance",
                "kernel methods"
            ],
            "author": [
                "Roni Khardon",
                "Gabriel Wachman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/khardon07a/khardon07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The xyz algorithm for fast interaction search in high-dimensional data",
            "abstract": [
                "When performing regression on a data set with p variables, it is often of interest to go beyond using main linear effects and include interactions as products between individual variables. For small-scale problems, these interactions can be computed explicitly but this leads to a computational complexity of at least O(p 2) if done naively. This cost can be prohibitive if p is very large. We introduce a new randomised algorithm that is able to discover interactions with high probability and under mild conditions has a runtime that is subquadratic in p. We show that strong interactions can be discovered in almost linear time, whilst finding weaker interactions requires O(p α) operations for 1 < α < 2 depending on their strength. The underlying idea is to transform interaction search into a closest pair problem which can be solved efficiently in subquadratic time. The algorithm is called xyz and is implemented in the language R. We demonstrate its efficiency for application to genome-wide association studies, where more than 10 11 interactions can be screened in under 280 seconds with a single-core 1.2 GHz CPU."
            ],
            "keywords": [
                "interactions",
                "high-dimensional data",
                "regression",
                "computational tradeoffs",
                "close pairs"
            ],
            "author": [
                "Gian-Andrea Thanei",
                "Nicolai Meinshausen",
                "Rajen D Shah"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-515/16-515.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning",
            "abstract": [
                "A problem for many kernel-based methods is that the amount of computation required to find the solution scales as O(n 3), where n is the number of training examples. We develop and analyze an algorithm to compute an easily-interpretable low-rank approximation to an n × n Gram matrix G such that computations of interest may be performed more rapidly. The approximation is of the formG k = CW + k C T , where C is a matrix consisting of a small number c of columns of G and W k is the best rank-k approximation to W , the matrix formed by the intersection between those c columns of G and the corresponding c rows of G. An important aspect of the algorithm is the probability distribution used to randomly sample the columns; we will use a judiciously-chosen and data-dependent nonuniform probability distribution. Let • and • F denote the spectral norm and the Frobenius norm, respectively, of a matrix, and let G k be the best rank-k approximation to G. We prove that by choosing O(k/ε 4) columns"
            ],
            "keywords": [
                "kernel methods",
                "randomized algorithms",
                "Gram matrix",
                "Nyström method"
            ],
            "author": [
                "Petros Drineas",
                "Michael W Mahoney",
                "G − Cw + K C T Ξ ≤ G − G K Ξ +"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/drineas05a/drineas05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nearly optimal classification for semimetrics *",
            "abstract": [
                "We initiate the rigorous study of classification in semimetric spaces, which are point sets with a distance function that is non-negative and symmetric, but need not satisfy the triangle inequality. We define the density dimension dens and discover that it plays a central role in the statistical and algorithmic feasibility of learning in semimetric spaces. We compute this quantity for several widely used semimetrics and present nearly optimal sample compression algorithms, which are then used to obtain generalization guarantees, including fast rates. Our claim of near-optimality holds in both computational and statistical senses. When the sample has radius R and margin γ, we show that it can be compressed down to roughly d = (R/γ) dens points, and further that finding a significantly better compression is algorithmically intractable unless P=NP. This compression implies generalization via standard Occam-type arguments, to which we provide a nearly matching lower bound."
            ],
            "keywords": [
                "semimetric",
                "classification",
                "compression",
                "generalization"
            ],
            "author": [
                "Lee-Ad Gottlieb",
                "Aryeh Kontorovich"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-217/16-217.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Statistical Perspective on Algorithmic Leveraging",
            "abstract": [
                "One popular method for dealing with large-scale data sets is sampling. For example, by using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. This method has been successful in improving computational efficiency of algorithms for matrix problems such as least-squares approximation, least absolute deviations approximation, and low-rank matrix approximation. Existing work has focused on algorithmic issues such as worst-case running times and numerical issues associated with providing high-quality implementations, but none of it addresses statistical aspects of this method."
            ],
            "keywords": [
                "randomized algorithm",
                "leverage scores",
                "subsampling",
                "least squares",
                "linear regression"
            ],
            "author": [
                "Ping Ma",
                "Michael W Mahoney",
                "Mahoney Bin Yu Ma"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/ma15a/ma15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Undercomplete Blind Subspace Deconvolution",
            "abstract": [
                "We introduce the blind subspace deconvolution (BSSD) problem, which is the extension of both the blind source deconvolution (BSD) and the independent subspace analysis (ISA) tasks. We examine the case of the undercomplete BSSD (uBSSD). Applying temporal concatenation we reduce this problem to ISA. The associated 'high dimensional' ISA problem can be handled by a recent technique called joint f-decorrelation (JFD). Similar decorrelation methods have been used previously for kernel independent component analysis (kernel-ICA). More precisely, the kernel canonical correlation (KCCA) technique is a member of this family, and, as is shown in this paper, the kernel generalized variance (KGV) method can also be seen as a decorrelation method in the feature space. These kernel based algorithms will be adapted to the ISA task. In the numerical examples, we (i) examine how efficiently the emerging higher dimensional ISA tasks can be tackled, and (ii) explore the working and advantages of the derived kernel-ISA methods."
            ],
            "keywords": [
                "undercomplete blind subspace deconvolution",
                "independent subspace analysis",
                "joint decorrelation",
                "kernel methods"
            ],
            "author": [
                "Zoltán Szabó"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/szabo07a/szabo07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning the Kernel Function via Regularization",
            "abstract": [
                "We study the problem of finding an optimal kernel from a prescribed convex set of kernels K for learning a real-valued function by regularization. We establish for a wide variety of regularization functionals that this leads to a convex optimization problem and, for square loss regularization, we characterize the solution of this problem. We show that, although K may be an uncountable set, the optimal kernel is always obtained as a convex combination of at most m + basic kernels, where m is the number of data examples. In particular, our results apply to learning the optimal radial kernel or the optimal dot product kernel."
            ],
            "keywords": [],
            "author": [
                "Charles A Micchelli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/micchelli05a/micchelli05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A model of fake data in data-driven analysis * †",
            "abstract": [
                "Data-driven analysis has been increasingly used in various decision making processes. With more sources, including reviews, news, and pictures, can now be used for data analysis, the authenticity of data sources is in doubt. While previous literature attempted to detect fake data piece by piece, in the current work, we try to capture the fake data sender's strategic behavior to detect the fake data source. Specifically, we model the tension between a data receiver who makes data-driven decisions and a fake data sender who benefits from misleading the receiver. We propose a potentially infinite horizon continuous time gametheoretic model with asymmetric information to capture the fact that the receiver does not initially know the existence of fake data and learns about it during the course of the game. We use point processes to model the data traffic, where each piece of data can occur at any discrete moment in a continuous time flow. We fully solve the model and employ numerical examples to illustrate the players' strategies and payoffs for insights. Specifically, our results show that maintaining some suspicion about the data sources and understanding that the sender can be strategic are very helpful to the data receiver. In addition, based on our model, we propose a methodology of detecting fake data that is complementary to the previous studies on this topic, which suggested various approaches on analyzing the data piece by piece. We show that after analyzing each piece of data, understanding a source by looking at the its whole history of pushing data can be helpful."
            ],
            "keywords": [
                "data-driven analysis",
                "fake data",
                "game theory",
                "point process"
            ],
            "author": [
                "Xiaofan Li",
                "Andrew B Whinston"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/17-360/17-360.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors",
            "abstract": [
                "In recursive linear models, the multivariate normal joint distribution of all variables exhibits a dependence structure induced by a recursive (or acyclic) system of linear structural equations. These linear models have a long tradition and appear in seemingly unrelated regressions, structural equation modelling, and approaches to causal inference. They are also related to Gaussian graphical models via a classical representation known as a path diagram. Despite the models' long history, a number of problems remain open. In this paper, we address the problem of computing maximum likelihood estimates in the subclass of 'bow-free' recursive linear models. The term 'bow-free' refers to the condition that the errors for variables i and j be uncorrelated if variable i occurs in the structural equation for variable j. We introduce a new algorithm, termed Residual Iterative Conditional Fitting (RICF), that can be implemented using only least squares computations. In contrast to existing algorithms, RICF has clear convergence properties and yields exact maximum likelihood estimates after the first iteration whenever the MLE is available in closed form."
            ],
            "keywords": [
                "linear regression",
                "maximum likelihood estimation",
                "path diagram",
                "structural equation model",
                "recursive semi-Markov model",
                "residual iterative conditional fitting"
            ],
            "author": [
                "Mathias Drton",
                "Michael Eichler",
                "Maastrichtuniversity Nl",
                "Thomas S Richardson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/drton09a/drton09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure",
            "abstract": [
                "We study the problem of learning an optimal Bayesian network in a constrained search space; skeletons are compelled to be subgraphs of a given undirected graph called the superstructure. The previously derived constrained optimal search (COS) remains limited even for sparse superstructures. To extend its feasibility, we propose to divide the superstructure into several clusters and perform an optimal search on each of them. Further, to ensure acyclicity, we introduce the concept of ancestral constraints (ACs) and derive an optimal algorithm satisfying a given set of ACs. Finally, we theoretically derive the necessary and sufficient sets of ACs to be considered for finding an optimal constrained graph. Empirical evaluations demonstrate that our algorithm can learn optimal Bayesian networks for some graphs containing several hundreds of vertices, and even for superstructures having a high average degree (up to four), which is a drastic improvement in feasibility over the previous optimal algorithm. Learnt networks are shown to largely outperform state-of-the-art heuristic algorithms both in terms of score and structural hamming distance."
            ],
            "keywords": [
                "Bayesian networks",
                "structure learning",
                "constrained optimal search"
            ],
            "author": [
                "Kaname Kojima",
                "Eric Perrier",
                "Satoru Miyano"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/kojima10a/kojima10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Discriminative Clustering with Sparse Regularizers",
            "abstract": [
                "Clustering high-dimensional data often requires some form of dimensionality reduction, where clustered variables are separated from \"noise-looking\" variables. We cast this problem as finding a low-dimensional projection of the data which is well-clustered. This yields a one-dimensional projection in the simplest situation with two clusters, and extends naturally to a multi-label scenario for more than two clusters. In this paper, (a) we first show that this joint clustering and dimension reduction formulation is equivalent to previously proposed discriminative clustering frameworks, thus leading to convex relaxations of the problem; (b) we propose a novel sparse extension, which is still cast as a convex relaxation and allows estimation in higher dimensions; (c) we propose a natural extension for the multi-label scenario; (d) we provide a new theoretical analysis of the performance of these formulations with a simple probabilistic model, leading to scalings over the form d = O(√ n) for the affine invariant case and d = O(n) for the sparse case, where n is the number of examples and d the ambient dimension; and finally, (e) we propose an efficient iterative algorithm with running-time complexity proportional to O(nd), improving on earlier algorithms for discriminative clustering with the square loss, which had quadratic complexity in the number of examples."
            ],
            "keywords": [],
            "author": [
                "Nicolas Flammarion",
                "Francis Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-429/16-429.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stationary Features and Cat Detection",
            "abstract": [
                "Most discriminative techniques for detecting instances from object categories in still images consist of looping over a partition of a pose space with dedicated binary classifiers. The efficiency of this strategy for a complex pose, that is, for fine-grained descriptions, can be assessed by measuring the effect of sample size and pose resolution on accuracy and computation. Two conclusions emerge: (1) fragmenting the training data, which is inevitable in dealing with high in-class variation, severely reduces accuracy; (2) the computational cost at high resolution is prohibitive due to visiting a massive pose partition. To overcome data-fragmentation we propose a novel framework centered on pose-indexed features which assign a response to a pair consisting of an image and a pose, and are designed to be stationary: the probability distribution of the response is always the same if an object is actually present. Such features allow for efficient, one-shot learning of pose-specific classifiers. To avoid expensive scene processing, we arrange these classifiers in a hierarchy based on nested partitions of the pose; as in previous work on coarse-to-fine search, this allows for efficient processing. The hierarchy is then \"folded\" for training: all the classifiers at each level are derived from one base predictor learned from all the data. The hierarchy is \"unfolded\" for testing: parsing a scene amounts to examining increasingly finer object descriptions only when there is sufficient evidence for coarser ones. In this way, the detection results are equivalent to an exhaustive search at high resolution. We illustrate these ideas by detecting and localizing cats in highly cluttered greyscale scenes."
            ],
            "keywords": [
                "supervised learning",
                "computer vision",
                "image interpretation",
                "cats",
                "stationary features",
                "hierarchical search"
            ],
            "author": [
                "François Fleuret",
                "Donald Geman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/fleuret08a/fleuret08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Bilinear Model for Matching Queries and Documents",
            "abstract": [
                "The task of matching data from two heterogeneous domains naturally arises in various areas such as web search, collaborative filtering, and drug design. In web search, existing work has designed relevance models to match queries and documents by exploiting either user clicks or content of queries and documents. To the best of our knowledge, however, there has been little work on principled approaches to leveraging both clicks and content to learn a matching model for search. In this paper, we propose a framework for learning to match heterogeneous objects. The framework learns two linear mappings for two objects respectively, and matches them via the dot product of their images after mapping. Moreover, when different regularizations are enforced, the framework renders a rich family of matching models. With orthonormal constraints on mapping functions, the framework subsumes Partial Least Squares (PLS) as a special case. Alternatively, with a ℓ 1 +ℓ 2 regularization, we obtain a new model called Regularized Mapping to Latent Structures (RMLS). RMLS enjoys many advantages over PLS, including lower time complexity and easy parallelization. To further understand the matching framework, we conduct generalization analysis and apply the result to both PLS and RMLS. We apply the framework to web search and implement both PLS and RMLS using a click-through bipartite with metadata representing features of queries and documents. We test the efficacy and scalability of RMLS and PLS on large scale web search problems. The results show that both PLS and RMLS can significantly outperform baseline methods, while RMLS substantially speeds up the learning process."
            ],
            "keywords": [
                "web search",
                "partial least squares",
                "regularized mapping to latent structures",
                "generalization analysis"
            ],
            "author": [
                "Wei Wu",
                "Zhengdong Lu",
                "Hang Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/wu13a/wu13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantification Under Prior Probability Shift: the Ratio Estimator and its Extensions",
            "abstract": [
                "The quantification problem consists of determining the prevalence of a given label in a target population. However, one often has access to the labels in a sample from the training population but not in the target population. A common assumption in this situation is that of prior probability shift, that is, once the labels are known, the distribution of the features is the same in the training and target populations. In this paper, we derive a new lower bound for the risk of the quantification problem under the prior shift assumption. Complementing this lower bound, we present a new approximately minimax class of estimators, ratio estimators, which generalize several previous proposals in the literature. Using a weaker version of the prior shift assumption, which can be tested, we show that ratio estimators can be used to build confidence intervals for the quantification problem. We also extend the ratio estimator so that it can: (i) incorporate labels from the target population, when they are available and (ii) estimate how the prevalence of positive labels varies according to a function of certain covariates."
            ],
            "keywords": [
                "quantification",
                "prior probability shift",
                "data set shift",
                "domain shift",
                "semisupervised learning"
            ],
            "author": [
                "Afonso Fernandes Vaz",
                "Rafael Izbicki"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-456/18-456.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "libDAI: A Free and Open Source C++ Library for Discrete Approximate Inference in Graphical Models",
            "abstract": [
                "This paper describes the software package libDAI, a free & open source C++ library that provides implementations of various exact and approximate inference methods for graphical models with discrete-valued variables. libDAI supports directed graphical models (Bayesian networks) as well as undirected ones (Markov random fields and factor graphs). It offers various approximations of the partition sum, marginal probability distributions and maximum probability states. Parameter learning is also supported. A feature comparison with other open source software packages for approximate inference is given. libDAI is licensed under the GPL v2+ license and is available at http://www.libdai.org."
            ],
            "keywords": [
                "probabilistic graphical models",
                "approximate inference",
                "open source software",
                "factor graphs",
                "Markov random fields",
                "Bayesian networks"
            ],
            "author": [
                "Joris M Mooij"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/mooij10a/mooij10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Geometric Variance Reduction in Markov Chains: Application to Value Function and Gradient Estimation",
            "abstract": [
                "We study a variance reduction technique for Monte Carlo estimation of functionals in Markov chains. The method is based on designing sequential control variates using successive approximations of the function of interest V. Regular Monte Carlo estimates have a variance of O(1/N), where N is the number of sample trajectories of the Markov chain. Here, we obtain a geometric variance reduction O(ρ N) (with ρ < 1) up to a threshold that depends on the approximation error V − AV , where A is an approximation operator linear in the values. Thus, if V belongs to the right approximation space (i.e. AV = V), the variance decreases geometrically to zero. An immediate application is value function estimation in Markov chains, which may be used for policy evaluation in a policy iteration algorithm for solving Markov Decision Processes. Another important domain, for which variance reduction is highly needed, is gradient estimation, that is computing the sensitivity ∂ α V of the performance measure V with respect to some parameter α of the transition probabilities. For example, in policy parametric optimization, computing an estimate of the policy gradient is required to perform a gradient optimization method. We show that, using two approximations for the value function and the gradient, a geometric variance reduction is also achieved, up to a threshold that depends on the approximation errors of both of those representations."
            ],
            "keywords": [],
            "author": [
                "Rémi Munos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/munos06a/munos06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "AI Explainability 360: An Extensible Toolkit for Understanding Data and Machine Learning Models",
            "abstract": [
                "As artificial intelligence algorithms make further inroads in high-stakes societal applications, there are increasing calls from multiple stakeholders for these algorithms to explain their outputs. To make matters more challenging, different personas of consumers of explanations have different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360, an open-source Python toolkit featuring ten diverse and state-of-the-art explainability methods and two evaluation metrics (http://aix360.mybluemix.net). Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of interpretation and explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline. The toolkit is not only the software, but also guidance material, tutorials, and an interactive web demo to introduce AI explainability to different audiences. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed."
            ],
            "keywords": [
                "explainability",
                "interpretability",
                "transparency",
                "taxonomy",
                "open source"
            ],
            "author": [
                "Vijay Arya",
                "Rachel K E Bellamy",
                "Samuel C Hoffman",
                "Stephanie Houde",
                "Q Vera Liao",
                "Ronny Luss",
                "Pablo Pedemonte",
                "Moninder Singh",
                "Dennis Wei"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-1035/19-1035.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Estimation of Low Rank Density Matrices",
            "abstract": [
                "The density matrices are positively semi-definite Hermitian matrices of unit trace that describe the state of a quantum system. The goal of the paper is to develop minimax lower bounds on error rates of estimation of low rank density matrices in trace regression models used in quantum state tomography (in particular, in the case of Pauli measurements) with explicit dependence of the bounds on the rank and other complexity parameters. Such bounds are established for several statistically relevant distances, including quantum versions of Kullback-Leibler divergence (relative entropy distance) and of Hellinger distance (so called Bures distance), and Schatten p-norm distances. Sharp upper bounds and oracle inequalities for least squares estimator with von Neumann entropy penalization are obtained showing that minimax lower bounds are attained (up to logarithmic factors) for these distances."
            ],
            "keywords": [
                "quantum state tomography",
                "low rank density matrix",
                "minimax lower bounds"
            ],
            "author": [
                "Vladimir Koltchinskii",
                "Dong Xia",
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/koltchinskii15a/koltchinskii15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Spectral Approach for the Design of Experiments: Design, Analysis and Algorithms",
            "abstract": [
                "This paper proposes a new approach to construct high quality space-filling sample designs. First, we propose a novel technique to quantify the space-filling property and optimally trade-off uniformity and randomness in sample designs in arbitrary dimensions. Second, we connect the proposed metric (defined in the spatial domain) to the quality metric of the design performance (defined in the spectral domain). This connection serves as an analytic framework for evaluating the qualitative properties of space-filling designs in general. Using the theoretical insights provided by this spatial-spectral analysis, we derive the notion of optimal space-filling designs, which we refer to as space-filling spectral designs. Third, we propose an efficient estimator to evaluate the space-filling properties of sample designs in arbitrary dimensions and use it to develop an optimization framework for generating high quality space-filling designs. Finally, we carry out a detailed performance comparison on two different applications in varying dimensions: a) image reconstruction and b) surrogate modeling for several benchmark optimization functions and a physics simulation code for inertial confinement fusion (ICF). Our results clearly evidence the superiority of the proposed space-filling designs over existing approaches, particularly in high dimensions."
            ],
            "keywords": [
                "design of experiments",
                "space-filling",
                "poisson-disk sampling",
                "surrogate modeling",
                "regression"
            ],
            "author": [
                "Bhavya Kailkhura",
                "Jayaraman J Thiagarajan",
                "Charvi Rastogi",
                "Pramod K Varshney"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-735/17-735.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Recursive Method for Structural Learning of Directed Acyclic Graphs",
            "abstract": [
                "In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is first decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efficiency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method."
            ],
            "keywords": [
                "Bayesian network",
                "conditional independence",
                "decomposition",
                "directed acyclic graph",
                "structural learning"
            ],
            "author": [
                "Xianchao Xie"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/xie08a/xie08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scikit-learn: Machine Learning in Python",
            "abstract": [
                "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."
            ],
            "keywords": [
                "Python",
                "supervised learning",
                "unsupervised learning",
                "model selection"
            ],
            "author": [
                "Fabian Pedregosa",
                "Vincent Michel",
                "Bertrand Thirion",
                "Olivier Grisel",
                "Mathieu Blondel",
                "Peter Prettenhofer",
                "Ron Weiss",
                "Vincent Dubourg",
                "Jake Vanderplas",
                "David Cournapeau",
                "Enthought J J Thompson",
                "Gaël Varoquaux",
                "Alexandre Gramfort",
                "Alexandre Passos",
                "Matthieu Brucher",
                "Matthieu Perrot",
                "VAROQUAUX Duchesnay Pedregosa",
                "Al Matthieu Brucher",
                "Edouard Duchesnay"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/pedregosa11a/pedregosa11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Discriminant Analysis, Ridge Regression and Beyond",
            "abstract": [
                "Fisher linear discriminant analysis (FDA) and its kernel extension-kernel discriminant analysis (KDA)-are well known methods that consider dimensionality reduction and classification jointly. While widely deployed in practical problems, there are still unresolved issues surrounding their efficient implementation and their relationship with least mean squares procedures. In this paper we address these issues within the framework of regularized estimation. Our approach leads to a flexible and efficient implementation of FDA as well as KDA. We also uncover a general relationship between regularized discriminant analysis and ridge regression. This relationship yields variations on conventional FDA based on the pseudoinverse and a direct equivalence to an ordinary least squares estimator."
            ],
            "keywords": [
                "Fisher discriminant analysis",
                "reproducing kernel",
                "generalized eigenproblems",
                "ridge regression",
                "singular value decomposition",
                "eigenvalue decomposition"
            ],
            "author": [
                "Zhihua Zhang",
                "Guang Dai",
                "Congfu Xu",
                "Michael I Jordan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/zhang10b/zhang10b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Multinomial Logit Gaussian Process",
            "abstract": [
                "Gaussian process prior with an appropriate likelihood function is a flexible non-parametric model for a variety of learning tasks. One important and standard task is multi-class classification, which is the categorization of an item into one of several fixed classes. A usual likelihood function for this is the multinomial logistic likelihood function. However, exact inference with this model has proved to be difficult because high-dimensional integrations are required. In this paper, we propose a variational approximation to this model, and we describe the optimization of the variational parameters. Experiments have shown our approximation to be tight. In addition, we provide dataindependent bounds on the marginal likelihood of the model, one of which is shown to be much tighter than the existing variational mean-field bound in the experiments. We also derive a proper lower bound on the predictive likelihood that involves the Kullback-Leibler divergence between the approximating and the true posterior. We combine our approach with a recently proposed sparse approximation to give a variational sparse approximation to the Gaussian process multi-class model. We also derive criteria which can be used to select the inducing set, and we show the effectiveness of these criteria over random selection in an experiment."
            ],
            "keywords": [
                "Gaussian process",
                "probabilistic classification",
                "multinomial logistic",
                "variational approximation",
                "sparse approximation"
            ],
            "author": [
                "Ming A Kian"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/chai12a/chai12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds",
            "abstract": [
                "The presence of asymmetry in the misclassification costs or class prevalences is a common occurrence in the pattern classification domain. While much interest has been devoted to the study of cost-sensitive learning techniques, the relationship between cost-sensitive learning and the specification of the model set in a parametric estimation framework remains somewhat unclear. To that end, we differentiate between the case of the model including the true posterior, and that in which the model is misspecified. In the former case, it is shown that thresholding the maximum likelihood (ML) estimate is an asymptotically optimal solution to the risk minimization problem. On the other hand, under model misspecification, it is demonstrated that thresholded ML is suboptimal and that the risk-minimizing solution varies with the misclassification cost ratio. Moreover, we analytically show that the negative weighted log likelihood (Elkan, 2001) is a tight, convex upper bound of the empirical loss. Coupled with empirical results on several real-world data sets, we argue that weighted ML is the preferred cost-sensitive technique."
            ],
            "keywords": [
                "empirical risk minimization",
                "loss function",
                "cost-sensitive learning",
                "imbalanced data sets"
            ],
            "author": [
                "Jacek P Dmochowski",
                "Paul Sajda",
                "Lucas C Parra"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/dmochowski10a/dmochowski10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Set-Valued Approachability and Online Learning with Partial Monitoring",
            "abstract": [
                "Approachability has become a standard tool in analyzing learning algorithms in the adversarial online learning setup. We develop a variant of approachability for games where there is ambiguity in the obtained reward: it belongs to a set rather than being a single vector. Using this variant we tackle the problem of approachability in games with partial monitoring and develop a simple and generally efficient strategy (i.e., with constant per-step complexity) for this setup. As an important example, we instantiate our general strategy to the case when external regret or internal regret is to be minimized under partial monitoring."
            ],
            "keywords": [
                "online learning",
                "approachability",
                "regret",
                "partial monitoring"
            ],
            "author": [
                "Shie Mannor",
                "Gilles Stoltz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/mannor14a/mannor14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lower Bounds and Aggregation in Density Estimation",
            "abstract": [
                "In this paper we prove the optimality of an aggregation procedure. We prove lower bounds for aggregation of model selection type of M density estimators for the Kullback-Leibler divergence (KL), the Hellinger's distance and the L 1-distance. The lower bound, with respect to the KL distance, can be achieved by the on-line type estimate suggested, among others, by Yang (2000a). Combining these results, we state that log M/n is an optimal rate of aggregation in the sense of Tsybakov (2003), where n is the sample size."
            ],
            "keywords": [
                "aggregation",
                "optimal rates",
                "Kullback-Leibler divergence"
            ],
            "author": [
                "Guillaume Lecué"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/lecue06a/lecue06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Linear Non-Gaussian Acyclic Model for Causal Discovery",
            "abstract": [
                "In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data. Such methods make various assumptions on the data generating process to facilitate its identification from purely observational data. Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assumptions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-Gaussian distributions of non-zero variances. The solution relies on the use of the statistical method known as independent component analysis, and does not require any pre-specified time-ordering of the variables. We provide a complete Matlab package for performing this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artificially generated data and real-world data."
            ],
            "keywords": [
                "independent component analysis",
                "non-Gaussianity",
                "causal discovery",
                "directed acyclic graph",
                "non-experimental data"
            ],
            "author": [
                "Shohei Shimizu",
                "Patrik O Hoyer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/shimizu06a/shimizu06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient State-Space Inference of Periodic Latent Force Models",
            "abstract": [
                "Latent force models (LFM) are principled approaches to incorporating solutions to differential equations within non-parametric inference methods. Unfortunately, the development and application of LFMs can be inhibited by their computational cost, especially when closed-form solutions for the LFM are unavailable, as is the case in many real world problems where these latent forces exhibit periodic behaviour. Given this, we develop a new sparse representation of LFMs which considerably improves their computational efficiency, as well as broadening their applicability, in a principled way, to domains with periodic or near periodic latent forces. Our approach uses a linear basis model to approximate one generative model for each periodic force. We assume that the latent forces are generated from Gaussian process priors and develop a linear basis model which fully expresses these priors. We apply our approach to model the thermal dynamics of domestic buildings and show that it is effective at predicting day-ahead temperatures within the homes. We also apply our approach within queueing theory in which quasi-periodic arrival rates are modelled as latent forces. In both cases, we demonstrate that our approach can be implemented"
            ],
            "keywords": [
                "latent force models",
                "Gaussian processes",
                "Kalman filter",
                "kernel principle component analysis",
                "queueing theory"
            ],
            "author": [
                "Steven Reece",
                "Siddhartha Ghosh",
                "Alex Rogers",
                "Stephen Roberts",
                "Nicholas R Jennings"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/reece14a/reece14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation",
            "abstract": [
                "Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a nonnegligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds."
            ],
            "keywords": [
                "model selection",
                "performance evaluation",
                "bias-variance trade-off",
                "selection bias",
                "overfitting"
            ],
            "author": [
                "Gavin C Cawley",
                "Nicola L C Talbot"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/cawley10a/cawley10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Analysis of Metric Graph Reconstruction",
            "abstract": [
                "A metric graph is a 1-dimensional stratified metric space consisting of vertices and edges or loops glued together. Metric graphs can be naturally used to represent and model data that take the form of noisy filamentary structures, such as street maps, neurons, networks of rivers and galaxies. We consider the statistical problem of reconstructing the topology of a metric graph embedded in R D from a random sample. We derive lower and upper bounds on the minimax risk for the noiseless case and tubular noise case. The upper bound is based on the reconstruction algorithm given in Aanjaneya et al. (2012)."
            ],
            "keywords": [
                "metric graph",
                "filament",
                "reconstruction",
                "manifold learning",
                "minimax estimation"
            ],
            "author": [
                "Fabrizio Lecci",
                "Alessandro Rinaldo",
                "Larry Wasserman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/lecci14a/lecci14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Posterior Regularization for Structured Latent Variable Models",
            "abstract": [
                "We present posterior regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efficiently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efficiency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efficient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment."
            ],
            "keywords": [
                "posterior regularization framework",
                "unsupervised learning",
                "latent variables models",
                "prior knowledge",
                "natural language processing"
            ],
            "author": [
                "Kuzman Ganchev",
                "Jennifer Gillenwater",
                "Ben Taskar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/ganchev10a/ganchev10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": null,
            "abstract": [
                "It is a well-known problem that obtaining a correct bandwidth and/or smoothing parameter in nonparametric regression is difficult in the presence of correlated errors. There exist a wide variety of methods coping with this problem, but they all critically depend on a tuning procedure which requires accurate information about the correlation structure. We propose a bandwidth selection procedure based on bimodal kernels which successfully removes the correlation without requiring any prior knowledge about its structure and its parameters. Further, we show that the form of the kernel is very important when errors are correlated which is in contrast to the independent and identically distributed (i.i.d.) case. Finally, some extensions are proposed to use the proposed criterion in support vector machines and least squares support vector machines for regression."
            ],
            "keywords": [
                "nonparametric regression",
                "correlated errors",
                "bandwidth choice",
                "cross-validation",
                "shortrange dependence",
                "bimodal kernel"
            ],
            "author": [
                "Kris De Brabanter",
                "Jos De Brabanter",
                "Johan A K Suykens",
                "Bart De"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/debrabanter11a/debrabanter11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stable Graphical Models",
            "abstract": [
                "Stable random variables are motivated by the central limit theorem for densities with (potentially) unbounded variance and can be thought of as natural generalizations of the Gaussian distribution to skewed and heavy-tailed phenomenon. In this paper, we introduce α-stable graphical (α-SG) models, a class of multivariate stable densities that can also be represented as Bayesian networks whose edges encode linear dependencies between random variables. One major hurdle to the extensive use of stable distributions is the lack of a closed-form analytical expression for their densities. This makes penalized maximumlikelihood based learning computationally demanding. We establish theoretically that the Bayesian information criterion (BIC) can asymptotically be reduced to the computationally more tractable minimum dispersion criterion (MDC) and develop StabLe, a structure learning algorithm based on MDC. We use simulated datasets for five benchmark network topologies to empirically demonstrate how StabLe improves upon ordinary least squares (OLS) regression. We also apply StabLe to microarray gene expression data for lymphoblastoid cells from 727 individuals belonging to eight global population groups. We establish that StabLe improves test set performance relative to OLS via tenfold cross-validation. Finally, we develop SGEX, a method for quantifying differential expression of genes between different population groups."
            ],
            "keywords": [
                "Bayesian networks",
                "stable distributions",
                "linear regression",
                "structure learning",
                "gene expression",
                "differential expression"
            ],
            "author": [
                "Navodit Misra",
                "Ercan E Kuruoglu",
                "Max Planck"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-150/14-150.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters",
            "abstract": [
                "Tuning the regularisation and kernel hyperparameters is a vital step in optimising the generalisation performance of kernel methods, such as the support vector machine (SVM). This is most often performed by minimising a resampling/cross-validation based model selection criterion, however there seems little practical guidance on the most suitable form of resampling. This paper presents the results of an extensive empirical evaluation of resampling procedures for SVM hyperparameter selection, designed to address this gap in the machine learning literature. We tested 15 different resampling procedures on 121 binary classification data sets in order to select the best SVM hyperparameters. We used three very different statistical procedures to analyse the results: the standard multi-classifier/multidata set procedure proposed by Demšar, the confidence intervals on the excess loss of each procedure in relation to 5-fold cross validation, and the Bayes factor analysis proposed by Barber. We conclude that a 2-fold procedure is appropriate to select the hyperparameters of an SVM for data sets for 1000 or more datapoints, while a 3-fold procedure is appropriate for smaller data sets."
            ],
            "keywords": [
                "Hyperparameters",
                "SVM",
                "resampling",
                "cross-validation",
                "k-fold",
                "bootstrap"
            ],
            "author": [
                "Jacques Wainer",
                "Gavin Cawley"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-174/16-174.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Probabilistic Characterization of Random Decision Trees",
            "abstract": [
                "In this paper we use the methodology introduced by Dhurandhar and Dobra (2009) for analyzing the error of classifiers and the model selection measures, to analyze decision tree algorithms. The methodology consists of obtaining parametric expressions for the moments of the generalization error (GE) for the classification model of interest, followed by plotting these expressions for interpretability. The major challenge in applying the methodology to decision trees, the main theme of this work, is customizing the generic expressions for the moments of GE to this particular classification algorithm. The specific contributions we make in this paper are: (a) we primarily characterize a subclass of decision trees namely, Random decision trees, (b) we discuss how the analysis extends to other decision tree algorithms and (c) in order to extend the analysis to certain model selection measures, we generalize the relationships between the moments of GE and moments of the model selection measures given in (Dhurandhar and Dobra, 2009) to randomized classification algorithms. An empirical comparison of the proposed method with Monte Carlo and distribution free bounds obtained using Breiman's formula, depicts the advantages of the method in terms of running time and accuracy. It thus showcases the use of the deployed methodology as an exploratory tool to study learning algorithms."
            ],
            "keywords": [
                "moments",
                "generalization error",
                "decision trees"
            ],
            "author": [
                "Amit Dhurandhar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/dhurandhar08a/dhurandhar08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Inference for Linear Support Vector Machine",
            "abstract": [
                "The growing size of modern data brings many new challenges to existing statistical inference methodologies and theories, and calls for the development of distributed inferential approaches. This paper studies distributed inference for linear support vector machine (SVM) for the binary classification task. Despite a vast literature on SVM, much less is known about the inferential properties of SVM, especially in a distributed setting. In this paper, we propose a multi-round distributed linear-type (MDL) estimator for conducting inference for linear SVM. The proposed estimator is computationally efficient. In particular, it only requires an initial SVM estimator and then successively refines the estimator by solving simple weighted least squares problem. Theoretically, we establish the Bahadur representation of the estimator. Based on the representation, the asymptotic normality is further derived, which shows that the MDL estimator achieves the optimal statistical efficiency, i.e., the same efficiency as the classical linear SVM applying to the entire data set in a single machine setup. Moreover, our asymptotic result avoids the condition on the number of machines or data batches, which is commonly assumed in distributed estimation literature, and allows the case of diverging dimension. We provide simulation studies to demonstrate the performance of the proposed MDL estimator."
            ],
            "keywords": [
                "Linear support vector machine",
                "distributed inference",
                "Bahadur representation",
                "asymptotic theory"
            ],
            "author": [
                "Xiaozhou Wang",
                "Zhuoyi Yang",
                "Xi Chen",
                "Weidong Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-801/18-801.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PyOD: A Python Toolbox for Scalable Outlier Detection",
            "abstract": [
                "PyOD is an open-source Python toolbox for performing scalable outlier detection on multivariate data. Uniquely, it provides access to a wide range of outlier detection algorithms, including established outlier ensembles and more recent neural network-based approaches, under a single, well-documented API designed for use by both practitioners and researchers. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, maintainability checks, interactive examples and parallelization are emphasized as core components in the toolbox's development. PyOD is compatible with both Python 2 and 3 and can be installed through Python Package Index (PyPI) or https://github.com/yzhao062/pyod."
            ],
            "keywords": [
                "anomaly detection",
                "outlier detection",
                "outlier ensembles",
                "neural networks",
                "machine learning",
                "data mining",
                "Python"
            ],
            "author": [
                "Yue Zhao",
                "Zain Nasrullah",
                "Zheng Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-011/19-011.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Managing Diversity in Regression Ensembles",
            "abstract": [
                "Ensembles are a widely used and effective technique in machine learning-their success is commonly attributed to the degree of disagreement, or 'diversity', within the ensemble. For ensembles where the individual estimators output crisp class labels, this 'diversity' is not well understood and remains an open research issue. For ensembles of regression estimators, the diversity can be exactly formulated in terms of the covariance between individual estimator outputs, and the optimum level is expressed in terms of a bias-variance-covariance trade-off. Despite this, most approaches to learning ensembles use heuristics to encourage the right degree of diversity. In this work we show how to explicitly control diversity through the error function. The first contribution of this paper is to show that by taking the combination mechanism for the ensemble into account we can derive an error function for each individual that balances ensemble diversity with individual accuracy. We show the relationship between this error function and an existing algorithm called negative correlation learning, which uses a heuristic penalty term added to the mean squared error function. It is demonstrated that these methods control the bias-variance-covariance trade-off systematically, and can be utilised with any estimator capable of minimising a quadratic error function, for example MLPs, or RBF networks. As a second contribution, we derive a strict upper bound on the coefficient of the penalty term, which holds for any estimator that can be cast in a generalised linear regression framework, with mild assumptions on the basis functions. Finally we present the results of an empirical study, showing significant improvements over simple ensemble learning, and finding that this technique is competitive with a variety of methods, including boosting, bagging, mixtures of experts, and Gaussian processes, on a number of tasks."
            ],
            "keywords": [
                "ensemble",
                "diversity",
                "regression estimators",
                "neural networks",
                "hessian matrix",
                "negative correlation learning"
            ],
            "author": [
                "Gavin Brown",
                "Jeremy L Wyatt",
                "Peter Tino"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/brown05a/brown05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dual Principal Component Pursuit",
            "abstract": [
                "We consider the problem of learning a linear subspace from data corrupted by outliers. Classical approaches are typically designed for the case in which the subspace dimension is small relative to the ambient dimension. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement; as such, it is particularly suitable for subspaces whose dimension is close to the ambient dimension (subspaces of high relative dimension). We pose the problem of computing normal vectors to the inlier subspace as a non-convex 1 minimization problem on the sphere, which we call Dual Principal Component Pursuit (DPCP) problem. We provide theoretical guarantees under which every global solution to DPCP is a vector in the orthogonal complement of the inlier subspace. Moreover, we relax the non-convex DPCP problem to a recursion of linear programs whose solutions are shown to converge in a finite number of steps to a vector orthogonal to the subspace. In particular, when the inlier subspace is a hyperplane, the solutions to the recursion of linear programs converge to the global minimum of the non-convex DPCP problem in a finite number of steps. We also propose algorithms based on alternating minimization and iteratively re-weighted least squares, which are suitable for dealing with large-scale data. Experiments on synthetic data show that the proposed methods are able to handle more outliers and higher relative dimensions than current state-of-the-art methods, while experiments in the context of the three-view geometry problem in computer vision suggest that the proposed methods can be a useful or even superior alternative to traditional RANSAC-based approaches for computer vision and other applications."
            ],
            "keywords": [
                "Outliers",
                "Robust Principal Component Analysis",
                "High Relative Dimension",
                "1 Minimization",
                "Non-Convex Optimization",
                "Linear Programming",
                "Trifocal Tensor"
            ],
            "author": [
                "Manolis C Tsakiris",
                "René Vidal"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-436/17-436.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Strong Consistency of the Prototype Based Clustering in Probabilistic Space",
            "abstract": [
                "In this paper we formulate in general terms an approach to prove strong consistency of the Empirical Risk Minimisation inductive principle applied to the prototype or distance based clustering. This approach was motivated by the Divisive Information-Theoretic Feature Clustering model in probabilistic space with Kullback-Leibler divergence, which may be regarded as a special case within the Clustering Minimisation framework."
            ],
            "keywords": [
                "clustering",
                "probabilistic space",
                "consistency"
            ],
            "author": [
                "Vladimir Nikulin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/nikulin15a/nikulin15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Compact Convex Projections",
            "abstract": [
                "We study the usefulness of conditional gradient like methods for determining projections onto convex sets, in particular, projections onto naturally arising convex sets in reproducing kernel Hilbert spaces. Our work is motivated by the recently introduced kernel herding algorithm which is closely related to the Conditional Gradient Method (CGM). It is known that the herding algorithm converges with a rate of 1=t , where t counts the number of iterations, when a point in the interior of a convex set is approximated. We generalize this result and we provide a necessary and sufficient condition for the algorithm to approximate projections with a rate of 1=t. The CGM, which is in general vastly superior to the herding algorithm, achieves only an inferior rate of 1= p t in this setting. We study the usefulness of such projection algorithms further by exploring ways to use these for solving concrete machine learning problems. In particular, we derive non-parametric regression algorithms which use at their core a slightly modified kernel herding algorithm to determine projections. We derive bounds to control approximation errors of these methods and we demonstrate via experiments that the developed regressors are en-par with state-of-the-art regression algorithms for large scale problems."
            ],
            "keywords": [],
            "author": [
                "Steffen Grünewälder"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-147/16-147.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Absent Data Generating Classifier for Imbalanced Class Sizes",
            "abstract": [
                "We propose an algorithm for two-class classification problems when the training data are imbalanced. This means the number of training instances in one of the classes is so low that the conventional classification algorithms become ineffective in detecting the minority class. We present a modification of the kernel Fisher discriminant analysis such that the imbalanced nature of the problem is explicitly addressed in the new algorithm formulation. The new algorithm exploits the properties of the existing minority points to learn the effects of other minority data points, had they actually existed. The algorithm proceeds iteratively by employing the learned properties and conditional sampling in such a way that it generates sufficient artificial data points for the minority set, thus enhancing the detection probability of the minority class. Implementing the proposed method on a number of simulated and real data sets, we show that our proposed method performs competitively compared to a set of alternative state-of-the-art imbalanced classification algorithms."
            ],
            "keywords": [
                "kernel Fisher discriminant analysis",
                "imbalanced data",
                "two-class classification"
            ],
            "author": [
                "Arash Pourhabib",
                "Bani K Mallick",
                "Yu Ding"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/pourhabib15a/pourhabib15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classification of Time Sequences using Graphs of Temporal Constraints",
            "abstract": [
                "We introduce two algorithms that learn to classify Symbolic and Scalar Time Sequences (SSTS); an extension of multivariate time series. An SSTS is a set of events and a set of scalars. An event is defined by a symbol and a time-stamp. A scalar is defined by a symbol and a function mapping a number for each possible time stamp of the data. The proposed algorithms rely on temporal patterns called Graph of Temporal Constraints (GTC). A GTC is a directed graph in which vertices express occurrences of specific events, and edges express temporal constraints between occurrences of pairs of events. Additionally, each vertex of a GTC can be augmented with numeric constraints on scalar values. We allow GTCs to be cyclic and/or disconnected. The first of the introduced algorithms extracts sets of co-dependent GTCs to be used in a voting mechanism. The second algorithm builds decision forest like representations where each node is a GTC. In both algorithms, extraction of GTCs and model building are interleaved. Both algorithms are closely related to each other and they exhibit complementary properties including complexity, performance, and interpretability. The main novelties of this work reside in direct building of the model and efficient learning of GTC structures. We explain the proposed algorithms and evaluate their performance against a diverse collection of 59 benchmark data sets. In these experiments, our algorithms come across as highly competitive and in most cases closely match or outperform state-of-the-art alternatives in terms of the computational speed while dominating in terms of the accuracy of classification of time sequences."
            ],
            "keywords": [
                "classification",
                "temporal data",
                "sequential data",
                "graphical constraint models",
                "decision forests",
                "symbolic and scalar time sequences",
                "supervised learning"
            ],
            "author": [
                "Mathieu Guillame-Bert",
                "Artur Dubrawski"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-403/15-403.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fairness Constraints: A Flexible Approach for Fair Classification",
            "abstract": [
                "Algorithmic decision making is employed in an increasing number of real-world applications to aid human decision making. While it has shown considerable promise in terms of improved decision accuracy, in some scenarios, its outcomes have been also shown to impose an unfair (dis)advantage on people from certain social groups (e.g., women, blacks). In this context, there is a need for computational techniques to limit unfairness in algorithmic decision making. In this work, we take a step forward to fulfill that need and introduce a flexible constraint-based framework to enable the design of fair margin-based classifiers. The main technical innovation of our framework is a general and intuitive measure of decision boundary unfairness, which serves as a tractable proxy to several of the most popular computational definitions of unfairness from the literature. Leveraging our measure, we can reduce the design of fair margin-based classifiers to adding tractable constraints on their decision boundaries. Experiments on multiple synthetic and real-world datasets show that our framework is able to successfully limit unfairness, often at a small cost in terms of accuracy."
            ],
            "keywords": [
                "Supervised learning",
                "margin-based classifiers",
                "fairness",
                "discrimination",
                "disparate impact"
            ],
            "author": [
                "Muhammad Bilal Zafar",
                "Isabel Valera",
                "Manuel Gomez-Rodriguez",
                "Krishna P Gummadi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-262/18-262.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear Programming Relaxations and Belief Propagation -An Empirical Study",
            "abstract": [
                "The problem of finding the most probable (MAP) configuration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for finding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the fields of computer vision and computational biology. We find that TRBP almost always finds the solution significantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can find the MAP configurations in a matter of minutes for a large range of real world problems."
            ],
            "keywords": [],
            "author": [
                "Chen Yanover",
                "Talya Meltzer",
                "Yair Weiss",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/yanover06a/yanover06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rounding-based Moves for Semi-Metric Labeling",
            "abstract": [
                "Semi-metric labeling is a special case of energy minimization for pairwise Markov random fields. The energy function consists of arbitrary unary potentials, and pairwise potentials that are proportional to a given semi-metric distance function over the label set. Popular methods for solving semi-metric labeling include (i) move-making algorithms, which iteratively solve a minimum st-cut problem; and (ii) the linear programming (LP) relaxation based approach. In order to convert the fractional solution of the LP relaxation to an integer solution, several randomized rounding procedures have been developed in the literature. We consider a large class of parallel rounding procedures, and design move-making algorithms that closely mimic them. We prove that the multiplicative bound of a move-making algorithm exactly matches the approximation factor of the corresponding rounding procedure for any arbitrary distance function. Our analysis includes all known results for move-making algorithms as special cases."
            ],
            "keywords": [
                "semi-metric labeling",
                "move-making algorithms",
                "linear programming relaxation",
                "multiplicative bounds"
            ],
            "author": [
                "M Pawan Kumar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-454/14-454.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Iterative Kernel Principal Component Analysis",
            "abstract": [
                "We develop gain adaptation methods that improve convergence of the kernel Hebbian algorithm (KHA) for iterative kernel PCA (Kim et al., 2005). KHA has a scalar gain parameter which is either held constant or decreased according to a predetermined annealing schedule, leading to slow convergence. We accelerate it by incorporating the reciprocal of the current estimated eigenvalues as part of a gain vector. An additional normalization term then allows us to eliminate a tuning parameter in the annealing schedule. Finally we derive and apply stochastic meta-descent (SMD) gain vector adaptation (Schraudolph, 1999, 2002) in reproducing kernel Hilbert space to further speed up convergence. Experimental results on kernel PCA and spectral clustering of USPS digits, motion capture and image denoising, and image super-resolution tasks confirm that our methods converge substantially faster than conventional KHA. To demonstrate scalability, we perform kernel PCA on the entire MNIST data set."
            ],
            "keywords": [
                "step size adaptation",
                "gain vector adaptation",
                "stochastic meta-descent",
                "kernel Hebbian algorithm",
                "online learning"
            ],
            "author": [
                "Simon Günter",
                "Nicol N Schraudolph"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/guenter07a/guenter07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Machine Learning for Computer Security *",
            "abstract": [
                "The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems."
            ],
            "keywords": [
                "computer security",
                "spam",
                "images with embedded text",
                "malicious executables",
                "network protocols",
                "encrypted traffic"
            ],
            "author": [
                "Philip K Chan",
                "Richard P Lippmann",
                "Richard P Lippman"
            ],
            "ref": "http://www.jmlr.org/papers/volume7/MLSEC-intro06a/MLSEC-intro06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hybrid MPI/OpenMP Parallel Linear Support Vector Machine Training",
            "abstract": [
                "Support vector machines are a powerful machine learning technology, but the training process involves a dense quadratic optimization problem and is computationally challenging. A parallel implementation of linear Support Vector Machine training has been developed, using a combination of MPI and OpenMP. Using an interior point method for the optimization and a reformulation that avoids the dense Hessian matrix, the structure of the augmented system matrix is exploited to partition data and computations amongst parallel processors efficiently. The new implementation has been applied to solve problems from the PASCAL Challenge on Large-scale Learning. We show that our approach is competitive, and is able to solve problems in the Challenge many times faster than other parallel approaches. We also demonstrate that the hybrid version performs more efficiently than the version using pure MPI."
            ],
            "keywords": [
                "linear SVM training",
                "hybrid parallelism",
                "largescale learning",
                "interior point method"
            ],
            "author": [
                "Kristian Woodsend",
                "Sören Sonnenburg",
                "Elad Vojtech Franc",
                "Michele Sebag"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/woodsend09a/woodsend09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PAC-Bayesian Analysis of Co-clustering and Beyond",
            "abstract": [
                "We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering, and pairwise clustering. We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for finding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved stateof-the-art performance in the MovieLens collaborative filtering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization."
            ],
            "keywords": [],
            "author": [
                "Yevgeny Seldin",
                "Naftali Tishby"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/seldin10a/seldin10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning from Comparisons and Choices",
            "abstract": [
                "When tracking user-specific online activities, each user's preference is revealed in the form of choices and comparisons. For example, a user's purchase history is a record of her choices, i.e. which item was chosen among a subset of offerings. A user's preferences can be observed either explicitly as in movie ratings or implicitly as in viewing times of news articles. Given such individualized ordinal data in the form of comparisons and choices, we address the problem of collaboratively learning representations of the users and the items. The learned features can be used to predict a user's preference of an unseen item to be used in recommendation systems. This also allows one to compute similarities among users and items to be used for categorization and search. Motivated by the empirical successes of the MultiNomial Logit (MNL) model in marketing and transportation, and also more recent successes in word embedding and crowdsourced image embedding, we pose this problem as learning the MNL model parameters that best explain the data. We propose a convex relaxation for learning the MNL model, and show that it is minimax optimal up to a logarithmic factor by comparing its performance to a fundamental lower bound. This characterizes the minimax sample complexity of the problem, and proves that the proposed estimator cannot be improved upon other than by a logarithmic factor. Further, the analysis identifies how the accuracy depends on the topology of sampling via the spectrum of the sampling graph. This provides a guideline for designing surveys when one can choose which items are to be compared. This is accompanied by numerical simulations on synthetic and real data sets, confirming our theoretical predictions."
            ],
            "keywords": [
                "Collaborative Ranking",
                "Nuclear Norm Minimization",
                "Multi-Nomial Logit Model"
            ],
            "author": [
                "Sahand Negahban",
                "Kiran K Thekumparampil",
                "Jiaming Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-607/17-607.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Algorithms for Sparse Linear Classifiers in the Massive Data Setting",
            "abstract": [
                "Classifiers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classifiers, etc., provide competitive methods for classification problems in high dimensions. However, current algorithms for training sparse classifiers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classifiers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function."
            ],
            "keywords": [
                "Laplace approximation",
                "expectation propagation",
                "LASSO"
            ],
            "author": [
                "Suhrid Balakrishnan",
                "David Madigan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/balakrishnan08a/balakrishnan08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Estimation of High-dimensional Factor-Augmented Vector Autoregressive (FAVAR) Models",
            "abstract": [
                "A factor-augmented vector autoregressive (FAVAR) model is defined by a VAR equation that captures lead-lag correlations amongst a set of observed variables X and latent factors F , and a calibration equation that relates another set of observed variables Y with F and X. The latter equation is used to estimate the factors that are subsequently used in estimating the parameters of the VAR system. The FAVAR model has become popular in applied economic research, since it can summarize a large number of variables of interest as a few factors through the calibration equation and subsequently examine their influence on core variables of primary interest through the VAR equation. However, there is increasing need for examining lead-lag relationships between a large number of time series, while incorporating information from another high-dimensional set of variables. Hence, in this paper we investigate the FAVAR model under high-dimensional scaling. We introduce an appropriate identification constraint for the model parameters, which when incorporated into the formulated optimization problem yields estimates with good statistical properties. Further, we address a number of technical challenges introduced by the fact that estimates of the VAR system model parameters are based on estimated rather than directly observed quantities. The performance of the proposed estimators is evaluated on synthetic data. Further, the model is applied to commodity prices and reveals interesting and interpretable relationships between the prices and the factors extracted from a set of global macroeconomic indicators."
            ],
            "keywords": [
                "Model Identifiability",
                "Compactness",
                "Low-rank plus Sparse Decomposition",
                "Finite-Sample Bounds"
            ],
            "author": [
                "Jiahe Lin",
                "George Michailidis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-874/19-874.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Complexity of Representation and Inference in Compositional Models with Part Sharing",
            "abstract": [
                "This paper performs a complexity analysis of a class of serial and parallel compositional models of multiple objects and shows that they enable efficient representation and rapid inference. Compositional models are generative and represent objects in a hierarchically distributed manner in terms of parts and subparts, which are constructed recursively by part-subpart compositions. Parts are represented more coarsely at higher level of the hierarchy, so that the upper levels give coarse summary descriptions (e.g., there is a horse in the image) while the lower levels represents the details (e.g., the positions of the legs of the horse). This hierarchically distributed representation obeys the executive summary principle, meaning that a high level executive only requires a coarse summary description and can, if necessary, get more details by consulting lower level executives. The parts and subparts are organized in terms of hierarchical dictionaries which enables part sharing between different objects allowing efficient representation of many objects. The first main contribution of this paper is to show that compositional models can be mapped onto a parallel visual architecture similar to that used by bio-inspired visual models such as deep convolutional networks but more explicit in terms of representation, hence enabling part detection as well as object detection, and suitable for complexity analysis. Inference algorithms can be run on this architecture to exploit the gains caused by part sharing and executive summary. Effectively, this compositional architecture enables us to perform exact inference simultaneously over a large class of generative models of objects. The second contribution is an analysis of the complexity of compositional models in terms of computation time (for serial computers) and numbers of nodes (e.g., \"neurons\") for parallel computers. In particular, we compute the complexity gains by part sharing and executive summary and their dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level of the hierarchy, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can enable linear processing on parallel computers."
            ],
            "keywords": [
                "compositional models",
                "object detection",
                "hierarchical architectures",
                "part sharing"
            ],
            "author": [
                "Alan Yuille",
                "Roozbeh Mottaghi",
                "Aaron Courville",
                "Rob Fergus",
                "Christopher Manning"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/yuille16a/yuille16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximal Causes for Non-linear Component Extraction",
            "abstract": [
                "We study a generative model in which hidden causes combine competitively to produce observations. Multiple active causes combine to determine the value of an observed variable through a max function, in the place where algorithms such as sparse coding, independent component analysis, or non-negative matrix factorization would use a sum. This max rule can represent a more realistic model of non-linear interaction between basic components in many settings, including acoustic and image data. While exact maximum-likelihood learning of the parameters of this model proves to be intractable, we show that efficient approximations to expectation-maximization (EM) can be found in the case of sparsely active hidden causes. One of these approximations can be formulated as a neural network model with a generalized softmax activation function and Hebbian learning. Thus, we show that learning in recent softmax-like neural networks may be interpreted as approximate maximization of a data likelihood. We use the bars benchmark test to numerically verify our analytical results and to demonstrate the competitiveness of the resulting algorithms. Finally, we show results of learning model parameters to fit acoustic and visual data sets in which max-like component combinations arise naturally."
            ],
            "keywords": [
                "component extraction",
                "maximum likelihood",
                "approximate EM",
                "competitive learning",
                "neural networks"
            ],
            "author": [
                "Jörg Lücke"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/luecke08a/luecke08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "metric-learn: Metric Learning Algorithms in Python",
            "abstract": [
                "metric-learn is an open source Python package implementing supervised and weaklysupervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface compatible with scikit-learn which allows to easily perform cross-validation, model selection, and pipelining with other machine learning estimators. metric-learn is thoroughly tested and available on PyPi under the MIT license."
            ],
            "keywords": [
                "machine learning",
                "python",
                "metric learning",
                "scikit-learn"
            ],
            "author": [
                "William De Vazelhes",
                "C J Carey",
                "Yuan Tang",
                "Nathalie Vauquier"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-678/19-678.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Local Dependence In Ordered Data",
            "abstract": [
                "In many applications, data come with a natural ordering. This ordering can often induce local dependence among nearby variables. However, in complex data, the width of this dependence may vary, making simple assumptions such as a constant neighborhood size unrealistic. We propose a framework for learning this local dependence based on estimating the inverse of the Cholesky factor of the covariance matrix. Penalized maximum likelihood estimation of this matrix yields a simple regression interpretation for local dependence in which variables are predicted by their neighbors. Our proposed method involves solving a convex, penalized Gaussian likelihood problem with a hierarchical group lasso penalty. The problem decomposes into independent subproblems which can be solved efficiently in parallel using first-order methods. Our method yields a sparse, symmetric, positive definite estimator of the precision matrix, encoding a Gaussian graphical model. We derive theoretical results not found in existing methods attaining this structure. In particular, our conditions for signed support recovery and estimation consistency rates in multiple norms are as mild as those in a regression problem. Empirical results show our method performing favorably compared to existing methods. We apply our method to genomic data to flexibly model linkage disequilibrium. Our method is also applied to improve the performance of discriminant analysis in sound recording classification."
            ],
            "keywords": [
                "Local dependence",
                "Gaussian graphical models",
                "precision matrices",
                "Cholesky factor",
                "hierarchical group lasso"
            ],
            "author": [
                "Guo Yu",
                "Jacob Bien"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-198/16-198.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Manifold Learning: The Price of Normalization",
            "abstract": [
                "We analyze the performance of a class of manifold-learning algorithms that find their output by minimizing a quadratic form under some normalization constraints. This class consists of Locally Linear Embedding (LLE), Laplacian Eigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and Diffusion maps. We present and prove conditions on the manifold that are necessary for the success of the algorithms. Both the finite sample case and the limit case are analyzed. We show that there are simple manifolds in which the necessary conditions are violated, and hence the algorithms cannot recover the underlying manifolds. Finally, we present numerical results that demonstrate our claims."
            ],
            "keywords": [
                "dimensionality reduction",
                "manifold learning",
                "Laplacian eigenmap",
                "diffusion maps",
                "locally linear embedding",
                "local tangent space alignment",
                "Hessian eigenmap"
            ],
            "author": [
                "Yair Goldberg",
                "Dan Kushnir"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/goldberg08a/goldberg08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Theory of Curriculum Learning, with Convex Loss Functions",
            "abstract": [
                "Curriculum Learning is motivated by human cognition, where teaching often involves gradually exposing the learner to examples in a meaningful order, from easy to hard. Although methods based on this concept have been empirically shown to improve performance of several machine learning algorithms, no theoretical analysis has been provided even for simple cases. To address this shortfall, we start by formulating an ideal definition of difficulty score-the loss of the optimal hypothesis at a given datapoint. We analyze the possible contribution of curriculum learning based on this score in two convex problemslinear regression, and binary classification by hinge loss minimization. We show that in both cases, the convergence rate of SGD optimization decreases monotonically with the difficulty score, in accordance with earlier empirical results. We also prove that when the difficulty score is fixed, the convergence rate of SGD optimization is monotonically increasing with respect to the loss of the current hypothesis at each point. We discuss how these results settle some confusion in the literature where two apparently opposing heuristics are reported to improve performance: curriculum learning in which easier points are given priority, vs hard data mining where the more difficult points are sought out."
            ],
            "keywords": [
                "curriculum learning",
                "linear regression",
                "hinge loss minimization"
            ],
            "author": [
                "Daphna Weinshall",
                "Dan Amir"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-751/18-751.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adjusting for Chance Clustering Comparison Measures",
            "abstract": [
                "Adjusted for chance measures are widely used to compare partitions/clusterings of the same data set. In particular, the Adjusted Rand Index (ARI) based on pair-counting, and the Adjusted Mutual Information (AMI) based on Shannon information theory are very popular in the clustering community. Nonetheless it is an open problem as to what are the best application scenarios for each measure and guidelines in the literature for their usage are sparse, with the result that users often resort to using both. Generalized Information Theoretic (IT) measures based on the Tsallis entropy have been shown to link pair-counting and Shannon IT measures. In this paper, we aim to bridge the gap between adjustment of measures based on pair-counting and measures based on information theory. We solve the key technical challenge of analytically computing the expected value and variance of generalized IT measures. This allows us to propose adjustments of generalized IT measures, which reduce to well known adjusted clustering comparison measures as special cases. Using the theory of generalized IT measures, we are able to propose the following guidelines for using ARI and AMI as external validation indices: ARI should be used when the reference clustering has large equal sized clusters; AMI should be used when the reference clustering is unbalanced and there exist small clusters."
            ],
            "keywords": [
                "Clustering Comparison",
                "Clustering Validation",
                "Adjustment for Chance",
                "Generalized Information Theoretic Measures",
                "Pair-Counting Measures"
            ],
            "author": [
                "Simone Romano",
                "Xuan Nguyen",
                "James Bailey",
                "Karin Verspoor"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-627/15-627.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimation of a Low-rank Topic-Based Model for Information Cascades",
            "abstract": [
                "We consider the problem of estimating the latent structure of a social network based on the observed information diffusion events, or cascades, where the observations for a given cascade consist of only the timestamps of infection for infected nodes but not the source of the infection. Most of the existing work on this problem has focused on estimating a diffusion matrix without any structural assumptions on it. In this paper, we propose a novel model based on the intuition that an information is more likely to propagate among two nodes if they are interested in similar topics which are also prominent in the information content. In particular, our model endows each node with an influence vector (which measures how authoritative the node is on each topic) and a receptivity vector (which measures how susceptible the node is for each topic). We show how this node-topic structure can be estimated from the observed cascades, and prove the consistency of the estimator. Experiments on synthetic and real data demonstrate the improved performance and better interpretability of our model compared to existing state-of-the-art methods."
            ],
            "keywords": [
                "alternating gradient descent",
                "low-rank models",
                "information diffusion",
                "influencereceptivity model",
                "network science",
                "nonconvex optimization"
            ],
            "author": [
                "Ming Yu",
                "Varun Gupta"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-496/19-496.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Linear Ranking Functions for Beam Search with Application to Planning",
            "abstract": [
                "Beam search is commonly used to help maintain tractability in large search spaces at the expense of completeness and optimality. Here we study supervised learning of linear ranking functions for controlling beam search. The goal is to learn ranking functions that allow for beam search to perform nearly as well as unconstrained search, and hence gain computational efficiency without seriously sacrificing optimality. In this paper, we develop theoretical aspects of this learning problem and investigate the application of this framework to learning in the context of automated planning. We first study the computational complexity of the learning problem, showing that even for exponentially large search spaces the general consistency problem is in NP. We also identify tractable and intractable subclasses of the learning problem, giving insight into the problem structure. Next, we analyze the convergence of recently proposed and modified online learning algorithms, where we introduce several notions of problem margin that imply convergence for the various algorithms. Finally, we present empirical results in automated planning, where ranking functions are learned to guide beam search in a number of benchmark planning domains. The results show that our approach is often able to outperform an existing state-of-the-art planning heuristic as well as a recent approach to learning such heuristics."
            ],
            "keywords": [
                "beam search",
                "speedup learning",
                "automated planning",
                "structured classification"
            ],
            "author": [
                "Yuehua Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/xu09c/xu09c.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Forests, Decision Trees, and Categorical Predictors: The \"Absent Levels\" Problem",
            "abstract": [
                "One advantage of decision tree based methods like random forests is their ability to natively handle categorical predictors without having to first transform them (e.g., by using feature engineering techniques). However, in this paper, we show how this capability can lead to an inherent \"absent levels\" problem for decision tree based methods that has never been thoroughly discussed, and whose consequences have never been carefully explored. This problem occurs whenever there is an indeterminacy over how to handle an observation that has reached a categorical split which was determined when the observation in question's level was absent during training. Although these incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's random forests FORTRAN code and the randomForest R package (Liaw and Wiener, 2002) as motivating case studies, we examine how overlooking the absent levels problem can systematically bias a model. Furthermore, by using three real data examples, we illustrate how absent levels can dramatically alter a model's performance in practice, and we empirically demonstrate how some simple heuristics can be used to help mitigate the effects of the absent levels problem until a more robust theoretical solution is found."
            ],
            "keywords": [
                "absent levels",
                "categorical predictors",
                "decision trees",
                "CART",
                "random forests"
            ],
            "author": [
                "Timothy C Au"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-474/16-474.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The huge Package for High-dimensional Undirected Graph Estimation in R",
            "abstract": [
                "We describe an R package named huge which provides easy-to-use functions for estimating high dimensional undirected graphs from data. This package implements recent results in the literature, including Friedman et al. (2007), Liu et al. (2009, 2012) and Liu et al. (2010). Compared with the existing graph estimation package glasso, the huge package provides extra features: (1) instead of using Fortan, it is written in C, which makes the code more portable and easier to modify; (2) besides fitting Gaussian graphical models, it also provides functions for fitting high dimensional semiparametric Gaussian copula models; (3) more functions like data-dependent model selection, data generation and graph visualization; (4) a minor convergence problem of the graphical lasso algorithm is corrected; (5) the package allows the user to apply both lossless and lossy screening rules to scale up large-scale problems, making a tradeoff between computational and statistical efficiency."
            ],
            "keywords": [
                "high-dimensional undirected graph estimation",
                "glasso",
                "huge",
                "semiparametric graph estimation",
                "data-dependent model selection",
                "lossless screening",
                "lossy screening"
            ],
            "author": [
                "Tuo Zhao",
                "Han Liu",
                "Kathryn Roeder",
                "John Lafferty",
                "Larry Wasserman",
                "Liu, Roeder, Lafferty Wasserman Zhao",
                "LIU, ROEDER, LAFFERTY AND WASSERMAN Zhao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/zhao12a/zhao12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Learning in Approximately Linear Regression Based on Conditional Expectation of Generalization Error",
            "abstract": [
                "The goal of active learning is to determine the locations of training input points so that the generalization error is minimized. We discuss the problem of active learning in linear regression scenarios. Traditional active learning methods using least-squares learning often assume that the model used for learning is correctly specified. In many practical situations, however, this assumption may not be fulfilled. Recently, active learning methods using \"importance\"-weighted least-squares learning have been proposed, which are shown to be robust against misspecification of models. In this paper, we propose a new active learning method also using the weighted least-squares learning, which we call ALICE (Active Learning using the Importance-weighted least-squares learning based on Conditional Expectation of the generalization error). An important difference from existing methods is that we predict the conditional expectation of the generalization error given training input points, while existing methods predict the full expectation of the generalization error. Due to this difference, the training input design can be fine-tuned depending on the realization of training input points. Theoretically, we prove that the proposed active learning criterion is a more accurate predictor of the single-trial generalization error than the existing criterion. Numerical studies with toy and benchmark data sets show that the proposed method compares favorably to existing methods."
            ],
            "keywords": [
                "Active Learning",
                "Conditional Expectation of Generalization Error",
                "Misspecification of Models",
                "Importance-Weighted Least-Squares Learning",
                "Covariate Shift"
            ],
            "author": [
                "Masashi Sugiyama"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/sugiyama06a/sugiyama06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Reproducing Kernel Banach Spaces for Machine Learning",
            "abstract": [
                "We introduce the notion of reproducing kernel Banach spaces (RKBS) and study special semiinner-product RKBS by making use of semi-inner-products and the duality mapping. Properties of an RKBS and its reproducing kernel are investigated. As applications, we develop in the framework of RKBS standard learning schemes including minimal norm interpolation, regularization network, support vector machines, and kernel principal component analysis. In particular, existence, uniqueness and representer theorems are established."
            ],
            "keywords": [
                "reproducing kernel Banach spaces",
                "reproducing kernels",
                "learning theory",
                "semi-innerproducts",
                "representer theorems"
            ],
            "author": [
                "Haizhang Zhang",
                "Yuesheng Xu",
                "Jun Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/zhang09b/zhang09b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exploring Strategies for Training Deep Neural Networks",
            "abstract": [
                "Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms."
            ],
            "keywords": [
                "artificial neural networks",
                "deep belief networks",
                "restricted Boltzmann machines",
                "autoassociators",
                "unsupervised learning"
            ],
            "author": [
                "Hugo Larochelle",
                "Yoshua Bengio",
                "Pascal Lamblin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/larochelle09a/larochelle09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Class of Parallel Doubly Stochastic Algorithms for Large-Scale Learning",
            "abstract": [
                "We consider learning problems over training sets in which both, the number of training examples and the dimension of the feature vectors, are large. To solve these problems we propose the random parallel stochastic algorithm (RAPSA). We call the algorithm random parallel because it utilizes multiple parallel processors to operate on a randomly chosen subset of blocks of the feature vector. RAPSA is doubly stochastic since each processor utilizes a random set of functions to compute the stochastic gradient associated with a randomly chosen sets of variable coordinates. Algorithms that are parallel in either of these dimensions exist, but RAPSA is the first attempt at a methodology that is parallel in both the selection of blocks and the selection of elements of the training set. In RAPSA, processors utilize the randomly chosen functions to compute the stochastic gradient component associated with a randomly chosen block. The technical contribution of this paper is to show that this minimally coordinated algorithm converges to the optimal classifier when the training objective is strongly convex. Moreover, we present an accelerated version of RAPSA (ARAPSA) that incorporates the objective function curvature information by premultiplying the descent direction by a Hessian approximation matrix. We further extend the results for asynchronous settings and show that if the processors perform their updates without any coordination the algorithms are still convergent to the optimal argument. RAPSA and its extensions are then numerically evaluated on a linear estimation problem and a binary image classification task using the MNIST handwritten digit dataset."
            ],
            "keywords": [
                "Stochastic optimization",
                "large-scale learning",
                "asynchronous methods",
                "parallel algorithms"
            ],
            "author": [
                "Aryan Mokhtari",
                "Alec Koppel",
                "Martin Takáč",
                "Alejandro Ribeiro"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/16-311/16-311.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Mahalanobis Distance in Functional Settings",
            "abstract": [
                "Mahalanobis distance is a classical tool in multivariate analysis. We suggest here an extension of this concept to the case of functional data. More precisely, the proposed definition concerns those statistical problems where the sample data are real functions defined on a compact interval of the real line. The obvious difficulty for such a functional extension is the non-invertibility of the covariance operator in infinite-dimensional cases. Unlike other recent proposals, our definition is suggested and motivated in terms of the Reproducing Kernel Hilbert Space (RKHS) associated with the stochastic process that generates the data. The proposed distance is a true metric; it depends on a unique real smoothing parameter which is fully motivated in RKHS terms. Moreover, it shares some properties of its finite dimensional counterpart: it is invariant under isometries, it can be consistently estimated from the data and its sampling distribution is known under Gaussian models. An empirical study for two statistical applications, outliers detection and binary classification, is included. The results are quite competitive when compared to other recent proposals in the literature."
            ],
            "keywords": [
                "Functional data",
                "Mahalanobis distance",
                "reproducing kernel Hilbert spaces",
                "kernel methods in statistics",
                "square root operator"
            ],
            "author": [
                "José R Berrendero",
                "Beatriz Bueno-Larraz",
                "Antonio Cuevas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-156/18-156.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Introduction to the Special Topic on Grammar Induction, Representation of Language and Language Learning",
            "abstract": [
                "Grammar induction refers to the process of learning grammars and languages from data; this finds a variety of applications in syntactic pattern recognition, the modeling of natural language acquisition, data mining and machine translation. This special topic contains several papers presenting some of recent developments in the area of grammar induction and language learning, as applied to various problems in Natural Language Processing, including supervised and unsupervised parsing and statistical machine translation."
            ],
            "keywords": [
                "machine translation",
                "Bayesian inference",
                "grammar induction",
                "natural language parsing"
            ],
            "author": [
                "Dorota Głowacka",
                "Alexander Clark",
                "Mark Johnson"
            ],
            "ref": "http://www.jmlr.org/papers/volume12/glowacka11a/glowacka11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bootstrap Method for Error Estimation in Randomized Matrix Multiplication",
            "abstract": [
                "In recent years, randomized methods for numerical linear algebra have received growing interest as a general approach to large-scale problems. Typically, the essential ingredient of these methods is some form of randomized dimension reduction, which accelerates computations, but also creates random approximation error. In this way, the dimension reduction step encodes a tradeoff between cost and accuracy. However, the exact numerical relationship between cost and accuracy is typically unknown, and consequently, it may be difficult for the user to precisely know (1) how accurate a given solution is, or (2) how much computation is needed to achieve a given level of accuracy. In the current paper, we study randomized matrix multiplication (sketching) as a prototype setting for addressing these general problems. As a solution, we develop a bootstrap method for directly estimating the accuracy as a function of the reduced dimension (as opposed to deriving worst-case bounds on the accuracy in terms of the reduced dimension). From a computational standpoint, the proposed method does not substantially increase the cost of standard sketching methods, and this is made possible by an \"extrapolation\" technique. In addition, we provide both theoretical and empirical results to demonstrate the effectiveness of the proposed method."
            ],
            "keywords": [
                "matrix sketching",
                "randomized matrix multiplication",
                "bootstrap methods"
            ],
            "author": [
                "Miles E Lopes",
                "Shusen Wang",
                "Michael W Mahoney"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-451/17-451.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distribution-Specific Hardness of Learning Neural Networks",
            "abstract": [
                "Although neural networks are routinely and successfully trained in practice using simple gradient-based methods, most existing theoretical results are negative, showing that learning such networks is difficult, in a worst-case sense over all data distributions. In this paper, we take a more nuanced view, and consider whether specific assumptions on the \"niceness\" of the input distribution, or \"niceness\" of the target function (e.g. in terms of smoothness, non-degeneracy, incoherence, random choice of parameters etc.), are sufficient to guarantee learnability using gradient-based methods. We provide evidence that neither class of assumptions alone is sufficient: On the one hand, for any member of a class of \"nice\" target functions, there are difficult input distributions. On the other hand, we identify a family of simple target functions, which are difficult to learn even if the input distribution is \"nice\". To prove our results, we develop some tools which may be of independent interest, such as extending Fourier-based hardness techniques developed in the context of statistical queries (Blum et al., 1994), from the Boolean cube to Euclidean space and to more general classes of functions."
            ],
            "keywords": [
                "neural networks",
                "computational hardness",
                "distributional assumptions",
                "gradientbased methods"
            ],
            "author": [
                "Ohad Shamir"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-537/17-537.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Algorithms for Decision Tree Cross-validation",
            "abstract": [
                "Cross-validation is a useful and generally applicable technique often employed in machine learning, including decision tree induction. An important disadvantage of straightforward implementation of the technique is its computational overhead. In this paper we show that, for decision trees, the computational overhead of cross-validation can be reduced significantly by integrating the cross-validation with the normal decision tree induction process. We discuss how existing decision tree algorithms can be adapted to this aim, and provide an analysis of the speedups these adaptations may yield. We identify a number of parameters that influence the obtainable speedups, and validate and refine our analysis with experiments on a variety of data sets with two different implementations. Besides cross-validation, we also briefly explore the usefulness of these techniques for bagging. We conclude with some guidelines concerning when these optimizations should be considered."
            ],
            "keywords": [
                "Decision trees",
                "cross-validation",
                "inductive logic programming"
            ],
            "author": [
                "Hendrik Blockeel",
                "Jan Struyf",
                "Carla E Brodley",
                "Andrea Danyluk"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/blockeel02a/blockeel02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Pricing in High-dimensions",
            "abstract": [
                "We study the pricing problem faced by a firm that sells a large number of products, described via a wide range of features, to customers that arrive over time. Customers independently make purchasing decisions according to a general choice model that includes products features and customers' characteristics, encoded as d-dimensional numerical vectors, as well as the price offered. The parameters of the choice model are a priori unknown to the firm, but can be learned as the (binary-valued) sales data accrues over time. The firm's objective is to maximize its revenue. We benchmark the performance using the classic regret minimization framework where the regret is defined as the expected revenue loss against a clairvoyant policy that knows the parameters of the choice model in advance, and always offers the revenue-maximizing price. This setting is motivated in part by the prevalence of online marketplaces that allow for real-time pricing. We assume a structured choice model, parameters of which depend on s 0 out of the d product features. Assuming that the market noise distribution is known, we propose a dynamic policy, called Regularized Maximum Likelihood Pricing (RMLP) that leverages the (sparsity) structure of the high-dimensional model and obtains a logarithmic regret in T. More specifically, the regret of our algorithm is of O(s 0 log d • log T). Furthermore, we show that no policy can obtain regret better than O(s 0 (log d + log T)). In addition, we propose a generalization of our policy to a setting that the market noise distribution is unknown but belongs to a parametrized family of distributions. This policy obtains regret of O((log d)T). We further show that no policy can obtain regret better than Ω(√ T) in such environments."
            ],
            "keywords": [
                "Revenue Management",
                "Dynamic Pricing",
                "High-dimensional Regression",
                "Maximum Likelihood",
                "Regret",
                "Sparsity"
            ],
            "author": [
                "Adel Javanmard",
                "Hamid Nazerzadeh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-357/17-357.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning with the Maximum Correntropy Criterion Induced Losses for Regression",
            "abstract": [
                "Within the statistical learning framework, this paper studies the regression model associated with the correntropy induced losses. The correntropy, as a similarity measure, has been frequently employed in signal processing and pattern recognition. Motivated by its empirical successes, this paper aims at presenting some theoretical understanding towards the maximum correntropy criterion in regression problems. Our focus in this paper is twofold: first, we are concerned with the connections between the regression model associated with the correntropy induced loss and the least squares regression model. Second, we study its convergence property. A learning theory analysis which is centered around the above two aspects is conducted. From our analysis, we see that the scale parameter in the loss function balances the convergence rates of the regression model and its robustness. We then make some efforts to sketch a general view on robust loss functions when being applied into the learning for regression problems. Numerical experiments are also implemented to verify the effectiveness of the model."
            ],
            "keywords": [
                "correntropy",
                "the maximum correntropy criterion",
                "robust regression",
                "robust loss function",
                "least squares regression",
                "statistical learning theory"
            ],
            "author": [
                "Yunlong Feng",
                "Xiaolin Huang",
                "Lei Shi",
                "Yuning Yang",
                "Johan A K Suykens"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/feng15a/feng15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Covariance in Unsupervised Learning of Probabilistic Grammars",
            "abstract": [
                "Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text. Their symbolic component is amenable to inspection by humans, while their probabilistic component helps resolve ambiguity. They also permit the use of well-understood, generalpurpose learning algorithms. There has been an increased interest in using probabilistic grammars in the Bayesian setting. To date, most of the literature has focused on using a Dirichlet prior. The Dirichlet prior has several limitations, including that it cannot directly model covariance between the probabilistic grammar's parameters. Yet, various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties. In this paper, we suggest an alternative to the Dirichlet prior, a family of logistic normal distributions. We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction, demonstrating performance improvements with our priors on a set of six treebanks in different natural languages. Our covariance framework permits soft parameter tying within grammars and across grammars for text in different languages, and we show empirical gains in a novel learning setting using bilingual, non-parallel data."
            ],
            "keywords": [
                "dependency grammar induction",
                "variational inference",
                "logistic normal distribution",
                "Bayesian inference"
            ],
            "author": [
                "Shay B Cohen",
                "Noah A Smith",
                "Alex Clark",
                "Dorota Glowacka",
                "Colin De La Higuera",
                "Mark Johnson",
                "John Shawe-Taylor"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/cohen10a/cohen10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Dictionary for Least Squares Representation",
            "abstract": [
                "Dictionaries are collections of vectors used for the representation of a class of vectors in Euclidean spaces. Recent research on optimal dictionaries is focused on constructing dictionaries that offer sparse representations, i.e., 0-optimal representations. Here we consider the problem of finding optimal dictionaries with which representations of a given class of vectors is optimal in an 2-sense: optimality of representation is defined as attaining the minimal average 2-norm of the coefficients used to represent the vectors in the given class. With the help of recent results on rank-1 decompositions of symmetric positive semidefinite matrices, we provide an explicit description of 2-optimal dictionaries as well as their algorithmic constructions in polynomial time."
            ],
            "keywords": [
                "2 -optimal dictionary",
                "rank-1 decomposition",
                "finite tight frames"
            ],
            "author": [
                "Mohammed Rayyan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-046/16-046.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Using Privileged Information: Similarity Control and Knowledge Transfer",
            "abstract": [
                "This paper describes a new paradigm of machine learning, in which Intelligent Teacher is involved. During training stage, Intelligent Teacher provides Student with information that contains, along with classification of each example, additional privileged information (for example, explanation) of this example. The paper describes two mechanisms that can be used for significantly accelerating the speed of Student's learning using privileged information: (1) correction of Student's concepts of similarity between examples, and (2) direct Teacher-Student knowledge transfer."
            ],
            "keywords": [
                "intelligent teacher",
                "privileged information",
                "similarity control",
                "knowledge transfer",
                "knowledge representation",
                "frames",
                "support vector machines",
                "SVM+",
                "classification",
                "learning theory",
                "kernel functions",
                "similarity functions",
                "regression"
            ],
            "author": [
                "Vladimir Vapnik",
                "Rauf Izmailov",
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/vapnik15b/vapnik15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lyapunov Design for Safe Reinforcement Learning",
            "abstract": [
                "Lyapunov design methods are used widely in control engineering to design controllers that achieve qualitative objectives, such as stabilizing a system or maintaining a system's state in a desired operating range. We propose a method for constructing safe, reliable reinforcement learning agents based on Lyapunov design principles. In our approach, an agent learns to control a system by switching among a number of given, base-level controllers. These controllers are designed using Lyapunov domain knowledge so that any switching policy is safe and enjoys basic performance guarantees. Our approach thus ensures qualitatively satisfactory agent behavior for virtually any reinforcement learning algorithm and at all times, including while the agent is learning and taking exploratory actions. We demonstrate the process of designing safe agents for four different control problems. In simulation experiments, we find that our theoretically motivated designs also enjoy a number of practical benefits, including reasonable performance initially and throughout learning, and accelerated learning."
            ],
            "keywords": [
                "Reinforcement Learning",
                "Lyapunov Functions",
                "Safety",
                "Stability"
            ],
            "author": [
                "Theodore J Perkins",
                "Andrew G Barto",
                "Carla E Brodley",
                "Andrea Danyluk"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/perkins02a/perkins02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "tick: a Python Library for Statistical Learning, with an emphasis on Hawkes Processes and Time-Dependent Models",
            "abstract": [
                "This paper introduces tick, is a statistical learning library for Python 3, with a particular emphasis on time-dependent models, such as point processes, tools for generalized linear models and survival analysis. The core of the library provides model computational classes, solvers and proximal operators for regularization. It relies on a C ++ implementation and state-of-the-art optimization algorithms to provide very fast computations in a single node multi-core setting. Source code and documentation can be downloaded from https:// github.com/X-DataInitiative/tick."
            ],
            "keywords": [
                "Statistical Learning",
                "Python",
                "Hawkes processes",
                "Optimization",
                "Generalized linear models",
                "Point Process",
                "Survival Analysis"
            ],
            "author": [
                "Emmanuel Bacry",
                "Martin Bompaire",
                "Philip Deegan",
                "Søren V Poulsen",
                "Stéphane Gaïffas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-381/17-381.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast SDP Relaxations of Graph Cut Clustering, Transduction,and Other Combinatorial Problems",
            "abstract": [
                "The rise of convex programming has changed the face of many research fields in recent years, machine learning being one of the ones that benefitted the most. A very recent developement, the relaxation of combinatorial problems to semi-definite programs (SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning. In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade off computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example."
            ],
            "keywords": [
                "convex transduction",
                "normalized graph cut",
                "semi-definite programming",
                "semisupervised learning",
                "relaxation",
                "combinatorial optimization",
                "max-cut"
            ],
            "author": [
                "Tijl De Bie",
                "OKP K U Leuven",
                "Nello Cristianini",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/debie06a/debie06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Ranking and Generalization Bounds",
            "abstract": [
                "The problem of ranking is to predict or to guess the ordering between objects on the basis of their observed features. In this paper we consider ranking estimators that minimize the empirical convex risk. We prove generalization bounds for the excess risk of such estimators with rates that are faster than 1 √ n. We apply our results to commonly used ranking algorithms, for instance boosting or support vector machines. Moreover, we study the performance of considered estimators on real data sets."
            ],
            "keywords": [
                "convex risk minimization",
                "excess risk",
                "support vector machine",
                "empirical process",
                "U-process"
            ],
            "author": [
                "Wojciech Rejchel",
                "Wrejchel @ Gmail"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/rejchel12a/rejchel12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analysis of Perceptron-Based Active Learning",
            "abstract": [
                "We start by showing that in an active learning setting, the Perceptron algorithm needs Ω(1 ε 2) labels to learn linear separators within generalization error ε. We then present a simple active learning algorithm for this problem, which combines a modification of the Perceptron update with an adaptive filtering rule for deciding which points to query. For data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error ε after asking for justÕ(d log 1 ε) labels. This exponential improvement over the usual sample complexity of supervised learning had previously been demonstrated only for the computationally more complex query-by-committee algorithm."
            ],
            "keywords": [
                "active learning",
                "perceptron",
                "label complexity bounds",
                "online learning"
            ],
            "author": [
                "Sanjoy Dasgupta",
                "Adam Tauman Kalai",
                "Claire Monteleoni"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/dasgupta09a/dasgupta09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bilinear Discriminant Component Analysis",
            "abstract": [
                "Factor analysis and discriminant analysis are often used as complementary approaches to identify linear components in two dimensional data arrays. For three dimensional arrays, which may organize data in dimensions such as space, time, and trials, the opportunity arises to combine these two approaches. A new method, Bilinear Discriminant Component Analysis (BDCA), is derived and demonstrated in the context of functional brain imaging data for which it seems ideally suited. The work suggests to identify a subspace projection which optimally separates classes while ensuring that each dimension in this space captures an independent contribution to the discrimination."
            ],
            "keywords": [
                "bilinear",
                "decomposition",
                "component",
                "classification",
                "regularization"
            ],
            "author": [
                "Mads Dyrholm",
                "Lucas C Parra"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/dyrholm07a/dyrholm07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scaling-up Empirical Risk Minimization: Optimization of Incomplete U-statistics",
            "abstract": [
                "In a wide range of statistical learning problems such as ranking, clustering or metric learning among others, the risk is accurately estimated by U-statistics of degree d ≥ 1, i.e. functionals of the training data with low variance that take the form of averages over k-tuples. From a computational perspective, the calculation of such statistics is highly expensive even for a moderate sample size n, as it requires averaging O(n d) terms. This makes learning procedures relying on the optimization of such data functionals hardly feasible in practice. It is the major goal of this paper to show that, strikingly, such empirical risks can be replaced by drastically computationally simpler Monte-Carlo estimates based on O(n) terms only, usually referred to as incomplete U-statistics, without damaging the O P (1/ √ n) learning rate of Empirical Risk Minimization (ERM) procedures. For this purpose, we establish uniform deviation results describing the error made when approximating a Uprocess by its incomplete version under appropriate complexity assumptions. Extensions to model selection, fast rate situations and various sampling techniques are also considered, as well as an application to stochastic gradient descent for ERM. Finally, numerical examples are displayed in order to provide strong empirical evidence that the approach we promote largely surpasses more naive subsampling techniques."
            ],
            "keywords": [
                "big data",
                "empirical risk minimization",
                "U-processes",
                "rate bound analysis",
                "sampling design",
                "stochastic gradient descent"
            ],
            "author": [
                "Stephan Clémençon",
                "Igor Colin",
                "Aurélien Bellet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-012/15-012.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification",
            "abstract": [
                "The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner."
            ],
            "keywords": [
                "convex optimization",
                "semi-definite programming",
                "Mahalanobis distance",
                "metric learning",
                "multi-class classification",
                "support vector machines"
            ],
            "author": [
                "Kilian Q Weinberger",
                "Lawrence K Saul"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/weinberger09a/weinberger09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Concave Penalized Estimation of Sparse Gaussian Bayesian Networks",
            "abstract": [
                "We develop a penalized likelihood estimation framework to learn the structure of Gaussian Bayesian networks from observational data. In contrast to recent methods which accelerate the learning problem by restricting the search space, our main contribution is a fast algorithm for score-based structure learning which does not restrict the search space in any way and works on high-dimensional data sets with thousands of variables. Our use of concave regularization, as opposed to the more popular 0 (e.g. BIC) penalty, is new. Moreover, we provide theoretical guarantees which generalize existing asymptotic results when the underlying distribution is Gaussian. Most notably, our framework does not require the existence of a so-called faithful DAG representation, and as a result, the theory must handle the inherent nonidentifiability of the estimation problem in a novel way. Finally, as a matter of independent interest, we provide a comprehensive comparison of our approach to several standard structure learning methods using open-source packages developed for the R language. Based on these experiments, we show that our algorithm obtains higher sensitivity with comparable false discovery rates for high-dimensional data and scales efficiently as the number of nodes increases. In particular, the total runtime for our method to generate a solution path of 20 estimates for DAGs with 8000 nodes is around one hour."
            ],
            "keywords": [
                "Bayesian networks",
                "concave penalization",
                "directed acyclic graphs",
                "coordinate descent",
                "nonconvex optimization"
            ],
            "author": [
                "Bryon Aragam",
                "Qing Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/aragam15a/aragam15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonlinear Boosting Projections for Ensemble Construction Nicolás García-Pedrajas",
            "abstract": [
                "In this paper we propose a novel approach for ensemble construction based on the use of nonlinear projections to achieve both accuracy and diversity of individual classifiers. The proposed approach combines the philosophy of boosting, putting more effort on difficult instances, with the basis of the random subspace method. Our main contribution is that instead of using a random subspace, we construct a projection taking into account the instances which have posed most difficulties to previous classifiers. In this way, consecutive nonlinear projections are created by a neural network trained using only incorrectly classified instances. The feature subspace induced by the hidden layer of this network is used as the input space to a new classifier. The method is compared with bagging and boosting techniques, showing an improved performance on a large set of 44 problems from the UCI Machine Learning Repository. An additional study showed that the proposed approach is less sensitive to noise in the data than boosting methods."
            ],
            "keywords": [
                "classifier ensembles",
                "boosting",
                "neural networks",
                "nonlinear projections"
            ],
            "author": [
                "Npedrajas @ Uco",
                "Colin Fyfe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/garcia-pedrajas07a/garcia-pedrajas07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Importance Sampling for Minibatches",
            "abstract": [
                "Minibatching is a very well studied and highly popular technique in supervised learning, used by practitioners due to its ability to accelerate training through better utilization of parallel processing power and reduction of stochastic variance. Another popular technique is importance sampling-a strategy for preferential sampling of more important examples also capable of accelerating the training process. However, despite considerable effort by the community in these areas, and due to the inherent technical difficulty of the problem, there is virtually no existing work combining the power of importance sampling with the strength of minibatching. In this paper we propose the first practical importance sampling for minibatches and give simple and rigorous complexity analysis of its performance. We illustrate on synthetic problems that for training data of certain properties, our sampling can lead to several orders of magnitude improvement in training time. We then test the new sampling on several popular data sets, and show that the improvement can reach an order of magnitude."
            ],
            "keywords": [
                "empirical risk minimization",
                "importance sampling",
                "minibatching",
                "variancereduced methods",
                "convex optimization"
            ],
            "author": [
                "Dominik Csiba",
                "Peter Richtárik"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-241/16-241.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization * Vojtěch Franc",
            "abstract": [
                "We have developed an optimized cutting plane algorithm (OCA) for solving large-scale risk minimization problems. We prove that the number of iterations OCA requires to converge to a ε precise solution is approximately linear in the sample size. We also derive OCAS, an OCA-based linear binary Support Vector Machine (SVM) solver, and OCAM, a linear multi-class SVM solver. In an extensive empirical evaluation we show that OCAS outperforms current state-of-the-art SVM solvers like SVM light , SVM perf and BMRM, achieving speedup factor more than 1,200 over SVM light on some data sets and speedup factor of 29 over SVM perf , while obtaining the same precise support vector solution. OCAS, even in the early optimization steps, often shows faster convergence than the currently prevailing approximative methods in this domain, SGD and Pegasos. In addition, our proposed linear multi-class SVM solver, OCAM, achieves speedups of factor of up to 10 compared to SVM multi−class. Finally, we use OCAS and OCAM in two real-world applications, the problem of human acceptor splice site detection and malware detection. Effectively parallelizing OCAS, we achieve state-of-the-art results on an acceptor splice site recognition problem only by being able to learn from all the available 50 million examples in a 12-million-dimensional feature space. Source code, data sets and scripts to reproduce the experiments are available at"
            ],
            "keywords": [
                "risk minimization",
                "linear support vector machine",
                "multi-class classification",
                "binary classification",
                "large-scale learning",
                "parallelization"
            ],
            "author": [],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/franc09a/franc09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Achieving Optimal Misclassification Proportion in Stochastic Block Models",
            "abstract": [
                "Community detection is a fundamental statistical problem in network data analysis. In this paper, we present a polynomial time two-stage method that provably achieves optimal statistical performance in misclassification proportion for stochastic block model under weak regularity conditions. Our two-stage procedure consists of a refinement stage motivated by penalized local maximum likelihood estimation. This stage can take a wide range of weakly consistent community detection procedures as its initializer, to which it applies and outputs a community assignment that achieves optimal misclassification proportion with high probability. The theoretical property is confirmed by simulated examples."
            ],
            "keywords": [
                "Clustering",
                "Community detection",
                "Minimax rates",
                "Network analysis",
                "Spectral clustering"
            ],
            "author": [
                "Chao Gao",
                "Zongming Ma",
                "Anderson Y Zhang",
                "Harrison H Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-245/16-245.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SVMTorch: Support Vector Machines for Large-Scale Regression Problems",
            "abstract": [
                "Support Vector Machines SVMs for regression problems are trained by solving a quadratic optimization problem which needs on the order of l 2 memory and time resources to solve, where l is the number of training examples. In this paper, we propose a decomposition algorithm, SVMTorch 1 , which is similar to SVM-Light proposed by Joachims 1999 for classi cation problems, but adapted to regression problems. With this algorithm, one can now e ciently solve large-scale regression problems more than 20000 examples. Comparisons with Nodelib, another publicly available SVM algorithm for large-scale regression problems from Flake and Lawrence 2000 yielded signi cant time improvements. Finally, based on a recent paper from Lin 2000, we show that a convergence proof exists for our algorithm."
            ],
            "keywords": [],
            "author": [
                "Ronan Collobert",
                "Robert C Williamson"
            ],
            "ref": "http://www.jmlr.org/papers/volume1/collobert01a/collobert01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Topological Inference: Distance To a Measure and Kernel Distance",
            "abstract": [
                "Let P be a distribution with support S. The salient features of S can be quantified with persistent homology, which summarizes topological features of the sublevel sets of the distance function (the distance of any point x to S). Given a sample from P we can infer the persistent homology using an empirical version of the distance function. However, the empirical distance function is highly non-robust to noise and outliers. Even one outlier is deadly. The distance-to-a-measure (DTM), introduced by Chazal et al. (2011), and the kernel distance, introduced by Phillips et al. (2014), are smooth functions that provide useful topological information but are robust to noise and outliers. Chazal et al. (2015) derived concentration bounds for DTM. Building on these results, we derive limiting distributions and confidence sets, and we propose a method for choosing tuning parameters."
            ],
            "keywords": [
                "Topological data analysis",
                "persistent homology",
                "RKHS"
            ],
            "author": [
                "Frédéric Chazal",
                "Brittany Fasy",
                "Fabrizio Lecci",
                "Bertrand Michel",
                "Alessandro Rinaldo",
                "Larry Wasserman",
                "Frederic Chazal"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-484/15-484.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Feature Extraction by Non-Parametric Mutual Information Maximization",
            "abstract": [
                "We present a method for learning discriminative feature transforms using as criterion the mutual information between class labels and transformed features. Instead of a commonly used mutual information measure based on Kullback-Leibler divergence, we use a quadratic divergence measure, which allows us to make an efficient non-parametric implementation and requires no prior assumptions about class densities. In addition to linear transforms, we also discuss nonlinear transforms that are implemented as radial basis function networks. Extensions to reduce the computational complexity are also presented, and a comparison to greedy feature selection is made."
            ],
            "keywords": [
                "Mutual information",
                "Renyi entropy",
                "Quadratic divergence measures",
                "Parzen estimation",
                "Feature transform",
                "Feature extraction",
                "Non-parametric estimation"
            ],
            "author": [
                "Kari Torkkola",
                "Isabelle Guyon",
                "André Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/torkkola03a/torkkola03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Chaining Meets Chain Rule: Multilevel Entropic Regularization and Training of Neural Networks",
            "abstract": [
                "We derive generalization and excess risk bounds for neural networks using a family of complexity measures based on a multilevel relative entropy. The bounds are obtained by introducing the notion of generated hierarchical coverings of neural networks and by using the technique of chaining mutual information introduced by Asadi et al. '18. The resulting bounds are algorithm-dependent and multiscale: they exploit the multilevel structure of neural networks. This, in turn, leads to an empirical risk minimization problem with a multilevel entropic regularization. The minimization problem is resolved by introducing a multiscale extension of the celebrated Gibbs posterior distribution, proving that the derived distribution achieves the unique minimum. This leads to a new training procedure for neural networks with performance guarantees, which exploits the chain rule of relative entropy rather than the chain rule of derivatives (as in backpropagation), and which takes into account the interactions between different scales of the hypothesis sets of neural networks corresponding to different depths of the hidden layers. To obtain an efficient implementation of the latter, we further develop a multilevel Metropolis algorithm simulating the multiscale Gibbs distribution, with an experiment for a two-layer neural network on the MNIST data set."
            ],
            "keywords": [
                "neural networks",
                "multilevel relative entropy",
                "chaining mutual information",
                "multiscale generalization bound",
                "multiscale Gibbs distribution"
            ],
            "author": [
                "Amir R Asadi",
                "Emmanuel Abbe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-794/19-794.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improving Prediction from Dirichlet Process Mixtures via Enrichment * †",
            "abstract": [
                "Flexible covariate-dependent density estimation can be achieved by modelling the joint density of the response and covariates as a Dirichlet process mixture. An appealing aspect of this approach is that computations are relatively easy. In this paper, we examine the predictive performance of these models with an increasing number of covariates. Even for a moderate number of covariates, we find that the likelihood for x tends to dominate the posterior of the latent random partition, degrading the predictive performance of the model. To overcome this, we suggest using a different nonparametric prior, namely an enriched Dirichlet process. Our proposal maintains a simple allocation rule, so that computations remain relatively simple. Advantages are shown through both predictive equations and examples, including an application to diagnosis Alzheimer's disease."
            ],
            "keywords": [
                "Bayesian nonparametrics",
                "density regression",
                "predictive distribution",
                "random partition",
                "urn scheme"
            ],
            "author": [
                "Sara Wade",
                "David B Dunson",
                "Sonia Petrone"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/wade14a/wade14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency of Semi-Supervised Learning Algorithms on Graphs: Probit and One-Hot Methods",
            "abstract": [
                "Graph-based semi-supervised learning is the problem of propagating labels from a small number of labelled data points to a larger set of unlabelled data. This paper is concerned with the consistency of optimization-based techniques for such problems, in the limit where the labels have small noise and the underlying unlabelled data is well clustered. We study graph-based probit for binary classification, and a natural generalization of this method to multi-class classification using one-hot encoding. The resulting objective function to be optimized comprises the sum of a quadratic form defined through a rational function of the graph Laplacian, involving only the unlabelled data, and a fidelity term involving only the labelled data. The consistency analysis sheds light on the choice of the rational function defining the optimization."
            ],
            "keywords": [
                "Semi-supervised learning",
                "classification",
                "consistency",
                "graph Laplacian",
                "probit",
                "spectral analysis"
            ],
            "author": [
                "Franca Hoffmann",
                "Zhi Ren",
                "Andrew M Stuart",
                "Franca ©2020",
                "Bamdad Hoffmann",
                "Zhi Hosseini",
                "Andrew M Ren"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/15-900/15-900.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers",
            "abstract": [
                "We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classifiers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classification rate (i.e., risk), respectively. Given an ordering, we can find a discriminative structure with O N k+1 score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classification using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classification performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures significantly outperforms generatively produced structures, and achieves a classification accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of ∼10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classifiers still hold in the case of missing features, a case where generative classifiers have an advantage over discriminative classifiers."
            ],
            "keywords": [
                "Bayesian networks",
                "classification",
                "discriminative learning",
                "structure learning",
                "graphical model",
                "missing feature"
            ],
            "author": [
                "Franz Pernkopf",
                "Jeff A Bilmes"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/pernkopf10a/pernkopf10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large Scale Visual Recognition through Adaptation using Joint Representation and Multiple Instance Learning",
            "abstract": [
                "A major barrier towards scaling visual recognition systems is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) trained used 1.2M+ labeled images have emerged as clear winners on object classification benchmarks. Unfortunately, only a small fraction of those labels are available with bounding box localization for training the detection task and even fewer pixel level annotations are available for semantic segmentation. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect scene-centric images with precisely localized labels. We develop methods for learning large scale recognition models which exploit joint training over both weak (image-level) and strong (bounding box) labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. We provide a novel formulation of a joint multiple instance learning method that includes examples from object-centric data with image-level labels when available, and also performs domain transfer learning to improve the underlying detector representation. We then show how to use our large scale detectors to produce pixel level annotations. Using our method, we produce a >7.6K category detector and release code and models at lsda.berkeleyvision.org."
            ],
            "keywords": [
                "Computer Vision",
                "Deep Learning",
                "Transfer Learning",
                "Large Scale Learning"
            ],
            "author": [
                "Judy Hoffman",
                "Deepak Pathak",
                "Eric Tzeng",
                "Jonathan Long",
                "Sergio Guadarrama",
                "Trevor Darrell",
                "Kate Saenko"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-223/15-223.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Anytime Learning of Decision Trees",
            "abstract": [
                "The majority of existing algorithms for learning decision trees are greedy-a tree is induced topdown, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Even the few non-greedy learners cannot learn good trees when the concept is difficult. Furthermore, they require a fixed amount of time and are not able to generate a better tree if additional time is available. We introduce a framework for anytime induction of decision trees that overcomes these problems by trading computation speed for better tree quality. Our proposed family of algorithms employs a novel strategy for evaluating candidate splits. A biased sampling of the space of consistent trees rooted at an attribute is used to estimate the size of the minimal tree under that attribute, and an attribute with the smallest expected tree is selected. We present two types of anytime induction algorithms: a contract algorithm that determines the sample size on the basis of a pre-given allocation of time, and an interruptible algorithm that starts with a greedy tree and continuously improves subtrees by additional sampling. Experimental results indicate that, for several hard concepts, our proposed approach exhibits good anytime behavior and yields significantly better decision trees when more time is available."
            ],
            "keywords": [
                "anytime algorithms",
                "decision tree induction",
                "lookahead",
                "hard concepts",
                "resource-bounded reasoning"
            ],
            "author": [
                "Saher Esmeir"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/esmeir07a/esmeir07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Maximum Separation Subspace in Sufficient Dimension Reduction with Categorical Response",
            "abstract": [
                "Sufficient dimension reduction (SDR) is a very useful concept for exploratory analysis and data visualization in regression, especially when the number of covariates is large. Many SDR methods have been proposed for regression with a continuous response, where the central subspace (CS) is the target of estimation. Various conditions, such as the linearity condition and the constant covariance condition, are imposed so that these methods can estimate at least a portion of the CS. In this paper we study SDR for regression and discriminant analysis with categorical response. Motivated by the exploratory analysis and data visualization aspects of SDR, we propose a new geometric framework to reformulate the SDR problem in terms of manifold optimization and introduce a new concept called Maximum Separation Subspace (MASES). The MASES naturally preserves the \"sufficiency\" in SDR without imposing additional conditions on the predictor distribution, and directly inspires a semi-parametric estimator. Numerical studies show MASES exhibits superior performance as compared with competing SDR methods in specific settings."
            ],
            "keywords": [
                "Categorical data analysis",
                "Hellinger distance",
                "semi-parametric",
                "single index models",
                "sliced inverse regression",
                "sufficient dimension reduction"
            ],
            "author": [
                "Xin Zhang",
                "Qing Mai",
                "Hui Zou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/17-788/17-788.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Partial Least Squares for Stationary Data",
            "abstract": [
                "We consider the kernel partial least squares algorithm for non-parametric regression with stationary dependent data. Probabilistic convergence rates of the kernel partial least squares estimator to the true regression function are established under a source and an effective dimensionality condition. It is shown both theoretically and in simulations that long range dependence results in slower convergence rates. A protein dynamics example shows high predictive power of kernel partial least squares."
            ],
            "keywords": [
                "effective dimensionality",
                "long range dependence",
                "nonparametric regression",
                "source condition",
                "protein dynamics"
            ],
            "author": [
                "Marco Singer",
                "Tatyana Krivobokova",
                "Axel Munk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-306/17-306.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fourier Theoretic Probabilistic Inference over Permutations",
            "abstract": [
                "Permutations are ubiquitous in many real-world problems, such as voting, ranking, and data association. Representing uncertainty over permutations is challenging, since there are n! possibilities, and typical compact and factorized probability distribution representations, such as graphical models, cannot capture the mutual exclusivity constraints associated with permutations. In this paper, we use the \"low-frequency\" terms of a Fourier decomposition to represent distributions over permutations compactly. We present Kronecker conditioning, a novel approach for maintaining and updating these distributions directly in the Fourier domain, allowing for polynomial time bandlimited approximations. Low order Fourier-based approximations, however, may lead to functions that do not correspond to valid distributions. To address this problem, we present a quadratic program defined directly in the Fourier domain for projecting the approximation onto a relaxation of the polytope of legal marginal distributions. We demonstrate the effectiveness of our approach on a real camera-based multi-person tracking scenario."
            ],
            "keywords": [
                "identity management",
                "permutations",
                "approximate inference",
                "group theoretical methods",
                "sensor networks"
            ],
            "author": [
                "Jonathan Huang",
                "Carlos Guestrin",
                "Leonidas Guibas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/huang09a/huang09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Geomstats: A Python Package for Riemannian Geometry in Machine Learning",
            "abstract": [
                "We introduce Geomstats, an open-source Python package for computations and statistics on nonlinear manifolds such as hyperbolic spaces, spaces of symmetric positive definite matrices, Lie groups of transformations, and many more. We provide object-oriented and extensively unit-tested implementations. Manifolds come equipped with families of Riemannian metrics with associated exponential and logarithmic maps, geodesics, and parallel transport. Statistics and learning algorithms provide methods for estimation, clustering, and dimension reduction on manifolds. All associated operations are vectorized for batch computation and provide support for different execution backends-namely NumPy, Py-Torch, and TensorFlow. This paper presents the package, compares it with related libraries, and provides relevant code examples. We show that Geomstats provides reliable building blocks to both foster research in differential geometry and statistics and democratize the use of Riemannian geometry in machine learning applications. The source code is freely available under the MIT license at geomstats.ai."
            ],
            "keywords": [
                "differential geometry",
                "Riemannian geometry",
                "statistics",
                "machine learning",
                "manifold"
            ],
            "author": [
                "Nina Miolane",
                "Nicolas Guigui",
                "Alice Le",
                "Johan Mathe",
                "Benjamin Hou",
                "Stefan Heyder",
                "Olivier Peltre",
                "Niklas Koep",
                "Hadi Zaatiti",
                "Hatem Hajri",
                "Thomas Gerald",
                "Paul Chauchat",
                "Claire Donnat",
                "Susan Holmes",
                "Xavier Pennec",
                "Alice Le Brigant",
                "Yann Thanwerdas",
                "Yann Cabanes",
                "Christian Shewmake",
                "Daniel Brooks",
                "Bernhard Kainz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-027/19-027.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Streaming kernel regression with provably adaptive mean, variance, and regularization",
            "abstract": [
                "We consider the problem of streaming kernel regression, when the observations arrive sequentially and the goal is to recover the underlying mean function, assumed to belong to an RKHS. The variance of the noise is not assumed to be known. In this context, we tackle the problem of tuning the regularization parameter adaptively at each time step, while maintaining tight confidence bounds estimates on the value of the mean function at each point. To this end, we first generalize existing results for finite-dimensional linear regression with fixed regularization and known variance to the kernel setup with a regularization parameter allowed to be a measurable function of past observations. Then, using appropriate self-normalized inequalities we build upper and lower bound estimates for the variance, leading to Bernstein-like concentration bounds. The latter is used in order to define the adaptive regularization. The bounds resulting from our technique are valid uniformly over all observation points and all time steps, and are compared against the literature with numerical experiments. Finally, the potential of these tools is illustrated by an application to kernelized bandits, where we revisit the Kernel UCB and Kernel Thompson Sampling procedures, and show the benefits of the novel adaptive kernel tuning strategy."
            ],
            "keywords": [
                "kernel",
                "regression",
                "online learning",
                "adaptive tuning",
                "bandits"
            ],
            "author": [
                "Audrey Durand",
                "Joelle Pineau"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-404/17-404.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Support Vector Clustering",
            "abstract": [
                "We present a novel clustering method using the approach of support vector machines. Data points are mapped by means of a Gaussian kernel to a high dimensional feature space, where we search for the minimal enclosing sphere. This sphere, when mapped back to data space, can separate into several components, each enclosing a separate cluster of points. We present a simple algorithm for identifying these clusters. The width of the Gaussian kernel controls the scale at which the data is probed while the soft margin constant helps coping with outliers and overlapping clusters. The structure of a dataset is explored by varying the two parameters, maintaining a minimal number of support vectors to assure smooth cluster boundaries. We demonstrate the performance of our algorithm on several datasets."
            ],
            "keywords": [
                "Clustering",
                "Support Vectors Machines",
                "Gaussian Kernel"
            ],
            "author": [
                "Asa Ben-Hur",
                "David Horn",
                "Hava T Siegelmann",
                "Vladimir Vapnik"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/horn01a/horn01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Attribute Patterns in High-Dimensional Structured Latent Attribute Models",
            "abstract": [
                "Structured latent attribute models (SLAMs) are a special family of discrete latent variable models widely used in social and biological sciences. This paper considers the problem of learning significant attribute patterns from a SLAM with potentially high-dimensional configurations of the latent attributes. We address the theoretical identifiability issue, propose a penalized likelihood method for the selection of the attribute patterns, and further establish the selection consistency in such an overfitted SLAM with a diverging number of latent patterns. The good performance of the proposed methodology is illustrated by simulation studies and two real datasets in educational assessments."
            ],
            "keywords": [],
            "author": [
                "Yuqi Gu",
                "Gongjun Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-197/19-197.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Rate of Optimal Quantization and Application to the Clustering Performance of the Empirical Measure",
            "abstract": [
                "We study the convergence rate of the optimal quantization for a probability measure sequence (µ n) n∈N * on R d converging in the Wasserstein distance in two aspects: the first one is the convergence rate of optimal quantizer x (n) ∈ (R d) K of µ n at level K; the other one is the convergence rate of the distortion function valued at x (n) , called the \"performance\" of x (n). Moreover, we also study the mean performance of the optimal quantization for the empirical measure of a distribution µ with finite second moment but possibly unbounded support. As an application, we show an upper bound with a convergence rate O(log n √ n) of the mean performance for the empirical measure of the multidimensional normal distribution N (m, Σ) and of distributions with hyper-exponential tails. This extends the results from Biau et al. (2008) obtained for compactly supported distribution. We also derive an upper bound which is sharper in the quantization level K but suboptimal in n by applying results in Fournier and Guillin (2015)."
            ],
            "keywords": [
                "clustering performance",
                "convergence rate of optimal quantization",
                "distortion function",
                "empirical measure",
                "optimal quantization"
            ],
            "author": [
                "Yating Liu",
                "Gilles Pagès"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-804/18-804.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Posterior Sparsity in Unsupervised Dependency Parsing",
            "abstract": [
                "A strong inductive bias is essential in unsupervised grammar induction. In this paper, we explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. We use part-of-speech (POS) tags to group dependencies by parent-child types and investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In experiments with different languages, we achieve significant gains in directed attachment accuracy over the standard expectation maximization (EM) baseline, with an average accuracy improvement of 6.5%, outperforming EM by at least 1% for 9 out of 12 languages. Furthermore, the new method outperforms models based on standard Bayesian sparsity-inducing parameter priors with an average improvement of 5% and positive gains of at least 1% for 9 out of 12 languages. On English text in particular, we show that our approach improves performance over other state-of-the-art techniques."
            ],
            "keywords": [],
            "author": [
                "Jennifer Gillenwater",
                "Fernando Pereira",
                "Ben Taskar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/gillenwater11a/gillenwater11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Discrete Restricted Boltzmann Machines",
            "abstract": [
                "We describe discrete restricted Boltzmann machines: probabilistic graphical models with bipartite interactions between visible and hidden discrete variables. Examples are binary restricted Boltzmann machines and discrete naïve Bayes models. We detail the inference functions and distributed representations arising in these models in terms of configurations of projected products of simplices and normal fans of products of simplices. We bound the number of hidden variables, depending on the cardinalities of their state spaces, for which these models can approximate any probability distribution on their visible states to any given accuracy. In addition, we use algebraic methods and coding theory to compute their dimension."
            ],
            "keywords": [
                "restricted Boltzmann machine",
                "naïve Bayes model",
                "representational power",
                "distributed representation",
                "expected dimension"
            ],
            "author": [
                "Guido Montúfar",
                "Jason Morton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/montufar15a/montufar15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ellipsoidal Rounding for Nonnegative Matrix Factorization Under Noisy Separability",
            "abstract": [
                "We present a numerical algorithm for nonnegative matrix factorization (NMF) problems under noisy separability. An NMF problem under separability can be stated as one of finding all vertices of the convex hull of data points. The research interest of this paper is to find the vectors as close to the vertices as possible in a situation in which noise is added to the data points. Our algorithm is designed to capture the shape of the convex hull of data points by using its enclosing ellipsoid. We show that the algorithm has correctness and robustness properties from theoretical and practical perspectives; correctness here means that if the data points do not contain any noise, the algorithm can find the vertices of their convex hull; robustness means that if the data points contain noise, the algorithm can find the near-vertices. Finally, we apply the algorithm to document clustering, and report the experimental results."
            ],
            "keywords": [
                "nonnegative matrix factorization",
                "separability",
                "robustness to noise",
                "enclosing ellipsoid",
                "document clustering"
            ],
            "author": [
                "Tomohiko Mizutani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/mizutani14a/mizutani14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Alternating Linearization for Structured Regularization Problems",
            "abstract": [
                "We adapt the alternating linearization method for proximal decomposition to structured regularization problems, in particular, to the generalized lasso problems. The method is related to two well-known operator splitting methods, the Douglas-Rachford and the Peaceman-Rachford method, but it has descent properties with respect to the objective function. This is achieved by employing a special update test, which decides whether it is beneficial to make a Peaceman-Rachford step, any of the two possible Douglas-Rachford steps, or none. The convergence mechanism of the method is related to that of bundle methods of nonsmooth optimization. We also discuss implementation for very large problems, with the use of specialized algorithms and sparse data structures. Finally, we present numerical results for several synthetic and real-world examples, including a three-dimensional fused lasso problem, which illustrate the scalability, efficacy, and accuracy of the method."
            ],
            "keywords": [
                "lasso",
                "fused lasso",
                "nonsmooth optimization",
                "operator splitting"
            ],
            "author": [
                "Xiaodong Lin",
                "Minh Pham",
                "Andrzej Ruszczyński",
                "S V N Vishwanathan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/lin14a/lin14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Reverse Iterative Volume Sampling for Linear Regression *",
            "abstract": [
                "We study the following basic machine learning task: Given a fixed set of input points in R d for a linear regression problem, we wish to predict a hidden response value for each of the points. We can only afford to attain the responses for a small subset of the points that are then used to construct linear predictions for all points in the dataset. The performance of the predictions is evaluated by the total square loss on all responses (the attained as well as the remaining hidden ones). We show that a good approximate solution to this least squares problem can be obtained from just dimension d many responses by using a joint sampling technique called volume sampling. Moreover, the least squares solution obtained for the volume sampled subproblem is an unbiased estimator of optimal solution based on all n responses. This unbiasedness is a desirable property that is not shared by other common subset selection techniques. Motivated by these basic properties, we develop a theoretical framework for studying volume sampling, resulting in a number of new matrix expectation equalities and statistical guarantees which are of importance not only to least squares regression but also to numerical linear algebra in general. Our methods also lead to a regularized variant of volume sampling, and we propose the first efficient algorithm for volume sampling which makes this technique a practical tool in the machine learning toolbox. Finally, we provide experimental evidence which confirms our theoretical findings."
            ],
            "keywords": [
                "volume sampling",
                "linear regression",
                "row sampling",
                "active learning",
                "optimal design"
            ],
            "author": [
                "Michał Dereziński",
                "Manfred K Warmuth",
                "Santa Cruz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-781/17-781.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Linear Cyclic Causal Models with Latent Variables",
            "abstract": [
                "Identifying cause-effect relationships between variables of interest is a central problem in science. Given a set of experiments we describe a procedure that identifies linear models that may contain cycles and latent variables. We provide a detailed description of the model family, full proofs of the necessary and sufficient conditions for identifiability, a search algorithm that is complete, and a discussion of what can be done when the identifiability conditions are not satisfied. The algorithm is comprehensively tested in simulations, comparing it to competing algorithms in the literature. Furthermore, we adapt the procedure to the problem of cellular network inference, applying it to the biologically realistic data of the DREAM challenges. The paper provides a full theoretical foundation for the causal discovery procedure first presented by Eberhardt et al. (2010) and Hyttinen et al. (2010)."
            ],
            "keywords": [
                "causality",
                "graphical models",
                "randomized experiments",
                "structural equation models",
                "latent variables",
                "latent confounders",
                "cycles"
            ],
            "author": [
                "Antti Hyttinen",
                "Frederick Eberhardt",
                "Baker Hall",
                "Patrik O Hoyer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/hyttinen12a/hyttinen12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "NetSDM: Semantic Data Mining with Network Analysis",
            "abstract": [
                "Semantic data mining (SDM) is a form of relational data mining that uses annotated data together with complex semantic background knowledge to learn rules that can be easily interpreted. The drawback of SDM is a high computational complexity of existing SDM algorithms, resulting in long run times even when applied to relatively small data sets. This paper proposes an effective SDM approach, named NetSDM, which first transforms the available semantic background knowledge into a network format, followed by network analysis based node ranking and pruning to significantly reduce the size of the original background knowledge. The experimental evaluation of the NetSDM methodology on acute lymphoblastic leukemia and breast cancer data demonstrates that NetSDM achieves radical time efficiency improvements and that learned rules are comparable or better than the rules obtained by the original SDM algorithms."
            ],
            "keywords": [
                "data mining",
                "semantic data mining",
                "ontologies",
                "subgroup discovery",
                "network analysis"
            ],
            "author": [
                "Jan Kralj",
                "Marko Robnik-Šikonja",
                "Nada Lavrač"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-066/17-066.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Smooth ε-Insensitive Regression by Loss Symmetrization",
            "abstract": [
                "We describe new loss functions for regression problems along with an accompanying algorithmic framework which utilizes these functions. These loss functions are derived by symmetrization of margin-based losses commonly used in boosting algorithms, namely, the logistic loss and the exponential loss. The resulting symmetric logistic loss can be viewed as a smooth approximation to the ε-insensitive hinge loss used in support vector regression. We describe and analyze two parametric families of batch learning algorithms for minimizing these symmetric losses. The first family employs an iterative log-additive update which can be viewed as a regression counterpart to recent boosting algorithms. The second family utilizes an iterative additive update step. We also describe and analyze online gradient descent (GD) and exponentiated gradient (EG) algorithms for the symmetric logistic loss. A byproduct of our work is a new simple form of regularization for boosting-based classification and regression algorithms. Our regression framework also has implications on classification algorithms, namely, a new additive update boosting algorithm for classification. We demonstrate the merits of our algorithms in a series of experiments."
            ],
            "keywords": [],
            "author": [
                "Ofer Dekel",
                "Yoram Singer",
                "Kristin P Bennett",
                "Nicolò Cesa-Bianchi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/dekel05a/dekel05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Introduction to Special Issue on Independent Components Analysis Te-Won Lee",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "TEWON@UCSD Edu",
                "Jean-François Cardoso"
            ],
            "ref": "http://www.jmlr.org/papers/volume4/lee03a/lee03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Revisiting Bayesian Blind Deconvolution",
            "abstract": [
                "Blind deconvolution involves the estimation of a sharp signal or image given only a blurry observation. Because this problem is fundamentally ill-posed, strong priors on both the sharp image and blur kernel are required to regularize the solution space. While this naturally leads to a standard MAP estimation framework, performance is compromised by unknown trade-off parameter settings, optimization heuristics, and convergence issues stemming from non-convexity and/or poor prior selections. To mitigate some of these problems, a number of authors have recently proposed substituting a variational Bayesian (VB) strategy that marginalizes over the high-dimensional image space leading to better estimates of the blur kernel. However, the underlying cost function now involves both integrals with no closed-form solution and complex, function-valued arguments, thus losing the transparency of MAP. Beyond standard Bayesian-inspired intuitions, it thus remains unclear by exactly what mechanism these methods are able to operate, rendering understanding, improvements and extensions more difficult. To elucidate these issues, we demonstrate that the VB methodology can be recast as an unconventional MAP problem with a very particular penalty/prior that conjoins the image, blur kernel, and noise level in a principled way. This unique penalty has a number of useful characteristics pertaining to relative concavity, local minima avoidance, normalization, and scale-invariance that allow us to rigorously explain the success of VB including its existing implementational heuristics and approximations. It also provides strict criteria for learning the noise level and choosing the optimal image prior that, perhaps counter-intuitively, need not reflect the statistics of natural scenes. In so doing we challenge the prevailing notion of why VB is successful for blind deconvolution while providing a transparent platform for introducing enhancements and extensions. Moreover, the underlying insights carry over to a wide variety of other bilinear models common in the machine learning literature such as independent component analysis, dictionary learning/sparse coding, and non-negative matrix factorization."
            ],
            "keywords": [
                "blind deconvolution",
                "blind image deblurring",
                "variational Bayes",
                "sparse priors",
                "sparse estimation"
            ],
            "author": [
                "David Wipf"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/wipf14a/wipf14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "In All Likelihood, Deep Belief Is Not Enough",
            "abstract": [
                "Statistical models of natural images provide an important tool for researchers in the fields of machine learning and computational neuroscience. The canonical measure to quantitatively assess and compare the performance of statistical models is given by the likelihood. One class of statistical models which has recently gained increasing popularity and has been applied to a variety of complex data is formed by deep belief networks. Analyses of these models, however, have often been limited to qualitative analyses based on samples due to the computationally intractable nature of their likelihood. Motivated by these circumstances, the present article introduces a consistent estimator for the likelihood of deep belief networks which is computationally tractable and simple to apply in practice. Using this estimator, we quantitatively investigate a deep belief network for natural image patches and compare its performance to the performance of other models for natural image patches. We find that the deep belief network is outperformed with respect to the likelihood even by very simple mixture models."
            ],
            "keywords": [
                "deep belief network",
                "restricted Boltzmann machine",
                "likelihood estimation",
                "natural image statistics",
                "potential log-likelihood"
            ],
            "author": [
                "Lucas Theis",
                "Sebastian Gerwinn",
                "Fabian Sinz",
                "Matthias Bethge",
                "Werner Reichardt"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/theis11a/theis11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": null,
            "abstract": NaN,
            "keywords": [],
            "author": [],
            "ref": "http://www.jmlr.org/papers/volume1/herbster01a/herbster01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Grafting: Fast, Incremental Feature Selection by Gradient Descent in Function Space",
            "abstract": [
                "We present a novel and flexible approach to the problem of feature selection, called grafting. Rather than considering feature selection as separate from learning, grafting treats the selection of suitable features as an integral part of learning a predictor in a regularized learning framework. To make this regularized learning process sufficiently fast for large scale problems, grafting operates in an incremental iterative fashion, gradually building up a feature set while training a predictor model using gradient descent. At each iteration, a fast gradient-based heuristic is used to quickly assess which feature is most likely to improve the existing model, that feature is then added to the model, and the model is incrementally optimized using gradient descent. The algorithm scales linearly with the number of data points and at most quadratically with the number of features. Grafting can be used with a variety of predictor model classes, both linear and non-linear, and can be used for both classification and regression. Experiments are reported here on a variant of grafting for classification, using both linear and non-linear models, and using a logistic regression-inspired loss function. Results on a variety of synthetic and real world data sets are presented. Finally the relationship between grafting, stagewise additive modelling, and boosting is explored."
            ],
            "keywords": [
                "Feature selection",
                "functional gradient descent",
                "loss functions",
                "margin space",
                "boosting"
            ],
            "author": [
                "Simon Perkins",
                "Kevin Lacker",
                "James Theiler",
                "Isabelle Guyon",
                "André Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/perkins03a/perkins03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets",
            "abstract": [
                "Classification algorithms are frequently used on data with a natural hierarchical structure. For instance, classifiers are often trained and tested on trial-wise measurements, separately for each subject within a group. One important question is how classification outcomes observed in individual subjects can be generalized to the population from which the group was sampled. To address this question, this paper introduces novel statistical models that are guided by three desiderata. First, all models explicitly respect the hierarchical nature of the data, that is, they are mixed-effects models that simultaneously account for within-subjects (fixed-effects) and across-subjects (random-effects) variance components. Second, maximum-likelihood estimation is replaced by full Bayesian inference in order to enable natural regularization of the estimation problem and to afford conclusions in terms of posterior probability statements. Third, inference on classification accuracy is complemented by inference on the balanced accuracy, which avoids inflated accuracy estimates for imbalanced data sets. We introduce hierarchical models that satisfy these criteria and demonstrate their advantages over conventional methods using MCMC implementations for model inversion and model selection on both synthetic and empirical data. We envisage that our approach will improve the sensitivity and validity of statistical inference in future hierarchical classification studies."
            ],
            "keywords": [
                "beta-binomial",
                "normal-binomial",
                "balanced accuracy",
                "Bayesian inference",
                "group studies"
            ],
            "author": [
                "Kay H Brodersen",
                "Christoph Mathys",
                "Justin R Chumbley",
                "Jean Daunizeau",
                "Soon Cheng",
                "Joachim M Buhmann",
                "H Brodersen",
                "Joachim M Ong",
                "Klaas E Stephan Buhmann",
                "MATHYS, CHUMBLEY, DAUNIZEAU, ONG, BUHMANN Stephan Brodersen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/brodersen12a/brodersen12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso",
            "abstract": [
                "We consider the sparse inverse covariance regularization problem or graphical lasso with regularization parameter λ. Suppose the sample covariance graph formed by thresholding the entries of the sample covariance matrix at λ is decomposed into connected components. We show that the vertex-partition induced by the connected components of the thresholded sample covariance graph (at λ) is exactly equal to that induced by the connected components of the estimated concentration graph, obtained by solving the graphical lasso problem for the same λ. This characterizes a very interesting property of a path of graphical lasso solutions. Furthermore, this simple rule, when used as a wrapper around existing algorithms for the graphical lasso, leads to enormous performance gains. For a range of values of λ, our proposal splits a large graphical lasso problem into smaller tractable problems, making it possible to solve an otherwise infeasible large-scale problem. We illustrate the graceful scalability of our proposal via synthetic and real-life microarray examples."
            ],
            "keywords": [
                "sparse inverse covariance selection",
                "sparsity",
                "graphical lasso",
                "Gaussian graphical models",
                "graph connected components",
                "concentration graph",
                "large scale covariance estimation"
            ],
            "author": [
                "Rahul Mazumder",
                "Trevor Hastie",
                "Stanford Edu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/mazumder12a/mazumder12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Markov Properties for Linear Causal Models with Correlated Errors",
            "abstract": [
                "A linear causal model with correlated errors, represented by a DAG with bi-directed edges, can be tested by the set of conditional independence relations implied by the model. A global Markov property specifies, by the d-separation criterion, the set of all conditional independence relations holding in any model associated with a graph. A local Markov property specifies a much smaller set of conditional independence relations which will imply all other conditional independence relations which hold under the global Markov property. For DAGs with bi-directed edges associated with arbitrary probability distributions, a local Markov property is given in Richardson (2003) which may invoke an exponential number of conditional independencies. In this paper, we show that for a class of linear structural equation models with correlated errors, there is a local Markov property which will invoke only a linear number of conditional independence relations. For general linear models, we provide a local Markov property that often invokes far fewer conditional independencies than that in Richardson (2003). The results have applications in testing linear structural equation models with correlated errors."
            ],
            "keywords": [
                "Markov properties",
                "linear causal models",
                "linear structural equation models",
                "graphical models"
            ],
            "author": [
                "Changsung Kang",
                "Jin Tian"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/kang09a/kang09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Data-driven Rank Breaking for Efficient Rank Aggregation",
            "abstract": [
                "Rank aggregation systems collect ordinal preferences from individuals to produce a global ranking that represents the social preference. Rank-breaking is a common practice to reduce the computational complexity of learning the global ranking. The individual preferences are broken into pairwise comparisons and applied to efficient algorithms tailored for independent paired comparisons. However, due to the ignored dependencies in the data, naive rank-breaking approaches can result in inconsistent estimates. The key idea to produce accurate and consistent estimates is to treat the pairwise comparisons unequally, depending on the topology of the collected data. In this paper, we provide the optimal rank-breaking estimator, which not only achieves consistency but also achieves the best error bound. This allows us to characterize the fundamental tradeoff between accuracy and complexity. Further, the analysis identifies how the accuracy depends on the spectral gap of a corresponding comparison graph."
            ],
            "keywords": [
                "Rank aggregation",
                "Plackett-Luce model",
                "Sample complexity"
            ],
            "author": [
                "Ashish Khetan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-209/16-209.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Intersection Trees",
            "abstract": [
                "Finding interactions between variables in large and high-dimensional data sets is often a serious computational challenge. Most approaches build up interaction sets incrementally, adding variables in a greedy fashion. The drawback is that potentially informative high-order interactions may be overlooked. Here, we propose an alternative approach for classification problems with binary predictor variables, called Random Intersection Trees. It works by starting with a maximal interaction that includes all variables, and then gradually removing variables if they fail to appear in randomly chosen observations of a class of interest. We show that informative interactions are retained with high probability, and the computational complexity of our procedure is of order p κ , where p is the number of predictor variables. The value of κ can reach values as low as 1 for very sparse data; in many more general settings, it will still beat the exponent s obtained when using a brute force search constrained to order s interactions. In addition, by using some new ideas based on min-wise hash schemes, we are able to further reduce the computational cost. Interactions found by our algorithm can be used for predictive modelling in various forms, but they are also often of interest in their own right as useful characterisations of what distinguishes a certain class from others."
            ],
            "keywords": [
                "high-dimensional classification",
                "interactions",
                "min-wise hashing",
                "sparse data"
            ],
            "author": [
                "Rajen Dinesh",
                "Nicolai Meinshausen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/shah14a/shah14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Theoretical Analysis of the Optimal Free Responses of Graph-Based SFA for the Design of Training Graphs",
            "abstract": [
                "Slow feature analysis (SFA) is an unsupervised learning algorithm that extracts slowly varying features from a multi-dimensional time series. Graph-based SFA (GSFA) is an extension to SFA for supervised learning that can be used to successfully solve regression problems if combined with a simple supervised post-processing step on a small number of slow features. The objective function of GSFA minimizes the squared output differences between pairs of samples specified by the edges of a structure called training graph. The edges of current training graphs, however, are derived only from the relative order of the labels. Exploiting the exact numerical value of the labels enables further improvements in label estimation accuracy. In this article, we propose the exact label learning (ELL) method to create a more precise training graph that encodes the desired labels explicitly and allows GSFA to extract a normalized version of them directly (i.e., without supervised post-processing). The ELL method is used for three tasks: (1) We estimate gender from artificial images of human faces (regression) and show the advantage of coding additional labels, particularly skin color. (2) We analyze two existing graphs for regression. (3) We extract compact discriminative features to classify traffic sign images. When the number of output features is limited, such compact features provide a higher classification rate compared to a graph that generates features equivalent to those of nonlinear Fisher discriminant analysis. The method is versatile, directly supports multiple labels, and provides higher accuracy compared to current graphs for the problems considered."
            ],
            "keywords": [
                "slow feature analysis",
                "nonlinear regression",
                "image analysis",
                "pattern recognition",
                "many classes"
            ],
            "author": [
                "Alberto N Escalante-B",
                "Laurenz Wiskott"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-311/15-311.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Stochastic Variance Reduced Gradient Methods by Sampling Extra Data with Replacement",
            "abstract": [
                "We study the round complexity of minimizing the average of convex functions under a new setting of distributed optimization where each machine can receive two subsets of functions. The first subset is from a random partition and the second subset is randomly sampled with replacement. Under this setting, we define a broad class of distributed algorithms whose local computation can utilize both subsets and design a distributed stochastic variance reduced gradient method belonging to in this class. When the condition number of the problem is small, our method achieves the optimal parallel runtime, amount of communication and rounds of communication among all distributed first-order methods up to constant factors. When the condition number is relatively large, a lower bound is provided for the number of rounds of communication needed by any algorithm in this class. Then, we present an accelerated version of our method whose the rounds of communication matches the lower bound up to logarithmic terms, which establishes that this accelerated algorithm has the lowest round complexity among all algorithms in our class under this new setting."
            ],
            "keywords": [
                "distributed optimization",
                "communication complexity",
                "first-order method",
                "lower bound",
                "stochastic variance reduced gradient"
            ],
            "author": [
                "Jason D Lee",
                "Qihang Lin",
                "Tengyu Ma",
                "Tianbao Yang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-640/16-640.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models",
            "abstract": [
                "Maximum entropy (Maxent) is useful in natural language processing and many other areas. Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent. With many variants of IS methods, it is difficult to understand them and see the differences. In this paper, we create a general and unified framework for iterative scaling methods. This framework also connects iterative scaling and coordinate descent methods. We prove general convergence results for IS methods and analyze their computational complexity. Based on the proposed framework, we extend a coordinate descent method for linear SVM to Maxent. Results show that it is faster than existing iterative scaling methods."
            ],
            "keywords": [
                "maximum entropy",
                "iterative scaling",
                "coordinate descent",
                "natural language processing",
                "optimization"
            ],
            "author": [
                "Lan Huang",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang",
                "Chih-Jen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/huang10a/huang10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts *",
            "abstract": [
                "We present an ensemble method for concept drift that dynamically creates and removes weighted experts in response to changes in performance. The method, dynamic weighted majority (DWM), uses four mechanisms to cope with concept drift: It trains online learners of the ensemble, it weights those learners based on their performance, it removes them, also based on their performance, and it adds new experts based on the global performance of the ensemble. After an extensive evaluationconsisting of five experiments, eight learners, and thirty data sets that varied in type of target concept, size, presence of noise, and the like-we concluded that DWM outperformed other learners that only incrementally learn concept descriptions, that maintain and use previously encountered examples, and that employ an unweighted, fixed-size ensemble of experts."
            ],
            "keywords": [
                "concept learning",
                "online learning",
                "ensemble methods",
                "concept drift"
            ],
            "author": [
                "J Zico Kolter",
                "Marcus A Maloof"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/kolter07a/kolter07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Thompson Sampling Guided Stochastic Searching on the Line for Deceptive Environments with Applications to Root-Finding Problems *",
            "abstract": [
                "The multi-armed bandit problem forms the foundation for solving a wide range of online stochastic optimization problems through a simple, yet effective mechanism. One simply casts the problem as a gambler who repeatedly pulls one out of N slot machine arms, eliciting random rewards. Learning of reward probabilities is then combined with reward maximization, by carefully balancing reward exploration against reward exploitation. In this paper, we address a particularly intriguing variant of the multi-armed bandit problem, referred to as the Stochastic Point Location (SPL) problem. The gambler is here only told whether the optimal arm (point) lies to the \"left\" or to the \"right\" of the arm pulled, with the feedback being erroneous with probability 1 − π. This formulation thus targets optimization in continuous action spaces with both informative and deceptive feedback. To tackle this class of problems, we formulate a compact and scalable Bayesian representation of the solution space that simultaneously captures both the location of the optimal arm as well as the probability of receiving correct feedback. We further introduce the accompanying Thompson Sampling guided Stochastic Point Location (TS-SPL) scheme for balancing exploration against exploitation. By learning π, TS-SPL also supports deceptive environments that are lying about the direction of the optimal arm. This, in turn, allows us to address the fundamental Stochastic Root Finding (SRF) problem. Empirical results demonstrate that our scheme deals with both deceptive and informative environments, significantly outperforming competing algorithms both for SRF and SPL."
            ],
            "keywords": [
                "thompson sampling",
                "searching on the line",
                "probabilistic bisection search",
                "deceptive environment",
                "stochastic point location"
            ],
            "author": [
                "Sondre Glimsdal",
                "Ole-Christoffer Granmo"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-263/18-263.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Direct Approach for Sparse Quadratic Discriminant Analysis",
            "abstract": [
                "Quadratic discriminant analysis (QDA) is a standard tool for classification due to its simplicity and flexibility. Because the number of its parameters scales quadratically with the number of the variables, QDA is not practical, however, when the dimensionality is relatively large. To address this, we propose a novel procedure named DA-QDA for QDA in analyzing high-dimensional data. Formulated in a simple and coherent framework, DA-QDA aims to directly estimate the key quantities in the Bayes discriminant function including quadratic interactions and a linear index of the variables for classification. Under appropriate sparsity assumptions, we establish consistency results for estimating the interactions and the linear index, and further demonstrate that the misclassification rate of our procedure converges to the optimal Bayes risk, even when the dimensionality is exponentially high with respect to the sample size. An efficient algorithm based on the alternating direction method of multipliers (ADMM) is developed for finding interactions, which is much faster than its competitor in the literature. The promising performance of DA-QDA is illustrated via extensive simulation studies and the analysis of four real datasets."
            ],
            "keywords": [
                "Bayes Risk",
                "Consistency",
                "High Dimensional Data",
                "Linear Discriminant Analysis",
                "Quadratic Discriminant Analysis",
                "Sparsity"
            ],
            "author": [
                "Binyan Jiang",
                "Chenlei Leng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-285/17-285.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Trust Region Newton Method for Large-Scale Logistic Regression",
            "abstract": [
                "Large-scale logistic regression arises in many applications such as document classification and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM)."
            ],
            "keywords": [
                "logistic regression",
                "newton method",
                "trust region",
                "conjugate gradient",
                "support vector machines"
            ],
            "author": [
                "Chih-Jen Lin",
                "S Sathiya Keerthi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/lin08b/lin08b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classifier Selection using the Predicate Depth Ran Gilad-Bachrach",
            "abstract": [
                "Typically, one approaches a supervised machine learning problem by writing down an objective function and finding a hypothesis that minimizes it. This is equivalent to finding the Maximum A Posteriori (MAP) hypothesis for a Boltzmann distribution. However, MAP is not a robust statistic. We present an alternative approach by defining a median of the distribution, which we show is both more robust, and has good generalization guarantees. We present algorithms to approximate this median. One contribution of this work is an efficient method for approximating the Tukey median. The Tukey median, which is often used for data visualization and outlier detection, is a special case of the family of medians we define: however, computing it exactly is exponentially slow in the dimension. Our algorithm approximates such medians in polynomial time while making weaker assumptions than those required by previous work."
            ],
            "keywords": [
                "classification",
                "estimation",
                "median",
                "Tukey depth"
            ],
            "author": [
                "Christopher J C Burges"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/gilad-bachrach13a/gilad-bachrach13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Reconstructing Undirected Graphs from Eigenspaces",
            "abstract": [
                "We aim at recovering the weighted adjacency matrix W of an undirected graph from a perturbed version of its eigenspaces. This situation arises for instance when working with stationary signals on graphs or Markov chains observed at random times. Our approach relies on minimizing a cost function based on the Frobenius norm of the commutator AB − BA between symmetric matrices A and B. We describe a particular framework in which we have access to an estimation of the eigenspaces and provide support selection procedures from theoretical and practical points of view. In the Erdős-Rényi model on N vertices with no self-loops, we show that identifiability (i.e., the ability to reconstruct W from the knowledge of its eigenspaces) follows a sharp phase transition on the expected number of edges with threshold function N log N/2. Simulated and real life numerical experiments assert our methodology."
            ],
            "keywords": [
                "Support recovery",
                "Identifiability",
                "Stationary signal processing",
                "Graphs",
                "Backward selection algorithm"
            ],
            "author": [
                "Yohann De Castro",
                "Thibault Espinasse",
                "Paul Rochet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-146/16-146.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lenient Learning in Independent-Learner Stochastic Cooperative Games",
            "abstract": [
                "We introduce the Lenient Multiagent Reinforcement Learning 2 (LMRL2) algorithm for independent-learner stochastic cooperative games. LMRL2 is designed to overcome a pathology called relative overgeneralization, and to do so while still performing well in games with stochastic transitions, stochastic rewards, and miscoordination. We discuss the existing literature, then compare LMRL2 against other algorithms drawn from the literature which can be used for games of this kind: traditional (\"Distributed\") Q-learning, Hysteretic Q-learning, WoLF-PHC, SOoN, and (for repeated games only) FMQ. The results show that LMRL2 is very effective in both of our measures (complete and correct policies), and is found in the top rank more often than any other technique. LMRL2 is also easy to tune: though it has many available parameters, almost all of them stay at default settings. Generally the algorithm is optimally tuned with a single parameter, if any. We then examine and discuss a number of side-issues and options for LMRL2."
            ],
            "keywords": [
                "multiagent learning",
                "reinforcement learning",
                "game theory",
                "lenient learning",
                "independent learner"
            ],
            "author": [
                "Ermo Wei",
                "Sean Luke"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-417/15-417.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Near-optimal Individualized Treatment Recommendations",
            "abstract": [
                "The individualized treatment recommendation (ITR) is an important analytic framework for precision medicine. The goal of ITR is to assign the best treatments to patients based on their individual characteristics. From the machine learning perspective, the solution to the ITR problem can be formulated as a weighted classification problem to maximize the mean benefit from the recommended treatments given patients' characteristics. Several ITR methods have been proposed in both the binary setting and the multicategory setting. In practice, one may prefer a more flexible recommendation that includes multiple treatment options. This motivates us to develop methods to obtain a set of near-optimal individualized treatment recommendations alternative to each other, called alternative individualized treatment recommendations (A-ITR). We propose two methods to estimate the optimal A-ITR within the outcome weighted learning (OWL) framework. Simulation studies and a real data analysis for Type diabetic patients with injectable antidiabetic treatments are conducted to show the usefulness of the proposed A-ITR framework. We also show the consistency of these methods and obtain an upper bound for the risk between the theoretically optimal recommendation and the estimated one. An R package aitr has been developed, found at https://github.com/menghaomiao/aitr."
            ],
            "keywords": [
                "individualized treatment recommendation",
                "set-valued classification",
                "anglebased classification",
                "reproducing kernel Hilbert space",
                "statistical learning theory"
            ],
            "author": [
                "Haomiao Meng",
                "Ying-Qi Zhao",
                "Haoda Fu",
                "Xingye Qiao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-334/20-334.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Equilibria of Games via Payoff Queries",
            "abstract": [
                "A recent body of experimental literature has studied empirical game-theoretical analysis, in which we have partial knowledge of a game, consisting of observations of a subset of the pure-strategy profiles and their associated payoffs to players. The aim is to find an exact or approximate Nash equilibrium of the game, based on these observations. It is usually assumed that the strategy profiles may be chosen in an on-line manner by the algorithm. We study a corresponding computational learning model, and the query complexity of learning equilibria for various classes of games. We give basic results for exact equilibria of bimatrix and graphical games. We then study the query complexity of approximate equilibria in bimatrix games. Finally, we study the query complexity of exact equilibria in symmetric network congestion games. For directed acyclic networks, we can learn the cost functions (and hence compute an equilibrium) while querying just a small fraction of pure-strategy profiles. For the special case of parallel links, we have the stronger result that an equilibrium can be identified while only learning a small fraction of the cost values."
            ],
            "keywords": [
                "query complexity",
                "bimatrix game",
                "congestion game",
                "equilibrium computation",
                "approximate Nash equilibrium"
            ],
            "author": [
                "John Fearnley",
                "Martin Gairing",
                "Paul W Goldberg",
                "Rahul Savani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/fearnley15a/fearnley15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research",
            "abstract": [
                "This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature."
            ],
            "keywords": [
                "crowdsourcing",
                "data generation",
                "model evaluation",
                "hybrid intelligence",
                "behavioral experiments",
                "incentives",
                "mechanical turk"
            ],
            "author": [
                "Jennifer Wortman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-234/17-234.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Robust Procedure For Gaussian Graphical Model Search From Microarray Data With p Larger Than n",
            "abstract": [
                "Learning of large-scale networks of interactions from microarray data is an important and challenging problem in bioinformatics. A widely used approach is to assume that the available data constitute a random sample from a multivariate distribution belonging to a Gaussian graphical model. As a consequence, the prime objects of inference are full-order partial correlations which are partial correlations between two variables given the remaining ones. In the context of microarray data the number of variables exceed the sample size and this precludes the application of traditional structure learning procedures because a sampling version of full-order partial correlations does not exist. In this paper we consider limited-order partial correlations, these are partial correlations computed on marginal distributions of manageable size, and provide a set of rules that allow one to assess the usefulness of these quantities to derive the independence structure of the underlying Gaussian graphical model. Furthermore, we introduce a novel structure learning procedure based on a quantity, obtained from limited-order partial correlations, that we call the non-rejection rate. The applicability and usefulness of the procedure are demonstrated by both simulated and real data."
            ],
            "keywords": [
                "Gaussian distribution",
                "gene network",
                "graphical model",
                "microarray data",
                "non-rejection rate",
                "partial correlation",
                "small-sample inference"
            ],
            "author": [
                "Robert Castelo",
                "Alberto Roverato"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/castelo06a/castelo06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Value Function Based Reinforcement Learning in Changing Markovian Environments",
            "abstract": [
                "The paper investigates the possibility of applying value function based reinforcement learning (RL) methods in cases when the environment may change over time. First, theorems are presented which show that the optimal value function of a discounted Markov decision process (MDP) Lipschitz continuously depends on the immediate-cost function and the transition-probability function. Dependence on the discount factor is also analyzed and shown to be non-Lipschitz. Afterwards, the concept of (ε, δ)-MDPs is introduced, which is a generalization of MDPs and ε-MDPs. In this model the environment may change over time, more precisely, the transition function and the cost function may vary from time to time, but the changes must be bounded in the limit. Then, learning algorithms in changing environments are analyzed. A general relaxed convergence theorem for stochastic iterative algorithms is presented. We also demonstrate the results through three classical RL methods: asynchronous value iteration, Q-learning and temporal difference learning. Finally, some numerical experiments concerning changing environments are presented."
            ],
            "keywords": [
                "Markov decision processes",
                "reinforcement learning",
                "changing environments",
                "(ε",
                "δ)-MDPs",
                "value function bounds",
                "stochastic iterative algorithms"
            ],
            "author": [
                "Balázs Csanád Csáji",
                "László Monostori",
                "Laszlo Monostori",
                "@ Sztaki"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/csaji08a/csaji08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Foundations of Noise-free Selective Classification",
            "abstract": [
                "We consider selective classification, a term we adopt here to refer to 'classification with a reject option.' The essence in selective classification is to trade-off classifier coverage for higher accuracy. We term this trade-off the risk-coverage (RC) trade-off. Our main objective is to characterize this trade-off and to construct algorithms that can optimally or near optimally achieve the best possible trade-offs in a controlled manner. For noise-free models we present in this paper a thorough analysis of selective classification including characterizations of RC trade-offs in various interesting settings."
            ],
            "keywords": [
                "classification with a reject option",
                "selective classification",
                "perfect learning",
                "high performance classification",
                "risk-coverage trade-off"
            ],
            "author": [
                "Ran El-Yaniv",
                "Yair Wiener"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/el-yaniv10a/el-yaniv10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Collective Inference for Extraction MRFs Coupled with Symmetric Clique Potentials",
            "abstract": [
                "Many structured information extraction tasks employ collective graphical models that capture interinstance associativity by coupling them with various clique potentials. We propose tractable families of such potentials that are invariant under permutations of their arguments, and call them symmetric clique potentials. We present three families of symmetric potentials-MAX, SUM, and MAJORITY. We propose cluster message passing for collective inference with symmetric clique potentials, and present message computation algorithms tailored to such potentials. Our first message computation algorithm, called α-pass, is sub-quadratic in the clique size, outputs exact messages for MAX, and computes-approximate messages for Potts, a popular member of the SUM family. Empirically, it is upto two orders of magnitude faster than existing algorithms based on graph-cuts or belief propagation. Our second algorithm, based on Lagrangian relaxation, operates on MAJORITY potentials and provides close to exact solutions while being two orders of magnitude faster. We show that the cluster message passing framework is more principled, accurate and converges faster than competing approaches. We extend our collective inference framework to exploit associativity of more general intradomain properties of instance labelings, which opens up interesting applications in domain adaptation. Our approach leads to significant error reduction on unseen domains without incurring any overhead of model retraining."
            ],
            "keywords": [
                "graphical models",
                "collective inference",
                "clique potentials",
                "cluster graphs",
                "message passing"
            ],
            "author": [
                "Rahul Gupta"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/gupta10a/gupta10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Clustering from General Pairwise Observations with Applications to Time-varying Graphs *",
            "abstract": [
                "We present a general framework for graph clustering and bi-clustering where we are given a general observation (called a label) between each pair of nodes. This framework allows a rich encoding of various types of pairwise interactions between nodes. We propose a new tractable and robust approach to this problem based on convex optimization and maximum likelihood estimators. We analyze our algorithms under a general statistical model extending the planted partition and stochastic block models. Both sufficient and necessary conditions are provided for successful recovery of the underlying clusters. Our theoretical results subsume many existing graph clustering results for a wide range of settings, including planted partition, weighted clustering, submatrix localization and partially observed graphs. Furthermore, our results are applicable to novel settings including time-varying graphs, providing new insights to solutions of these problems. We provide empirical results on both synthetic and real data that corroborate with our theoretical findings."
            ],
            "keywords": [
                "graph clustering",
                "convex optimization",
                "low-rank matrix",
                "information divergence",
                "timevarying graphs",
                "pairwise observation",
                "dynamic graphs"
            ],
            "author": [
                "Hong Shiau",
                "Yudong Chen",
                "Huan Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-659/15-659.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Query Strategies for Evading Convex-Inducing Classifiers",
            "abstract": [
                "Classifiers are often used to detect miscreant activities. We study how an adversary can systematically query a classifier to elicit information that allows the attacker to evade detection while incurring a near-minimal cost of modifying their intended malfeasance. We generalize the theory of Lowd and Meek (2005) to the family of convex-inducing classifiers that partition their feature space into two sets, one of which is convex. We present query algorithms for this family that construct undetected instances of approximately minimal cost using only polynomially-many queries in the dimension of the space and in the level of approximation. Our results demonstrate that nearoptimal evasion can be accomplished for this family without reverse engineering the classifier's decision boundary. We also consider general ℓ p costs and show that near-optimal evasion on the family of convex-inducing classifiers is generally efficient for both positive and negative convexity for all levels of approximation if p = 1."
            ],
            "keywords": [
                "query algorithms",
                "evasion",
                "reverse engineering",
                "adversarial learning"
            ],
            "author": [
                "Blaine Nelson",
                "Benjamin I P Rubinstein",
                "Ling Huang",
                "Anthony D Joseph",
                "Satish Rao",
                "J D Tygar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/nelson12a/nelson12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Mode-Seeking Clustering and Density Ridge Estimation via Direct Estimation of Density-Derivative-Ratios",
            "abstract": [
                "Modes and ridges of the probability density function behind observed data are useful geometric features. Mode-seeking clustering assigns cluster labels by associating data samples with the nearest modes, and estimation of density ridges enables us to find lower-dimensional structures hidden in data. A key technical challenge both in mode-seeking clustering and density ridge estimation"
            ],
            "keywords": [],
            "author": [
                "Hiroaki Sasaki"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-380/17-380.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Partial Least Squares Regression in Reproducing Kernel Hilbert Space",
            "abstract": [
                "A family of regularized least squares regression models in a Reproducing Kernel Hilbert Space is extended by the kernel partial least squares (PLS) regression model. Similar to principal components regression (PCR), PLS is a method based on the projection of input (explanatory) variables to the latent variables (components). However, in contrast to PCR, PLS creates the components by modeling the relationship between input and output variables while maintaining most of the information in the input variables. PLS is useful in situations where the number of explanatory variables exceeds the number of observations and/or a high level of multicollinearity among those variables is assumed. Motivated by this fact we will provide a kernel PLS algorithm for construction of nonlinear regression models in possibly high-dimensional feature spaces. We give the theoretical description of the kernel PLS algorithm and we experimentally compare the algorithm with the existing kernel PCR and kernel ridge regression techniques. We will demonstrate that on the data sets employed kernel PLS achieves the same results as kernel PCR but uses significantly fewer, qualitatively different components."
            ],
            "keywords": [],
            "author": [
                "Roman Rosipal",
                "Leonard J Trejo",
                "Nello Cristianini",
                "Bob Williamson"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/rosipal01a/rosipal01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Markov chain Monte Carlo methods for tall data",
            "abstract": [
                "Markov chain Monte Carlo methods are often deemed too computationally intensive to be of any practical use for big data applications, and in particular for inference on datasets containing a large number n of individual data points, also known as tall datasets. In scenarios where data are assumed independent, various approaches to scale up the Metropolis-Hastings algorithm in a Bayesian inference context have been recently proposed in machine learning and computational statistics. These approaches can be grouped into two categories: divide-and-conquer approaches and, subsampling-based algorithms. The aims of this article are as follows. First, we present a comprehensive review of the existing literature, commenting on the underlying assumptions and theoretical guarantees of each method. Second, by leveraging our understanding of these limitations, we propose an original subsampling-based approach relying on a control variate method which samples under regularity conditions from a distribution provably close to the posterior distribution of interest, yet can require less than O(n) data point likelihood evaluations at each iteration for certain statistical models in favourable scenarios. Finally, we emphasize that we have only been able so far to propose subsampling-based methods which display good performance in scenarios where the Bernstein-von Mises approximation of the target posterior distribution is excellent. It remains an open challenge to develop such methods in scenarios where the Bernstein-von Mises approximation is poor. Contents 1 Introduction 2 2 Bayesian inference, MCMC, and tall data 3 2."
            ],
            "keywords": [],
            "author": [
                "Rémi Bardenet",
                "Arnaud Doucet",
                "Chris Holmes"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-205/15-205.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Hierarchical Clustering *",
            "abstract": [
                "One of the most widely used techniques for data clustering is agglomerative clustering. Such algorithms have been long used across many different fields ranging from computational biology to social sciences to computer vision in part because their output is easy to interpret. Unfortunately, it is well known, however, that many of the classic agglomerative clustering algorithms are not robust to noise. In this paper we propose and analyze a new robust algorithm for bottom-up agglomerative clustering. We show that our algorithm can be used to cluster accurately in cases where the data satisfies a number of natural properties and where the traditional agglomerative algorithms fail. We also show how to adapt our algorithm to the inductive setting where our given data is only a small random sample of the entire data set. Experimental evaluations on synthetic and real world data sets show that our algorithm achieves better performance than other hierarchical algorithms in the presence of noise."
            ],
            "keywords": [
                "unsupervised learning",
                "clustering",
                "agglomerative algorithms",
                "robustness"
            ],
            "author": [
                "Maria-Florina Balcan",
                "Yingyu Liang",
                "Pramod Gupta"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/balcan14a/balcan14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Towards an Axiomatic Approach to Hierarchical Clustering of Measures",
            "abstract": [
                "We propose some axioms for hierarchical clustering of probability measures and investigate their ramifications. The basic idea is to let the user stipulate the clusters for some elementary measures. This is done without the need of any notion of metric, similarity or dissimilarity. Our main results then show that for each suitable choice of user-defined clustering on elementary measures we obtain a unique notion of clustering on a large set of distributions satisfying a set of additivity and continuity axioms. We illustrate the developed theory by numerous examples including some with and some without a density."
            ],
            "keywords": [
                "axiomatic clustering",
                "hierarchical clustering",
                "infinite samples clustering",
                "density level set clustering",
                "mixed Hausdorff-dimensions"
            ],
            "author": [
                "Philipp Thomann",
                "Ingo Steinwart",
                "Nico Schmid",
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/thomann15a/thomann15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Minimum Volume Sets",
            "abstract": [
                "Given a probability measure P and a reference measure µ, one is often interested in the minimum µ-measure set with P-measure at least α. Minimum volume sets of this type summarize the regions of greatest probability mass of P, and are useful for detecting anomalies and constructing confidence regions. This paper addresses the problem of estimating minimum volume sets based on independent samples distributed according to P. Other than these samples, no other information is available regarding P, but the reference measure µ is assumed to be known. We introduce rules for estimating minimum volume sets that parallel the empirical risk minimization and structural risk minimization principles in classification. As in classification, we show that the performances of our estimators are controlled by the rate of uniform convergence of empirical to true probabilities over the class from which the estimator is drawn. Thus we obtain finite sample size performance bounds in terms of VC dimension and related quantities. We also demonstrate strong universal consistency, an oracle inequality, and rates of convergence. The proposed estimators are illustrated with histogram and decision tree set estimation rules."
            ],
            "keywords": [
                "minimum volume sets",
                "anomaly detection",
                "statistical learning theory",
                "uniform deviation bounds",
                "sample complexity",
                "universal consistency"
            ],
            "author": [
                "Clayton D Scott",
                "Robert D Nowak"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/scott06a/scott06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Spectral Algorithm for Inference in Hidden semi-Markov Models",
            "abstract": [
                "Hidden semi-Markov models (HSMMs) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models (HMMs). In this paper, we introduce a novel spectral algorithm to perform inference in HSMMs. Unlike expectation maximization (EM), our approach correctly estimates the probability of given observation sequence based on a set of training sequences. Our approach is based on estimating moments from the sample, whose number of dimensions depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few matrix inversions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the advantage of the algorithm over EM in terms of speed and accuracy, especially for large data sets."
            ],
            "keywords": [
                "Graphical models",
                "hidden semi-Markov model",
                "spectral algorithm",
                "tensor analysis",
                "aviation safety"
            ],
            "author": [
                "Igor Melnyk",
                "Ibm T J Watson",
                "Arindam Banerjee"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-468/15-468.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SMART: An Open Source Data Labeling Platform for Supervised Learning",
            "abstract": [
                "SMART is an open source web application designed to help data scientists and research teams efficiently build labeled training data sets for supervised machine learning tasks. SMART provides users with an intuitive interface for creating labeled data sets, supports active learning to help reduce the required amount of labeled data, and incorporates interrater reliability statistics to provide insight into label quality. SMART is designed to be platform agnostic and easily deployable to meet the needs of as many different research teams as possible. The project website 1 contains links to the code repository and extensive user documentation."
            ],
            "keywords": [
                "software",
                "supervised learning",
                "data labeling",
                "active learning",
                "open source"
            ],
            "author": [
                "Rob Chew",
                "Michael Wenger",
                "Caroline Kery",
                "Jason Nance",
                "Emily Hadley",
                "Peter Baumgartner",
                "Keith Richards"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-859/18-859.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernels for Sequentially Ordered Data",
            "abstract": [
                "We present a novel framework for learning with sequential data of any kind, such as multivariate time series, strings, or sequences of graphs. The main result is a \"sequentialization\" that transforms any kernel on a given domain into a kernel for sequences in that domain. This procedure preserves properties such as positive definiteness, the associated kernel feature map is an ordered variant of sample (cross-)moments, and this sequentialized kernel is consistent in the sense that it converges to a kernel for paths if sequences converge to paths (by discretization). Further, classical kernels for sequences arise as special cases of this method. We use dynamic programming and low-rank techniques for tensors to provide efficient algorithms to compute this sequentialized kernel."
            ],
            "keywords": [
                "Sequential data",
                "kernels",
                "signature",
                "ordered moments",
                "signature kernels"
            ],
            "author": [
                "Franz J Király",
                "Harald Oberhauser"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/16-314/16-314.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Learning of Complex Prediction Problems Using Simultaneous Projections",
            "abstract": [
                "We describe and analyze an algorithmic framework for online classification where each online trial consists of multiple prediction tasks that are tied together. We tackle the problem of updating the online predictor by defining a projection problem in which each prediction task corresponds to a single linear constraint. These constraints are tied together through a single slack parameter. We then introduce a general method for approximately solving the problem by projecting simultaneously and independently on each constraint which corresponds to a prediction sub-problem, and then averaging the individual solutions. We show that this approach constitutes a feasible, albeit not necessarily optimal, solution of the original projection problem. We derive concrete simultaneous projection schemes and analyze them in the mistake bound model. We demonstrate the power of the proposed algorithm in experiments with synthetic data and with multiclass text categorization tasks."
            ],
            "keywords": [
                "online learning",
                "parallel computation",
                "mistake bounds",
                "structured prediction"
            ],
            "author": [
                "Yonatan Amit",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/amit08a/amit08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient and Accurate Methods for Updating Generalized Linear Models with Multiple Feature Additions",
            "abstract": [
                "In this paper, we propose an approach for learning regression models efficiently in an environment where multiple features and data-points are added incrementally in a multistep process. At each step, any finite number of features maybe added and hence, the setting is not amenable to low rank updates. We show that our approach is not only efficient and optimal for ordinary least squares, weighted least squares, generalized least squares and ridge regression, but also more generally for generalized linear models and lasso regression that use iterated re-weighted least squares for maximum likelihood estimation. Our approach instantiated to linear settings has close relations to the partitioned matrix inversion mechanism based on Schur's complement. For arbitrary regression methods, even a relaxation of the approach is no worse than using the model from the previous step or using a model that learns on the additional features and optimizes the residual of the model at the previous step. Such problems are commonplace in complex manufacturing operations consisting of hundreds of steps, where multiple measurements are taken at each step to monitor the quality of the final product. Accurately predicting if the finished product will meet specifications at each or, at least, important intermediate steps can be extremely useful in enhancing productivity. We further validate our claims through experiments on synthetic and real industrial data sets."
            ],
            "keywords": [
                "linear regression",
                "logistic regressions",
                "lasso",
                "group lasso",
                "feature selection",
                "manufacturing"
            ],
            "author": [
                "Amit Dhurandhar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/dhurandhar14a/dhurandhar14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Particle-Based Variational Approach to Bayesian Non-negative Matrix Factorization",
            "abstract": [
                "Bayesian Non-negative Matrix Factorization (BNMF) is a promising approach for understanding uncertainty and structure in matrix data. However, a large volume of applied work optimizes traditional non-Bayesian NMF objectives that fail to provide a principled understanding of the non-identifiability inherent in NMF-an issue ideally addressed by a Bayesian approach. Despite their suitability, current BNMF approaches have failed to gain popularity in an applied setting; they sacrifice flexibility in modeling for tractable computation, tend to get stuck in local modes, and can require many thousands of samples for meaningful uncertainty estimates. We address these issues through a particle-based variational approach to BNMF that only requires the joint likelihood to be differentiable for computational tractability, uses a novel transfer-based initialization technique to identify multiple modes in the posterior, and thus allows domain experts to inspect a small set of factorizations that faithfully represent the posterior. On several real datasets, we obtain better particle approximations to the BNMF posterior in less time than baselines and demonstrate the significant role that multimodality plays in NMF-related tasks."
            ],
            "keywords": [
                "Bayesian",
                "Non-negative Matrix Factorization",
                "Stein discrepancy",
                "Non-identifiability",
                "Transfer Learning"
            ],
            "author": [
                "Muhammad A Masood",
                "John A Paulson",
                "Finale Doshi-Velez"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-153/18-153.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Rates of Efficient Global Optimization Algorithms",
            "abstract": [
                "In the efficient global optimization problem, we minimize an unknown function f , using as few observations f (x) as possible. It can be considered a continuum-armed-bandit problem, with noiseless data, and simple regret. Expected-improvement algorithms are perhaps the most popular methods for solving the problem; in this paper, we provide theoretical results on their asymptotic behaviour. Implementing these algorithms requires a choice of Gaussian-process prior, which determines an associated space of functions, its reproducing-kernel Hilbert space (RKHS). When the prior is fixed, expected improvement is known to converge on the minimum of any function in its RKHS. We provide convergence rates for this procedure, optimal for functions of low smoothness, and describe a modified algorithm attaining optimal rates for smoother functions. In practice, however, priors are typically estimated sequentially from the data. For standard estimators, we show this procedure may never find the minimum of f. We then propose alternative estimators, chosen to minimize the constants in the rate of convergence, and show these estimators retain the convergence rates of a fixed prior."
            ],
            "keywords": [
                "convergence rates",
                "efficient global optimization",
                "expected improvement",
                "continuumarmed bandit",
                "Bayesian optimization"
            ],
            "author": [
                "Adam D Bull"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/bull11a/bull11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Weight Function in the Subtree Kernel is Decisive",
            "abstract": [
                "Tree data are ubiquitous because they model a large variety of situations, e.g., the architecture of plants, the secondary structure of RNA, or the hierarchy of XML files. Nevertheless, the analysis of these non-Euclidean data is difficult per se. In this paper, we focus on the subtree kernel that is a convolution kernel for tree data introduced by Vishwanathan and Smola in the early 2000's. More precisely, we investigate the influence of the weight function from a theoretical perspective and in real data applications. We establish on a 2-classes stochastic model that the performance of the subtree kernel is improved when the weight of leaves vanishes, which motivates the definition of a new weight function, learned from the data and not fixed by the user as usually done. To this end, we define a unified framework for computing the subtree kernel from ordered or unordered trees, that is particularly suitable for tuning parameters. We show through eight real data classification problems the great efficiency of our approach, in particular for small data sets, which also states the high importance of the weight function. Finally, a visualization tool of the significant features is derived."
            ],
            "keywords": [
                "classification of tree data",
                "kernel methods",
                "subtree kernel",
                "weight function",
                "tree compression"
            ],
            "author": [
                "Romain Azaïs",
                "Florian Ingels"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-290/19-290.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Lasso and group-Lasso for functional Poisson regression",
            "abstract": [
                "High dimensional Poisson regression has become a standard framework for the analysis of massive counts datasets. In this work we estimate the intensity function of the Poisson regression model by using a dictionary approach, which generalizes the classical basis approach, combined with a Lasso or a group-Lasso procedure. Selection depends on penalty weights that need to be calibrated. Standard methodologies developed in the Gaussian framework can not be directly applied to Poisson models due to heteroscedasticity. Here we provide data-driven weights for the Lasso and the group-Lasso derived from concentration inequalities adapted to the Poisson case. We show that the associated Lasso and group-Lasso procedures satisfy fast and slow oracle inequalities. Simulations are used to assess the empirical performance of our procedure, and an original application to the analysis of Next Generation Sequencing data is provided."
            ],
            "keywords": [
                "Functional Poisson regression",
                "adaptive lasso",
                "adaptive group-lasso",
                "calibration",
                "concentration"
            ],
            "author": [
                "Stéphane Ivanoff",
                "Franck Picard",
                "Vincent Rivoirard"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-021/15-021.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A General Framework for Constrained Bayesian Optimization using Information-based Search",
            "abstract": [
                "We present an information-theoretic framework for solving global black-box optimization problems that also have black-box constraints. Of particular interest to us is to efficiently solve problems with decoupled constraints, in which subsets of the objective and constraint functions may be evaluated independently. For example, when the objective is evaluated on a CPU and the constraints are evaluated independently on a GPU. These problems require an acquisition function that can be separated into the contributions of the individual function evaluations. We develop one such acquisition function and call it Predictive Entropy Search with Constraints (PESC). PESC is an approximation to the expected information gain criterion and it compares favorably to alternative approaches based on improvement in several synthetic and real-world problems. In addition to this, we consider problems with a mix of functions that are fast and slow to evaluate. These problems require balancing the amount of time spent in the meta-computation of PESC and in the actual evaluation of the target objective. We take a bounded rationality approach and develop a partial update for PESC which trades off accuracy against speed. We then propose a method for adaptively switching between the partial and full updates for PESC. This allows us to interpolate between versions of PESC that are efficient in terms of function evaluations and those that are efficient in terms of wall-clock time. Overall, we demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization."
            ],
            "keywords": [
                "Bayesian optimization",
                "constraints",
                "predictive entropy search"
            ],
            "author": [
                "José Miguel Hernández-Lobato",
                "Michael A Gelbart",
                "Ryan P Adams",
                "Matthew W Hoffman",
                "M Hernández-Lobato",
                "Z Ghahramani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-616/15-616.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Bayes via Barycenter in Wasserstein Space",
            "abstract": [
                "Divide-and-conquer based methods for Bayesian inference provide a general approach for tractable posterior inference when the sample size is large. These methods divide the data into smaller subsets, sample from the posterior distribution of parameters in parallel on all the subsets, and combine posterior samples from all the subsets to approximate the full data posterior distribution. The smaller size of any subset compared to the full data implies that posterior sampling on any subset is computationally more efficient than sampling from the true posterior distribution. Since the combination step takes negligible time relative to sampling, posterior computations can be scaled to massive data by dividing the full data into sufficiently large number of data subsets. One such approach relies on the geometry of posterior distributions estimated across different subsets and combines them through their barycenter in a Wasserstein space of probability measures. We provide theoretical guarantees on the accuracy of approximation that are valid in many applications. We show that the geometric method approximates the full data posterior distribution better than its competitors across diverse simulations and reproduces known results when applied to a movie ratings database."
            ],
            "keywords": [
                "barycenter",
                "big data",
                "distributed Bayesian computations",
                "empirical measures",
                "linear programming",
                "optimal transportation",
                "Wasserstein distance",
                "Wasserstein space"
            ],
            "author": [
                "Sanvesh Srivastava",
                "Cheng Li",
                "David B Dunson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-084/17-084.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spam Filtering Based On The Analysis Of Text Information Embedded Into Images",
            "abstract": [
                "In recent years anti-spam filters have become necessary tools for Internet service providers to face up to the continuously growing spam phenomenon. Current server-side anti-spam filters are made up of several modules aimed at detecting different features of spam e-mails. In particular, text categorisation techniques have been investigated by researchers for the design of modules for the analysis of the semantic content of e-mails, due to their potentially higher generalisation capability with respect to manually derived classification rules used in current server-side filters. However, very recently spammers introduced a new trick consisting of embedding the spam message into attached images, which can make all current techniques based on the analysis of digital text in the subject and body fields of e-mails ineffective. In this paper we propose an approach to antispam filtering which exploits the text information embedded into images sent as attachments. Our approach is based on the application of state-of-the-art text categorisation techniques to the analysis of text extracted by OCR tools from images attached to e-mails. The effectiveness of the proposed approach is experimentally evaluated on two large corpora of spam e-mails."
            ],
            "keywords": [
                "spam filtering",
                "e-mail",
                "images",
                "text categorisation"
            ],
            "author": [
                "Giorgio Fumera",
                "Ignazio Pillai",
                "Fabio Roli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/fumera06a/fumera06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality",
            "abstract": [
                "The statistical learning theory of risk minimization depends heavily on probability bounds for uniform deviations of the empirical risks. Classical probability bounds using Hoeffding's inequality cannot accommodate more general situations with unbounded loss and dependent data. The current paper introduces an inequality that extends Hoeffding's inequality to handle these more general situations. We will apply this inequality to provide probability bounds for uniform deviations in a very general framework, which can involve discrete decision rules, unbounded loss, and a dependence structure that can be more general than either martingale or strong mixing. We will consider two examples with high dimensional predictors: autoregression (AR) with ℓ 1-loss, and ARX model with variable selection for sign classification, which uses both lagged responses and exogenous predictors."
            ],
            "keywords": [
                "dependence",
                "empirical risk",
                "probability bound",
                "unbounded loss",
                "uniform deviation"
            ],
            "author": [
                "Wenxin Jiang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/jiang09a/jiang09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
            "abstract": [
                "In many real-world settings, a team of agents must coordinate its behaviour while acting in a decentralised fashion. At the same time, it is often possible to train the agents in a centralised fashion where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a mixing network that estimates joint action-values as a monotonic combination of per-agent values. We structurally enforce that the joint-action value is monotonic in the per-agent values, through the use of non-negative weights in the mixing network, which guarantees consistency between the centralised and decentralised policies. To evaluate the performance of QMIX, we propose the StarCraft Multi-Agent Challenge (SMAC) as a new benchmark for deep multi-agent reinforcement learning. We evaluate QMIX on a challenging set of SMAC scenarios and show that it significantly outperforms existing multi-agent reinforcement learning methods."
            ],
            "keywords": [
                "Reinforcement Learning",
                "Multi-Agent Learning",
                "Multi-Agent Coordination"
            ],
            "author": [
                "Tabish Rashid",
                "Mikayel Samvelyan",
                "Christian Schroeder De Witt",
                "Gregory Farquhar",
                "Jakob Foerster",
                "Shimon Whiteson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-081/20-081.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SGDLibrary: A MATLAB library for stochastic optimization algorithms",
            "abstract": [
                "We consider the problem of finding the minimizer of a function f : R d → R of the finite-sum form min f (w) = 1/n n i f i (w). This problem has been studied intensively in recent years in the field of machine learning (ML). One promising approach for large-scale data is to use a stochastic optimization algorithm to solve the problem. SGDLibrary is a readable, flexible and extensible pure-MATLAB library of a collection of stochastic optimization algorithms. The purpose of the library is to provide researchers and implementers a comprehensive evaluation environment for the use of these algorithms on various ML problems."
            ],
            "keywords": [
                "Stochastic optimization",
                "stochastic gradient",
                "finite-sum minimization problem",
                "large-scale optimization problem"
            ],
            "author": [
                "Hiroyuki Kasai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-632/17-632.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variance-based Regularization with Convex Objectives",
            "abstract": [
                "We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error. Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certificates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance. We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems."
            ],
            "keywords": [
                "variance regularization",
                "robust optimization",
                "empirical likelihood"
            ],
            "author": [
                "John Duchi",
                "Hongseok Namkoong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-750/17-750.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Reasoning with Ancestral Graphs",
            "abstract": [
                "Causal reasoning is primarily concerned with what would happen to a system under external interventions. In particular, we are often interested in predicting the probability distribution of some random variables that would result if some other variables were forced to take certain values. One prominent approach to tackling this problem is based on causal Bayesian networks, using directed acyclic graphs as causal diagrams to relate post-intervention probabilities to pre-intervention probabilities that are estimable from observational data. However, such causal diagrams are seldom fully testable given observational data. In consequence, many causal discovery algorithms based on data-mining can only output an equivalence class of causal diagrams (rather than a single one). This paper is concerned with causal reasoning given an equivalence class of causal diagrams, represented by a (partial) ancestral graph. We present two main results. The first result extends Pearl (1995)'s celebrated do-calculus to the context of ancestral graphs. In the second result, we focus on a key component of Pearl's calculus-the property of invariance under interventions, and give stronger graphical conditions for this property than those implied by the first result. The second result also improves the earlier, similar results due to Spirtes et al. (1993)."
            ],
            "keywords": [
                "ancestral graphs",
                "causal Bayesian network",
                "do-calculus",
                "intervention"
            ],
            "author": [
                "Jiji Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/zhang08a/zhang08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Regularization-Based Adaptive Test for High-Dimensional Generalized Linear Models",
            "abstract": [
                "In spite of its urgent importance in the era of big data, testing high-dimensional parameters in generalized linear models (GLMs) in the presence of high-dimensional nuisance parameters has been largely under-studied, especially with regard to constructing powerful tests for general (and unknown) alternatives. Most existing tests are powerful only against certain alternatives and may yield incorrect Type I error rates under high-dimensional nuisance parameter situations. In this paper, we propose the adaptive interaction sum of powered score (aiSPU) test in the framework of penalized regression with a non-convex penalty, called truncated Lasso penalty (TLP), which can maintain correct Type I error rates while yielding high statistical power across a wide range of alternatives. To calculate its p-values analytically, we derive its asymptotic null distribution. Via simulations, its superior finite-sample performance is demonstrated over several representative existing methods. In addition, we apply it and other representative tests to an Alzheimer's Disease Neuroimaging Initiative (ADNI) data set, detecting possible gene-gender interactions for Alzheimer's disease. We also put R package \"aispu\" implementing the proposed test on GitHub."
            ],
            "keywords": [
                "Adaptive Test",
                "Truncated Lasso Penalty",
                "Gene-Environmental Interaction"
            ],
            "author": [
                "Chong Wu",
                "Gongjun Xu",
                "Xiaotong Shen",
                "Wei Pan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-807/18-807.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multimodal Learning with Deep Boltzmann Machines",
            "abstract": [
                "Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bi-modal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time."
            ],
            "keywords": [
                "Boltzmann machines",
                "unsupervised learning",
                "multimodal learning",
                "neural networks",
                "deep learning"
            ],
            "author": [
                "Nitish Srivastava",
                "Ruslan Salakhutdinov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/srivastava14b/srivastava14b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "From Dependency to Causality: A Machine Learning Approach",
            "abstract": [
                "The relationship between statistical dependency and causality lies at the heart of all statistical approaches to causal inference. Recent results in the ChaLearn cause-effect pair challenge have shown that causal directionality can be inferred with good accuracy also in Markov indistinguishable configurations thanks to data driven approaches. This paper proposes a supervised machine learning approach to infer the existence of a directed causal link between two variables in multivariate settings with n > 2 variables. The approach relies on the asymmetry of some conditional (in)dependence relations between the members of the Markov blankets of two variables causally connected. Our results show that supervised learning methods may be successfully used to extract causal information on the basis of asymmetric statistical descriptors also for n > 2 variate distributions."
            ],
            "keywords": [
                "causal inference",
                "information theory",
                "machine learning"
            ],
            "author": [
                "Gianluca Bontempi",
                "Isabelle Guyon",
                "Alexander Statnikov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/bontempi15a/bontempi15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Properties of Monotonic Effects on Directed Acyclic Graphs",
            "abstract": [
                "Various relationships are shown hold between monotonic effects and weak monotonic effects and the monotonicity of certain conditional expectations. Counterexamples are provided to show that the results do not hold under less restrictive conditions. Monotonic effects are furthermore used to relate signed edges on a causal directed acyclic graph to qualitative effect modification. The theory is applied to an example concerning the direct effect of smoking on cardiovascular disease controlling for hypercholesterolemia. Monotonicity assumptions are used to construct a test for whether there is a variable that confounds the relationship between the mediator, hypercholesterolemia, and the outcome, cardiovascular disease."
            ],
            "keywords": [
                "Bayesian networks",
                "conditional expectation",
                "covariance",
                "directed acyclic graphs",
                "effect modification",
                "monotonicity"
            ],
            "author": [
                "Tyler J Vanderweele",
                "James M Robins"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/vanderweele09a/vanderweele09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimating Functions for Blind Separation When Sources Have Variance Dependencies",
            "abstract": [
                "A blind separation problem where the sources are not independent, but have variance dependencies is discussed. For this scenario Hyvärinen and Hurri (2004) proposed an algorithm which requires no assumption on distributions of sources and no parametric model of dependencies between components. In this paper, we extend the semiparametric approach of Amari and Cardoso (1997) to variance dependencies and study estimating functions for blind separation of such dependent sources. In particular, we show that many ICA algorithms are applicable to the variance-dependent model as well under mild conditions, although they should in principle not. Our results indicate that separation can be done based only on normalized sources which are adjusted to have stationary variances and is not affected by the dependent activity levels. We also study the asymptotic distribution of the quasi maximum likelihood method and the stability of the natural gradient learning in detail. Simulation results of artificial and realistic examples match well with our theoretical findings."
            ],
            "keywords": [
                "blind source separation",
                "variance dependencies",
                "independent component analysis",
                "semiparametric statistical models",
                "estimating functions"
            ],
            "author": [
                "Motoaki Kawanabe",
                "Klaus- Robert Müller"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/kawanabe05a/kawanabe05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Learning with Regularized Least Squares",
            "abstract": [
                "We study distributed learning with the least squares regularization scheme in a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach, the algorithm partitions a data set into disjoint data subsets, applies the least squares regularization scheme to each data subset to produce an output function, and then takes an average of the individual output functions as a final global estimator or predictor. We show with error bounds and learning rates in expectation in both the L 2-metric and RKHS-metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine. Our derived learning rates in expectation are optimal and stated in a general setting without any eigenfunction assumption. The analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach. Even for the classical least squares regularization scheme in the RKHS associated with a general kernel, we give the best learning rate in expectation in the literature."
            ],
            "keywords": [
                "Distributed learning",
                "divide-and-conquer",
                "error analysis",
                "integral operator",
                "second order decomposition"
            ],
            "author": [
                "Shao-Bo Lin",
                "Ding-Xuan Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-586/15-586.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "bandicoot: a Python Toolbox for Mobile Phone Metadata",
            "abstract": [
                "bandicoot is an open-source Python toolbox to extract more than 1442 features from standard mobile phone metadata. bandicoot makes it easy for machine learning researchers and practitioners to load mobile phone data, to analyze and visualize them, and to extract robust features which can be used for various classification and clustering tasks. Emphasis is put on ease of use, consistency, and documentation. bandicoot has no dependencies and is distributed under MIT license."
            ],
            "keywords": [
                "Python",
                "feature engineering",
                "mobile phone metadata",
                "CDR",
                "visualization"
            ],
            "author": [
                "Luc Rocher"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-593/15-593.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sharp Restricted Isometry Bounds for the Inexistence of Spurious Local Minima in Nonconvex Matrix Recovery",
            "abstract": [
                "Nonconvex matrix recovery is known to contain no spurious local minima under a restricted isometry property (RIP) with a sufficiently small RIP constant δ. If δ is too large, however, then counterexamples containing spurious local minima are known to exist. In this paper, we introduce a proof technique that is capable of establishing sharp thresholds on δ to guarantee the inexistence of spurious local minima. Using the technique, we prove that in the case of a rank-1 ground truth, an RIP constant of δ < 1/2 is both necessary and sufficient for exact recovery from any arbitrary initial point (such as a random point). We also prove a local recovery result: given an initial point x 0 satisfying f (x 0) ≤ (1 − δ) 2 f (0), any descent algorithm that converges to second-order optimality guarantees exact recovery."
            ],
            "keywords": [
                "matrix factorization",
                "nonconvex optimization",
                "Restricted Isometry Property",
                "matrix sensing",
                "spurious local minima"
            ],
            "author": [
                "Richard Y Zhang",
                "Somayeh Sojoudi",
                "Javad Lavaei"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-020/19-020.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Revisiting Stein's Paradox: Multi-Task Averaging",
            "abstract": [
                "We present a multi-task learning approach to jointly estimate the means of multiple independent distributions from samples. The proposed multi-task averaging (MTA) algorithm results in a convex combination of the individual task's sample averages. We derive the optimal amount of regularization for the two task case for the minimum risk estimator and a minimax estimator, and show that the optimal amount of regularization can be practically estimated without cross-validation. We extend the practical estimators to an arbitrary number of tasks. Simulations and real data experiments demonstrate the advantage of the proposed MTA estimators over standard averaging and James-Stein estimation."
            ],
            "keywords": [
                "multi-task learning",
                "James-Stein",
                "Stein's paradox"
            ],
            "author": [
                "Sergey Feldman",
                "Maya R Gupta",
                "Bela A Frigyik",
                "Ben Taskar",
                "Massimiliano Pontil"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/feldman14a/feldman14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Semi-Analytic Resampling in Lasso",
            "abstract": [
                "An approximate method for conducting resampling in Lasso, the penalized linear regression, in a semi-analytic manner is developed, whereby the average over the resampled datasets is directly computed without repeated numerical sampling, thus enabling an inference free of the statistical fluctuations due to sampling finiteness, as well as a significant reduction of computational time. The proposed method is based on a message passing type algorithm, and its fast convergence is guaranteed by the state evolution analysis, when covariates are provided as zero-mean independently and identically distributed Gaussian random variables. It is employed to implement bootstrapped Lasso (Bolasso) and stability selection, both of which are variable selection methods using resampling in conjunction with Lasso, and resolves their disadvantage regarding computational cost. To examine approximation accuracy and efficiency, numerical experiments were carried out using simulated datasets. Moreover, an application to a real-world dataset, the wine quality dataset, is presented. To process such real-world datasets, an objective criterion for determining the relevance of selected variables is also introduced by the addition of noise variables and resampling. MATLAB codes implementing the proposed method are distributed in (Obuchi, 2018)."
            ],
            "keywords": [
                "bootstrap method",
                "Lasso",
                "variable selection",
                "message passing algorithm",
                "replica method"
            ],
            "author": [
                "Tomoyuki Obuchi",
                "Yoshiyuki Kabashima"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-109/18-109.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning with Structured Sparsity",
            "abstract": [
                "This paper investigates a learning formulation called structured sparsity, which is a natural extension of the standard sparsity concept in statistical learning and compressive sensing. By allowing arbitrary structures on the feature set, this concept generalizes the group sparsity idea that has become popular in recent years. A general theory is developed for learning with structured sparsity, based on the notion of coding complexity associated with the structure. It is shown that if the coding complexity of the target signal is small, then one can achieve improved performance by using coding complexity regularization methods, which generalize the standard sparse regularization. Moreover, a structured greedy algorithm is proposed to efficiently solve the structured sparsity problem. It is shown that the greedy algorithm approximately solves the coding complexity optimization problem under appropriate conditions. Experiments are included to demonstrate the advantage of structured sparsity over standard sparsity on some real applications."
            ],
            "keywords": [
                "structured sparsity",
                "standard sparsity",
                "group sparsity",
                "tree sparsity",
                "graph sparsity",
                "sparse learning",
                "feature selection",
                "compressive sensing"
            ],
            "author": [
                "Junzhou Huang",
                "Tong Zhang",
                "Dimitris Metaxas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/huang11b/huang11b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Tight Bound of Hard Thresholding",
            "abstract": [
                "This paper is concerned with the hard thresholding operator which sets all but the k largest absolute elements of a vector to zero. We establish a tight bound to quantitatively characterize the deviation of the thresholded solution from a given signal. Our theoretical result is universal in the sense that it holds for all choices of parameters, and the underlying analysis depends only on fundamental arguments in mathematical optimization. We discuss the implications for two domains: Compressed Sensing. On account of the crucial estimate, we bridge the connection between the restricted isometry property (RIP) and the sparsity parameter for a vast volume of hard thresholding based algorithms, which renders an improvement on the RIP condition especially when the true sparsity is unknown. This suggests that in essence, many more kinds of sensing matrices or fewer measurements are admissible for the data acquisition procedure. Machine Learning. In terms of large-scale machine learning, a significant yet challenging problem is learning accurate sparse models in an efficient manner. In stark contrast to prior work that attempted the 1-relaxation for promoting sparsity, we present a novel stochastic algorithm which performs hard thresholding in each iteration, hence ensuring such parsimonious solutions. Equipped with the developed bound, we prove the global linear convergence for a number of prevalent statistical models under mild assumptions, even though the problem turns out to be non-convex."
            ],
            "keywords": [
                "sparsity",
                "hard thresholding",
                "compressed sensing",
                "stochastic optimization"
            ],
            "author": [
                "Jie Shen",
                "Ping Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-299/16-299.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Newton-Stein Method: An Optimization Method for GLMs via Stein's Lemma",
            "abstract": [
                "We consider the problem of efficiently computing the maximum likelihood estimator in Generalized Linear Models (GLMs) when the number of observations is much larger than the number of coefficients (n p 1). In this regime, optimization algorithms can immensely benefit from approximate second order information. We propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a Stein-type lemma, which allows further improvements through sub-sampling and eigenvalue thresholding. Our algorithm enjoys fast convergence rates, resembling that of second order methods, with modest per-iteration cost. We provide its convergence analysis for the general case where the rows of the design matrix are samples from a sub-Gaussian distribution. We show that the convergence has two phases, a quadratic phase followed by a linear phase. Finally, we empirically demonstrate that our algorithm achieves the highest performance compared to various optimization algorithms on several data sets."
            ],
            "keywords": [
                "Optimization",
                "Generalized Linear Models",
                "Newton's method",
                "Sub-sampling"
            ],
            "author": [
                "Murat A Erdogdu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-062/16-062.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Revisiting the Nyström Method for Improved Large-scale Machine Learning",
            "abstract": [
                "We reconsider randomized algorithms for the low-rank approximation of symmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods; they characterize the effects of common data preprocessing steps on the performance of these algorithms; and they point to important differences between uniform sampling and nonuniform sampling methods based on leverage scores. In addition, our empirical results illustrate that existing theory is so weak that it does not provide even a qualitative guide to practice. Thus, we complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds-e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error-and they point to future directions to make these algorithms useful in even larger-scale machine learning applications."
            ],
            "keywords": [
                "Nyström approximation",
                "low-rank approximation",
                "kernel methods",
                "randomized algorithms",
                "numerical linear algebra"
            ],
            "author": [
                "Alex Gittens",
                "Michael W Mahoney"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/gittens16a/gittens16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis",
            "abstract": [
                "Supervised learning from high-dimensional data, for example, multimedia data, is a challenging task. We propose an extension of slow feature analysis (SFA) for supervised dimensionality reduction called graph-based SFA (GSFA). The algorithm extracts a label-predictive low-dimensional set of features that can be post-processed by typical supervised algorithms to generate the final label or class estimation. GSFA is trained with a so-called training graph, in which the vertices are the samples and the edges represent similarities of the corresponding labels. A new weighted SFA optimization problem is introduced, generalizing the notion of slowness from sequences of samples to such training graphs. We show that GSFA computes an optimal solution to this problem in the considered function space and propose several types of training graphs. For classification, the most straightforward graph yields features equivalent to those of (nonlinear) Fisher discriminant analysis. Emphasis is on regression, where four different graphs were evaluated experimentally with a subproblem of face detection on photographs. The method proposed is promising particularly when linear models are insufficient as well as when feature selection is difficult."
            ],
            "keywords": [
                "slow feature analysis",
                "feature extraction",
                "classification",
                "regression",
                "pattern recognition",
                "training graphs",
                "nonlinear dimensionality reduction",
                "supervised learning",
                "implicitly supervised",
                "high-dimensional data",
                "image analysis"
            ],
            "author": [
                "Alberto N Escalante-B",
                "Laurenz Wiskott"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/escalante13a/escalante13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Step Size Adaptation in Reproducing Kernel Hilbert Space",
            "abstract": [
                "This paper presents an online support vector machine (SVM) that uses the stochastic meta-descent (SMD) algorithm to adapt its step size automatically. We formulate the online learning problem as a stochastic gradient descent in reproducing kernel Hilbert space (RKHS) and translate SMD to the nonparametric setting, where its gradient trace parameter is no longer a coefficient vector but an element of the RKHS. We derive efficient updates that allow us to perform the step size adaptation in linear time. We apply the online SVM framework to a variety of loss functions, and in particular show how to handle structured output spaces and achieve efficient online multiclass classification. Experiments show that our algorithm outperforms more primitive methods for setting the gradient step size."
            ],
            "keywords": [
                "online SVM",
                "stochastic meta-descent",
                "structured output spaces"
            ],
            "author": [
                "S V N Vishwanathan",
                "Nicol N Schraudolph",
                "Alex J Smola"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/schraudolph06a/schraudolph06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rationality, Optimism and Guarantees in General Reinforcement Learning",
            "abstract": [
                "In this article, 1 we present a top-down theoretical study of general reinforcement learning agents. We begin with rational agents with unlimited resources and then move to a setting where an agent can only maintain a limited number of hypotheses and optimizes plans over a horizon much shorter than what the agent designer actually wants. We axiomatize what is rational in such a setting in a manner that enables optimism, which is important to achieve systematic explorative behavior. Then, within the class of agents deemed rational, we achieve convergence and finite-error bounds. Such results are desirable since they imply that the agent learns well from its experiences, but the bounds do not directly guarantee good performance and can be achieved by agents doing things one should obviously not. Good performance cannot in fact be guaranteed for any agent in fully general settings. Our approach is to design agents that learn well from experience and act rationally. We introduce a framework for general reinforcement learning agents based on rationality axioms for a decision function and an hypothesis-generating function designed so as to achieve guarantees on the number errors. We will consistently use an optimistic decision function but the hypothesis-generating function needs to change depending on what is known/assumed. We investigate a number of natural situations having either a frequentist or Bayesian flavor, deterministic or stochastic environments and either finite or countable hypothesis class. Further, to achieve sufficiently good bounds as to hold promise for practical success we introduce a notion of a class of environments being generated by a set of laws. None of the above has previously been done for fully general reinforcement learning environments."
            ],
            "keywords": [
                "reinforcement learning",
                "rationality",
                "optimism",
                "optimality",
                "error bounds"
            ],
            "author": [
                "Peter Sunehag",
                "Marcus Hutter"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/sunehag15a/sunehag15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Penalized Model-Based Clustering with Application to Variable Selection",
            "abstract": [
                "Variable selection in clustering analysis is both challenging and important. In the context of modelbased clustering analysis with a common diagonal covariance matrix, which is especially suitable for \"high dimension, low sample size\" settings, we propose a penalized likelihood approach with an L 1 penalty function, automatically realizing variable selection via thresholding and delivering a sparse solution. We derive an EM algorithm to fit our proposed model, and propose a modified BIC as a model selection criterion to choose the number of components and the penalization parameter. A simulation study and an application to gene function prediction with gene expression profiles demonstrate the utility of our method."
            ],
            "keywords": [
                "BIC",
                "EM",
                "mixture model",
                "penalized likelihood",
                "soft-thresholding",
                "shrinkage"
            ],
            "author": [
                "Wei Pan",
                "Xiaotong Shen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/pan07a/pan07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Updates",
            "abstract": [
                "We analyze (stochastic) gradient descent (SGD) with delayed updates on smooth quasiconvex and non-convex functions and derive concise, non-asymptotic, convergence rates. We show that the rate of convergence in all cases consists of two terms: (i) a stochastic term which is not affected by the delay, and (ii) a higher order deterministic term which is only linearly slowed down by the delay. Thus, in the presence of noise, the effects of the delay become negligible after a few iterations and the algorithm converges at the same optimal rate as standard SGD. This result extends a line of research that showed similar results in the asymptotic regime or for strongly-convex quadratic functions only. We further show similar results for SGD with more intricate form of delayed gradientscompressed gradients under error compensation and for local SGD where multiple workers perform local steps before communicating with each other. In all of these settings, we improve upon the best known rates. These results show that SGD is robust to compressed and/or delayed stochastic gradient updates. This is in particular important for distributed parallel implementations, where asynchronous and communication efficient methods are the key to achieve linear speedups for optimization with multiple devices."
            ],
            "keywords": [
                "Delayed Gradients",
                "Error-Compensation",
                "Error-Feedback",
                "Gradient Compression",
                "Local SGD",
                "Machine Learning",
                "Optimization",
                "Stochastic Gradient Descent"
            ],
            "author": [
                "Sebastian U Stich",
                "Sai Praneeth Karimireddy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-748/19-748.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Characterization, Stability and Convergence of Hierarchical Clustering Methods",
            "abstract": [
                "We study hierarchical clustering schemes under an axiomatic view. We show that within this framework, one can prove a theorem analogous to one of Kleinberg (2002), in which one obtains an existence and uniqueness theorem instead of a non-existence result. We explore further properties of this unique scheme: stability and convergence are established. We represent dendrograms as ultrametric spaces and use tools from metric geometry, namely the Gromov-Hausdorff distance, to quantify the degree to which perturbations in the input metric space affect the result of hierarchical methods."
            ],
            "keywords": [
                "clustering",
                "hierarchical clustering",
                "stability of clustering",
                "Gromov-Hausdorff distance"
            ],
            "author": [
                "Gunnar Carlsson",
                "Facundo Mémoli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/carlsson10a/carlsson10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Benefitting from the Variables that Variable Selection Discards",
            "abstract": [
                "In supervised learning variable selection is used to find a subset of the available inputs that accurately predict the output. This paper shows that some of the variables that variable selection discards can beneficially be used as extra outputs for inductive transfer. Using discarded input variables as extra outputs forces the model to learn mappings from the variables that were selected as inputs to these extra outputs. Inductive transfer makes what is learned by these mappings available to the model that is being trained on the main output, often resulting in improved performance on that main output. We present three synthetic problems (two regression problems and one classification problem) where performance improves if some variables discarded by variable selection are used as extra outputs. We then apply variable selection to two real problems (DNA splice-junction and pneumonia risk prediction) and demonstrate the same effect: using some of the discarded input variables as extra outputs yields somewhat better performance on both of these problems than can be achieved by variable selection alone. This new approach enhances the benefit of variable selection by allowing the learner to benefit from variables that would otherwise have been discarded by variable selection, but without suffering the loss in performance that occurs when these variables are used as inputs."
            ],
            "keywords": [
                "multitask learning",
                "output variable selection",
                "inductive transfer"
            ],
            "author": [
                "Rich Caruana",
                "Virginia R De Sa",
                "Isabelle Guyon",
                "André Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/caruana03a/caruana03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Combining PAC-Bayesian and Generic Chaining Bounds",
            "abstract": [
                "There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, first, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester (1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand (see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds."
            ],
            "keywords": [
                "statistical learning theory",
                "PAC-Bayes theorems",
                "generalization error bounds"
            ],
            "author": [
                "Jean-Yves Audibert",
                "Olivier Bousquet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/audibert07a/audibert07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Megaman: Scalable Manifold Learning in Python",
            "abstract": [
                "Manifold Learning (ML) is a class of algorithms seeking a low-dimensional non-linear representation of high-dimensional data. Thus, ML algorithms are most applicable to highdimensional data and require large sample sizes to accurately estimate the manifold. Despite this, most existing manifold learning implementations are not particularly scalable. Here we present a Python package that implements a variety of manifold learning algorithms in a modular and scalable fashion, using fast approximate neighbors searches and fast sparse eigendecompositions. The package incorporates theoretical advances in manifold learning, such as the unbiased Laplacian estimator introduced by Coifman and Lafon (2006) and the estimation of the embedding distortion by the Riemannian metric method introduced by Perrault-Joncas and Meila (2013). In benchmarks, even on a single-core desktop computer, our code embeds millions of data points in minutes, and takes just 200 minutes to embed the main sample of galaxy spectra from the Sloan Digital Sky Surveyconsisting of 0.6 million samples in 3750-dimensions-a task which has not previously been possible."
            ],
            "keywords": [
                "manifold learning",
                "dimension reduction",
                "Riemannian metric",
                "graph embedding",
                "scalable methods",
                "python"
            ],
            "author": [
                "James Mcqueen",
                "Marina Meilȃ",
                "Jacob Vanderplas",
                "Zhongyue Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-109/16-109.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the optimality of the Hedge algorithm in the stochastic regime",
            "abstract": [
                "In this paper, we study the behavior of the Hedge algorithm in the online stochastic setting. We prove that anytime Hedge with decreasing learning rate, which is one of the simplest algorithm for the problem of prediction with expert advice, is remarkably both worst-case optimal and adaptive to the easier stochastic and adversarial with a gap problems. This shows that, in spite of its small, non-adaptive learning rate, Hedge possesses the same optimal regret guarantee in the stochastic case as recently introduced adaptive algorithms. Moreover, our analysis exhibits qualitative differences with other versions of the Hedge algorithm, such as the fixed-horizon variant (with constant learning rate) and the one based on the so-called \"doubling trick\", both of which fail to adapt to the easier stochastic setting. Finally, we determine the intrinsic limitations of anytime Hedge in the stochastic case, and discuss the improvements provided by more adaptive algorithms."
            ],
            "keywords": [
                "Online learning",
                "prediction with expert advice",
                "Hedge",
                "adaptive algorithms"
            ],
            "author": [
                "Jaouad Mourtada"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-869/18-869.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improving Structure MCMC for Bayesian Networks through Markov Blanket Resampling",
            "abstract": [
                "Algorithms for inferring the structure of Bayesian networks from data have become an increasingly popular method for uncovering the direct and indirect influences among variables in complex systems. A Bayesian approach to structure learning uses posterior probabilities to quantify the strength with which the data and prior knowledge jointly support each possible graph feature. Existing Markov Chain Monte Carlo (MCMC) algorithms for estimating these posterior probabilities are slow in mixing and convergence, especially for large networks. We present a novel Markov blanket resampling (MBR) scheme that intermittently reconstructs the Markov blanket of nodes, thus allowing the sampler to more effectively traverse low-probability regions between local maxima. As we can derive the complementary forward and backward directions of the MBR proposal distribution, the Metropolis-Hastings algorithm can be used to account for any asymmetries in these proposals. Experiments across a range of network sizes show that the MBR scheme outperforms other state-of-the-art algorithms, both in terms of learning performance and convergence rate. In particular, MBR achieves better learning performance than the other algorithms when the number of observations is relatively small and faster convergence when the number of variables in the network is large."
            ],
            "keywords": [
                "probabilistic graphical models",
                "directed acyclic graph",
                "Bayesian inference",
                "Markov chain Monte Carlo"
            ],
            "author": [
                "Chengwei Su",
                "Mark E Borsuk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/su16a/su16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife",
            "abstract": [
                "We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2013) that are based on the jackknife and the infinitesimal jackknife (IJ). In practice, bagged predictors are computed using a finite number B of bootstrap replicates, and working with a large B can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require B = Θ(n 1.5) bootstrap replicates to converge, where n is the size of the training set. We propose improved versions that only require B = Θ(n) replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our findings with multiple experiments and simulation studies."
            ],
            "keywords": [
                "bagging",
                "jackknife methods",
                "Monte Carlo noise",
                "variance estimation"
            ],
            "author": [
                "Stefan Wager",
                "Trevor Hastie",
                "Bradley Efron"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/wager14a/wager14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stability of Density-Based Clustering",
            "abstract": [
                "High density clusters can be characterized by the connected components of a level set L(λ) = {x : p(x) > λ} of the underlying probability density function p generating the data, at some appropriate level λ ≥ 0. The complete hierarchical clustering can be characterized by a cluster tree T = λ L(λ). In this paper, we study the behavior of a density level set estimate L(λ) and cluster tree estimate T based on a kernel density estimator with kernel bandwidth h. We define two notions of instability to measure the variability of L(λ) and T as a function of h, and investigate the theoretical properties of these instability measures."
            ],
            "keywords": [
                "clustering",
                "density estimation",
                "level sets",
                "stability",
                "model selection"
            ],
            "author": [
                "Alessandro Rinaldo",
                "Aarti Singh",
                "Rebecca Nugent",
                "Larry Wasserman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/rinaldo12a/rinaldo12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models",
            "abstract": [
                "Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an informationtheoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing sparse outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds of unknown dimension obscured by gross corruptions."
            ],
            "keywords": [
                "Variational Autoencoder",
                "Deep Generative Model",
                "Robust PCA"
            ],
            "author": [
                "Bin Dai",
                "Yu Wang",
                "John Aston",
                "David Wipf"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-704/17-704.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayes Point Machines",
            "abstract": [
                "Kernel-classifiers comprise a powerful class of non-linear decision functions for binary classification. The support vector machine is an example of a learning algorithm for kernel classifiers that singles out the consistent classifier with the largest margin, i.e. minimal real-valued output on the training sample, within the set of consistent hypotheses, the so-called version space. We suggest the Bayes point machine as a well-founded improvement which approximates the Bayes-optimal decision by the centre of mass of version space. We present two algorithms to stochastically approximate the centre of mass of version space: a billiard sampling algorithm and a sampling algorithm based on the well known perceptron algorithm. It is shown how both algorithms can be extended to allow for soft-boundaries in order to admit training errors. Experimentally, we find that-for the zero training error case-Bayes point machines consistently outperform support vector machines on both surrogate data and real-world benchmark data sets. In the soft-boundary/soft-margin case, the improvement over support vector machines is shown to be reduced. Finally, we demonstrate that the realvalued output of single Bayes points on novel test points is a valid confidence measure and leads to a steady decrease in generalisation error when used as a rejection criterion."
            ],
            "keywords": [],
            "author": [
                "Ralf Herbrich",
                "Colin Campbell",
                "Christopher K I Williams"
            ],
            "ref": "http://www.jmlr.org/papers/volume1/herbrich01a/herbrich01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Stationary-Point Hitting Time and Ergodicity of Stochastic Gradient Langevin Dynamics",
            "abstract": [
                "Stochastic gradient Langevin dynamics (SGLD) is a fundamental algorithm in stochastic optimization. Recent work by Zhang et al. (2017) presents an analysis for the hitting time of SGLD for the first and second order stationary points. The proof in Zhang et al. (2017) is a two-stage procedure through bounding the Cheeger's constant, which is rather complicated and leads to loose bounds. In this paper, using intuitions from stochastic differential equations, we provide a direct analysis for the hitting times of SGLD to the first and second order stationary points. Our analysis is straightforward. It only relies on basic linear algebra and probability theory tools. Our direct analysis also leads to tighter bounds comparing to Zhang et al. (2017) and shows the explicit dependence of the hitting time on different factors, including dimensionality, smoothness, noise strength, and step size effects. Under suitable conditions, we show that the hitting time of SGLD to firstorder stationary points can be dimension-independent. Moreover, we apply our analysis to study several important online estimation problems in machine learning, including linear regression, matrix factorization, and online PCA. 1. See Section 1.1 for the precise definition."
            ],
            "keywords": [],
            "author": [
                "Xi Chen",
                "Simon S Du",
                "Xin T Tong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-327/19-327.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bounds for Linear Multi-Task Learning",
            "abstract": [
                "We give dimension-free and data-dependent bounds for linear multi-task learning where a common linear operator is chosen to preprocess data for a vector of task specific linear-thresholding classifiers. The complexity penalty of multi-task learning is bounded by a simple expression involving the margins of the task-specific classifiers, the Hilbert-Schmidt norm of the selected preprocessor and the Hilbert-Schmidt norm of the covariance operator for the total mixture of all task distributions, or, alternatively, the Frobenius norm of the total Gramian matrix for the data-dependent version. The results can be compared to state-of-the-art results on linear single-task learning."
            ],
            "keywords": [
                "learning to learn",
                "transfer learning",
                "multi-task learning"
            ],
            "author": [
                "Andreas Maurer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/maurer06a/maurer06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiclass Anomaly Detector: the CS++ Support Vector Machine",
            "abstract": [
                "A new support vector machine (SVM) variant, called CS++-SVM, is presented combining multiclass classification and anomaly detection in a single-step process to create a trained machine that can simultaneously classify test data belonging to classes represented in the training set and label as anomalous test data belonging to classes not represented in the training set. A theoretical analysis of the properties of the new method, showing how it combines properties inherited both from the conic-segmentation SVM (CS-SVM) and the 1-class SVM (to which the method described reduces to in the case of unlabelled training data), is given. Finally, experimental results are presented to demonstrate the effectiveness of the algorithm for both simulated and real-world data."
            ],
            "keywords": [
                "Multiclass",
                "Anomaly Detection",
                "SVM",
                "Kernel Machines",
                "1-class SVM"
            ],
            "author": [
                "Alistair Shilton",
                "Sutharshan Rajasegarar",
                "Marimuthu Palaniswami"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/16-122/16-122.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs",
            "abstract": [
                "We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to significant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is significantly more accurate than previous approximations and should become exact in the limit of large random graphs."
            ],
            "keywords": [
                "Gaussian process",
                "generalisation error",
                "learning curve",
                "cavity method",
                "belief propagation",
                "graph",
                "random walk kernel"
            ],
            "author": [
                "Matthew J Urry",
                "Peter Sollich"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/urry13a/urry13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Task Learning for Straggler Avoiding Predictive Job Scheduling",
            "abstract": [
                "Parallel processing frameworks (Dean and Ghemawat, 2004) accelerate jobs by breaking them into tasks that execute in parallel. However, slow running or straggler tasks can run up to 8 times slower than the median task on a production cluster (Ananthanarayanan et al., 2013), leading to delayed job completion and inefficient use of resources. Existing straggler mitigation techniques wait to detect stragglers and then relaunch them, delaying straggler detection and wasting resources. We built Wrangler (Yadwadkar et al., 2014), a system that predicts when stragglers are going to occur and makes scheduling decisions to avoid such situations. To capture node and workload variability, Wrangler built separate models for every node and workload, requiring the time-consuming collection of substantial training data. In this paper, we propose multi-task learning formulations that share information between the various models, allowing us to use less training data and bring training time down from 4 hours to 40 minutes. Unlike naive multi-task learning formulations, our formulations capture the shared structure in our data, improving generalization performance on limited data. Finally, we extend these formulations using group sparsity inducing norms to automatically discover the similarities between tasks and improve interpretability."
            ],
            "keywords": [],
            "author": [
                "Neeraja J Yadwadkar",
                "Joseph E Gonzalez",
                "Randy Katz",
                "Urun Dogan",
                "Marius Kloft",
                "Francesco Orabona",
                "Tatiana Tommasi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-149/15-149.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Volume Clustering: A New Discriminative Clustering Approach *",
            "abstract": [
                "The large volume principle proposed by Vladimir Vapnik, which advocates that hypotheses lying in an equivalence class with a larger volume are more preferable, is a useful alternative to the large margin principle. In this paper, we introduce a new discriminative clustering model based on the large volume principle called maximum volume clustering (MVC), and then propose two approximation schemes to solve this MVC model: A soft-label MVC method using sequential quadratic programming and a hard-label MVC method using semi-definite programming, respectively. The proposed MVC is theoretically advantageous for three reasons. The optimization involved in hardlabel MVC is convex, and under mild conditions, the optimization involved in soft-label MVC is akin to a convex one in terms of the resulting clusters. Secondly, the soft-label MVC method pos-* ."
            ],
            "keywords": [
                "discriminative clustering",
                "large volume principle",
                "sequential quadratic programming",
                "semi-definite programming",
                "finite sample stability",
                "clustering error bound"
            ],
            "author": [
                "Gang Niu",
                "Bo Dai",
                "Lin Shang",
                "Masashi Sugiyama",
                "Bo Niu",
                "Lin Dai",
                "Masashi Shang",
                "DAI, SHANG AND SUGIYAMA Niu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/niu13a/niu13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Conic Optimization for Quadratic Regression Under Sparse Noise",
            "abstract": [
                "This paper is concerned with the quadratic regression problem, where the goal is to find the unknown state (numerical parameters) of a system modeled by a set of equations that are quadratic in the state. We focus on the setting when a subset of equations of fixed cardinality is subject to errors of arbitrary magnitudes (potentially adversarial). We develop two methods to address this problem, which are both based on conic optimization and are able to accept any available prior knowledge on the solution as an input. We derive sufficient conditions for guaranteeing the correct recovery of the unknown state for each method and show that one method provides a better accuracy while the other one scales better to large-scale systems. The obtained conditions consist in bounds on the number of bad measurements each method can tolerate without producing a nonzero estimation error. In the case when no prior knowledge is available, we develop an iterative-based conic optimization technique. It is proved that the proposed methods allow up to half of the total number of measurements to be grossly erroneous.The efficacy of the developed methods is demonstrated in different case studies, including data analytics for a European power grid."
            ],
            "keywords": [
                "nonlinear regression",
                "conic programming",
                "bad data detection"
            ],
            "author": [
                "Igor Molybog",
                "Ramtin Madani",
                "Javad Lavaei"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-881/18-881.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Bundle Methods for Convex and Non-Convex Risks",
            "abstract": [
                "Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efficient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efficient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random fields, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy ε with a rate O(1/λε) where λ is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difficult and requires a stronger and more disputable assumption. Yet we provide experimental results on artificial test problems, and on five standard and difficult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms."
            ],
            "keywords": [
                "optimization",
                "non-convex",
                "non-smooth",
                "cutting plane",
                "bundle method",
                "regularized risk"
            ],
            "author": [
                "Trinh-Minh-Tri Do",
                "Thierry Artières"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/do12a/do12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-Asymptotic Analysis of a New Bandit Algorithm for Semi-Bounded Rewards",
            "abstract": [
                "In this paper we consider a stochastic multiarmed bandit problem. It is known in this problem that Deterministic Minimum Empirical Divergence (DMED) policy achieves the asymptotic theoretical bound for the model where each reward distribution is supported in a known bounded interval, say [0, 1]. However, the regret bound of DMED is described in an asymptotic form and the performance in finite time has been unknown. We modify this policy and derive a finite-time regret bound for the new policy, Indexed Minimum Empirical Divergence (IMED), by refining large deviation probabilities to a simple nonasymptotic form. Further, the refined analysis reveals that the finite-time regret bound is valid even in the case that the reward is not bounded from below. Therefore, our finitetime result applies to the case that the minimum reward (that is, the maximum loss) is unknown or unbounded. We also present some simulation results which shows that IMED much improves DMED and performs competitively to other state-of-the-art policies."
            ],
            "keywords": [
                "stochastic bandit",
                "finite-time regret",
                "large deviation principle"
            ],
            "author": [
                "Junya Honda"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/honda15a/honda15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximate Newton Methods for Policy Search in Markov Decision Processes",
            "abstract": [
                "Approximate Newton methods are standard optimization tools which aim to maintain the benefits of Newton's method, such as a fast rate of convergence, while alleviating its drawbacks, such as computationally expensive calculation or estimation of the inverse Hessian. In this work we investigate approximate Newton methods for policy optimization in Markov decision processes (MDPs). We first analyse the structure of the Hessian of the total expected reward, which is a standard objective function for MDPs. We show that, like the gradient, the Hessian exhibits useful structure in the context of MDPs and we use this analysis to motivate two Gauss-Newton methods for MDPs. Like the Gauss-Newton method for non-linear least squares, these methods drop certain terms in the Hessian. The approximate Hessians possess desirable properties, such as negative definiteness, and we demonstrate several important performance guarantees including guaranteed ascent directions, invariance to affine transformation of the parameter space and convergence guarantees. We finally provide a unifying perspective of key policy search algorithms, demonstrating that our second Gauss-Newton algorithm is closely related to both the EMalgorithm and natural gradient ascent applied to MDPs, but performs significantly better in practice on a range of challenging domains."
            ],
            "keywords": [
                "Markov decision processes",
                "reinforcement learning",
                "Newton method",
                "function approximation"
            ],
            "author": [
                "Thomas Furmston",
                "David Barber"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-414/15-414.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Noisy-OR Component Analysis and its Application to Link Analysis",
            "abstract": [
                "We develop a new component analysis framework, the Noisy-Or Component Analyzer (NOCA), that targets high-dimensional binary data. NOCA is a probabilistic latent variable model that assumes the expression of observed high-dimensional binary data is driven by a small number of hidden binary sources combined via noisy-or units. The component analysis procedure is equivalent to learning of NOCA parameters. Since the classical EM formulation of the NOCA learning problem is intractable, we develop its variational approximation. We test the NOCA framework on two problems: (1) a synthetic image-decomposition problem and (2) a co-citation data analysis problem for thousands of CiteSeer documents. We demonstrate good performance of the new model on both problems. In addition, we contrast the model to two mixture-based latent-factor models: the probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA). Differing assumptions underlying these models cause them to discover different types of structure in co-citation data, thus illustrating the benefit of NOCA in building our understanding of highdimensional data sets."
            ],
            "keywords": [
                "component analysis",
                "vector quantization",
                "variational learning",
                "link analysis"
            ],
            "author": [],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/singliar06a/singliar06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Support Vector Machinery for Infinite Ensemble Learning",
            "abstract": [
                "Ensemble learning algorithms such as boosting can achieve better performance by averaging over the predictions of some base hypotheses. Nevertheless, most existing algorithms are limited to combining only a finite number of hypotheses, and the generated ensemble is usually sparse. Thus, it is not clear whether we should construct an ensemble classifier with a larger or even an infinite number of hypotheses. In addition, constructing an infinite ensemble itself is a challenging task. In this paper, we formulate an infinite ensemble learning framework based on the support vector machine (SVM). The framework can output an infinite and nonsparse ensemble through embedding infinitely many hypotheses into an SVM kernel. We use the framework to derive two novel kernels, the stump kernel and the perceptron kernel. The stump kernel embodies infinitely many decision stumps, and the perceptron kernel embodies infinitely many perceptrons. We also show that the Laplacian radial basis function kernel embodies infinitely many decision trees, and can thus be explained through infinite ensemble learning. Experimental results show that SVM with these kernels is superior to boosting with the same base hypothesis set. In addition, SVM with the stump kernel or the perceptron kernel performs similarly to SVM with the Gaussian radial basis function kernel, but enjoys the benefit of faster parameter selection. These properties make the novel kernels favorable choices in practice."
            ],
            "keywords": [
                "ensemble learning",
                "boosting",
                "support vector machine",
                "kernel"
            ],
            "author": [
                "Hsuan-Tien Lin",
                "Ling Li",
                "Peter L Bartlett"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/lin08a/lin08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Discerning the Linear Convergence of ADMM for Structured Convex Optimization through the Lens of Variational Analysis",
            "abstract": [
                "Despite the rich literature, the linear convergence of alternating direction method of multipliers (ADMM) has not been fully understood even for the convex case. For example, the linear convergence of ADMM can be empirically observed in a wide range of applications arising in statistics, machine learning, and related areas, while existing theoretical results seem to be too stringent to be satisfied or too ambiguous to be checked and thus why the ADMM performs linear convergence for these applications still seems to be unclear. In this paper, we systematically study the local linear convergence of ADMM in the context of convex optimization through the lens of variational analysis. We show that the local linear convergence of ADMM can be guaranteed without the strong convexity of objective functions together with the full rank assumption of the coefficient matrices, or the full polyhedricity assumption of their subdifferential; and it is possible to discern the local linear convergence for various concrete applications, especially for some representative models arising in statistical learning. We use some variational analysis techniques sophisticatedly; and our analysis is conducted in the most general proximal version of ADMM with Fortin and Glowinski's larger step size so that all major variants of the ADMM known in the literature are covered."
            ],
            "keywords": [
                "Convex programming",
                "variational analysis",
                "alternating direction method of multipliers",
                "linear convergence",
                "calmness",
                "metric subregularity",
                "machine learning",
                "statistics"
            ],
            "author": [
                "Xiaoming Yuan",
                "Shangzhi Zeng",
                "Jin Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-562/18-562.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DSCOVR: Randomized Primal-Dual Block Coordinate Algorithms for Asynchronous Distributed Optimization",
            "abstract": [
                "Machine learning with big data often involves large optimization models. For distributed optimization over a cluster of machines, frequent communication and synchronization of all model parameters (optimization variables) can be very costly. A promising solution is to use parameter servers to store different subsets of the model parameters, and update them asynchronously at different machines using local datasets. In this paper, we focus on distributed optimization of large linear models with convex loss functions, and propose a family of randomized primal-dual block coordinate algorithms that are especially suitable for asynchronous distributed implementation with parameter servers. In particular, we work with the saddle-point formulation of such problems which allows simultaneous data and model partitioning, and exploit its structure by doubly stochastic coordinate optimization with variance reduction (DSCOVR). Compared with other first-order distributed algorithms, we show that DSCOVR may require less amount of overall computation and communication, and less or no synchronization. We discuss the implementation details of the DSCOVR algorithms, and present numerical experiments on an industrial distributed computing system."
            ],
            "keywords": [],
            "author": [
                "Adams Wei",
                "Qihang Lin",
                "Weizhu Chen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-608/17-608.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graph-Dependent Implicit Regularisation for Distributed Stochastic Subgradient Descent",
            "abstract": [
                "We propose graph-dependent implicit regularisation strategies for synchronised distributed stochastic subgradient descent (Distributed SGD) for convex problems in multi-agent learning. Under the standard assumptions of convexity, Lipschitz continuity, and smoothness, we establish statistical learning rates that retain, up to logarithmic terms, single-machine serial statistical guarantees through implicit regularisation (step size tuning and early stopping) with appropriate dependence on the graph topology. Our approach avoids the need for explicit regularisation in decentralised learning problems, such as adding constraints to the empirical risk minimisation rule. Particularly for distributed methods, the use of implicit regularisation allows the algorithm to remain simple, without projections or dual methods. To prove our results, we establish graph-independent generalisation bounds for Distributed SGD that match the single-machine serial SGD setting (using algorithmic stability), and we establish graph-dependent optimisation bounds that are of independent interest. We present numerical experiments to show that the qualitative nature of the upper bounds we derive can be representative of real behaviours."
            ],
            "keywords": [
                "Distributed machine learning",
                "implicit regularisation",
                "generalisation bounds",
                "algorithmic stability",
                "multi-agent optimisation"
            ],
            "author": [
                "Dominic Richards",
                "Patrick Rebeschini"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-638/18-638.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Worst-Case Analysis of Selective Sampling for Linear Classification",
            "abstract": [
                "A selective sampling algorithm is a learning algorithm for classification that, based on the past observed data, decides whether to ask the label of each new instance to be classified. In this paper, we introduce a general technique for turning linear-threshold classification algorithms from the general additive family into randomized selective sampling algorithms. For the most popular algorithms in this family we derive mistake bounds that hold for individual sequences of examples. These bounds show that our semi-supervised algorithms can achieve, on average, the same accuracy as that of their fully supervised counterparts, but using fewer labels. Our theoretical results are corroborated by a number of experiments on real-world textual data. The outcome of these experiments is essentially predicted by our theoretical results: Our selective sampling algorithms tend to perform as well as the algorithms receiving the true label after each classification, while observing in practice substantially fewer labels."
            ],
            "keywords": [
                "selective sampling",
                "semi-supervised learning",
                "on-line learning",
                "kernel algorithms",
                "linear-threshold classifiers"
            ],
            "author": [
                "Nicolò Cesa-Bianchi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/cesa-bianchi06b/cesa-bianchi06b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GraSPy: Graph Statistics in Python",
            "abstract": [
                "We introduce GraSPy, a Python library devoted to statistical inference, machine learning, and visualization of random graphs and graph populations. This package provides flexible and easy-to-use algorithms for analyzing and understanding graphs with a scikit-learn compliant API."
            ],
            "keywords": [
                "Python",
                "graph analysis",
                "network analysis",
                "statistical inference",
                "machine learning"
            ],
            "author": [
                "Jaewon Chung",
                "Benjamin D Pedigo",
                "Eric W Bridgeford",
                "Hayden S Helm",
                "Joshua T Vogelstein",
                "Bijan K Varjavand"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-490/19-490.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classification Methods with Reject Option Based on Convex Risk Minimization",
            "abstract": [
                "In this paper, we investigate the problem of binary classification with a reject option in which one can withhold the decision of classifying an observation at a cost lower than that of misclassification. Since the natural loss function is non-convex so that empirical risk minimization easily becomes infeasible, the paper proposes minimizing convex risks based on surrogate convex loss functions. A necessary and sufficient condition for infinite sample consistency (both risks share the same minimizer) is provided. Moreover, we show that the excess risk can be bounded through the excess surrogate risk under appropriate conditions. These bounds can be tightened by a generalized margin condition. The impact of the results is illustrated on several commonly used surrogate loss functions."
            ],
            "keywords": [
                "classification",
                "convex surrogate loss",
                "empirical risk minimization",
                "generalized margin condition",
                "reject option"
            ],
            "author": [
                "Ming Yuan",
                "H Milton",
                "Marten Wegkamp"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/yuan10a/yuan10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Seeded Graph Matching for Correlated Erdős-Rényi Graphs",
            "abstract": [
                "Graph matching is an important problem in machine learning and pattern recognition. Herein, we present theoretical and practical results on the consistency of graph matching for estimating a latent alignment function between the vertex sets of two graphs, as well as subsequent algorithmic implications when the latent alignment is partially observed. In the correlated Erdős-Rényi graph setting, we prove that graph matching provides a strongly consistent estimate of the latent alignment in the presence of even modest correlation. We then investigate a tractable, restricted-focus version of graph matching, which is only concerned with adjacency involving vertices in a partial observation of the latent alignment; we prove that a logarithmic number of vertices whose alignment is known is sufficient for this restricted-focus version of graph matching to yield a strongly consistent estimate of the latent alignment of the remaining vertices. We show how Frank-Wolfe methodology for approximate graph matching, when there is a partially observed latent alignment, inherently incorporates this restricted-focus graph matching. Lastly, we illustrate the relationship between seeded graph matching and restricted-focus graph matching by means of an illuminating example from human connectomics."
            ],
            "keywords": [
                "graph matching",
                "Erdős-Rényi graph",
                "consistency",
                "estimation",
                "seeded vertices",
                "Frank-Wolfe",
                "assignment problem"
            ],
            "author": [
                "Vince Lyzinski"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/lyzinski14a/lyzinski14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multivariate Spearman's ρ for Aggregating Ranks Using Copulas",
            "abstract": [
                "We study the problem of rank aggregation: given a set of ranked lists, we want to form a consensus ranking. Furthermore, we consider the case of extreme lists: i.e., only the rank of the best or worst elements are known. We impute missing ranks and generalise Spearman's ρ to extreme ranks. Our main contribution is the derivation of a non-parametric estimator for rank aggregation based on multivariate extensions of Spearman's ρ, which measures correlation between a set of ranked lists. Multivariate Spearman's ρ is defined using copulas, and we show that the geometric mean of normalised ranks maximises multivariate correlation. Motivated by this, we propose a weighted geometric mean approach for learning to rank which has a closed form least squares solution. When only the best (top-k) or worst (bottomk) elements of a ranked list are known, we impute the missing ranks by the average value, allowing us to apply Spearman's ρ. We discuss an optimistic and pessimistic imputation of missing values, which respectively maximise and minimise correlation, and show its effect on aggregating university rankings. Finally, we demonstrate good performance on the rank aggregation benchmarks MQ2007 and MQ2008."
            ],
            "keywords": [],
            "author": [
                "Justin Bedő",
                "Soon Cheng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-625/15-625.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling",
            "abstract": [
                "We study parameter inference in large-scale latent variable models. We first propose a unified treatment of online inference for latent variable models from a non-canonical exponential family, and draw explicit links between several previously proposed frequentist or Bayesian methods. We then propose a novel inference method for the frequentist estimation of parameters, that adapts MCMC methods to online inference of latent variable models with the proper use of local Gibbs sampling. Then, for latent Dirichlet allocation,we provide an extensive set of experiments and comparisons with existing work, where our new approach outperforms all previously proposed methods. In particular, using Gibbs sampling for latent variable inference is superior to variational inference in terms of test log-likelihoods. Moreover, Bayesian inference through variational methods perform poorly, sometimes leading to worse fits with latent variables of higher dimensionality."
            ],
            "keywords": [
                "Latent Variables Models",
                "Online Learning",
                "Gibbs Sampling",
                "Topic Modelling",
                "Latent Dirichlet Allocation"
            ],
            "author": [
                "Christophe Dupuy",
                "Francis Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-374/16-374.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints",
            "abstract": [
                "In this paper we propose efficient algorithms for solving constrained online convex optimization problems. Our motivation stems from the observation that most algorithms proposed for online convex optimization require a projection onto the convex set K from which the decisions are made. While the projection is straightforward for simple shapes (e.g., Euclidean ball), for arbitrary complex sets it is the main computational challenge and may be inefficient in practice. In this paper, we consider an alternative online convex optimization problem. Instead of requiring that decisions belong to K for all rounds, we only require that the constraints, which define the set K , be satisfied in the long run. By turning the problem into an online convex-concave optimization problem, we propose an efficient algorithm which achieves O(√ T) regret bound and O(T 3/4) bound on the violation of constraints. Then, we modify the algorithm in order to guarantee that the constraints are satisfied in the long run. This gain is achieved at the price of getting O(T 3/4) regret bound. Our second algorithm is based on the mirror prox method (Nemirovski, 2005) to solve variational inequalities which achieves O(T 2/3) bound for both regret and the violation of constraints when the domain K can be described by a finite number of linear constraints. Finally, we extend the results to the setting where we only have partial access to the convex set K and propose a multipoint bandit feedback algorithm with the same bounds in expectation as our first algorithm."
            ],
            "keywords": [
                "online convex optimization",
                "convex-concave optimization",
                "bandit feedback",
                "variational inequality"
            ],
            "author": [
                "Mehrdad Mahdavi",
                "Rong Jin",
                "Tianbao Yang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/mahdavi12a/mahdavi12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bounding the Search Space for Global Optimization of Neural Networks Learning Error: An Interval Analysis Approach",
            "abstract": [
                "Training a multilayer perceptron (MLP) with algorithms employing global search strategies has been an important research direction in the field of neural networks. Despite a number of significant results, an important matter concerning the bounds of the search regiontypically defined as a box-where a global optimization method has to search for a potential global minimizer seems to be unresolved. The approach presented in this paper builds on interval analysis and attempts to define guaranteed bounds in the search space prior to applying a global search algorithm for training an MLP. These bounds depend on the machine precision and the term \"guaranteed\" denotes that the region defined surely encloses weight sets that are global minimizers of the neural network's error function. Although the solution set to the bounding problem for an MLP is in general non-convex, the paper presents the theoretical results that help deriving a box which is a convex set. This box is an outer approximation of the algebraic solutions to the interval equations resulting from the function implemented by the network nodes. An experimental study using well known benchmarks is presented in accordance with the theoretical results."
            ],
            "keywords": [
                "neural network training",
                "bound constrained global optimization",
                "interval analysis",
                "interval linear equations",
                "algebraic solution"
            ],
            "author": [
                "Stavros P Adam",
                "George D Magoulas",
                "Dimitrios A Karras",
                "Michael N Vrahatis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-350/14-350.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Instance-Specific Predictive Models",
            "abstract": [
                "This paper introduces a Bayesian algorithm for constructing predictive models from data that are optimized to predict a target variable well for a particular instance. This algorithm learns Markov blanket models, carries out Bayesian model averaging over a set of models to predict a target variable of the instance at hand, and employs an instance-specific heuristic to locate a set of suitable models to average over. We call this method the instance-specific Markov blanket (ISMB) algorithm. The ISMB algorithm was evaluated on 21 UCI data sets using five different performance measures and its performance was compared to that of several commonly used predictive algorithms, including nave Bayes, C4.5 decision tree, logistic regression, neural networks, k-Nearest Neighbor, Lazy Bayesian Rules, and AdaBoost. Over all the data sets, the ISMB algorithm performed better on average on all performance measures against all the comparison algorithms."
            ],
            "keywords": [
                "instance-specific",
                "Bayesian network",
                "Markov blanket",
                "Bayesian model averaging"
            ],
            "author": [
                "Shyam Visweswaran",
                "Gregory F Cooper"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/visweswaran10a/visweswaran10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Java-ML: A Machine Learning Library",
            "abstract": [
                "Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classifiers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license."
            ],
            "keywords": [
                "open source",
                "machine learning",
                "data mining",
                "java library",
                "clustering",
                "feature selection",
                "classification"
            ],
            "author": [
                "Thomas Abeel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/abeel09a/abeel09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parameter Screening and Optimisation for ILP using Designed Experiments",
            "abstract": [
                "Reports of experiments conducted with an Inductive Logic Programming system rarely describe how specific values of parameters of the system are arrived at when constructing models. Usually, no attempt is made to identify sensitive parameters, and those that are used are often given \"factory-supplied\" default values, or values obtained from some non-systematic exploratory analysis. The immediate consequence of this is, of course, that it is not clear if better models could have been obtained if some form of parameter selection and optimisation had been performed. Questions follow inevitably on the experiments themselves: specifically, are all algorithms being treated fairly, and is the exploratory phase sufficiently well-defined to allow the experiments to be replicated? In this paper, we investigate the use of parameter selection and optimisation techniques grouped under the study of experimental design. Screening and response surface methods determine, in turn, sensitive parameters and good values for these parameters. Screening is done here by constructing a stepwise regression model relating the utility of an ILP system's hypothesis to its input parameters, using systematic combinations of values of input parameters (technically speaking, we use a two-level fractional factorial design of the input parameters). The parameters used by the regression model are taken to be the sensitive parameters for the system for that application. We then seek an assignment of values to these sensitive parameters that maximise the utility of the ILP model. This is done using the technique of constructing a local \"response surface\". The parameters are then changed following the path of steepest ascent until a locally optimal value is reached. This combined use of parameter selection and response surface-driven optimisation has a long history of application in industrial engineering, and its role in ILP is demonstrated using well-known benchmarks. The results suggest that computational overheads from this preliminary phase are not substantial, and that much can be gained, both on improving system performance and on enabling controlled experimentation, by adopting well-established procedures such as the ones proposed here."
            ],
            "keywords": [
                "inductive logic programming",
                "parameter screening and optimisation",
                "experimental design"
            ],
            "author": [
                "Ashwin Srinivasan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/srinivasan11a/srinivasan11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online PCA with Optimal Regret *",
            "abstract": [
                "We investigate the online version of Principle Component Analysis (PCA), where in each trial t the learning algorithm chooses a k-dimensional subspace, and upon receiving the next instance vector x t , suffers the \"compression loss\", which is the squared Euclidean distance between this instance and its projection into the chosen subspace. When viewed in the right parameterization, this compression loss is linear, i.e. it can be rewritten as tr(W t x t x t), where W t is the parameter of the algorithm and the outer product x t x t (with x t ≤ 1) is the instance matrix. In this paper generalize PCA to arbitrary positive definite instance matrices X t with the linear loss tr(W t X t). We evaluate online algorithms in terms of their worst-case regret, which is a bound on the additional total loss of the online algorithm on all instances matrices over the compression loss of the best k-dimensional subspace (chosen in hindsight). We focus on two popular online algorithms for generalized PCA: the Gradient Descent (GD) and Matrix Exponentiated Gradient (MEG) algorithms. We show that if the regret is expressed as a function of the number of trials, then both algorithms are optimal to within a constant factor on worst-case sequences of positive definite instances matrices with trace norm at most one (which subsumes the original PCA problem with outer products). This is surprising because MEG is believed be suboptimal in this case. We also show that when considering regret bounds as a function of a loss budget, then MEG remains optimal and strictly outperforms GD when the instance matrices are trace norm bounded. Next, we consider online PCA when the adversary is allowed to present the algorithm with positive semidefinite instance matrices whose largest eigenvalue is bounded (rather than their trace which is the sum of their eigenvalues). Again we can show that MEG is optimal and strictly better than GD in this setting."
            ],
            "keywords": [
                "online learning",
                "regret bounds",
                "expert setting",
                "k-sets",
                "PCA",
                "Gradient Descent",
                "Matrix Exponentiated Gradient algorithm"
            ],
            "author": [
                "Jiazhong Nie",
                "Wojciech Kot",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-320/15-320.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unified View on Multi-class Support Vector Classification",
            "abstract": [
                "A unified view on multi-class support vector machines (SVMs) is presented, covering most prominent variants including the one-vs-all approach and the algorithms proposed by Weston & Watkins, Crammer & Singer, Lee, Lin, & Wahba, and Liu & Yuan. The unification leads to a template for the quadratic training problems and new multi-class SVM formulations. Within our framework, we provide a comparative analysis of the various notions of multi-class margin and margin-based loss. In particular, we demonstrate limitations of the loss function considered, for instance, in the Crammer & Singer machine. We analyze Fisher consistency of multi-class loss functions and universal consistency of the various machines. On the one hand, we give examples of SVMs that are, in a particular hyperparameter regime, universally consistent without being based on a Fisher consistent loss. These include the canonical extension of SVMs to multiple classes as proposed by Weston & Watkins and Vapnik as well as the one-vs-all approach. On the other hand, it is demonstrated that machines based on Fisher consistent loss functions can fail to identify proper decision boundaries in low-dimensional feature spaces. We compared the performance of nine different multi-class SVMs in a thorough empirical study. Our results suggest to use the Weston & Watkins SVM, which can be trained comparatively fast and gives good accuracies on benchmark functions. If training time is a major concern, the one-vs-all approach is the method of choice."
            ],
            "keywords": [
                "support vector machines",
                "multi-class classification",
                "consistency"
            ],
            "author": [
                "Urün Dogan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/11-229/11-229.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Stage Multi-Task Feature Learning Pinghua Gong",
            "abstract": [
                "Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an ℓ-type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel non-convex regularizer. To solve the non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm; we also provide intuitive interpretations, detailed convergence and reproducibility analysis for the proposed algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms."
            ],
            "keywords": [
                "multi-task learning",
                "multi-stage",
                "non-convex",
                "sparse learning"
            ],
            "author": [
                "Changshui Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/gong13a/gong13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dimensionality Estimation, Manifold Learning and Function Approximation using Tensor Voting",
            "abstract": [
                "We address instance-based learning from a perceptual organization standpoint and present methods for dimensionality estimation, manifold learning and function approximation. Under our approach, manifolds in high-dimensional spaces are inferred by estimating geometric relationships among the input instances. Unlike conventional manifold learning, we do not perform dimensionality reduction, but instead perform all operations in the original input space. For this purpose we employ a novel formulation of tensor voting, which allows an N-D implementation. Tensor voting is a perceptual organization framework that has mostly been applied to computer vision problems. Analyzing the estimated local structure at the inputs, we are able to obtain reliable dimensionality estimates at each instance, instead of a global estimate for the entire data set. Moreover, these local dimensionality and structure estimates enable us to measure geodesic distances and perform nonlinear interpolation for data sets with varying density, outliers, perturbation and intersections, that cannot be handled by state-of-the-art methods. Quantitative results on the estimation of local manifold structure using ground truth data are presented. In addition, we compare our approach with several leading methods for manifold learning at the task of measuring geodesic distances. Finally, we show competitive function approximation results on real data."
            ],
            "keywords": [
                "dimensionality estimation",
                "manifold learning",
                "geodesic distance",
                "function approximation",
                "high-dimensional processing",
                "tensor voting"
            ],
            "author": [
                "Philippos Mordohai",
                "Gérard Medioni"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/mordohai10a/mordohai10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Approximation of Matrix Coherence and Statistical Leverage",
            "abstract": [
                "The statistical leverage scores of a matrix A are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nyström-based low-rank matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n × d matrix A, with n ≫ d, and that returns as output relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of n and d) in O(nd log n) time, as opposed to the O(nd 2) time required by the naïve algorithm that involves computing an orthogonal basis for the range of A. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with n ≈ d, and the extension to streaming environments."
            ],
            "keywords": [
                "matrix coherence",
                "statistical leverage",
                "randomized algorithm"
            ],
            "author": [
                "Petros Drineas",
                "Malik Magdon-Ismail",
                "Michael W Mahoney",
                "David P Woodruff",
                "Mehryar Mohri"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/drineas12a/drineas12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convex Programming for Estimation in Nonlinear Recurrent Models",
            "abstract": [
                "We propose a formulation for nonlinear recurrent models that includes simple parametric models of recurrent neural networks as a special case. The proposed formulation leads to a natural estimator in the form of a convex program. We provide a sample complexity for this estimator in the case of stable dynamics, where the nonlinear recursion has a certain contraction property, and under certain regularity conditions on the input distribution. We evaluate the performance of the estimator by simulation on synthetic data. These numerical experiments also suggest the extent at which the imposed theoretical assumptions may be relaxed."
            ],
            "keywords": [
                "recurrent neural networks",
                "convex programming",
                "dynamical systems",
                "VC dimension"
            ],
            "author": [
                "Sohail Bahmani",
                "Justin Romberg"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-723/19-723.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improved Moves for Truncated Convex Models",
            "abstract": [
                "We consider the problem of obtaining an approximate maximum a posteriori estimate of a discrete random field characterized by pairwise potentials that form a truncated convex model. For this problem, we propose two st-MINCUT based move making algorithms that we call Range Swap and Range Expansion. Our algorithms can be thought of as extensions of αβ-Swap and α-Expansion respectively that fully exploit the form of the pairwise potentials. Specifically, instead of dealing with one or two labels at each iteration, our methods explore a large search space by considering a range of labels (that is, an interval of consecutive labels). Furthermore, we show that Range Expansion provides the same multiplicative bounds as the standard linear programming (LP) relaxation in polynomial time. Compared to previous approaches based on the LP relaxation, for example interior-point algorithms or tree-reweighted message passing (TRW), our methods are faster as they use only the efficient st-MINCUT algorithm in their design. We demonstrate the usefulness of the proposed approaches on both synthetic and standard real data problems."
            ],
            "keywords": [
                "truncated convex models",
                "move making algorithms",
                "range moves",
                "multiplicative bounds",
                "linear programming relaxation"
            ],
            "author": [
                "M Pawan Kumar",
                "Olga Veksler",
                "Philip H S Torr"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/kumar11a/kumar11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Boosted Kernel Ridge Regression: Optimal Learning Rates and Early Stopping",
            "abstract": [
                "In this paper, we introduce a learning algorithm, boosted kernel ridge regression (BKRR), that combines L 2-Boosting with the kernel ridge regression (KRR). We analyze the learning performance of this algorithm in the framework of learning theory. We show that BKRR provides a new bias-variance trade-off via tuning the number of boosting iterations, which is different from KRR via adjusting the regularization parameter. A (semi-)exponential bias-variance trade-off is derived for BKRR, exhibiting a stable relationship between the generalization error and the number of iterations. Furthermore, an adaptive stopping rule is proposed, with which BKRR achieves the optimal learning rate without saturation."
            ],
            "keywords": [
                "learning theory",
                "kernel ridge regression",
                "boosting",
                "integral operator"
            ],
            "author": [
                "Shao-Bo Lin",
                "Yunwen Lei",
                "Ding-Xuan Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-063/18-063.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimality of Graphlet Screening in High Dimensional Variable Selection",
            "abstract": [
                "Consider a linear model Y = Xβ+σz, where X has n rows and p columns and z ∼ N (0, I n). We assume both p and n are large, including the case of p n. The unknown signal vector β is assumed to be sparse in the sense that only a small fraction of its components is nonzero. The goal is to identify such nonzero coordinates (i.e., variable selection)."
            ],
            "keywords": [
                "asymptotic minimaxity",
                "graph of least favorables (GOLF)",
                "graph of strong dependence (GOSD)",
                "graphlet screening (GS)",
                "Hamming distance",
                "phase diagram",
                "rare and weak signal model",
                "screen and clean",
                "sparsity"
            ],
            "author": [
                "Jiashun Jin",
                "Cun-Hui Zhang",
                "Qi Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/jin14a/jin14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hierarchical Clustering via Spreading Metrics",
            "abstract": [
                "We study the cost function for hierarchical clusterings introduced by (Dasgupta, 2016) where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in (Dasgupta, 2016) that a top-down algorithm based on the uniform Sparsest Cut problem returns a hierarchical clustering of cost at most O (α n log n) times the cost of the optimal hierarchical clustering, where α n is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most O log 3/2 n times the cost of the optimal solution. We improve this by giving an O(log n)-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an O(log n)-approximate hierarchical clustering for a generalization of this cost function also studied in (Dasgupta, 2016). Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with constant factor inapproximability results for this problem: 1) no polynomial size LP or SDP can achieve a constant factor approximation for this problem and 2) no polynomial time algorithm can achieve a constant factor approximation under the Small Set Expansion hypothesis."
            ],
            "keywords": [
                "Hierarchical clustering",
                "clustering",
                "convex optimization",
                "linear programming",
                "approximation algorithms"
            ],
            "author": [
                "Aurko Roy",
                "Sebastian Pokutta"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-081/17-081.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Estimation and Completion of Matrices with Biclustering Structures",
            "abstract": [
                "Biclustering structures in data matrices were first formalized in a seminal paper by John Hartigan (Hartigan, 1972) where one seeks to cluster cases and variables simultaneously. Such structures are also prevalent in block modeling of networks. In this paper, we develop a theory for the estimation and completion of matrices with biclustering structures, where the data is a partially observed and noise contaminated matrix with a certain underlying biclustering structure. In particular, we show that a constrained least squares estimator achieves minimax rate-optimal performance in several of the most important scenarios. To this end, we derive unified high probability upper bounds for all sub-Gaussian data and also provide matching minimax lower bounds in both Gaussian and binary cases. Due to the close connection of graphon to stochastic block models, an immediate consequence of our general results is a minimax rate-optimal estimator for sparse graphons."
            ],
            "keywords": [
                "Biclustering",
                "graphon",
                "matrix completion",
                "missing data",
                "stochastic block models",
                "sparse network"
            ],
            "author": [
                "Chao Gao",
                "Yu Lu",
                "Zongming Ma",
                "Harrison H Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-617/15-617.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotic Analysis of Objectives Based on Fisher Information in Active Learning",
            "abstract": [
                "Obtaining labels can be costly and time-consuming. Active learning allows a learning algorithm to intelligently query samples to be labeled for a more efficient learning. Fisher information ratio (FIR) has been used as an objective for selecting queries. However, little is known about the theory behind the use of FIR for active learning. There is a gap between the underlying theory and the motivation of its usage in practice. In this paper, we attempt to fill this gap and provide a rigorous framework for analyzing existing FIR-based active learning methods. In particular, we show that FIR can be asymptotically viewed as an upper bound of the expected variance of the log-likelihood ratio. Additionally, our analysis suggests a unifying framework that not only enables us to make theoretical comparisons among the existing querying methods based on FIR, but also allows us to give insight into the development of new active learning approaches based on this objective."
            ],
            "keywords": [
                "classification active learning",
                "Fisher information ratio",
                "asymptotic log-loss",
                "upper-bound minimization"
            ],
            "author": [
                "Jamshid Sourati",
                "Murat Akcakaya",
                "Todd K Leen",
                "Deniz Erdogmus",
                "Jennifer G Dy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-104/15-104.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression",
            "abstract": [
                "This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper."
            ],
            "keywords": [
                "Gaussian process regression",
                "domain decomposition method",
                "partial independent conditional",
                "bagging for Gaussian process",
                "local probabilistic regression"
            ],
            "author": [
                "Chiwoo Park",
                "Jianhua Z Huang",
                "Yu Ding"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/park12a/park12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kronecker Graphs: An Approach to Modeling Networks",
            "abstract": [
                "How can we generate realistic networks? In addition, how can we do so with a mathematically tractable model that allows for rigorous analysis of network properties? Real networks exhibit a long list of surprising properties: Heavy tails for the in-and out-degree distribution, heavy tails for the eigenvalues and eigenvectors, small diameters, and densification and shrinking diameters over time. Current network models and generators either fail to match several of the above properties, are complicated to analyze mathematically, or both. Here we propose a generative model for networks that is both mathematically tractable and can generate networks that have all the above mentioned structural properties. Our main idea here is to use a non-standard matrix operation, the Kronecker product, to generate graphs which we refer to as \"Kronecker graphs\". First, we show that Kronecker graphs naturally obey common network properties. In fact, we rigorously prove that they do so. We also provide empirical evidence showing that Kronecker graphs can effectively model the structure of real networks. We then present KRONFIT, a fast and scalable algorithm for fitting the Kronecker graph generation model to large real networks. A naive approach to fitting would take super-exponential time. In contrast, KRONFIT takes linear time, by exploiting the structure of Kronecker matrix multiplication and by using statistical simulation techniques. Experiments on a wide range of large real and synthetic networks show that KRONFIT finds accurate parameters that very well mimic the properties of target networks. In fact, using just"
            ],
            "keywords": [
                "Kronecker graphs",
                "network analysis",
                "network models",
                "social networks",
                "graph generators",
                "graph mining",
                "network evolution"
            ],
            "author": [
                "Jure Leskovec",
                "Jon Kleinberg",
                "Christos Faloutsos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/leskovec10a/leskovec10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Deep Exploration via Randomized Value Functions",
            "abstract": [
                "We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation."
            ],
            "keywords": [
                "Reinforcement learning",
                "exploration",
                "value function",
                "neural network"
            ],
            "author": [
                "Ian Osband",
                "Benjamin Van Roy",
                "Daniel J Russo",
                "Zheng Wen",
                "Ian ©2019",
                "Benjamin Van Osband",
                "Daniel J Roy",
                "Zheng Russo"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-339/18-339.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Experience Selection in Deep Reinforcement Learning for Control",
            "abstract": [
                "Experience replay is a technique that allows off-policy reinforcement-learning methods to reuse past experiences. The stability and speed of convergence of reinforcement learning, as well as the eventual performance of the learned policy, are strongly dependent on the experiences being replayed. Which experiences are replayed depends on two important choices. The first is which and how many experiences to retain in the experience replay buffer. The second choice is how to sample the experiences that are to be replayed from that buffer. We propose new methods for the combined problem of experience retention and experience sampling. We refer to the combination as experience selection. We focus our investigation specifically on the control of physical systems, such as robots, where exploration is costly. To determine which experiences to keep and which to replay, we investigate different proxies for their immediate and long-term utility. These proxies include age, temporal difference error and the strength of the applied exploration noise. Since no currently available method works in all situations, we propose guidelines for using prior knowledge about the characteristics of the control problem at hand to choose the appropriate experience replay strategy."
            ],
            "keywords": [
                "reinforcement learning",
                "deep learning",
                "experience replay",
                "control",
                "robotics"
            ],
            "author": [
                "Tim De Bruin",
                "Jens Kober",
                "Karl Tuyls",
                "Robert Babuška"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-131/17-131.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stable and Efficient Gaussian Process Calculations",
            "abstract": [
                "The use of Gaussian processes can be an effective approach to prediction in a supervised learning environment. For large data sets, the standard Gaussian process approach requires solving very large systems of linear equations and approximations are required for the calculations to be practical. We will focus on the subset of regressors approximation technique. We will demonstrate that there can be numerical instabilities in a well known implementation of the technique. We discuss alternate implementations that have better numerical stability properties and can lead to better predictions. Our results will be illustrated by looking at an application involving prediction of galaxy redshift from broadband spectrum data."
            ],
            "keywords": [
                "Gaussian processes",
                "low rank approximations",
                "numerical stability",
                "photometric redshift",
                "subset of regressors method"
            ],
            "author": [
                "Leslie Foster",
                "Alex Waagen",
                "Nabeela Aijaz",
                "Michael Hurley",
                "Joel Rinsky",
                "Chandrika Satyavolu",
                "Michael J Way",
                "Paul Gazis",
                "Ashok Srivastava"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/foster09a/foster09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Optimal Ridge Penalty for Real-world High-dimensional Data Can Be Zero or Negative due to the Implicit Ridge Regularization",
            "abstract": [
                "A conventional wisdom in statistical learning is that large models require strong regularization to prevent overfitting. Here we show that this rule can be violated by linear regression in the underdetermined n p situation under realistic conditions. Using simulations and real-life highdimensional datasets, we demonstrate that an explicit positive ridge penalty can fail to provide any improvement over the minimum-norm least squares estimator. Moreover, the optimal value of ridge penalty in this situation can be negative. This happens when the high-variance directions in the predictor space can predict the response variable, which is often the case in the real-world highdimensional data. In this regime, low-variance directions provide an implicit ridge regularization and can make any further positive ridge penalty detrimental. We prove that augmenting any linear model with random covariates and using minimum-norm estimator is asymptotically equivalent to adding the ridge penalty. We use a spiked covariance model as an analytically tractable example and prove that the optimal ridge penalty in this case is negative when n p."
            ],
            "keywords": [
                "High-dimensional",
                "ridge regression",
                "regularization"
            ],
            "author": [
                "Dmitry Kobak",
                "Jonathan Lomond",
                "Benoit Sanchez",
                "France Paris"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-844/19-844.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Automatic Differentiation in Machine Learning: a Survey",
            "abstract": [
                "Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply \"autodiff\", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names \"dynamic computational graphs\" and \"differentiable programming\". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms \"autodiff\", \"automatic differentiation\", and \"symbolic differentiation\" as these are encountered more and more in machine learning settings."
            ],
            "keywords": [],
            "author": [
                "Atılım Güneş Baydin",
                "Jeffrey Mark Siskind"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-468/17-468.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Geometric Intuition and Algorithms for Eν-SVḾ",
            "abstract": [
                "In this work we address the Eν-SVM model proposed by Pérez-Cruz et al. as an extension of the traditional ν support vector classification model (ν-SVM). Through an enhancement of the range of admissible values for the regularization parameter ν, the Eν-SVM has been shown to be able to produce a wider variety of decision functions, giving rise to a better adaptability to the data. However, while a clear and intuitive geometric interpretation can be given for the ν-SVM model as a nearest-point problem in reduced convex hulls (RCH-NPP), no previous work has been made in developing such intuition for the Eν-SVM model. In this paper we show how Eν-SVM can be reformulated as a geometrical problem that generalizes RCH-NPP, providing new insights into this model. Under this novel point of view, we propose the RapMinos algorithm, able to solve Eν-SVM more efficiently than the current methods. Furthermore, we show how RapMinos is able to address the Eν-SVM model for any choice of regularization norm p≥1 seamlessly, which further extends the SVM model flexibility beyond the usual Eν-SVM models."
            ],
            "keywords": [
                "SVM",
                "Eν-SVM",
                "nearest point problem",
                "reduced convex hulls",
                "classification"
            ],
            "author": [
                "Alvaro Barbero",
                "Akiko Takeda",
                "Jorge López"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/barbero15a/barbero15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Poisson Random Fields for Dynamic Feature Models",
            "abstract": [
                "We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic model for time-dependent data assumed to have been generated by an unknown number of latent features. This model is suitable as a prior in Bayesian nonparametric feature allocation models in which the features underlying the observed data exhibit a dependency structure over time. More specifically, we establish a new framework for generating dependent Indian buffet processes, where the Poisson random field model from population genetics is used as a way of constructing dependent beta processes. Inference in the model is complex, and we describe a sophisticated Markov Chain Monte Carlo algorithm for exact posterior simulation. We apply our construction to develop a nonparametric focused topic model for collections of time-stamped text documents and test it on the full corpus of NIPS papers published from 1987 to 2015."
            ],
            "keywords": [
                "Bayesian nonparametrics",
                "Indian buffet process",
                "topic model",
                "Markov chain Monte Carlo",
                "Poisson random field"
            ],
            "author": [
                "Valerio Perrone",
                "Paul A Jenkins",
                "Dario Spanò",
                "Yee Whye Teh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-541/16-541.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Geometric Approach to Sample Compression",
            "abstract": [
                "The Sample Compression Conjecture of Littlestone & Warmuth has remained unsolved for a quarter century. While maximum classes (concept classes meeting Sauer's Lemma with equality) can be compressed, the compression of general concept classes reduces to compressing maximal classes (classes that cannot be expanded without increasing VC dimension). Two promising ways forward are: embedding maximal classes into maximum classes with at most a polynomial increase to VC dimension, and compression via operating on geometric representations. This paper presents positive results on the latter approach and a first negative result on the former, through a systematic investigation of finite maximum classes. Simple arrangements of hyperplanes in hyperbolic space are shown to represent maximum classes, generalizing the corresponding Euclidean result. We show that sweeping a generic hyperplane across such arrangements forms an unlabeled compression scheme of size VC dimension and corresponds to a special case of peeling the one-inclusion graph, resolving a recent conjecture of Kuzmin & Warmuth. A bijection between finite maximum classes and certain arrangements of piecewise-linear (PL) hyperplanes in either a ball or Euclidean space is established. Finally we show that d-maximum classes corresponding to PL-hyperplane arrangements in R d have cubical complexes homeomorphic to a d-ball, or equivalently complexes that are manifolds with boundary. A main result is that PL arrangements can be swept by a moving hyperplane to unlabeled d-compress any finite maximum class, forming a peeling scheme as conjectured by Kuzmin & Warmuth. A corollary is that some d-maximal classes cannot be embedded into any maximum class of VC-dimension d + k, for any constant k. The construction of the PL sweeping involves Pachner moves on the one-inclusion graph, corresponding to moves of a hyperplane across the intersection of d other hyperplanes. This extends the well known Pachner moves for triangulations to cubical complexes."
            ],
            "keywords": [
                "sample compression",
                "hyperplane arrangements",
                "hyperbolic and piecewise-linear geometry",
                "one-inclusion graphs"
            ],
            "author": [
                "Benjamin I P Rubinstein",
                "J Hyam Rubinstein",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/rubinstein12a/rubinstein12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Language-Motivated Approaches to Action Recognition",
            "abstract": [
                "We present language-motivated approaches to detecting, localizing and classifying activities and gestures in videos. In order to obtain statistical insight into the underlying patterns of motions in activities, we develop a dynamic, hierarchical Bayesian model which connects low-level visual features in videos with poses, motion patterns and classes of activities. This process is somewhat analogous to the method of detecting topics or categories from documents based on the word content of the documents, except that our documents are dynamic. The proposed generative model harnesses both the temporal ordering power of dynamic Bayesian networks such as hidden Markov models (HMMs) and the automatic clustering power of hierarchical Bayesian models such as the latent Dirichlet allocation (LDA) model. We also introduce a probabilistic framework for detecting and localizing pre-specified activities (or gestures) in a video sequence, analogous to the use of filler models for keyword detection in speech processing. We demonstrate the robustness of our classification model and our spotting framework by recognizing activities in unconstrained real-life video sequences and by spotting gestures via a one-shot-learning approach."
            ],
            "keywords": [
                "dynamic hierarchical Bayesian networks",
                "topic models",
                "activity recognition",
                "gesture spotting",
                "generative models"
            ],
            "author": [
                "Manavender R Malgireddy",
                "Isabelle Guyon",
                "Vassilis Athitsos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/malgireddy13a/malgireddy13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Building Support Vector Machines with Reduced Classifier Complexity",
            "abstract": [
                "Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classification speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily finds a set of kernel basis functions of a specified maximum size (d max) to approximate the SVM primal cost function well; (3) it is efficient and roughly scales as O(nd 2 max) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors."
            ],
            "keywords": [
                "SVMs",
                "classification",
                "sparse design"
            ],
            "author": [
                "S Sathiya Keerthi",
                "Olivier Chapelle",
                "Dennis Decoste",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/keerthi06a/keerthi06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Generalized Path Integral Control Approach to Reinforcement Learning",
            "abstract": [
                "With the goal to generate more scalable algorithms with higher efficiency and fewer open parameters, reinforcement learning (RL) has recently moved towards combining classical techniques from optimal control and dynamic programming with modern learning techniques from statistical estimation theory. In this vein, this paper suggests to use the framework of stochastic optimal control with path integrals to derive a novel approach to RL with parameterized policies. While solidly grounded in value function estimation and optimal control based on the stochastic Hamilton-Jacobi-Bellman (HJB) equations, policy improvements can be transformed into an approximation problem of a path integral which has no open algorithmic parameters other than the exploration noise. The resulting algorithm can be conceived of as model-based, semi-model-based, or even model free, depending on how the learning problem is structured. The update equations have no danger of numerical instabilities as neither matrix inversions nor gradient learning rates are required. Our new algorithm demonstrates interesting similarities with previous RL research in the framework of probability matching and provides intuition why the slightly heuristically motivated probability matching approach can actually perform well. Empirical evaluations demonstrate significant performance improvements over gradient-based policy learning and scalability to high-dimensional control problems. Finally, a learning experiment on a simulated 12 degree-of-freedom robot dog illustrates the functionality of our algorithm in a complex robot learning scenario. We believe that Policy Improvement with Path Integrals (PI 2) offers currently one of the most efficient, numerically robust, and easy to implement algorithms for RL based on trajectory roll-outs."
            ],
            "keywords": [
                "stochastic optimal control",
                "reinforcement learning",
                "parameterized policies"
            ],
            "author": [
                "Evangelos A Theodorou",
                "Jonas Buchli",
                "Stefan Schaal"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/theodorou10a/theodorou10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bundle Methods for Regularized Risk Minimization",
            "abstract": [
                "A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Gaussian Processes, Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as L 1 and L 2 penalties. In addition to the unified framework we present tight convergence bounds, which show that our algorithm converges in O(1/ε) steps to ε precision for general convex problems and in O(log(1/ε)) steps for continuously differentiable problems. We demonstrate the performance of our general purpose solver on a variety of publicly available data sets."
            ],
            "keywords": [
                "optimization",
                "subgradient methods",
                "cutting plane method",
                "bundle methods",
                "regularized risk minimization",
                "parallel optimization"
            ],
            "author": [
                "Choon Hui Teo",
                "Alex Smola",
                "Quoc V Le Quocle",
                "Stanford Edu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/teo10a/teo10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-task Reinforcement Learning in Partially Observable Stochastic Environments",
            "abstract": [
                "We consider the problem of multi-task reinforcement learning (MTRL) in multiple partially observable stochastic environments. We introduce the regionalized policy representation (RPR) to characterize the agent's behavior in each environment. The RPR is a parametric model of the conditional distribution over current actions given the history of past actions and observations; the agent's choice of actions is directly based on this conditional distribution, without an intervening model to characterize the environment itself. We propose off-policy batch algorithms to learn the parameters of the RPRs, using episodic data collected when following a behavior policy, and show their linkage to policy iteration. We employ the Dirichlet process as a nonparametric prior over the RPRs across multiple environments. The intrinsic clustering property of the Dirichlet process imposes sharing of episodes among similar environments, which effectively reduces the number of episodes required for learning a good policy in each environment, when data sharing is appropriate. The number of distinct RPRs and the associated clusters (the sharing patterns) are automatically discovered by exploiting the episodic data as well as the nonparametric nature of the Dirichlet process. We demonstrate the effectiveness of the proposed RPR as well as the RPR-based MTRL framework on various problems, including grid-world navigation and multi-aspect target classification. The experimental results show that the RPR is a competitive reinforcement learning algorithm in partially observable domains, and the MTRL consistently achieves better performance than single task reinforcement learning."
            ],
            "keywords": [
                "reinforcement learning",
                "partially observable Markov decision processes",
                "multi-task learning",
                "Dirichlet processes",
                "regionalized policy representation"
            ],
            "author": [
                "Hui Li",
                "Xuejun Liao",
                "Lawrence Carin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/li09b/li09b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models",
            "abstract": [
                "Summarising a high dimensional data set with a low dimensional embedding is a standard approach for exploring its structure. In this paper we provide an overview of some existing techniques for discovering such embeddings. We then introduce a novel probabilistic interpretation of principal component analysis (PCA) that we term dual probabilistic PCA (DPPCA). The DPPCA model has the additional advantage that the linear mappings from the embedded space can easily be nonlinearised through Gaussian processes. We refer to this model as a Gaussian process latent variable model (GP-LVM). Through analysis of the GP-LVM objective function, we relate the model to popular spectral techniques such as kernel PCA and multidimensional scaling. We then review a practical algorithm for GP-LVMs in the context of large data sets and develop it to also handle discrete valued data and missing attributes. We demonstrate the model on a range of real-world and artificially generated data sets."
            ],
            "keywords": [
                "Gaussian processes",
                "latent variable models",
                "principal component analysis",
                "spectral methods",
                "unsupervised learning",
                "visualisation"
            ],
            "author": [
                "Neil Lawrence"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/lawrence05a/lawrence05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Infinitely Imbalanced Logistic Regression",
            "abstract": [
                "In binary classification problems it is common for the two classes to be imbalanced: one case is very rare compared to the other. In this paper we consider the infinitely imbalanced case where one class has a finite sample size and the other class's sample size grows without bound. For logistic regression, the infinitely imbalanced case often has a useful solution. Under mild conditions, the intercept diverges as expected, but the rest of the coefficient vector approaches a non trivial and useful limit. That limit can be expressed in terms of exponential tilting and is the minimum of a convex objective function. The limiting form of logistic regression suggests a computational shortcut for fraud detection problems."
            ],
            "keywords": [
                "classification",
                "drug discovery",
                "fraud detection",
                "rare events",
                "unbalanced data"
            ],
            "author": [
                "Art B Owen",
                "Owen @ Stanford Stat"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/owen07a/owen07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-Dimensional Inference for Cluster-Based Graphical Models",
            "abstract": [
                "Motivated by modern applications in which one constructs graphical models based on a very large number of features, this paper introduces a new class of cluster-based graphical models, in which variable clustering is applied as an initial step for reducing the dimension of the feature space. We employ model assisted clustering, in which the clusters contain features that are similar to the same unobserved latent variable. Two different cluster-based Gaussian graphical models are considered: the latent variable graph, corresponding to the graphical model associated with the unobserved latent variables, and the cluster-average graph, corresponding to the vector of features averaged over clusters. Our study reveals that likelihood based inference for the latent graph, not analyzed previously, is analytically intractable. Our main contribution is the development and analysis of alternative estimation and inference strategies, for the precision matrix of an unobservable latent vector Z. We replace the likelihood of the data by an appropriate class of empirical risk functions, that can be specialized to the latent graphical model and to the simpler, but under-analyzed, cluster-average graphical model. The estimators thus derived can be used for inference on the graph structure, for instance on edge strength or pattern recovery. Inference is based on the asymptotic limits of the entry-wise estimates of the precision matrices associated with the conditional independence graphs under consideration. While taking the uncertainty induced by the clustering step into account, we establish Berry-Esseen central limit theorems for the proposed estimators. It is noteworthy that, although the clusters are estimated adaptively from the data, the central limit theorems regarding the entries of the estimated graphs are proved under the same conditions one would use if the clusters were known in advance. As an illustration of the usage of these newly developed inferential tools, we show that they can be reliably used for recovery of the sparsity pattern of the graphs we study, under FDR control, which is verified via simulation studies and an fMRI data analysis. These experimental results confirm the theoretically established difference between the two graph structures. Furthermore, the data analysis suggests that the latent variable graph, corresponding to the unobserved cluster centers, can help provide more insight into the"
            ],
            "keywords": [
                "Berry-Esseen bound",
                "Graphical model",
                "Latent variables",
                "High-dimensional inference",
                "Clustering",
                "False discovery rate"
            ],
            "author": [
                "Carson Eisenach",
                "Florentina Bunea",
                "Yang Ning"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-357/18-357.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "JNCC2: The Java Implementation Of Naive Credal Classifier 2",
            "abstract": [
                "JNCC2 implements the naive credal classifier 2 (NCC2). This is an extension of naive Bayes to imprecise probabilities that aims at delivering robust classifications also when dealing with small or incomplete data sets. Robustness is achieved by delivering set-valued classifications (that is, returning multiple classes) on the instances for which (i) the learning set is not informative enough to smooth the effect of choice of the prior density or (ii) the uncertainty arising from missing data prevents the reliable indication of a single class. JNCC2 is released under the GNU GPL license."
            ],
            "keywords": [
                "imprecise probabilities",
                "missing data",
                "naive Bayes",
                "naive credal classifier 2",
                "Java"
            ],
            "author": [
                "Giorgio Corani",
                "Marco Zaffalon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/corani08b/corani08b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximate Modified Policy Iteration and its Application to the Game of Tetris",
            "abstract": [
                "Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of the well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. 1. This paper is a significant extension of two conference papers by the authors (Scherrer et al., 2012; Gabillon et al., 2013). Here we discuss better the relation of the AMPI algorithms with other approximate DP methods, and provide more detailed description of the algorithms, proofs of the theorems, and report"
            ],
            "keywords": [
                "approximate dynamic programming",
                "reinforcement learning",
                "Markov decision processes",
                "finite-sample analysis",
                "performance bounds",
                "game of tetris"
            ],
            "author": [
                "Bruno Scherrer",
                "Mohammad Ghavamzadeh",
                "Victor Gabillon",
                "Boris Lesner",
                "Matthieu Geist",
                "Ghavamzadeh, Gabillon, Lesner Geist Scherrer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/scherrer15a/scherrer15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analyzing Tensor Power Method Dynamics in Overcomplete Regime",
            "abstract": [
                "We present a novel analysis of the dynamics of tensor power iterations in the overcomplete regime where the tensor CP rank is larger than the input dimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in general. We consider the case where the tensor components are randomly drawn, and show that the simple power iteration recovers the components with bounded error under mild initialization conditions. We apply our analysis to unsupervised learning of latent variable models, such as multi-view mixture models and spherical Gaussian mixtures. Given the third order moment tensor, we learn the parameters using tensor power iterations. We prove it can correctly learn the model parameters when the number of hidden components k is much larger than the data dimension d, up to k = o(d 1.5). We initialize the power iterations with data samples and prove its success under mild conditions on the signal-to-noise ratio of the samples. Our analysis significantly expands the class of latent variable models where spectral methods are applicable. Our analysis also deals with noise in the input tensor leading to sample complexity result in the application to learning latent variable models."
            ],
            "keywords": [
                "tensor decomposition",
                "tensor power iteration",
                "overcomplete representation",
                "unsupervised learning",
                "latent variable models"
            ],
            "author": [
                "Animashree Anandkumar",
                "Rong Ge",
                "Majid Janzamin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-486/15-486.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians",
            "abstract": [
                "We show that, given data from a mixture of k well-separated spherical Gaussians in R d , a simple two-round variant of EM will, with high probability, learn the parameters of the Gaussians to nearoptimal precision, if the dimension is high (d ln k). We relate this to previous theoretical and empirical work on the EM algorithm."
            ],
            "keywords": [
                "expectation maximization",
                "mixtures of Gaussians",
                "clustering",
                "unsupervised learning",
                "probabilistic analysis"
            ],
            "author": [
                "Sanjoy Dasgupta",
                "Leonard Schulman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/dasgupta07a/dasgupta07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convex and Scalable Weakly Labeled SVMs",
            "abstract": [
                "In this paper, we study the problem of learning from weakly labeled data, where labels of the training examples are incomplete. This includes, for example, (i) semi-supervised learning where labels are partially known; (ii) multi-instance learning where labels are implicitly known; and (iii) clustering where labels are completely unknown. Unlike supervised learning, learning with weak labels involves a difficult Mixed-Integer Programming (MIP) problem. Therefore, it can suffer from poor scalability and may also get stuck in local minimum. In this paper, we focus on SVMs and propose the WELLSVM via a novel label generation strategy. This leads to a convex relaxation of the original MIP, which is at least as tight as existing convex Semi-Definite Programming (SDP) relaxations. Moreover, the WELLSVM can be solved via a sequence of SVM subproblems that are much more scalable than previous convex SDP relaxations. Experiments on three weakly labeled learning tasks, namely, (i) semi-supervised learning; (ii) multi-instance learning for locating regions of interest in content-based information retrieval; and (iii) clustering, clearly demonstrate improved performance, and WELLSVM is also readily applicable on large data sets."
            ],
            "keywords": [
                "weakly labeled data",
                "semi-supervised learning",
                "multi-instance learning",
                "clustering",
                "cutting plane",
                "convex relaxation"
            ],
            "author": [
                "Yu-Feng Li",
                "Ivor W Tsang",
                "James T Kwok",
                "Hong Kong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/li13a/li13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Survey of Algorithms and Analysis for Adaptive Online Learning",
            "abstract": [
                "We present tools for the analysis of Follow-The-Regularized-Leader (FTRL), Dual Averaging, and Mirror Descent algorithms when the regularizer (equivalently, proxfunction or learning rate schedule) is chosen adaptively based on the data. Adaptivity can be used to prove regret bounds that hold on every round, and also allows for data-dependent regret bounds as in AdaGrad-style algorithms (e.g., Online Gradient Descent with adaptive per-coordinate learning rates). We present results from a large number of prior works in a unified manner, using a modular and tight analysis that isolates the key arguments in easily re-usable lemmas. This approach strengthens previously known FTRL analysis techniques to produce bounds as tight as those achieved by potential functions or primal-dual analysis. Further, we prove a general and exact equivalence between adaptive Mirror Descent algorithms and a corresponding FTRL update, which allows us to analyze Mirror Descent algorithms in the same framework. The key to bridging the gap between Dual Averaging and Mirror Descent algorithms lies in an analysis of the FTRL-Proximal algorithm family. Our regret bounds are proved in the most general form, holding for arbitrary norms and non-smooth regularizers with time-varying weight."
            ],
            "keywords": [
                "online learning",
                "online convex optimization",
                "regret analysis",
                "adaptive algorithms",
                "follow-the-regularized-leader",
                "mirror descent",
                "dual averaging"
            ],
            "author": [
                "H Brendan Mcmahan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-428/14-428.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximate Inference on Planar Graphs using Loop Calculus and Belief Propagation",
            "abstract": [
                "We introduce novel results for approximate inference on planar graphical models using the loop calculus framework. The loop calculus (Chertkov and Chernyak, 2006a) allows to express the exact partition function of a graphical model as a finite sum of terms that can be evaluated once the belief propagation (BP) solution is known. In general, full summation over all correction terms is intractable. We develop an algorithm for the approach presented in Chertkov et al. (2008) which represents an efficient truncation scheme on planar graphs and a new representation of the series in terms of Pfaffians of matrices. We analyze the performance of the algorithm for models with binary variables and pairwise interactions on grids and other planar graphs. We study in detail both the loop series and the equivalent Pfaffian series and show that the first term of the Pfaffian series for the general, intractable planar model, can provide very accurate approximations. The algorithm outperforms previous truncation schemes of the loop series and is competitive with other state of the art methods for approximate inference."
            ],
            "keywords": [
                "belief propagation",
                "loop calculus",
                "approximate inference",
                "partition function",
                "planar graphs",
                "Ising model"
            ],
            "author": [
                "Vicenç Gómez",
                "Hilbert J Kappen",
                "Michael Chertkov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/gomez10a/gomez10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity",
            "abstract": [
                "Analysis of causal effects between continuous-valued variables typically uses either autoregressive models or structural equation models with instantaneous effects. Estimation of Gaussian, linear structural equation models poses serious identifiability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. This is effectively what is called a structural vector autoregression (SVAR) model, and thus our work contributes to the long-standing problem of how to estimate SVAR's. We show that such a non-Gaussian model is identifiable without prior knowledge of network structure. We propose computationally efficient methods for estimating the model, as well as methods to assess the significance of the causal influences. The model is successfully applied on financial and brain imaging data."
            ],
            "keywords": [
                "structural vector autoregression",
                "structural equation models",
                "independent component analysis",
                "non-Gaussianity",
                "causality"
            ],
            "author": [
                "Aapo Hyvärinen",
                "Aapo Hyvarinen",
                "Helsinki Fi",
                "Kun Zhang",
                "Tuebingen Mpg De",
                "Patrik O Hoyer",
                "Shohei Shimizu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/hyvarinen10a/hyvarinen10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Noise Accumulation in High Dimensional Classification and Total Signal Index",
            "abstract": [
                "Great attention has been paid to Big Data in recent years. Such data hold promise for scientific discoveries but also pose challenges to analyses. One potential challenge is noise accumulation. In this paper, we explore noise accumulation in high dimensional two-group classification. First, we revisit a previous assessment of noise accumulation with principal component analyses, which yields a different threshold for discriminative ability than originally identified. Then we extend our scope to its impact on classifiers developed with three common machine learning approachesrandom forest, support vector machine, and boosted classification trees. We simulate four scenarios with differing amounts of signal strength to evaluate each method. After determining noise accumulation may affect the performance of these classifiers, we assess factors that impact it. We conduct simulations by varying sample size, signal strength, signal strength proportional to the number predictors, and signal magnitude with random forest classifiers. These simulations suggest that noise accumulation affects the discriminative ability of high-dimensional classifiers developed using common machine learning methods, which can be modified by sample size, signal strength, and signal magnitude. We developed the measure total signal index (TSI) to track the trends of total signal and noise accumulation."
            ],
            "keywords": [
                "Noise Accumulation",
                "Classification",
                "High Dimensional",
                "Random Forest",
                "Asymptotic",
                "Total Signal Index"
            ],
            "author": [
                "Miriam R Elman",
                "Jessica Minnier",
                "Xiaohui Chang",
                "Dongseok Choi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-117/19-117.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bounds for the Loss in Probability of Correct Classification Under Model Based Approximation",
            "abstract": [
                "In many pattern recognition/classification problem the true class conditional model and class probabilities are approximated for reasons of reducing complexity and/or of statistical estimation. The approximated classifier is expected to have worse performance, here measured by the probability of correct classification. We present an analysis valid in general, and easily computable formulas for estimating the degradation in probability of correct classification when compared to the optimal classifier. An example of an approximation is the Naïve Bayes classifier. We show that the performance of the Naïve Bayes depends on the degree of functional dependence between the features and labels. We provide a sufficient condition for zero loss of performance, too."
            ],
            "keywords": [
                "Bayesian networks",
                "naïve Bayes",
                "plug-in classifier",
                "Kolmogorov distance of variation",
                "variational learning"
            ],
            "author": [
                "Magnus Ekdahl"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/ekdahl06a/ekdahl06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Stochastic Algorithm for Feature Selection in Pattern Recognition",
            "abstract": [
                "We introduce a new model addressing feature selection from a large dictionary of variables that can be computed from a signal or an image. Features are extracted according to an efficiency criterion, on the basis of specified classification or recognition tasks. This is done by estimating a probability distribution P on the complete dictionary, which distributes its mass over the more efficient, or informative, components. We implement a stochastic gradient descent algorithm, using the probability as a state variable and optimizing a multi-task goodness of fit criterion for classifiers based on variable randomly chosen according to P. We then generate classifiers from the optimal distribution of weights learned on the training set. The method is first tested on several pattern recognition problems including face detection, handwritten digit recognition, spam classification and micro-array analysis. We then compare our approach with other step-wise algorithms like random forests or recursive feature elimination."
            ],
            "keywords": [
                "stochastic learning algorithms",
                "Robbins-Monro application",
                "pattern recognition",
                "classification algorithm",
                "feature selection"
            ],
            "author": [
                "Sébastien Gadat",
                "Laurent Younes"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/gadat07a/gadat07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-dimensional Quantile Tensor Regression",
            "abstract": [
                "Quantile regression is an indispensable tool for statistical learning. Traditional quantile regression methods consider vector-valued covariates and estimate the corresponding coefficient vector. Many modern applications involve data with a tensor structure. In this paper, we propose a quantile regression model which takes tensors as covariates, and present an estimation approach based on Tucker decomposition. It effectively reduces the number of parameters, leading to efficient estimation and feasible computation. We also use a sparse Tucker decomposition, which is a popular approach in the literature, to further reduce the number of parameters when the dimension of the tensor is large. We propose an alternating update algorithm combined with alternating direction method of multipliers (ADMM). The asymptotic properties of the estimators are established under suitable conditions. The numerical performances are demonstrated via simulations and an application to a crowd density estimation problem."
            ],
            "keywords": [
                "Multidimensional array",
                "Quantile regression",
                "Sparsity principle",
                "Tensor regression",
                "Tucker decomposition"
            ],
            "author": [
                "Wenqi Lu",
                "Zhongyi Zhu",
                "Heng Lian"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-383/20-383.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hamiltonian Monte Carlo with Energy Conserving Subsampling",
            "abstract": [
                "Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional posterior distributions with proposed parameter draws obtained by iterating on a discretized version of the Hamiltonian dynamics. The iterations make HMC computationally costly, especially in problems with large data sets, since it is necessary to compute posterior densities and their derivatives with respect to the parameters. Naively computing the Hamiltonian dynamics on a subset of the data causes HMC to lose its key ability to generate distant parameter proposals with high acceptance probability. The key insight in our article is that efficient subsampling HMC for the parameters is possible if both the dynamics and the acceptance probability are computed from the same data subsample in each complete HMC iteration. We show that this is possible to do in a principled way in a HMC-within-Gibbs framework where the subsample is updated using a pseudo marginal MH step and the parameters are then updated using an HMC step, based on the current subsample. We show that our subsampling methods are fast and compare favorably to two popular sampling algorithms that use gradient estimates from data subsampling. We also explore the current limitations of subsampling HMC algorithms by varying the quality of the variance reducing control variates used in the estimators of the posterior density and its gradients."
            ],
            "keywords": [
                "Bayesian inference",
                "Big Data",
                "Markov chain Monte Carlo",
                "Estimated likelihood",
                "Stochastic gradient Hamiltonian Monte Carlo",
                "Stochastic Gradient Langevin Dynamics"
            ],
            "author": [
                "Khue-Dung Dang",
                "Matias Quiroz",
                "Robert Kohn",
                "Minh-Ngoc Tran",
                "Mattias Villani",
                "Dung Dang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-452/17-452.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines",
            "abstract": [
                "In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy."
            ],
            "keywords": [
                "Multiclass problems",
                "SVM",
                "Kernel Machines"
            ],
            "author": [
                "Koby Crammer",
                "Yoram Singer",
                "Nello Cristianini",
                "Bob Williamson"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/crammer01a/crammer01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Generalization Error for Q-Learning",
            "abstract": [
                "Planning problems that involve learning a policy from a single training set of finite horizon trajectories arise in both social science and medical fields. We consider Q-learning with function approximation for this setting and derive an upper bound on the generalization error. This upper bound is in terms of quantities minimized by a Q-learning algorithm, the complexity of the approximation space and an approximation term due to the mismatch between Q-learning and the goal of learning a policy that maximizes the value function."
            ],
            "keywords": [
                "multistage decisions",
                "dynamic programming",
                "reinforcement learning",
                "batch data"
            ],
            "author": [
                "Susan A Murphy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/murphy05a/murphy05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Convex Parametrization of a New Class of Universal Kernel Functions",
            "abstract": [
                "The accuracy and complexity of kernel learning algorithms is determined by the set of kernels over which it is able to optimize. An ideal set of kernels should: admit a linear parameterization (tractability); be dense in the set of all kernels (accuracy); and every member should be universal so that the hypothesis space is infinite-dimensional (scalability). Currently, there is no class of kernel that meets all three criteria-e.g. Gaussians are not tractable or accurate; polynomials are not scalable. We propose a new class that meet all three criteria-the Tessellated Kernel (TK) class. Specifically, the TK class: admits a linear parameterization using positive matrices; is dense in all kernels; and every element in the class is universal. This implies that the use of TK kernels for learning the kernel can obviate the need for selecting candidate kernels in algorithms such as SimpleMKL and parameters such as the bandwidth. Numerical testing on soft margin Support Vector Machine (SVM) problems show that algorithms using TK kernels outperform other kernel learning algorithms and neural networks. Furthermore, our results show that when the ratio of the number of training data to features is high, the improvement of TK over MKL increases significantly."
            ],
            "keywords": [
                "kernel functions",
                "multiple kernel learning",
                "semi-definite programming",
                "supervised learning",
                "universal kernels"
            ],
            "author": [
                "Brendon K Colbert",
                "Matthew M Peet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-594/19-594.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Provable Convex Co-clustering of Tensors",
            "abstract": [
                "Cluster analysis is a fundamental tool for pattern discovery of complex heterogeneous data. Prevalent clustering methods mainly focus on vector or matrix-variate data and are not applicable to general-order tensors, which arise frequently in modern scientific and business applications. Moreover, there is a gap between statistical guarantees and computational efficiency for existing tensor clustering solutions due to the nature of their non-convex formulations. In this work, we bridge this gap by developing a provable convex formulation of tensor co-clustering. Our convex co-clustering (CoCo) estimator enjoys stability guarantees and its computational and storage costs are polynomial in the size of the data. We further establish a non-asymptotic error bound for the CoCo estimator, which reveals a surprising \"blessing of dimensionality\" phenomenon that does not exist in vector or matrix-variate cluster analysis. Our theoretical findings are supported by extensive simulated studies. Finally, we apply the CoCo estimator to the cluster analysis of advertisement click tensor data from a major online company. Our clustering results provide meaningful business insights to improve advertising effectiveness."
            ],
            "keywords": [
                "Clustering",
                "Fused lasso",
                "High-dimensional Statistical Learning",
                "Multiway Data",
                "Non-asymptotic Error"
            ],
            "author": [
                "Eric C Chi",
                "Brian R Gaines",
                "Will Wei Sun",
                "Hua Zhou",
                "Jian Yang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-155/18-155.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unified Formulation and Fast Accelerated Proximal Gradient Method for Classification",
            "abstract": [
                "Binary classification is the problem of predicting the class a given sample belongs to. To achieve a good prediction performance, it is important to find a suitable model for a given dataset. However, it is often time consuming and impractical for practitioners to try various classification models because each model employs a different formulation and algorithm. The difficulty can be mitigated if we have a unified formulation and an efficient universal algorithmic framework for various classification models to expedite the comparison of performance of different models for a given dataset. In this paper, we present a unified formulation of various classification models (including C-SVM, 2-SVM, ν-SVM, MM-FDA, MM-MPM, logistic regression, distance weighted discrimination) and develop a general optimization algorithm based on an accelerated proximal gradient (APG) method for the formulation. We design various techniques such as backtracking line search and adaptive restarting strategy in order to speed up the practical convergence of our method. We also give a theoretical convergence guarantee for the proposed fast APG algorithm. Numerical experiments show that our algorithm is stable and highly competitive to specialized algorithms designed for specific models (e.g., sequential minimal optimization (SMO) for SVM)."
            ],
            "keywords": [
                "restarted accelerated proximal gradient method",
                "binary classification",
                "minimum norm problem",
                "vector projection computation",
                "support vector machine"
            ],
            "author": [
                "Naoki Ito",
                "Akiko Takeda",
                "Kim-Chuan Toh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-274/16-274.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Nonconvex Approach for Phase Retrieval: Reshaped Wirtinger Flow and Incremental Algorithms",
            "abstract": [
                "We study the problem of solving a quadratic system of equations, i.e., recovering a vector signal x ∈ R n from its magnitude measurements y i = | a i , x |, i = 1, ..., m. We develop a gradient descent algorithm (referred to as RWF for reshaped Wirtinger flow) by minimizing the quadratic loss of the magnitude measurements. Comparing with Wirtinger flow (WF) (Candès et al., 2015), the loss function of RWF is nonconvex and nonsmooth, but better resembles the least-squares loss when the phase information is also available. We show that for random Gaussian measurements, RWF enjoys linear convergence to the true signal as long as the number of measurements is O(n). This improves the sample complexity of WF (O(n log n)), and achieves the same sample complexity as truncated Wirtinger flow (TWF) (Chen and Candès, 2015), but without any sophisticated truncation in the gradient loop. Furthermore, RWF costs less computationally than WF, and runs faster numerically than both WF and TWF. We further develop an incremental (stochastic) version of RWF (IRWF) and connect it with the randomized Kaczmarz method for phase retrieval. We demonstrate that IRWF outperforms existing incremental as well as batch algorithms with experiments."
            ],
            "keywords": [
                "gradient descent",
                "phase retrieval",
                "nonconvex optimization",
                "regularity condition",
                "stochastic algorithms"
            ],
            "author": [
                "Huishuai Zhang",
                "Yi Zhou",
                "Yingbin Liang",
                "Yuejie Chi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-572/16-572.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Sober Look at the Unsupervised Learning of Disentangled Representations and their Evaluation",
            "abstract": [
                "The idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train over 000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on eight data sets. We observe that while the different methods successfully enforce properties \"encouraged\" by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, different evaluation metrics do not always agree on what should be considered \"disentangled\" and exhibit systematic differences in the estimation. Finally, increased disentanglement does not seem to"
            ],
            "keywords": [
                "Disentangled representations",
                "impossibility",
                "evaluation",
                "reproducibility",
                "large scale experimental study"
            ],
            "author": [
                "Francesco Locatello",
                "Stefan Bauer",
                "Mario Lucic",
                "Gunnar Rätsch",
                "Sylvain Gelly",
                "Bernhard Schölkopf",
                "Olivier Bachem"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-976/19-976.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiplicative local linear hazard estimation and best one-sided cross-validation",
            "abstract": [
                "This paper develops detailed mathematical statistical theory of a new class of cross-validation techniques of local linear kernel hazards and their multiplicative bias corrections. The new class of cross-validation combines principles of local information and recent advances in indirect cross-validation. A few applications of cross-validating multiplicative kernel hazard estimation do exist in the literature. However, detailed mathematical statistical theory and small sample performance are introduced via this paper and further upgraded to our new class of best one-sided cross-validation. Best one-sided cross-validation turns out to have excellent performance in its practical illustrations, in its small sample performance and in its mathematical statistical theoretical performance."
            ],
            "keywords": [
                "Aalen's multiplicative model",
                "multiplicative bias correction",
                "bandwidth",
                "indirect cross-validation"
            ],
            "author": [
                "María Luz Gámiz",
                "Jens Perch Nielsen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-663/17-663.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Iterative and Active Graph Clustering Using Trace Norm Minimization Without Cluster Size Constraints *",
            "abstract": [
                "This paper investigates graph clustering under the planted partition model in the presence of small clusters. Traditional results dictate that for an algorithm to provably correctly recover the underlying clusters, all clusters must be sufficiently large-in particular, the cluster sizes need to beΩ(√ n), where n is the number of nodes of the graph. We show that this is not really a restriction: by a refined analysis of a convex-optimization-based recovery approach, we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones. Based on this result, we further devise an iterative algorithm to provably recover almost all clusters via a \"peeling strategy\": we recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the partial observation setting, in which only a (chosen) part of the graph is observed. The peeling strategy gives rise to an active learning algorithm, in which edges adjacent to smaller clusters are queried more often after large clusters are learned (and removed). We expect that the idea of iterative peeling-that is, sequentially identifying a subset of the clusters and reducing the problem to a smaller one-is useful more broadly beyond the specific implementations (based on convex optimization) used in this paper."
            ],
            "keywords": [
                "graph clustering",
                "community detection",
                "active clustering",
                "convex optimization",
                "planted partition model",
                "stochastic block model"
            ],
            "author": [
                "Nir Ailon",
                "Yudong Chen",
                "Huan Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/ailon15a/ailon15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Inner Product Spaces for Bayesian Networks",
            "abstract": [
                "Bayesian networks have become one of the major models used for statistical inference. We study the question whether the decisions computed by a Bayesian network can be represented within a low-dimensional inner product space. We focus on two-label classification tasks over the Boolean domain. As main results we establish upper and lower bounds on the dimension of the inner product space for Bayesian networks with an explicitly given (full or reduced) parameter collection. In particular, these bounds are tight up to a factor of 2. For some nontrivial cases of Bayesian networks we even determine the exact values of this dimension. We further consider logistic autoregressive Bayesian networks and show that every sufficiently expressive inner product space must have dimension at least Ω(n 2), where n is the number of network nodes. We also derive the bound 2 Ω(n) for an artificial variant of this network, thereby demonstrating the limits of our approach and raising an interesting open question. As a major technical contribution, this work reveals combinatorial and algebraic structures within Bayesian networks such that known methods for the derivation of lower bounds on the dimension of inner product spaces can be brought into play."
            ],
            "keywords": [
                "Bayesian network",
                "inner product space",
                "embedding",
                "linear arrangement",
                "Euclidean dimension"
            ],
            "author": [
                "Atsuyoshi Nakamura",
                "Michael Schmitt",
                "Niels Schmitt",
                "Hans Ulrich Simon",
                "Lehrstuhl Mathematik",
                "Informatik Fakultät Für Mathematik"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/nakamura05a/nakamura05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Target Curricula via Selection of Minimum Feature Sets: a Case Study in Boolean Networks",
            "abstract": [
                "We consider the effect of introducing a curriculum of targets when training Boolean models on supervised Multi Label Classification (MLC) problems. In particular, we consider how to order targets in the absence of prior knowledge, and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models. We show that hierarchical dependencies between targets can be exploited by enforcing an appropriate curriculum using hierarchical loss functions. On several multi-output circuit-inference problems with known target difficulties, Feedforward Boolean Networks (FBNs) trained with such a loss function achieve significantly lower out-of-sample error, up to 10% in some cases. This improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curricula. We also demonstrate the same improvements on three real-world models and two Gene Regulatory Network (GRN) inference problems. We posit a simple a-priori method for identifying an appropriate target order and estimating the strength of target relationships in Boolean MLCs. These methods use intrinsic dimension as a proxy for target difficulty, which is estimated using optimal solutions to a combinatorial optimisation problem known as the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same generalisation gains can be achieved without providing any knowledge of target difficulty."
            ],
            "keywords": [
                "Multi-Label Classification",
                "Target Curriculum",
                "Boolean Networks",
                "k-Feature Set"
            ],
            "author": [
                "Shannon Fenn",
                "Pablo Moscato"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-007/17-007.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Generalized Kernel Mixed Models",
            "abstract": [
                "We propose a fully Bayesian methodology for generalized kernel mixed models (GKMMs), which are extensions of generalized linear mixed models in the feature space induced by a reproducing kernel. We place a mixture of a point-mass distribution and Silverman's g-prior on the regression vector of a generalized kernel model (GKM). This mixture prior allows a fraction of the components of the regression vector to be zero. Thus, it serves for sparse modeling and is useful for Bayesian computation. In particular, we exploit data augmentation methodology to develop a Markov chain Monte Carlo (MCMC) algorithm in which the reversible jump method is used for model selection and a Bayesian model averaging method is used for posterior prediction. When the feature basis expansion in the reproducing kernel Hilbert space is treated as a stochastic process, this approach can be related to the Karhunen-Loève expansion of a Gaussian process (GP). Thus, our sparse modeling framework leads to a flexible approximation method for GPs."
            ],
            "keywords": [
                "reproducing kernel Hilbert spaces",
                "generalized kernel models",
                "Silverman's g-prior",
                "Bayesian model averaging",
                "Gaussian processes"
            ],
            "author": [
                "Zhihua Zhang",
                "Guang Dai",
                "Michael I Jordan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/zhang11a/zhang11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hyper-Sparse Optimal Aggregation",
            "abstract": [
                "Given a finite set F of functions and a learning sample, the aim of an aggregation procedure is to have a risk as close as possible to risk of the best function in F. Up to now, optimal aggregation procedures are convex combinations of every elements of F. In this paper, we prove that optimal aggregation procedures combining only two functions in F exist. Such algorithms are of particular interest when F contains many irrelevant functions that should not appear in the aggregation procedure. Since selectors are suboptimal aggregation procedures, this proves that two is the minimal number of elements of F required for the construction of an optimal aggregation procedure in every situations. Then, we perform a numerical study for the problem of selection of the regularization parameters of the Lasso and the Elastic-net estimators. We compare on simulated examples our aggregation algorithms to aggregation with exponential weights, to Mallow's C p and to cross-validation selection procedures."
            ],
            "keywords": [
                "aggregation",
                "exact oracle inequality",
                "empirical risk minimization",
                "empirical process theory",
                "sparsity",
                "Lasso",
                "Lars"
            ],
            "author": [
                "Stéphane Gaïffas",
                "Guillaume Lecué"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/gaiffas11a/gaiffas11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Rates in Statistical and Online Learning",
            "abstract": [
                "The speed with which a learning algorithm converges as it is presented with more data is a central problem in machine learning-a fast rate of convergence means less data is needed for the same level of performance. The pursuit of fast rates in online and statistical learning has led to the discovery of many conditions in learning theory under which fast learning is possible. We show that most of these conditions are special cases of a single, unifying condition, that comes in two forms: the central condition for 'proper' learning algorithms that always output a hypothesis in the given model, and stochastic mixability for online algorithms that may make predictions outside of the model. We show that under surprisingly weak assumptions both conditions are, in a certain sense, equivalent. The central condition has a re-interpretation in terms of convexity of a set of pseudoprobabilities, linking it to density estimation under misspecification. For bounded losses, we show how the central condition enables a direct proof of fast rates and we prove its equivalence to the Bernstein condition, itself a generalization of the Tsybakov margin condition, both of which have played a central role in obtaining fast rates in statistical learning. Yet, while the Bernstein condition is two-sided, the central condition is one-sided, making it more suitable to deal with unbounded losses. In its stochastic mixability form, our condition generalizes both a stochastic exp-concavity condition identified by Juditsky, Rigollet and Tsybakov and Vovk's notion of mixability. Our unifying conditions thus provide a substantial step towards a characterization of fast rates in statistical learning, similar to how classical mixability characterizes constant regret in the sequential prediction with expert advice setting."
            ],
            "keywords": [
                "statistical learning theory",
                "fast rates",
                "Tsybakov margin condition",
                "mixability",
                "exp-concavity"
            ],
            "author": [
                "Tim Van Erven",
                "Peter D Grünwald",
                "Nishant A Mehta",
                "Mark D Reid",
                "Robert C Williamson",
                "Alex Gammerman",
                "Vladimir Vovk",
                "Reid Mehta"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/vanerven15a/vanerven15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Closed Surface Fitting Through Tensor Products",
            "abstract": [
                "Closed surfaces provide a useful model for 3-d shapes, with the data typically consisting of a cloud of points in R 3. The existing literature on closed surface modeling focuses on frequentist point estimation methods that join surface patches along the edges, with surface patches created via Bézier surfaces or tensor products of B-splines. However, the resulting surfaces are not smooth along the edges and the geometric constraints required to join the surface patches lead to computational drawbacks. In this article, we develop a Bayesian model for closed surfaces based on tensor products of a cyclic basis resulting in infinitely smooth surface realizations. We impose sparsity on the control points through a doubleshrinkage prior. Theoretical properties of the support of our proposed prior are studied and it is shown that the posterior achieves the optimal rate of convergence under reasonable assumptions on the prior. The proposed approach is illustrated with some examples."
            ],
            "keywords": [
                "3-d shapes",
                "Bayesian nonparametrics",
                "Imaging",
                "Manifold learning",
                "Splines",
                "Tensors"
            ],
            "author": [
                "Olivier Binette",
                "Debdeep Pati",
                "David B Dunson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/13-233/13-233.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hinge-Loss Markov Random Fields and Probabilistic Soft Logic",
            "abstract": [
                "A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hingeloss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL-MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible."
            ],
            "keywords": [
                "Probabilistic graphical models",
                "statistical relational learning",
                "structured prediction"
            ],
            "author": [
                "Stephen H Bach",
                "Matthias Broecheler",
                "Bert Huang",
                "Lise Getoor"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-631/15-631.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DPPy: DPP Sampling with Python",
            "abstract": [
                "Determinantal point processes (DPPs) are specific probability distributions over clouds of points that are used as models and computational tools across physics, probability, statistics, and more recently machine learning. Sampling from DPPs is a challenge and therefore we present DPPy, a Python toolbox that gathers known exact and approximate sampling algorithms for both finite and continuous DPPs. The project is hosted on GitHub and equipped with an extensive documentation."
            ],
            "keywords": [
                "determinantal point processes",
                "sampling",
                "MCMC",
                "random matrices",
                "Python"
            ],
            "author": [
                "Guillaume Gautier",
                "Guillermo Polito",
                "Michal Valko"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-179/19-179.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Introduction to Artificial Prediction Markets for Classification",
            "abstract": [
                "Prediction markets are used in real life to predict outcomes of interest such as presidential elections. This paper presents a mathematical theory of artificial prediction markets for supervised learning of conditional probability estimators. The artificial prediction market is a novel method for fusing the prediction information of features or trained classifiers, where the fusion result is the contract price on the possible outcomes. The market can be trained online by updating the participants' budgets using training examples. Inspired by the real prediction markets, the equations that govern the market are derived from simple and reasonable assumptions. Efficient numerical algorithms are presented for solving these equations. The obtained artificial prediction market is shown to be a maximum likelihood estimator. It generalizes linear aggregation, existent in boosting and random forest, as well as logistic regression and some kernel methods. Furthermore, the market mechanism allows the aggregation of specialized classifiers that participate only on specific instances. Experimental comparisons show that the artificial prediction markets often outperform random forest and implicit online learning on synthetic data and real UCI data sets. Moreover, an extensive evaluation for pelvic and abdominal lymph node detection in CT data shows that the prediction market improves adaboost's detection rate from 79.6% to 81.2% at 3 false positives/volume."
            ],
            "keywords": [
                "online learning",
                "ensemble methods",
                "supervised learning",
                "random forest",
                "implicit online learning"
            ],
            "author": [
                "Adrian Barbu",
                "Nathan Lay"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/barbu12a/barbu12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Compression Technique for Analyzing Disagreement-Based Active Learning",
            "abstract": [
                "We introduce a new and improved characterization of the label complexity of disagreement-based active learning, in which the leading quantity is the version space compression set size. This quantity is defined as the size of the smallest subset of the training data that induces the same version space. We show various applications of the new characterization, including a tight analysis of CAL and refined label complexity bounds for linear separators under mixtures of Gaussians and axis-aligned rectangles under product densities. The version space compression set size, as well as the new characterization of the label complexity, can be naturally extended to agnostic learning problems, for which we show new speedup results for two well known active learning algorithms."
            ],
            "keywords": [
                "active learning",
                "selective sampling",
                "sequential design",
                "statistical learning theory",
                "PAC learning",
                "sample complexity",
                "selective prediction"
            ],
            "author": [
                "Yair Wiener",
                "Steve Hanneke"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/wiener15a/wiener15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Measuring the Effects of Data Parallelism on Neural Network Training",
            "abstract": [
                "Recent hardware developments have dramatically increased the scale of data parallelism available for neural network training. Among the simplest ways to harness next-generation hardware is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured by the number of steps necessary to reach a goal out-of-sample error. We study how this relationship varies with the training algorithm, model, and data set, and find extremely large variation between workloads. Along the way, we show that disagreements in the literature on how batch size affects model quality can largely be explained by differences in metaparameter tuning and compute budgets at different batch sizes. We find no evidence that larger batch sizes degrade out-of-sample performance. Finally, we discuss the implications of our results on efforts to train neural networks much faster in the future. Our experimental data is publicly available as a database of 71,638,836 loss measurements taken over the course of training for 168,160 individual models across 35 workloads."
            ],
            "keywords": [
                "neural networks",
                "stochastic gradient descent",
                "data parallelism",
                "batch size",
                "deep learning"
            ],
            "author": [
                "Christopher J Shallue",
                "Roy Frostig",
                "George E Dahl"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-789/18-789.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle",
            "abstract": [
                "While machine learning has proven to be a powerful data-driven solution to many reallife problems, its use in sensitive domains has been limited due to privacy concerns. A popular approach known as differential privacy offers provable privacy guarantees, but it is often observed in practice that it could substantially hamper learning accuracy. In this paper we study the learnability (whether a problem can be learned by any algorithm) under Vapnik's general learning setting with differential privacy constraint, and reveal some intricate relationships between privacy, stability and learnability. In particular, we show that a problem is privately learnable if an only if there is a private algorithm that asymptotically minimizes the empirical risk (AERM). In contrast, for non-private learning AERM alone is not sufficient for learnability. This result suggests that when searching for private learning algorithms, we can restrict the search to algorithms that are AERM. In light of this, we propose a conceptual procedure that always finds a universally consistent algorithm whenever the problem is learnable under privacy constraint. We also propose a generic and practical algorithm and show that under very general conditions it privately learns a wide class of learning problems. Lastly, we extend some of the results to the more practical (, δ)-differential privacy and establish the existence of a phase-transition on the class of problems that are approximately privately learnable with respect to how small δ needs to be."
            ],
            "keywords": [
                "differential privacy",
                "learnability",
                "characterization",
                "stability",
                "privacy-preserving machine learning"
            ],
            "author": [
                "Yu-Xiang Wang",
                "Stephen E Fienberg"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-313/15-313.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Equivalence of Linear Dimensionality-Reducing Transformations",
            "abstract": [
                "In this JMLR volume, Ye (2008) demonstrates the essential equivalence of two sets of solutions to a generalized Fisher criterion used for linear dimensionality reduction (see Ye, 2005; Loog, 2007). Here, I point out the basic flaw in this new contribution."
            ],
            "keywords": [
                "linear discriminant analysis",
                "equivalence relation",
                "linear subspaces",
                "Bayes error"
            ],
            "author": [
                "Marco Loog"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/loog08a/loog08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Inducing Tree-Substitution Grammars",
            "abstract": [
                "Inducing a grammar from text has proven to be a notoriously challenging learning task despite decades of research. The primary reason for its difficulty is that in order to induce plausible grammars, the underlying model must be capable of representing the intricacies of language while also ensuring that it can be readily learned from data. The majority of existing work on grammar induction has favoured model simplicity (and thus learnability) over representational capacity by using context free grammars and first order dependency grammars, which are not sufficiently expressive to model many common linguistic constructions. We propose a novel compromise by inferring a probabilistic tree substitution grammar, a formalism which allows for arbitrarily large tree fragments and thereby better represent complex linguistic structures. To limit the model's complexity we employ a Bayesian non-parametric prior which biases the model towards a sparse grammar with shallow productions. We demonstrate the model's efficacy on supervised phrase-structure parsing, where we induce a latent segmentation of the training treebank, and on unsupervised dependency grammar induction. In both cases the model uncovers interesting latent linguistic structures while producing competitive results."
            ],
            "keywords": [
                "grammar induction",
                "tree substitution grammar",
                "Bayesian non-parametrics",
                "Pitman-Yor process",
                "Chinese restaurant process"
            ],
            "author": [
                "Trevor Cohn",
                "Phil Blunsom",
                "Sharon Goldwater"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/cohn10b/cohn10b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Fast Hybrid Algorithm for Large-Scale ℓ 1 -Regularized Logistic Regression",
            "abstract": [
                "ℓ 1-regularized logistic regression, also known as sparse logistic regression, is widely used in machine learning, computer vision, data mining, bioinformatics and neural signal processing. The use of ℓ 1 regularization attributes attractive properties to the classifier, such as feature selection, robustness to noise, and as a result, classifier generality in the context of supervised learning. When a sparse logistic regression problem has large-scale data in high dimensions, it is computationally expensive to minimize the non-differentiable ℓ-norm in the objective function. Motivated by recent work (Koh et al., 2007; Hale et al., 2008), we propose a novel hybrid algorithm based on combining two types of optimization iterations: one being very fast and memory friendly while the other being slower but more accurate. Called hybrid iterative shrinkage (HIS), the resulting algorithm is comprised of a fixed point continuation phase and an interior point phase. The first phase is based completely on memory efficient operations such as matrix-vector multiplications, while the second phase is based on a truncated Newton's method. Furthermore, we show that various optimization techniques, including line search and continuation, can significantly accelerate convergence. The algorithm has global convergence at a geometric rate (a Q-linear rate in optimization terminology). We present a numerical comparison with several existing algorithms, including an analysis using benchmark data from the UCI machine learning repository, and show our algorithm is the most computationally efficient without loss of accuracy."
            ],
            "keywords": [
                "logistic regression",
                "ℓ 1 regularization",
                "fixed point continuation",
                "supervised learning",
                "large scale"
            ],
            "author": [
                "Jianing Shi",
                "Stanley Osher",
                "Paul Sajda"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/shi10a/shi10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks",
            "abstract": [
                "We present a procedure for effective estimation of entropy and mutual information from smallsample data, and apply it to the problem of inferring high-dimensional gene association networks. Specifically, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efficient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator."
            ],
            "keywords": [
                "entropy",
                "shrinkage estimation",
                "James-Stein estimator",
                "\"small n",
                "large p\" setting",
                "mutual information",
                "gene association network"
            ],
            "author": [
                "Jean Hausser",
                "Korbinian Strimmer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/hausser09a/hausser09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Introduction to the Special Issue on Learning Theory",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "Ralf Herbrich"
            ],
            "ref": "http://www.jmlr.org/papers/volume4/herbrich03a/herbrich03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems",
            "abstract": [
                "A metric between time-series distributions is proposed that can be evaluated using binary classification methods, which were originally developed to work on i.i.d. data. It is shown how this metric can be used for solving statistical problems that are seemingly unrelated to classification and concern highly dependent time series. Specifically, the problems of time-series clustering, homogeneity testing and the three-sample problem are addressed. Universal consistency of the resulting algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data."
            ],
            "keywords": [
                "time series",
                "reductions",
                "stationary ergodic",
                "clustering",
                "metrics between probability distributions"
            ],
            "author": [
                "Daniil Ryabko",
                "Jérémie Mary"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/ryabko13a/ryabko13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
            "abstract": [
                "In this paper we introduce the idea of improving the performance of parametric temporaldifference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, we show that varying the emphasis of linear TD(λ)'s updates in a particular way causes its expected update to become stable under off-policy training. The only prior model-free TD methods to achieve this with per-step computation linear in the number of function approximation parameters are the gradient-TD family of methods including TDC, GTD(λ), and GQ(λ). Compared to these methods, our emphatic TD(λ) is simpler and easier to use; it has only one learned parameter vector and one step-size parameter. Our treatment includes general state-dependent discounting and bootstrapping functions, and a way of specifying varying degrees of interest in accurately valuing different states."
            ],
            "keywords": [
                "Temporal-difference learning",
                "Off-policy learning",
                "Function approximation",
                "Stability",
                "Convergence"
            ],
            "author": [
                "Richard S Sutton",
                "A Rupam Mahmood",
                "Martha White"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-488/14-488.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improving the Reliability of Causal Discovery from Small Data Sets Using Argumentation",
            "abstract": [
                "We address the problem of improving the reliability of independence-based causal discovery algorithms that results from the execution of statistical independence tests on small data sets, which typically have low reliability. We model the problem as a knowledge base containing a set of independence facts that are related through Pearl's well-known axioms. Statistical tests on finite data sets may result in errors in these tests and inconsistencies in the knowledge base. We resolve these inconsistencies through the use of an instance of the class of defeasible logics called argumentation, augmented with a preference function, that is used to reason about and possibly correct errors in these tests. This results in a more robust conditional independence test, called an argumentative independence test. Our experimental evaluation shows clear positive improvements in the accuracy of argumentative over purely statistical tests. We also demonstrate significant improvements on the accuracy of causal structure discovery from the outcomes of independence tests both on sampled data from randomly generated causal models and on real-world data sets."
            ],
            "keywords": [
                "independence-based causal discovery",
                "causal Bayesian networks",
                "structure learning",
                "argumentation",
                "reliability improvement"
            ],
            "author": [
                "Facundo Bromberg",
                "Dimitris Margaritis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/bromberg09a/bromberg09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A General Framework for Consistency of Principal Component Analysis",
            "abstract": [
                "A general asymptotic framework is developed for studying consistency properties of principal component analysis (PCA). Our framework includes several previously studied domains of asymptotics as special cases and allows one to investigate interesting connections and transitions among the various domains. More importantly, it enables us to investigate asymptotic scenarios that have not been considered before, and gain new insights into the consistency, subspace consistency and strong inconsistency regions of PCA and the boundaries among them. We also establish the corresponding convergence rate within each region. Under general spike covariance models, the dimension (or number of variables) discourages the consistency of PCA, while the sample size and spike information (the relative size of the population eigenvalues) encourage PCA consistency. Our framework nicely illustrates the relationship among these three types of information in terms of dimension, sample size and spike size, and rigorously characterizes how their relationships affect PCA consistency."
            ],
            "keywords": [
                "High dimension low sample size",
                "PCA",
                "Random matrix",
                "Spike model"
            ],
            "author": [
                "Dan Shen",
                "Haipeng Shen",
                "J S Marron"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-229/14-229.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Word-Sequence Kernels",
            "abstract": [
                "We address the problem of categorising documents using kernel-based methods such as Support Vector Machines. Since the work of Joachims (1998), there is ample experimental evidence that SVM using the standard word frequencies as features yield state-of-the-art performance on a number of benchmark problems. Recently, Lodhi et al. (2002) proposed the use of string kernels, a novel way of computing document similarity based of matching non-consecutive subsequences of characters. In this article, we propose the use of this technique with sequences of words rather than characters. This approach has several advantages, in particular it is more efficient computationally and it ties in closely with standard linguistic pre-processing techniques. We present some extensions to sequence kernels dealing with symbol-dependent and match-dependent decay factors, and present empirical evaluations of these extensions on the Reuters-21578 datasets."
            ],
            "keywords": [
                "Kernel machines",
                "text categorisation",
                "linguistic processing",
                "string kernels",
                "sequence kernels"
            ],
            "author": [
                "Nicola Cancedda",
                "Eric Gaussier",
                "Jaz Kandola",
                "Thomas Hofmann",
                "Tomaso Poggio",
                "John Shawe-Taylor"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/cancedda03a/cancedda03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learnability of Gaussians with Flexible Variances",
            "abstract": [
                "Gaussian kernels with flexible variances provide a rich family of Mercer kernels for learning algorithms. We show that the union of the unit balls of reproducing kernel Hilbert spaces generated by Gaussian kernels with flexible variances is a uniform Glivenko-Cantelli (uGC) class. This result confirms a conjecture concerning learnability of Gaussian kernels and verifies the uniform convergence of many learning algorithms involving Gaussians with changing variances. Rademacher averages and empirical covering numbers are used to estimate sample errors of multi-kernel regularization schemes associated with general loss functions. It is then shown that the regularization error associated with the least square loss and the Gaussian kernels can be greatly improved when flexible variances are allowed. Finally, for regularization schemes generated by Gaussian kernels with flexible variances we present explicit learning rates for regression with least square loss and classification with hinge loss."
            ],
            "keywords": [
                "Gaussian kernel",
                "flexible variances",
                "learning theory",
                "Glivenko-Cantelli class",
                "regularization scheme",
                "empirical covering number"
            ],
            "author": [
                "Yiming Ying",
                "Ding-Xuan Zhou",
                "Peter L Bartlett"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/ying07a/ying07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Preface to this Special Issue",
            "abstract": [
                "This issue of JMLR is devoted to the memory of Alexey Chervonenkis. Over the period of a dozen years between 1962 and 1973 he and Vladimir Vapnik created a new discipline of statistical learning theory the foundation on which all our modern understanding of pattern recognition is based. Alexey was 28 years old when they made their most famous and original discovery, the uniform law of large numbers. In that short period Vapnik and Chervonenkis also introduced the main concepts of statistical learning theory, such as VCdimension, capacity control, and the Structural Risk Minimization principle, and designed two powerful pattern recognition methods, Generalised Portrait and Optimal Separating Hyperplane, later transformed by Vladimir Vapnik into Support Vector Machine arguably one of the best tools for pattern recognition and regression estimation. Thereafter Alexey continued to publish original and important contributions to learning theory. He was also active in research in several applied fields, including geology, bioinformatics, medicine, and advertising. Alexey tragically died in September 2014 after getting lost during a hike in the Elk Island park on the outskirts of Moscow. Vladimir Vapnik suggested to prepare an issue of JMLR to be published at the first anniversary of the death of his long-term collaborator and close friend. Vladimir and the editors contacted a few dozen leading researchers in the fields of machine learning related to Alexey's research interests and had many enthusiastic replies. In the end eleven papers were accepted. This issue also contains a first attempt at a complete bibliography of Alexey Chervonenkis's publications. Simultaneously with this special issue will appear Alexey's Festschrift (Vovk et al., 2015), to which the reader is referred for information about Alexey's research, life, and death. The Festschrift is based in part on a symposium held in Pathos, Cyprus, in 2013 to celebrate Alexey's 75th anniversary. Apart from research contributions, it contains Alexey's reminiscences about his early work on statistical learning with Vladimir Vapnik, a reprint of their seminal 1971 paper, a historical chapter by R. M. Dudley, reminiscences of Alexey's and Vladimir's close colleague Vasily Novoseltsev, and three reviews of various measures of complexity used in machine learning (\"Measures of Complexity\" is both the name of the symposium and the title of the book). Among Alexey's contributions to machine learning (mostly joint with Vladimir Vapnik) discussed in the book are:"
            ],
            "keywords": [],
            "author": [
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/gammerman15a/gammerman15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency",
            "abstract": [
                "We consider the problem of high-dimensional variable selection: given n noisy observations of a k-sparse vector β * ∈ R p , estimate the subset of non-zero entries of β *. A significant body of work has studied behavior of ℓ 1-relaxations when applied to random measurement matrices that are dense (e.g., Gaussian, Bernoulli). In this paper, we analyze sparsified measurement ensembles, and consider the trade-off between measurement sparsity, as measured by the fraction γ of nonzero entries, and the statistical efficiency, as measured by the minimal number of observations n required for correct variable selection with probability converging to one. Our main result is to prove that it is possible to let the fraction on non-zero entries γ → at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row, while retaining the same statistical efficiency as dense ensembles. A variety of simulation results confirm the sharpness of our theoretical predictions."
            ],
            "keywords": [
                "variable selection",
                "sparse random projections",
                "high-dimensional statistics",
                "Lasso",
                "consistency",
                "ℓ 1 -regularization"
            ],
            "author": [
                "Dapo Omidiran",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/omidiran10a/omidiran10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency and Localizability",
            "abstract": [
                "We show that all consistent learning methods-that is, that asymptotically achieve the lowest possible expected loss for any distribution on (X,Y)-are necessarily localizable, by which we mean that they do not significantly change their response at a particular point when we show them only the part of the training set that is close to that point. This is true in particular for methods that appear to be defined in a non-local manner, such as support vector machines in classification and least-squares estimators in regression. Aside from showing that consistency implies a specific form of localizability, we also show that consistency is logically equivalent to the combination of two properties: (1) a form of localizability, and (2) that the method's global mean (over the entire X distribution) correctly estimates the true mean. Consistency can therefore be seen as comprised of two aspects, one local and one global."
            ],
            "keywords": [
                "consistency",
                "local learning",
                "regression",
                "classification"
            ],
            "author": [
                "Alon Zakai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/zakai09a/zakai09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Preference-based Teaching",
            "abstract": [
                "We introduce a new model of teaching named \"preference-based teaching\" and a corresponding complexity parameter-the preference-based teaching dimension (PBTD)-representing the worstcase number of examples needed to teach any concept in a given concept class. Although the PBTD coincides with the well-known recursive teaching dimension (RTD) on finite classes, it is radically different on infinite ones: the RTD becomes infinite already for trivial infinite classes (such as half-intervals) whereas the PBTD evaluates to reasonably small values for a wide collection of infinite classes including classes consisting of so-called closed sets w.r.t. a given closure operator, including various classes related to linear sets over N 0 (whose RTD had been studied quite recently) and including the class of Euclidean half-spaces. On top of presenting these concrete results, we provide the reader with a theoretical framework (of a combinatorial flavor) which helps to derive bounds on the PBTD."
            ],
            "keywords": [
                "teaching dimension",
                "preference relation",
                "recursive teaching dimension",
                "learning halfspaces",
                "linear sets"
            ],
            "author": [
                "Ziyuan Gao",
                "Christoph Ries",
                "Hans U Simon",
                "Sandra Zilles"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-460/16-460.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Marginal Likelihood Integrals for Mixtures of Independence Models",
            "abstract": [
                "Inference in Bayesian statistics involves the evaluation of marginal likelihood integrals. We present algebraic algorithms for computing such integrals exactly for discrete data of small sample size. Our methods apply to both uniform priors and Dirichlet priors. The underlying statistical models are mixtures of independent distributions, or, in geometric language, secant varieties of Segre-Veronese varieties."
            ],
            "keywords": [
                "marginal likelihood",
                "exact integration",
                "mixture of independence model",
                "computational algebra"
            ],
            "author": [
                "Shaowei Lin",
                "Bernd Sturmfels",
                "Zhiqiang Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/lin09a/lin09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "pyDML: A Python Library for Distance Metric Learning",
            "abstract": [
                "pyDML is an open-source python library that provides a wide range of distance metric learning algorithms. Distance metric learning can be useful to improve similarity learning algorithms, such as the nearest neighbors classifier, and also has other applications, like dimensionality reduction. The pyDML package currently provides more than 20 algorithms, which can be categorized, according to their purpose, in: dimensionality reduction algorithms, algorithms to improve nearest neighbors or nearest centroids classifiers, information theory based algorithms or kernel based algorithms, among others. In addition, the library also provides some utilities for the visualization of classifier regions, parameter tuning and a stats website with the performance of the implemented algorithms. The package relies on the scipy ecosystem, it is fully compatible with scikit-learn, and is distributed under GPLv3 license. Source code and documentation can be found at https://github.com/jlsuarezdiaz/pyDML."
            ],
            "keywords": [
                "Distance Metric Learning",
                "Classification",
                "Mahalanobis Distance",
                "Dimensionality",
                "Python"
            ],
            "author": [
                "Juan Luis Suárez",
                "Salvador García",
                "Francisco Herrera"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-864/19-864.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Semi-Supervised Linear Regression in Covariate Shift Problems",
            "abstract": [
                "Semi-supervised learning approaches are trained using the full training (labeled) data and available testing (unlabeled) data. Demonstrations of the value of training with unlabeled data typically depend on a smoothness assumption relating the conditional expectation to high density regions of the marginal distribution and an inherent missing completely at random assumption for the labeling. So-called covariate shift poses a challenge for many existing semi-supervised or supervised learning techniques. Covariate shift models allow the marginal distributions of the labeled and unlabeled feature data to differ, but the conditional distribution of the response given the feature data is the same. An example of this occurs when a complete labeled data sample and then an unlabeled sample are obtained sequentially, as it would likely follow that the distributions of the feature data are quite different between samples. The value of using unlabeled data during training for the elastic net is justified geometrically in such practical covariate shift problems. The approach works by obtaining adjusted coefficients for unlabeled prediction which recalibrate the supervised elastic net to compromise: (i) maintaining elastic net predictions on the labeled data with (ii) shrinking unlabeled predictions to zero. Our approach is shown to dominate linear supervised alternatives on unlabeled response predictions when the unlabeled feature data are concentrated on a low dimensional manifold away from the labeled data and the true coefficient vector emphasizes directions away from this manifold. Large variance of the supervised predictions on the unlabeled set is reduced more than the increase in squared bias when the unlabeled responses are expected to be small, so an improved compromise within the bias-variance tradeoff is the rationale for this performance improvement. Performance is validated on simulated and real data."
            ],
            "keywords": [
                "joint optimization",
                "semi-supervised regression",
                "usefulness of unlabeled data"
            ],
            "author": [
                "Kenneth Joseph Ryan",
                "Mark Vere Culp"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/ryan15a/ryan15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Convergent Online Single Time Scale Actor Critic Algorithm",
            "abstract": [
                "Actor-Critic based approaches were among the first to address reinforcement learning in a general setting. Recently, these algorithms have gained renewed interest due to their generality, good convergence properties, and possible biological relevance. In this paper, we introduce an online temporal difference based actor-critic algorithm which is proved to converge to a neighborhood of a local maximum of the average reward. Linear function approximation is used by the critic in order estimate the value function, and the temporal difference signal, which is passed from the critic to the actor. The main distinguishing feature of the present convergence proof is that both the actor and the critic operate on a similar time scale, while in most current convergence proofs they are required to have very different time scales in order to converge. Moreover, the same temporal difference signal is used to update the parameters of both the actor and the critic. A limitation of the proposed approach, compared to results available for two time scale convergence, is that convergence is guaranteed only to a neighborhood of an optimal value, rather to an optimal value itself. The single time scale and identical temporal difference signal used by the actor and the critic, may provide a step towards constructing more biologically realistic models of reinforcement learning in the brain."
            ],
            "keywords": [
                "actor critic",
                "single time scale convergence",
                "temporal difference"
            ],
            "author": [
                "Dotan Di Castro",
                "Ron Meir"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/dicastro10a/dicastro10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Prototype Learning Algorithms: Theoretical and Experimental Studies",
            "abstract": [
                "In this paper, we propose a number of adaptive prototype learning (APL) algorithms. They employ the same algorithmic scheme to determine the number and location of prototypes, but differ in the use of samples or the weighted averages of samples as prototypes, and also in the assumption of distance measures. To understand these algorithms from a theoretical viewpoint, we address their convergence properties, as well as their consistency under certain conditions. We also present a soft version of APL, in which a non-zero training error is allowed in order to enhance the generalization power of the resultant classifier. Applying the proposed algorithms to twelve UCI benchmark data sets, we demonstrate that they outperform many instance-based learning algorithms, the k-nearest neighbor rule, and support vector machines in terms of average test accuracy."
            ],
            "keywords": [
                "adaptive prototype learning",
                "cluster-based prototypes",
                "consistency",
                "instance-based prototype",
                "pattern classification"
            ],
            "author": [
                "Fu ©2006",
                "Chin-Chin Chang",
                "Chi-Jen Lin",
                "Fu Chang",
                "Chi-Jen Lu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/chang06a/chang06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Memory Efficient Kernel Approximation",
            "abstract": [
                "Scaling kernel machines to massive data sets is a major challenge due to storage and computation issues in handling large kernel matrices, that are usually dense. Recently, many papers have suggested tackling this problem by using a low-rank approximation of the kernel matrix. In this paper, we first make the observation that the structure of shift-invariant kernels changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter. Based on this observation, we propose a new kernel approximation framework-Memory Efficient Kernel Approximation (MEKA), which considers both low-rank and clustering structure of the kernel matrix. We show that the resulting algorithm outperforms state-of-the-art low-rank kernel approximation methods in terms of speed, approximation error, and memory usage. As an example, on the covtype dataset with half a million samples, MEKA takes around 70 seconds and uses less than 80 MB memory on a single machine to achieve 10% relative approximation error, while standard Nyström approximation is about 6 times slower and uses more than 400MB memory to achieve similar approximation. We also present extensive experiments on applying MEKA to speed up kernel ridge regression."
            ],
            "keywords": [
                "kernel approximation",
                "Nyström method",
                "kernel methods"
            ],
            "author": [
                "Si Si",
                "Cho-Jui Hsieh",
                "Inderjit S Dhillon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-025/15-025.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dual Extrapolation for Sparse Generalized Linear Models",
            "abstract": [
                "Generalized Linear Models (GLM) form a wide class of regression and classification models, where prediction is a function of a linear combination of the input variables. For statistical inference in high dimension, sparsity inducing regularizations have proven to be useful while offering statistical guarantees. However, solving the resulting optimization problems can be challenging: even for popular iterative algorithms such as coordinate descent, one needs to loop over a large number of variables. To mitigate this, techniques known as screening rules and working sets diminish the size of the optimization problem at hand, either by progressively removing variables, or by solving a growing sequence of smaller problems. For both techniques, significant variables are identified thanks to convex duality arguments. In this paper, we show that the dual iterates of a GLM exhibit a Vector AutoRegressive (VAR) behavior after sign identification, when the primal problem is solved with proximal gradient descent or cyclic coordinate descent. Exploiting this regularity, one can construct dual points that offer tighter certificates of optimality, enhancing the performance of screening rules and working set algorithms."
            ],
            "keywords": [
                "Convex optimization",
                "extrapolation",
                "screening rules",
                "working sets",
                "Lasso",
                "sparse logistic regression",
                "generalized linear models"
            ],
            "author": [
                "Mathurin Massias",
                "Samuel Vaiter"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-587/19-587.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Neural Autoregressive Distribution Estimation",
            "abstract": [
                "We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE."
            ],
            "keywords": [
                "deep learning",
                "neural networks",
                "density modeling",
                "unsupervised learning"
            ],
            "author": [
                "Benigno Uria",
                "Marc-Alexandre Côté",
                "Karol Gregor",
                "Iain Murray"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-272/16-272.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Inference on Random Dot Product Graphs: a Survey",
            "abstract": [
                "The random dot product graph (RDPG) is an independent-edge random graph that is analytically tractable and, simultaneously, either encompasses or can successfully approximate"
            ],
            "keywords": [
                "Random dot product graph",
                "adjacency spectral embedding",
                "Laplacian spectral embedding",
                "multi-sample graph hypothesis testing",
                "semiparametric modeling"
            ],
            "author": [
                "Avanti Athreya",
                "Youngser Park",
                "Joshua T Vogelstein",
                "Keith Levin",
                "Vince Lyzinski",
                "Yichen Qin",
                "Daniel L Sussman",
                "Avanti ©2018",
                "Donniell E Athreya",
                "Keith Fishkind",
                "Vince Levin",
                "Yichen Lyzinski",
                "Youngser Qin",
                "Daniel L Park",
                "Minh Sussman",
                "Joshua T Tang",
                "Carey E Vogelstein"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-448/17-448.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Human Gesture Recognition on Product Manifolds",
            "abstract": [
                "Action videos are multidimensional data and can be naturally represented as data tensors. While tensor computing is widely used in computer vision, the geometry of tensor space is often ignored. The aim of this paper is to demonstrate the importance of the intrinsic geometry of tensor space which yields a very discriminating structure for action recognition. We characterize data tensors as points on a product manifold and model it statistically using least squares regression. To this aim, we factorize a data tensor relating to each order of the tensor using Higher Order Singular Value Decomposition (HOSVD) and then impose each factorized element on a Grassmann manifold. Furthermore, we account for underlying geometry on manifolds and formulate least squares regression as a composite function. This gives a natural extension from Euclidean space to manifolds. Consequently, classification is performed using geodesic distance on a product manifold where each factor manifold is Grassmannian. Our method exploits appearance and motion without explicitly modeling the shapes and dynamics. We assess the proposed method using three gesture databases, namely the Cambridge hand-gesture, the UMD Keck body-gesture, and the CHALEARN gesture challenge data sets. Experimental results reveal that not only does the proposed method perform well on the standard benchmark data sets, but also it generalizes well on the one-shot-learning gesture challenge. Furthermore, it is based on a simple statistical model and the intrinsic geometry of tensor space."
            ],
            "keywords": [
                "gesture recognition",
                "action recognition",
                "Grassmann manifolds",
                "product manifolds",
                "one-shot-learning",
                "kinect data"
            ],
            "author": [
                "Yui Man Lui",
                "Isabelle Guyon",
                "Vassilis Athitsos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/lui12a/lui12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gini Support Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression Shantanu Chakrabartty",
            "abstract": [
                "Many classification tasks require estimation of output class probabilities for use as confidence scores or for inference integrated with other models. Probability estimates derived from large margin classifiers such as support vector machines (SVMs) are often unreliable. We extend SVM large margin classification to GiniSVM maximum entropy multi-class probability regression. GiniSVM combines a quadratic (Gini-Simpson) entropy based agnostic model with a kernel based similarity model. A form of Huber loss in the GiniSVM primal formulation elucidates a connection to robust estimation, further corroborated by the impulsive noise filtering property of the reverse water-filling procedure to arrive at normalized classification margins. The GiniSVM normalized classification margins directly provide estimates of class conditional probabilities, approximating kernel logistic regression (KLR) but at reduced computational cost. As with other SVMs, GiniSVM produces a sparse kernel expansion and is trained by solving a quadratic program under linear constraints. GiniSVM training is efficiently implemented by sequential minimum optimization or by growth transformation on probability functions. Results on synthetic and benchmark data, including speaker verification and face detection data, show improved classification performance and increased tolerance to imprecision over soft-margin SVM and KLR."
            ],
            "keywords": [
                "support vector machines",
                "large margin classifiers",
                "kernel regression",
                "probabilistic models",
                "quadratic entropy",
                "Gini index",
                "growth transformation"
            ],
            "author": [
                "Shantanu @ Msu",
                "Gert Cauwenberghs",
                "Gert @ Ucsd"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/chakrabartty07a/chakrabartty07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Instrumental Variables with Structural and Non-Gaussianity Assumptions",
            "abstract": [
                "Learning a causal effect from observational data requires strong assumptions. One possible method is to use instrumental variables, which are typically justified by background knowledge. It is possible, under further assumptions, to discover whether a variable is structurally instrumental to a target causal effect X → Y. However, the few existing approaches are lacking on how general these assumptions can be, and how to express possible equivalence classes of solutions. We present instrumental variable discovery methods that systematically characterize which set of causal effects can and cannot be discovered under local graphical criteria that define instrumental variables, without reconstructing full causal graphs. We also introduce the first methods to exploit non-Gaussianity assumptions, highlighting identifiability problems and solutions. Due to the difficulty of estimating such models from finite data, we investigate how to strengthen assumptions in order to make the statistical problem more manageable."
            ],
            "keywords": [
                "causality",
                "causal discovery",
                "instrumental variables"
            ],
            "author": [
                "Ricardo Silva"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-014/17-014.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Locally Defined Principal Curves and Surfaces",
            "abstract": [
                "Principal curves are defined as self-consistent smooth curves passing through the middle of the data, and they have been used in many applications of machine learning as a generalization, dimensionality reduction and a feature extraction tool. We redefine principal curves and surfaces in terms of the gradient and the Hessian of the probability density estimate. This provides a geometric understanding of the principal curves and surfaces, as well as a unifying view for clustering, principal curve fitting and manifold learning by regarding those as principal manifolds of different intrinsic dimensionalities. The theory does not impose any particular density estimation method can be used with any density estimator that gives continuous first and second derivatives. Therefore, we first present our principal curve/surface definition without assuming any particular density estimation method. Afterwards, we develop practical algorithms for the commonly used kernel density estimation (KDE) and Gaussian mixture models (GMM). Results of these algorithms are presented in notional data sets as well as real applications with comparisons to other approaches in the principal curve literature. All in all, we present a novel theoretical understanding of principal curves and surfaces, practical algorithms as general purpose machine learning tools, and applications of these algorithms to several practical problems."
            ],
            "keywords": [
                "unsupervised learning",
                "dimensionality reduction",
                "principal curves",
                "principal surfaces",
                "subspace constrained mean-shift"
            ],
            "author": [
                "Umut Ozertem",
                "Deniz Erdogmus"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/ozertem11a/ozertem11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral Learning of Latent-Variable PCFGs: Algorithms and Sample Complexity",
            "abstract": [
                "We introduce a spectral learning algorithm for latent-variable PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006). Under a separability (singular value) condition, we prove that the method provides statistically consistent parameter estimates. Our result rests on three theorems: the first gives a tensor form of the inside-outside algorithm for PCFGs; the second shows that the required tensors can be estimated directly from training examples where hidden-variable values are missing; the third gives a PAC-style convergence bound for the estimation method."
            ],
            "keywords": [],
            "author": [
                "Shay B Cohen",
                "Michael Collins",
                "Dean P Foster",
                "Lyle Ungar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/cohen14a/cohen14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Network Community Detection with Continuous Optimization of Conductance and Weighted Kernel K-Means",
            "abstract": [
                "Local network community detection is the task of finding a single community of nodes concentrated around few given seed nodes in a localized way. Conductance is a popular objective function used in many algorithms for local community detection. This paper studies a continuous relaxation of conductance. We show that continuous optimization of this objective still leads to discrete communities. We investigate the relation of conductance with weighted kernel k-means for a single community, which leads to the introduction of a new objective function, σ-conductance. Conductance is obtained by setting σ to 0. Two algorithms, EMc and PGDc, are proposed to locally optimize σ-conductance and automatically tune the parameter σ. They are based on expectation maximization and projected gradient descent, respectively. We prove locality and give performance guarantees for EMc and PGDc for a class of dense and well separated communities centered around the seeds. Experiments are conducted on networks with ground-truth communities, comparing to state-of-the-art graph diffusion algorithms for conductance optimization. On large graphs, results indicate that EMc and PGDc stay localized and produce communities most similar to the ground, while graph diffusion algorithms generate large communities of lower quality. 1"
            ],
            "keywords": [
                "community detection",
                "conductance",
                "k-means"
            ],
            "author": [
                "Twan Van Laarhoven",
                "Elena Marchiori"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-043/16-043.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data",
            "abstract": [
                "In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges-a distributed state representation as in dynamic Bayesian networks (DBNs)-and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data. In addition to maximum conditional likelihood, we present two alternative approaches for training DCRFs: marginal likelihood training, for when we are primarily interested in predicting only a subset of the variables, and cascaded training, for when we have a distinct data set for each state variable, as in transfer learning. We evaluate marginal training and cascaded training on both synthetic data and real-world text data, finding that marginal training can improve accuracy when uncertainty exists over the latent variables, and that for transfer learning, a DCRF trained in a cascaded fashion performs better than a linear-chain CRF that predicts the final task directly."
            ],
            "keywords": [
                "conditional random fields",
                "graphical models",
                "sequence labeling"
            ],
            "author": [
                "Charles Sutton",
                "Andrew Mccallum",
                "Khashayar Rohanimanesh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/sutton07a/sutton07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pairwise Support Vector Machines and their Application to Large Scale Problems",
            "abstract": [
                "Pairwise classification is the task to predict whether the examples a, b of a pair (a, b) belong to the same class or to different classes. In particular, interclass generalization problems can be treated in this way. In pairwise classification, the order of the two input examples should not affect the classification result. To achieve this, particular kernels as well as the use of symmetric training sets in the framework of support vector machines were suggested. The paper discusses both approaches in a general way and establishes a strong connection between them. In addition, an efficient implementation is discussed which allows the training of several millions of pairs. The value of these contributions is confirmed by excellent results on the labeled faces in the wild benchmark."
            ],
            "keywords": [
                "pairwise support vector machines",
                "interclass generalization",
                "pairwise kernels",
                "large scale problems"
            ],
            "author": [
                "Carl Brunner",
                "Andreas Fischer",
                "Klaus Luig"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/brunner12a/brunner12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Clustering Partially Observed Graphs via Convex Optimization",
            "abstract": [
                "This paper considers the problem of clustering a partially observed unweighted graph-i.e., one where for some node pairs we know there is an edge between them, for some others we know there is no edge, and for the remaining we do not know whether or not there is an edge. We want to organize the nodes into disjoint clusters so that there is relatively dense (observed) connectivity within clusters, and sparse across clusters. We take a novel yet natural approach to this problem, by focusing on finding the clustering that minimizes the number of \"disagreements\"-i.e., the sum of the number of (observed) missing edges within clusters, and (observed) present edges across clusters. Our algorithm uses convex optimization; its basis is a reduction of disagreement minimization to the problem of recovering an (unknown) low-rank matrix and an (unknown) sparse matrix from their partially observed sum. We evaluate the performance of our algorithm on the classical Planted Partition/Stochastic Block Model. Our main theorem provides sufficient conditions for the success of our algorithm as a function of the minimum cluster size, edge density and observation probability; in particular, the results characterize the tradeoff between the observation probability and the edge density gap. When there are a constant number of clusters of equal size, our results are optimal up to logarithmic factors."
            ],
            "keywords": [
                "graph clustering",
                "convex optimization",
                "sparse and low-rank decomposition"
            ],
            "author": [
                "Yudong Chen",
                "Ali Jalali",
                "Huan Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/chen14a/chen14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonlinear Models Using Dirichlet Process Mixtures",
            "abstract": [
                "We introduce a new nonlinear model for classification, in which we model the joint distribution of response variable, y, and covariates, x, non-parametrically using Dirichlet process mixtures. We keep the relationship between y and x linear within each component of the mixture. The overall relationship becomes nonlinear if the mixture contains more than one component, with different regression coefficients. We use simulated data to compare the performance of this new approach to alternative methods such as multinomial logit (MNL) models, decision trees, and support vector machines. We also evaluate our approach on two classification problems: identifying the folding class of protein sequences and detecting Parkinson's disease. Our model can sometimes improve predictive accuracy. Moreover, by grouping observations into sub-populations (i.e., mixture components), our model can sometimes provide insight into hidden structure in the data."
            ],
            "keywords": [
                "mixture models",
                "Dirichlet process",
                "classification"
            ],
            "author": [
                "Babak Shahbaba",
                "Radford Neal",
                "Radford @ Stat"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/shahbaba09a/shahbaba09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergences of Regularized Algorithms and Stochastic Gradient Methods with Random Projections",
            "abstract": [
                "We study the least-squares regression problem over a Hilbert space, covering nonparametric regression over a reproducing kernel Hilbert space as a special case. We first investigate regularized algorithms adapted to a projection operator on a closed subspace of the Hilbert space. We prove convergence results with respect to variants of norms, under a capacity assumption on the hypothesis space and a regularity condition on the target function. As a result, we obtain optimal rates for regularized algorithms with randomized sketches, provided that the sketch dimension is proportional to the effective dimension up to a logarithmic factor. As a byproduct, we obtain similar results for Nyström regularized algorithms. Our results provide optimal, distribution-dependent rates that do not have any saturation effect for sketched/Nyström regularized algorithms, considering both the attainable and non-attainable cases, in the well-conditioned regimes. We then study stochastic gradient methods with projection over the subspace, allowing multi-pass over the data and minibatches, and we derive similar optimal statistical convergence results."
            ],
            "keywords": [
                "kernel methods",
                "regularized algorithms",
                "stochastic gradient methods",
                "random projection",
                "sketching"
            ],
            "author": [
                "Junhong Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-083/19-083.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Provably Accurate Double-Sparse Coding",
            "abstract": [
                "Sparse coding is a crucial subroutine in algorithms for various signal processing, deep learning, and other machine learning applications. The central goal is to learn an overcomplete dictionary that can sparsely represent a given input dataset. However, a key challenge is that storage, transmission, and processing of the learned dictionary can be untenably high if the data dimension is high. In this paper, we consider the double-sparsity model introduced by Rubinstein et al. (2010b) where the dictionary itself is the product of a fixed, known basis and a data-adaptive sparse component. First, we introduce a simple algorithm for double-sparse coding that can be amenable to efficient implementation via neural architectures. Second, we theoretically analyze its performance and demonstrate asymptotic sample complexity and running time benefits over existing (provable) approaches for sparse coding. To our knowledge, our work introduces the first computationally efficient algorithm for double-sparse coding that enjoys rigorous statistical guarantees. Finally, we corroborate our theory with several numerical experiments on simulated data, suggesting that our method may be useful for problem sizes encountered in practice."
            ],
            "keywords": [
                "Sparse coding",
                "provable algorithms",
                "unsupervised learning"
            ],
            "author": [
                "Thanh V Nguyen",
                "Raymond K W Wong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-728/17-728.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the consistency of graph-based Bayesian semi-supervised learning and the scalability of sampling algorithms",
            "abstract": [
                "This paper considers a Bayesian approach to graph-based semi-supervised learning. We show that if the graph parameters are suitably scaled, the graph-posteriors converge to a continuum limit as the size of the unlabeled data set grows. This consistency result has profound algorithmic implications: we prove that when consistency holds, carefully designed Markov chain Monte Carlo algorithms have a uniform spectral gap, independent of the number of unlabeled inputs. Numerical experiments illustrate and complement the theory."
            ],
            "keywords": [
                "semi-supervised learning",
                "graph-based learning",
                "Markov chain Monte Carlo",
                "spectral gap"
            ],
            "author": [
                "García Nicolás",
                "Zachary Kaplan",
                "Thabo Samakhoana",
                "Daniel Sanz-Alonso"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/17-698/17-698.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Consistency of Ordinal Regression Methods",
            "abstract": [
                "Many of the ordinal regression models that have been proposed in the literature can be seen as methods that minimize a convex surrogate of the zero-one, absolute, or squared loss functions. A key property that allows to study the statistical implications of such approximations is that of Fisher consistency. Fisher consistency is a desirable property for surrogate loss functions and implies that in the population setting, i.e., if the probability distribution that generates the data were available, then optimization of the surrogate would yield the best possible model. In this paper we will characterize the Fisher consistency of a rich family of surrogate loss functions used in the context of ordinal regression, including support vector ordinal regression, ORBoosting and least absolute deviation. We will see that, for a family of surrogate loss functions that subsumes support vector ordinal regression and ORBoosting, consistency can be fully characterized by the derivative of a real-valued function at zero, as happens for convex margin-based surrogates in binary classification. We also derive excess risk bounds for a surrogate of the absolute error that generalize existing risk bounds for binary classification. Finally, our analysis suggests a novel surrogate of the squared error loss. We compare this novel surrogate with competing approaches on 9 different datasets. Our method shows to be highly competitive in practice, outperforming the least squares loss on 7 out of 9 datasets."
            ],
            "keywords": [
                "Fisher consistency",
                "ordinal regression",
                "calibration",
                "surrogate loss",
                "excess risk bound"
            ],
            "author": [
                "Fabian Pedregosa",
                "Francis Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-495/15-495.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear State-Space Models for Blind Source Separation",
            "abstract": [
                "We apply a type of generative modelling to the problem of blind source separation in which prior knowledge about the latent source signals, such as time-varying auto-correlation and quasiperiodicity, are incorporated into a linear state-space model. In simulations, we show that in terms of signal-to-error ratio, the sources are inferred more accurately as a result of the inclusion of strong prior knowledge. We explore different schemes of maximum-likelihood optimization for the purpose of learning the model parameters. The Expectation Maximization algorithm, which is often considered the standard optimization method in this context, results in slow convergence when the noise variance is small. In such scenarios, quasi-Newton optimization yields substantial improvements in a range of signal to noise ratios. We analyze the performance of the methods on convolutive mixtures of speech signals."
            ],
            "keywords": [
                "blind source separation",
                "state-space model",
                "independent component analysis",
                "convolutive model",
                "EM",
                "speech modelling"
            ],
            "author": [
                "Rasmus Kongsgaard Olsson",
                "Lars Kai Hansen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/olsson06a/olsson06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PAC-Bayesian Generalisation Error Bounds for Gaussian Process Classification",
            "abstract": [
                "Approximate Bayesian Gaussian process (GP) classification techniques are powerful nonparametric learning methods, similar in appearance and performance to support vector machines. Based on simple probabilistic models, they render interpretable results and can be embedded in Bayesian frameworks for model selection, feature selection, etc. In this paper, by applying the PAC-Bayesian theorem of McAllester (1999a), we prove distributionfree generalisation error bounds for a wide range of approximate Bayesian GP classification techniques. We also provide a new and much simplified proof for this powerful theorem, making use of the concept of convex duality which is a backbone of many machine learning techniques. We instantiate and test our bounds for two particular GPC techniques, including a recent sparse method which circumvents the unfavourable scaling of standard GP algorithms. As is shown in experiments on a real-world task, the bounds can be very tight for moderate training sample sizes. To the best of our knowledge, these results provide the tightest known distribution-free error bounds for approximate Bayesian GPC methods, giving a strong learning-theoretical justification for the use of these techniques."
            ],
            "keywords": [
                "Gaussian Processes",
                "Generalisation Error Bounds",
                "PAC-Bayesian Framework",
                "Bayesian Learning",
                "Sparse Approximations",
                "Gibbs Classifier",
                "Kernel Machines",
                "Convex Duality"
            ],
            "author": [
                "Matthias Seeger"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/seeger02a/seeger02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Second-Order Bilinear Discriminant Analysis",
            "abstract": [
                "Traditional analysis methods for single-trial classification of electro-encephalography (EEG) focus on two types of paradigms: phase-locked methods, in which the amplitude of the signal is used as the feature for classification, that is, event related potentials; and second-order methods, in which the feature of interest is the power of the signal, that is, event related (de)synchronization. The process of deciding which paradigm to use is ad hoc and is driven by assumptions regarding the underlying neural generators. Here we propose a method that provides an unified framework for the analysis of EEG, combining first and second-order spatial and temporal features based on a bilinear model. Evaluation of the proposed method on simulated data shows that the technique outperforms state-of-the art techniques for single-trial classification for a broad range of signal-to-noise ratios. Evaluations on human EEG-including one benchmark data set from the Brain Computer Interface (BCI) competition-show statistically significant gains in classification accuracy, with a reduction in overall classification error from 26%-28% to 19%."
            ],
            "keywords": [
                "regularization",
                "classification",
                "bilinear decomposition",
                "neural signals",
                "brain computer interface"
            ],
            "author": [
                "Christoforos Christoforou",
                "Robert Haralick",
                "Paul Sajda",
                "Lucas C Parra"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/christoforou10a/christoforou10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Simpler Approach to Matrix Completion",
            "abstract": [
                "This paper provides the best bounds to date on the number of randomly sampled entries required to reconstruct an unknown low-rank matrix. These results improve on prior work by Candès and Recht (2009), Candès and Tao (2009), and Keshavan et al. (2009). The reconstruction is accomplished by minimizing the nuclear norm, or sum of the singular values, of the hidden matrix subject to agreement with the provided entries. If the underlying matrix satisfies a certain incoherence condition, then the number of entries required is equal to a quadratic logarithmic factor times the number of parameters in the singular value decomposition. The proof of this assertion is short, self contained, and uses very elementary analysis. The novel techniques herein are based on recent work in quantum information theory."
            ],
            "keywords": [
                "matrix completion",
                "low-rank matrices",
                "convex optimization",
                "nuclear norm minimization",
                "random matrices",
                "operator Chernoff bound",
                "compressed sensing"
            ],
            "author": [
                "Benjamin Recht"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/recht11a/recht11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Change Point Problems in Linear Dynamical Systems",
            "abstract": [
                "We study the problem of learning two regimes (we have a normal and a prefault regime in mind) based on a train set of non-Markovian observation sequences. Key to the model is that we assume that once the system switches from the normal to the prefault regime it cannot restore and will eventually result in a fault. We refer to the particular setting as semi-supervised since we assume the only information given to the learner is whether a particular sequence ended with a stop (implying that the sequence was generated by the normal regime) or with a fault (implying that there was a switch from the normal to the fault regime). In the latter case the particular time point at which a switch occurred is not known. The underlying model used is a switching linear dynamical system (SLDS). The constraints in the regime transition probabilities result in an exact inference procedure that scales quadratically with the length of a sequence. Maximum aposteriori (MAP) parameter estimates can be found using an expectation maximization (EM) algorithm with this inference algorithm in the E-step. For long sequences this will not be practically feasible and an approximate inference and an approximate EM procedure is called for. We describe a flexible class of approximations corresponding to different choices of clusters in a Kikuchi free energy with weak consistency constraints."
            ],
            "keywords": [
                "change point problems",
                "switching linear dynamical systems",
                "strong junction trees",
                "approximate inference",
                "expectation propagation",
                "Kikuchi free energies"
            ],
            "author": [
                "Onno Zoeter",
                "Tom Heskes"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/zoeter05a/zoeter05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Effective Ways to Build and Evaluate Individual Survival Distributions",
            "abstract": [
                "An accurate model of a patient's individual survival distribution can help determine the appropriate treatment for terminal patients. Unfortunately, risk scores (for example from Cox Proportional Hazard models) do not provide survival probabilities, single-time probability models (for instance the Gail model, predicting 5 year probability) only provide for a single time point, and standard Kaplan-Meier survival curves provide only population averages for a large class of patients, meaning they are not specific to individual patients. This motivates an alternative class of tools that can learn a model that provides an individual survival distribution for each subject, which gives survival probabilities across all times, such as extensions to the Cox model, Accelerated Failure Time, an extension to Random Survival Forests, and Multi-Task Logistic Regression. This paper first motivates such \"individual survival distribution\" (isd) models, and explains how they differ from standard models. It then discusses ways to evaluate such models-namely Concordance, 1-Calibration, Integrated Brier score, and versions of L1-loss-then motivates and defines a novel approach, \"D-Calibration\", which determines whether a model's probability estimates are meaningful. We also discuss how these measures differ, and use them to evaluate several isd prediction tools over a range of survival data sets. We also provide a code base for all of these survival models and evaluation measures, at https://github.com/haiderstats/ISDEvaluation."
            ],
            "keywords": [
                "Survival analysis",
                "risk model",
                "patient-specific survival prediction",
                "calibration",
                "discrimination"
            ],
            "author": [
                "Humza Haider",
                "Bret Hoehn",
                "Sarah Davis",
                "Russell Greiner"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-772/18-772.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Neural Architecture Search: A Survey",
            "abstract": [
                "Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy."
            ],
            "keywords": [
                "Neural Architecture Search",
                "AutoML",
                "AutoDL",
                "Search Space Design",
                "Search Strategy",
                "Performance Estimation Strategy"
            ],
            "author": [
                "Thomas Elsken",
                "Jan Hendrik Metzen",
                "Frank Hutter"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-598/18-598.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classification of Imbalanced Data with a Geometric Digraph Family",
            "abstract": [
                "We use a geometric digraph family called class cover catch digraphs (CCCDs) to tackle the class imbalance problem in statistical classification. CCCDs provide graph theoretic solutions to the class cover problem and have been employed in classification. We assess the classification performance of CCCD classifiers by extensive Monte Carlo simulations, comparing them with other classifiers commonly used in the literature. In particular, we show that CCCD classifiers perform relatively well when one class is more frequent than the other in a two-class setting, an example of the class imbalance problem. We also point out the relationship between class imbalance and class overlapping problems, and their influence on the performance of CCCD classifiers and other classification methods as well as some state-of-the-art algorithms which are robust to class imbalance by construction. Experiments on both simulated and real data sets indicate that CCCD classifiers are robust to the class imbalance problem. CCCDs substantially undersample from the majority class while preserving the information on the discarded points during the undersampling process. Many state-of-the-art methods, however, keep this information by means of ensemble classifiers, but CCCDs yield only a single classifier with the same property, making it both appealing and fast."
            ],
            "keywords": [
                "Class Cover Catch Digraphs",
                "Class Cover Problem",
                "Class Imbalance Problem",
                "Class Overlapping Problem",
                "Graph Domination",
                "Prototype Selection",
                "Support Estimation"
            ],
            "author": [
                "Artür Manukyan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-604/15-604.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming",
            "abstract": [
                "Sparse additive models are families of d-variate functions with the additive decomposition f * = ∑ j∈S f * j , where S is an unknown subset of cardinality s ≪ d. In this paper, we consider the case where each univariate component function f * j lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f * based on kernels combined with ℓ 1-type convex regularization. Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L 2 (P) and L 2 (P n) norms over the class F d,s,H of sparse additive models with each univariate function f * j in the unit ball of a univariate RKHS with bounded kernel function. We complement our upper bounds by deriving minimax lower bounds on the L (P) error, thereby showing the optimality of our method. Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much faster estimation rates are possible for any sparsity s = Ω(√ n), showing that global boundedness is a significant restriction in the high-dimensional setting."
            ],
            "keywords": [
                "sparsity",
                "kernel",
                "non-parametric",
                "convex",
                "minimax"
            ],
            "author": [
                "Garvesh Raskutti",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/raskutti12a/raskutti12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Frames, Reproducing Kernels, Regularization and Learning",
            "abstract": [
                "This work deals with a method for building a reproducing kernel Hilbert space (RKHS) from a Hilbert space with frame elements having special properties. Conditions on existence and a method of construction are given. Then, these RKHS are used within the framework of regularization theory for function approximation. Implications on semiparametric estimation are discussed and a multiscale scheme of regularization is also proposed. Results on toy and real-world approximation problems illustrate the effectiveness of such methods."
            ],
            "keywords": [
                "regularization",
                "kernel",
                "frames",
                "wavelets"
            ],
            "author": [
                "Alain Rakotomamonjy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/rakotomamonjy05a/rakotomamonjy05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Goodness-of-Fit Tests for Random Partitions via Symmetric Polynomials",
            "abstract": [
                "We consider goodness-of-fit tests with i.i.d. samples generated from a categorical distribution (p 1 , ..., p k). For a given (q 1 , ..., q k), we test the null hypothesis whether p j = q π(j) for some label permutation π. The uncertainty of label permutation implies that the null hypothesis is composite instead of being singular. In this paper, we construct a testing procedure using statistics that are defined as indefinite integrals of some symmetric polynomials. This method is aimed directly at the invariance of the problem, and avoids the need of matching the unknown labels. The asymptotic distribution of the testing statistic is shown to be chi-squared, and its power is proved to be nearly optimal under a local alternative hypothesis. Various degenerate structures of the null hypothesis are carefully analyzed in the paper. A two-sample version of the test is also studied."
            ],
            "keywords": [
                "hypothesis testing",
                "elementary symmetric polynomials",
                "Lagrange interpolating polynomials",
                "Vandermonde matrix",
                "minimax optimality"
            ],
            "author": [
                "Chao Gao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-624/17-624.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exploitation of Machine Learning Techniques in Modelling Phrase Movements for Machine Translation",
            "abstract": [
                "We propose a distance phrase reordering model (DPR) for statistical machine translation (SMT), where the aim is to learn the grammatical rules and context dependent changes using a phrase reordering classification framework. We consider a variety of machine learning techniques, including state-of-the-art structured prediction methods. Techniques are compared and evaluated on a Chinese-English corpus, a language pair known for the high reordering characteristics which cannot be adequately captured with current models. In the reordering classification task, the method significantly outperforms the baseline against which it was tested, and further, when integrated as a component of the state-of-the-art machine translation system, MOSES, it achieves improvement in translation results."
            ],
            "keywords": [
                "statistical machine translation (SMT)",
                "phrase reordering",
                "lexicalized reordering (LR)",
                "maximum entropy (ME)",
                "support vector machine (SVM)",
                "maximum margin regression (MMR)",
                "max-margin structure learning (MMS)"
            ],
            "author": [
                "Yizhao Ni",
                "Craig Saunders",
                "Sandor Szedmak"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/ni11a/ni11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets",
            "abstract": [
                "Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the 1 and 2 norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer f eature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable to sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. We also develop a screening method-called DPC (decomposition of convex set)-for nonnegative Lasso. Experiments on both synthetic and real data sets show that TLFre and DPC improve the efficiency of SGL and nonnegative Lasso by several orders of magnitude."
            ],
            "keywords": [
                "Sparse",
                "Sparse Group Lasso",
                "Screening",
                "Fenchel's Dual",
                "Decomposition",
                "Convex Sets",
                "Composite Function Optimization"
            ],
            "author": [
                "Jie Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/16-383/16-383.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Joint Estimation of Multiple Precision Matrices with Common Structures",
            "abstract": [
                "Estimation of inverse covariance matrices, known as precision matrices, is important in various areas of statistical analysis. In this article, we consider estimation of multiple precision matrices sharing some common structures. In this setting, estimating each precision matrix separately can be suboptimal as it ignores potential common structures. This article proposes a new approach to parameterize each precision matrix as a sum of common and unique components and estimate multiple precision matrices in a constrained l 1 minimization framework. We establish both estimation and selection consistency of the proposed estimator in the high dimensional setting. The proposed estimator achieves a faster convergence rate for the common structure in certain cases. Our numerical examples demonstrate that our new estimator can perform better than several existing methods in terms of the entropy loss and Frobenius loss. An application to a glioblastoma cancer data set reveals some interesting gene networks across multiple cancer subtypes."
            ],
            "keywords": [
                "covariance matrix",
                "graphical model",
                "high dimension",
                "joint estimation",
                "precision matrix"
            ],
            "author": [
                "Wonyul Lee",
                "Yufeng Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/lee15a/lee15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hash Kernels for Structured Data",
            "abstract": [
                "We propose hashing to facilitate efficient kernels. This generalizes previous work using sampling and we show a principled way to compute the kernel matrix for data streams and sparse feature spaces. Moreover, we give deviation bounds from the exact kernel matrix. This has applications to estimation on strings and graphs."
            ],
            "keywords": [
                "hashing",
                "stream",
                "string kernel",
                "graphlet kernel",
                "multiclass classification"
            ],
            "author": [
                "Qinfeng Shi",
                "James Petterson",
                "John Langford",
                "Alex Smola",
                "S V N Vishwanathan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/shi09a/shi09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Combination of Probabilistic Classifiers using Multivariate Normal Mixtures",
            "abstract": [
                "Ensemble methods are a powerful tool, often outperforming individual prediction models. Existing Bayesian ensembles either do not model the correlations between sources, or they are only capable of combining non-probabilistic predictions. We propose a new model, which overcomes these disadvantages. Transforming the probabilistic predictions with the inverse additive logistic transformation allows us to model the correlations with multivariate normal mixtures. We derive an efficient Gibbs sampler for the proposed model and implement a regularization method to make it more robust. We compare our method to related work and the classical linear opinion pool. Empirical evaluation on several toy and real-world data sets, including a case study on air-pollution forecasting, shows that the method outperforms other methods, while being robust and easy to use."
            ],
            "keywords": [
                "correlated classifiers",
                "ensemble",
                "probabilistic models",
                "additive logistic transformation",
                "Bayesian inference"
            ],
            "author": [
                "Gregor Pirš"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-241/18-241.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Smooth neighborhood recommender systems",
            "abstract": [
                "Recommender systems predict users' preferences over a large number of items by pooling similar information from other users and/or items in the presence of sparse observations. One major challenge is how to utilize user-item specific covariates and networks describing user-item interactions in a high-dimensional situation, for accurate personalized prediction. In this article, we propose a smooth neighborhood recommender in the framework of the latent factor models. A similarity kernel is utilized to borrow neighborhood information from continuous covariates over a user-item specific network, such as a user's social network, where the grouping information defined by discrete covariates is also integrated through the network. Consequently, user-item specific information is built into the recommender to battle the 'cold-start\" issue in the absence of observations in collaborative and contentbased filtering. Moreover, we utilize a \"divide-and-conquer\" version of the alternating least squares algorithm to achieve scalable computation, and establish asymptotic results for the proposed method, demonstrating that it achieves superior prediction accuracy. Finally, we illustrate that the proposed method improves substantially over its competitors in simulated examples and real benchmark data-Last.fm music data."
            ],
            "keywords": [
                "Blockwise coordinate decent",
                "Cold-start",
                "Kernel smoothing",
                "Neighborhood",
                "Personalized prediction",
                "Singular value decomposition",
                "Social networks"
            ],
            "author": [
                "Ben Dai",
                "Junhui Wang",
                "Xiaotong Shen",
                "Annie Qu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-629/17-629.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Semi-supervised Learning Using Conjugate Functions Shiliang Sun",
            "abstract": [
                "In this paper, we propose a general framework for sparse semi-supervised learning, which concerns using a small portion of unlabeled data and a few labeled data to represent target functions and thus has the merit of accelerating function evaluations when predicting the output of a new example. This framework makes use of Fenchel-Legendre conjugates to rewrite a convex insensitive loss involving a regularization with unlabeled data, and is applicable to a family of semi-supervised learning methods such as multi-view co-regularized least squares and single-view Laplacian support vector machines (SVMs). As an instantiation of this framework, we propose sparse multi-view SVMs which use a squared ε-insensitive loss. The resultant optimization is an inf-sup problem and the optimal solutions have arguably saddle-point properties. We present a globally optimal iterative algorithm to optimize the problem. We give the margin bound on the generalization error of the sparse multi-view SVMs, and derive the empirical Rademacher complexity for the induced function class. Experiments on artificial and real-world data show their effectiveness. We further give a sequential training approach to show their possibility and potential for uses in large-scale problems and provide encouraging experimental results indicating the efficacy of the margin bound and empirical Rademacher complexity on characterizing the roles of unlabeled data for semi-supervised learning."
            ],
            "keywords": [
                "semi-supervised learning",
                "Fenchel-Legendre conjugate",
                "representer theorem",
                "multiview regularization",
                "support vector machine",
                "statistical learning theory"
            ],
            "author": [
                "Shiliangsun @ Gmail"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/sun10a/sun10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fully Simplified Multivariate Normal Updates in Non-Conjugate Variational Message Passing",
            "abstract": [
                "Fully simplified expressions for Multivariate Normal updates in non-conjugate variational message passing approximate inference schemes are obtained. The simplicity of these expressions means that the updates can be achieved very efficiently. Since the Multivariate Normal family is the most common for approximating the joint posterior density function of a continuous parameter vector, these fully simplified updates are of great practical benefit."
            ],
            "keywords": [
                "Bayesian computing",
                "graphical models",
                "matrix differential calculus",
                "mean field variational Bayes",
                "variational approximation"
            ],
            "author": [
                "Matt P Wand",
                "David M Blei"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/wand14a/wand14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Time-Accuracy Tradeoffs in Kernel Prediction: Controlling Prediction Quality",
            "abstract": [
                "Kernel regression or classification (also referred to as weighted-NN methods in Machine Learning) are appealing for their simplicity and therefore ubiquitous in data analysis. However, practical implementations of kernel regression or classification consist of quantizing or sub-sampling data for improving time efficiency, often at the cost of prediction quality. While such tradeoffs are necessary in practice, their statistical implications are generally not well understood, hence practical implementations come with few performance guarantees. In particular, it is unclear whether it is possible to maintain the statistical accuracy of kernel prediction-crucial in some applications-while improving prediction time. The present work provides guiding principles for combining kernel prediction with dataquantization so as to guarantee good tradeoffs between prediction time and accuracy, and in particular so as to approximately maintain the good accuracy of vanilla kernel prediction. Furthermore, our tradeoff guarantees are worked out explicitly in terms of a tuning parameter which acts as a knob that favors either time or accuracy depending on practical needs. On one end of the knob, prediction time is of the same order as that of single-nearestneighbor prediction (which is statistically inconsistent) while maintaining consistency; on the other end of the knob, the prediction risk is nearly minimax-optimal (in terms of the original data size) while still reducing time complexity. The analysis thus reveals the interaction between the data-quantization approach and the kernel prediction method, and most importantly gives explicit control of the tradeoff to the practitioner rather than fixing the tradeoff in advance or leaving it opaque. The theoretical results are validated on data from a range of real-world application domains; in particular we demonstrate that the theoretical knob performs as expected."
            ],
            "keywords": [],
            "author": [
                "Samory Kpotufe",
                "Nakul Verma"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-538/16-538.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising",
            "abstract": [
                "This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine."
            ],
            "keywords": [
                "causation",
                "counterfactual reasoning",
                "computational advertising"
            ],
            "author": [
                "Léon Bottou",
                "Jonas Peters",
                "Joaquin Quiñonero-Candela",
                "Denis X Charles",
                "D Max Chickering",
                "Patrice Simard",
                "Ed Snelson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/bottou13a/bottou13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Method for Persistence Diagrams via Kernel Embedding and Weight Factor",
            "abstract": [
                "Topological data analysis (TDA) is an emerging mathematical concept for characterizing shapes in complicated data. In TDA, persistence diagrams are widely recognized as a useful descriptor of data, distinguishing robust and noisy topological properties. This paper introduces a kernel method for persistence diagrams to develop a statistical framework in TDA. The proposed kernel is stable under perturbation of data, enables one to explicitly control the effect of persistence by a weight function, and allows an efficient and accurate approximate computation. The method is applied into practical data on granular systems, oxide glasses and proteins, showing advantages of our method compared to other relevant methods for persistence diagrams."
            ],
            "keywords": [
                "topological data analysis",
                "persistence diagrams",
                "kernel method",
                "kernel embedding",
                "persistence weighted Gaussian kernel"
            ],
            "author": [
                "Genki Kusano",
                "Yasuaki Hiraoka"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-317/17-317.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The fastclime Package for Linear Programming and Large-Scale Precision Matrix Estimation in R Haotian Pang",
            "abstract": [
                "We develop an R package fastclime for solving a family of regularized linear programming (LP) problems. Our package efficiently implements the parametric simplex algorithm, which provides a scalable and sophisticated tool for solving large-scale linear programs. As an illustrative example, one use of our LP solver is to implement an important sparse precision matrix estimation method called CLIME (Constrained L 1 Minimization Estimator). Compared with existing packages for this problem such as clime and flare, our package has three advantages: (1) it efficiently calculates the full piecewise-linear regularization path; (2) it provides an accurate dual certificate as stopping criterion; (3) it is completely coded in C and is highly portable. This package is designed to be useful to statisticians and machine learning researchers for solving a wide range of problems."
            ],
            "keywords": [
                "high dimensional data",
                "sparse precision matrix",
                "linear programming",
                "parametric simplex method",
                "undirected graphical model"
            ],
            "author": [
                "Han Liu",
                "Robert Vanderbei"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/pang14a/pang14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "pyGPs -A Python Library for Gaussian Process Regression and Classification",
            "abstract": [
                "We introduce pyGPs, an object-oriented implementation of Gaussian processes (gps) for machine learning. The library provides a wide range of functionalities reaching from simple gp specification via mean and covariance and gp inference to more complex implementations of hyperparameter optimization, sparse approximations, and graph based learning. Using Python we focus on usability for both \"users\" and \"researchers\". Our main goal is to offer a user-friendly and flexible implementation of gps for machine learning."
            ],
            "keywords": [
                "Gaussian processes",
                "Python",
                "regression and classification"
            ],
            "author": [
                "Marion Neumann",
                "Shan Huang",
                "Daniel E Marthaler",
                "Kristian Kersting"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/neumann15a/neumann15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Integrating a Partial Model into Model Free Reinforcement Learning",
            "abstract": [
                "In reinforcement learning an agent uses online feedback from the environment in order to adaptively select an effective policy. Model free approaches address this task by directly mapping environmental states to actions, while model based methods attempt to construct a model of the environment, followed by a selection of optimal actions based on that model. Given the complementary advantages of both approaches, we suggest a novel procedure which augments a model free algorithm with a partial model. The resulting hybrid algorithm switches between a model based and a model free mode, depending on the current state and the agent's knowledge. Our method relies on a novel definition for a partially known model, and an estimator that incorporates such knowledge in order to reduce uncertainty in stochastic approximation iterations. We prove that such an approach leads to improved policy evaluation whenever environmental knowledge is available, without compromising performance when such knowledge is absent. Numerical simulations demonstrate the effectiveness of the approach on policy gradient and Q-learning algorithms, and its usefulness in solving a call admission control problem."
            ],
            "keywords": [
                "reinforcement learning",
                "temporal difference",
                "stochastic approximation",
                "markov decision processes",
                "hybrid model based model free algorithms"
            ],
            "author": [
                "Aviv Tamar",
                "Dotan Di Castro",
                "Ron Meir"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/tamar12a/tamar12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "General Latent Feature Models for Heterogeneous Datasets",
            "abstract": [
                "Latent variable models allow capturing the hidden structure underlying the data. In particular, feature allocation models represent each observation by a linear combination of latent variables. These models are often used to make predictions either for new observations or for missing information in the original data, as well as to perform exploratory data analysis. Although there is an extensive literature on latent feature allocation models for homogeneous datasets, where all the attributes that describe each object are of the same (continuous or discrete) type, there is no general framework for practical latent feature modeling for heterogeneous datasets. In this paper, we introduce a general Bayesian nonparametric latent feature allocation model suitable for heterogeneous datasets, where the attributes describing each object can be arbitrary combinations of real-valued, positive real-valued, categorical, ordinal and count variables. The proposed model presents several important properties. First, it is suitable for heterogeneous data while keeping the properties of conjugate models, which enables us to develop an inference algorithm that presents linear complexity with respect to the number of objects and attributes per MCMC iteration. Second, the Bayesian nonparametric component allows us to place a prior distribution on the number of features required to capture the latent structure in the data. Third, the latent features in the model are binary-valued, which facilitates the interpretability of the obtained latent features in exploratory data analysis. Finally, a software package, called GLFM toolbox, is made publicly available for other researchers to use and extend. It is"
            ],
            "keywords": [],
            "author": [
                "Isabel Valera",
                "Maria Lomeli",
                "Pradier, Lomeli, Ghahramani Valera"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/17-328/17-328.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression",
            "abstract": [
                "We show how to follow the path of cross validated solutions to families of regularized optimization problems, defined by a combination of a parameterized loss function and a regularization term. A primary example is kernel quantile regression, where the parameter of the loss function is the quantile being estimated. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on real and artificial data. This algorithm allows us to efficiently solve the whole family of bi-level problems. We show how it can be extended to cover other modeling problems, like support vector regression, and alternative in-sample model selection approaches."
            ],
            "keywords": [],
            "author": [
                "Saharon Rosset"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/rosset09a/rosset09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive False Discovery Rate Control under Independence and Dependence",
            "abstract": [
                "In the context of multiple hypothesis testing, the proportion π 0 of true null hypotheses in the pool of hypotheses to test often plays a crucial role, although it is generally unknown a priori. A testing procedure using an implicit or explicit estimate of this quantity in order to improve its efficency is called adaptive. In this paper, we focus on the issue of false discovery rate (FDR) control and we present new adaptive multiple testing procedures with control of the FDR. In a first part, assuming independence of the p-values, we present two new procedures and give a unified review of other existing adaptive procedures that have provably controlled FDR. We report extensive simulation results comparing these procedures and testing their robustness when the independence assumption is violated. The new proposed procedures appear competitive with existing ones. The overall best, though, is reported to be Storey's estimator, albeit for a specific parameter setting that does not appear to have been considered before. In a second part, we propose adaptive versions of step-up procedures that have provably controlled FDR under positive dependence and unspecified dependence of the p-values, respectively. In the latter case, while simulations only show an improvement over non-adaptive procedures in limited situations, these are to our knowledge among the first theoretically founded adaptive multiple testing procedures that control the FDR when the p-values are not independent."
            ],
            "keywords": [
                "multiple testing",
                "false discovery rate",
                "adaptive procedure",
                "positive regression dependence",
                "p-values"
            ],
            "author": [
                "Gilles Blanchard",
                "Etienne Roquain"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/blanchard09a/blanchard09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Inference through a Witness Protection Program",
            "abstract": [
                "One of the most fundamental problems in causal inference is the estimation of a causal effect when treatment and outcome are confounded. This is difficult in an observational study, because one has no direct evidence that all confounders have been adjusted for. We introduce a novel approach for estimating causal effects that exploits observational conditional independencies to suggest \"weak\" paths in an unknown causal graph. The widely used faithfulness condition of Spirtes et al. is relaxed to allow for varying degrees of \"path cancellations\" that imply conditional independencies but do not rule out the existence of confounding causal paths. The output is a posterior distribution over bounds on the average causal effect via a linear programming approach and Bayesian inference. We claim this approach should be used in regular practice as a complement to other tools in observational studies."
            ],
            "keywords": [
                "Causal inference",
                "instrumental variables",
                "Bayesian inference",
                "linear programming"
            ],
            "author": [
                "Ricardo Silva"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-130/15-130.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions",
            "abstract": [
                "In part I of this work we introduced and evaluated the Generalized Local Learning (GLL) framework for producing local causal and Markov blanket induction algorithms. In the present second part we analyze the behavior of GLL algorithms and provide extensions to the core methods. Specifically, we investigate the empirical convergence of GLL to the true local neighborhood as a function of sample size. Moreover, we study how predictivity improves with increasing sample size. Then we investigate how sensitive are the algorithms to multiple statistical testing, especially in the presence of many irrelevant features. Next we discuss the role of the algorithm parameters and also show that Markov blanket and causal graph concepts can be used to understand deviations from optimality of state-of-the-art non-causal algorithms. The present paper also introduces the following extensions to the core GLL framework: parallel and distributed versions of GLL algorithms, versions with false discovery rate control, strategies for constructing novel heuristics for specific domains, and divide-and-conquer local-to-global learning (LGL) strategies. We test the generality of the LGL approach by deriving a novel LGL-based algorithm that compares favorably"
            ],
            "keywords": [
                "local causal discovery",
                "Markov blanket induction",
                "feature selection",
                "classification",
                "causal structure learning",
                "learning of Bayesian networks"
            ],
            "author": [
                "Constantin F Aliferis",
                "Alexander Statnikov",
                "Xenofon D Koutsoukos",
                "Ioannis Tsamardinos",
                "Subramani Mani",
                "STATNIKOV, TSAMARDINOS, MANI AND KOUTSOUKOS Aliferis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/aliferis10b/aliferis10b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nieme: Large-Scale Energy-Based Models",
            "abstract": [
                "In this paper we introduce NIEME, 1 a machine learning library for large-scale classification, regression and ranking. NIEME relies on the framework of energy-based models (LeCun et al., 2006) which unifies several learning algorithms ranging from simple perceptrons to recent models such as the pegasos support vector machine or l1-regularized maximum entropy models. This framework also unifies batch and stochastic learning which are both seen as energy minimization problems. NIEME can hence be used in a wide range of situations, but is particularly interesting for large-scale learning tasks where both the examples and the features are processed incrementally. Being able to deal with new incoming features at any time within the learning process is another original feature of the NIEME toolbox. NIEME is released under the GPL license. It is efficiently implemented in C++, it works on Linux, Mac OS X and Windows and provides interfaces for C++, Java and Python."
            ],
            "keywords": [
                "large-scale machine learning",
                "classification",
                "ranking",
                "regression",
                "energy-based models",
                "machine learning software"
            ],
            "author": [
                "Francis Maes",
                "MAES@LIP6 Francis Fr"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/maes09a/maes09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gradient Descent Learns Linear Dynamical Systems",
            "abstract": [
                "We prove that stochastic gradient descent efficiently converges to the global optimizer of the maximum likelihood objective of an unknown linear time-invariant dynamical system from a sequence of noisy observations generated by the system. Even though the objective function is non-convex, we provide polynomial running time and sample complexity bounds under strong but natural assumptions. Linear systems identification has been studied for many decades, yet, to the best of our knowledge, these are the first polynomial guarantees for the problem we consider."
            ],
            "keywords": [
                "non-convex optimization",
                "linear dynamical system",
                "stochastic gradient descent",
                "generalization bounds",
                "time series",
                "over-parameterization"
            ],
            "author": [
                "Moritz Hardt",
                "Tengyu Ma",
                "Benjamin Recht"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-465/16-465.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Oger: Modular Learning Architectures For Large-Scale Sequential Processing",
            "abstract": [
                "Oger (OrGanic Environment for Reservoir computing) is a Python toolbox for building, training and evaluating modular learning architectures on large data sets. It builds on MDP for its modularity, and adds processing of sequential data sets, gradient descent training, several crossvalidation schemes and parallel parameter optimization methods. Additionally, several learning algorithms are implemented, such as different reservoir implementations (both sigmoid and spiking), ridge regression, conditional restricted Boltzmann machine (CRBM) and others, including GPU accelerated versions. Oger is released under the GNU LGPL, and is available from http: //organic.elis.ugent.be/oger."
            ],
            "keywords": [
                "Python",
                "modular architectures",
                "sequential processing"
            ],
            "author": [
                "David Verstraeten",
                "Benjamin Schrauwen",
                "Sander Dieleman",
                "Pieter Buteneers"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/verstraeten12a/verstraeten12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exact Inference on Gaussian Graphical Models of Arbitrary Topology using Path-Sums",
            "abstract": [
                "We present the path-sum formulation for exact statistical inference of marginals on Gaussian graphical models of arbitrary topology. The path-sum formulation gives the covariance between each pair of variables as a branched continued fraction of finite depth and breadth. Our method originates from the closed-form resummation of infinite families of terms of the walk-sum representation of the covariance matrix. We prove that the path-sum formulation always exists for models whose covariance matrix is positive definite: i.e. it is valid for both walk-summable and non-walk-summable graphical models of arbitrary topology. We show that for graphical models on trees the path-sum formulation is equivalent to Gaussian belief propagation. We also recover, as a corollary, an existing result that uses determinants to calculate the covariance matrix. We show that the path-sum formulation formulation is valid for arbitrary partitions of the inverse covariance matrix. We give detailed examples demonstrating our results."
            ],
            "keywords": [
                "Gaussian graphical models",
                "belief propagation",
                "path-sum",
                "walk-sum",
                "graphs of arbitrary topology",
                "block matrices"
            ],
            "author": [
                "P.-L Giscard",
                "Zheng Choo",
                "Simon James Thwaite",
                "D Jaksch",
                "Louis Giscard",
                "Dieter Jaksch Giscard",
                "Thwaite Jaksch Choo"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-445/14-445.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification",
            "abstract": [
                "We consider the problem of classification using high dimensional features' space. In a paper by Bickel and Levina (2004), it is recommended to use naive-Bayes classifiers, that is, to treat the features as if they are statistically independent. Consider now a sparse setup, where only a few of the features are informative for classification. Fan and Fan (2008), suggested a variable selection and classification method, called FAIR. The FAIR method improves the design of naive-Bayes classifiers in sparse setups. The improvement is due to reducing the noise in estimating the features' means. This reduction is since that only the means of a few selected variables should be estimated. We also consider the design of naive Bayes classifiers. We show that a good alternative to variable selection is estimation of the means through a certain non parametric empirical Bayes procedure. In sparse setups the empirical Bayes implicitly performs an efficient variable selection. It also adapts very well to non sparse setups, and has the advantage of making use of the information from many \"weakly informative\" variables, which variable selection type of classification procedures give up on using. We compare our method with FAIR and other classification methods in simulation for sparse and non sparse setups, and in real data examples involving classification of normal versus malignant tissues based on microarray data."
            ],
            "keywords": [
                "non parametric empirical Bayes",
                "high dimension",
                "classification"
            ],
            "author": [
                "Eitan Greenshtein"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/greenshtein09a/greenshtein09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights",
            "abstract": [
                "We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex."
            ],
            "keywords": [
                "Nesterov's accelerated scheme",
                "convex optimization",
                "first-order methods",
                "differential equation",
                "restarting"
            ],
            "author": [
                "Weijie Su",
                "Stephen Boyd",
                "Emmanuel J Candès"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-084/15-084.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Using Machine Learning to Guide Architecture Simulation",
            "abstract": [
                "An essential step in designing a new computer architecture is the careful examination of different design options. It is critical that computer architects have efficient means by which they may estimate the impact of various design options on the overall machine. This task is complicated by the fact that different programs, and even different parts of the same program, may have distinct behaviors that interact with the hardware in different ways. Researchers use very detailed simulators to estimate processor performance, which models every cycle of an executing program. Unfortunately, simulating every cycle of a real program can take weeks or months. To address this problem we have created a tool called SimPoint that uses data clustering algorithms from machine learning to automatically find repetitive patterns in a program's execution. By simulating one representative of each repetitive behavior pattern, simulation time can be reduced to minutes instead of weeks for standard benchmark programs, with very little cost in terms of accuracy. We describe this important problem, the data representation and preprocessing methods used by SimPoint, the clustering algorithm at the core of SimPoint, and we evaluate different options for tuning SimPoint."
            ],
            "keywords": [
                "k-means",
                "random projection",
                "Bayesian information criterion",
                "simulation",
                "SimPoint"
            ],
            "author": [
                "Greg Hamerly",
                "Erez Perelman",
                "Jeremy Lau",
                "Brad Calder",
                "Timothy Sherwood"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/hamerly06a/hamerly06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix",
            "abstract": [
                "Binary matrices, and their associated submatrices of 1s, play a central role in the study of random bipartite graphs and in core data mining problems such as frequent itemset mining (FIM). Motivated by these connections, this paper addresses several statistical questions regarding submatrices of 1s in a random binary matrix with independent Bernoulli entries. We establish a three-point concentration result, and a related probability bound, for the size of the largest square submatrix of 1s in a square Bernoulli matrix, and extend these results to non-square matrices and submatrices with fixed aspect ratios. We then consider the noise sensitivity of frequent itemset mining under a simple binary additive noise model, and show that, even at small noise levels, large blocks of 1s leave behind fragments of only logarithmic size. As a result, standard FIM algorithms, which search only for submatrices of 1s, cannot directly recover such blocks when noise is present. On the positive side, we show that an error-tolerant frequent itemset criterion can recover a submatrix of 1s against a background of 0s plus noise, even when the size of the submatrix of 1s is very small. 1"
            ],
            "keywords": [
                "frequent itemset mining",
                "bipartite graph",
                "biclique",
                "submatrix of 1s",
                "statistical significance"
            ],
            "author": [
                "Xing Sun",
                "Andrew B Nobel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/sun08a/sun08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structured Sparsity and Generalization",
            "abstract": [
                "We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an infinite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels."
            ],
            "keywords": [
                "empirical processes",
                "Rademacher average",
                "sparse estimation"
            ],
            "author": [
                "Andreas Maurer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/maurer12a/maurer12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-Parametric Modeling of Partially Ranked Data",
            "abstract": [
                "Statistical models on full and partial rankings of n items are often of limited practical use for large n due to computational consideration. We explore the use of non-parametric models for partially ranked data and derive computationally efficient procedures for their use for large n. The derivations are largely possible through combinatorial and algebraic manipulations based on the lattice of partial rankings. A bias-variance analysis and an experimental study demonstrate the applicability of the proposed method."
            ],
            "keywords": [
                "ranked data",
                "partially ordered sets",
                "kernel smoothing"
            ],
            "author": [
                "Guy Lebanon",
                "Yi Mao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/lebanon08a/lebanon08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Robust Learning Approach for Regression Models Based on Distributionally Robust Optimization",
            "abstract": [
                "We present a Distributionally Robust Optimization (DRO) approach to estimate a robustified regression plane in a linear regression setting, when the observed samples are potentially contaminated with adversarially corrupted outliers. Our approach mitigates the impact of outliers by hedging against a family of probability distributions on the observed data, some of which assign very low probabilities to the outliers. The set of distributions under consideration are close to the empirical distribution in the sense of the Wasserstein metric. We show that this DRO formulation can be relaxed to a convex optimization problem which encompasses a class of models. By selecting proper norm spaces for the Wasserstein metric, we are able to recover several commonly used regularized regression models. We provide new insights into the regularization term and give guidance on the selection of the regularization coefficient from the standpoint of a confidence region. We establish two types of performance guarantees for the solution to our formulation under mild conditions. One is related to its out-of-sample behavior (prediction bias), and the other concerns the discrepancy between the estimated and true regression planes (estimation bias). Extensive numerical results demonstrate the superiority of our approach to a host of regression models, in terms of the prediction and estimation accuracies. We also consider the application of our robust learning procedure to outlier detection, and show that our approach achieves a much higher AUC (Area Under the ROC Curve) than M-estimation (Huber, 1964, 1973)."
            ],
            "keywords": [
                "Robust Learning",
                "Distributionally Robust Optimization",
                "Wasserstein Metric",
                "Regularized Regression",
                "Generalization Guarantees"
            ],
            "author": [
                "Ruidi Chen",
                "Ioannis Ch"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-295/17-295.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Permutation Tests for Studying Classifier Performance",
            "abstract": [
                "We explore the framework of permutation-based p-values for assessing the performance of classifiers. In this paper we study two simple permutation tests. The first test assess whether the classifier has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classification problems in computational biology. The second test studies whether the classifier is exploiting the dependency between the features in classification; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classifier performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classifier performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classifier exploits the interdependency between the features in the data."
            ],
            "keywords": [
                "classification",
                "labeled data",
                "permutation tests",
                "restricted randomization",
                "significance testing"
            ],
            "author": [
                "Markus Ojala",
                "Gemma C Garriga"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/ojala10a/ojala10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ramp Loss Linear Programming Support Vector Machine",
            "abstract": [
                "The ramp loss is a robust but non-convex loss for classification. Compared with other non-convex losses, a local minimum of the ramp loss can be effectively found. The effectiveness of local search comes from the piecewise linearity of the ramp loss. Motivated by the fact that the 1-penalty is piecewise linear as well, the 1-penalty is applied for the ramp loss, resulting in a ramp loss linear programming support vector machine (ramp-LPSVM). The proposed ramp-LPSVM is a piecewise linear minimization problem and the related optimization techniques are applicable. Moreover, the 1-penalty can enhance the sparsity. In this paper, the corresponding misclassification error and convergence behavior are discussed. Generally, the ramp loss is a truncated hinge loss. Therefore ramp-LPSVM possesses some similar properties as hinge loss SVMs. A local minimization algorithm and a global search strategy are discussed. The good optimization capability of the proposed algorithms makes ramp-LPSVM perform well in numerical experiments: the result of ramp-LPSVM is more robust than that of hinge SVMs and is sparser than that of ramp-SVM, which consists of the • K-penalty and the ramp loss."
            ],
            "keywords": [
                "support vector machine",
                "ramp loss",
                "1 -regularization",
                "generalization error analysis",
                "global optimization"
            ],
            "author": [
                "Xiaolin Huang",
                "Lei Shi",
                "Johan A K Suykens"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/huang14a/huang14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training",
            "abstract": [
                "Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Specifically, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efficiency both in time and space during training and prediction."
            ],
            "keywords": [
                "SVM",
                "large-scale learning",
                "online learning",
                "stochastic gradient descent",
                "kernel methods"
            ],
            "author": [
                "Zhuang Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/wang12b/wang12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Theory of Distributed Regression with Bias Corrected Regularization Kernel Network",
            "abstract": [
                "Distributed learning is an effective way to analyze big data. In distributed regression, a typical approach is to divide the big data into multiple blocks, apply a base regression algorithm on each of them, and then simply average the output functions learnt from these blocks. Since the average process will decrease the variance, not the bias, bias correction is expected to improve the learning performance if the base regression algorithm is a biased one. Regularization kernel network is an effective and widely used method for nonlinear regression analysis. In this paper we will investigate a bias corrected version of regularization kernel network. We derive the error bounds when it is applied to a single data set and when it is applied as a base algorithm in distributed regression. We show that, under certain appropriate conditions, the optimal learning rates can be reached in both situations."
            ],
            "keywords": [
                "Distributed learning",
                "kernel method",
                "regularization",
                "bias correction",
                "error bound"
            ],
            "author": [
                "Zheng-Chu Guo",
                "Lei Shi",
                "Qiang Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-423/17-423.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Tensor Approach to Learning Mixed Membership Community Models",
            "abstract": [
                "Community detection is the task of detecting hidden communities from observed interactions. Guaranteed community detection has so far been mostly limited to models with non-overlapping communities such as the stochastic block model. In this paper, we remove this restriction, and provide guaranteed community detection for a family of probabilistic network models with overlapping communities, termed as the mixed membership Dirichlet model, first introduced by Airoldi et al. (2008). This model allows for nodes to have fractional memberships in multiple communities and assumes that the community memberships are drawn from a Dirichlet distribution. Moreover, it contains the stochastic block model as a special case. We propose a unified approach to learning these models via a tensor spectral decomposition method. Our estimator is based on low-order moment tensor of the observed network, consisting of 3-star counts. Our learning method is fast and is based on simple linear algebraic operations, e.g., singular value decomposition and tensor power iterations. We provide guaranteed recovery of community memberships and model parameters and present a careful finite sample analysis of our learning method. As an important special case, our results match the best known scaling requirements for the (homogeneous) stochastic block model."
            ],
            "keywords": [
                "community detection",
                "spectral methods",
                "tensor methods",
                "moment-based estimation",
                "mixed membership models"
            ],
            "author": [
                "Animashree Anandkumar",
                "Rong Ge",
                "Daniel Hsu",
                "Sham M Kakade"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/anandkumar14a/anandkumar14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "HPB: A Model for Handling BN Nodes with High Cardinality Parents",
            "abstract": [
                "We replaced the conditional probability tables of Bayesian network nodes whose parents have high cardinality with a multilevel empirical hierarchical Bayesian model called hierarchical pattern Bayes (HPB). 1 The resulting Bayesian networks achieved significant performance improvements over Bayesian networks with the same structure and traditional conditional probability tables, over Bayesian networks with simpler structures like naïve Bayes and tree augmented naïve Bayes, over Bayesian networks where traditional conditional probability tables were substituted by noisy-OR gates, default tables, decision trees and decision graphs and over Bayesian networks constructed after a cardinality reduction preprocessing phase using the agglomerative information bottleneck method. Our main tests took place in important fraud detection domains, which are characterized by the presence of high cardinality attributes and by the existence of relevant interactions among them. Other tests, over UCI data sets, show that HPB may have a quite wide applicability."
            ],
            "keywords": [
                "probabilistic reasoning",
                "Bayesian networks",
                "smoothing",
                "hierarchical Bayes",
                "empirical Bayes"
            ],
            "author": [
                "Jorge Jambeiro Filho",
                "Jacques Wainer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/jambeiro08a/jambeiro08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Training Highly Multiclass Classifiers",
            "abstract": [
                "Classification problems with thousands or more classes often have a large range of classconfusabilities, and we show that the more-confusable classes add more noise to the empirical loss that is minimized during training. We propose an online solution that reduces the effect of highly confusable classes in training the classifier parameters, and focuses the training on pairs of classes that are easier to differentiate at any given time in the training. We also show that the adagrad method, recently proposed for automatically decreasing step sizes for convex stochastic gradient descent, can also be profitably applied to the nonconvex joint training of supervised dimensionality reduction and linear classifiers as done in Wsabie. Experiments on ImageNet benchmark data sets and proprietary image recognition problems with 15,000 to 97,000 classes show substantial gains in classification accuracy compared to one-vs-all linear SVMs and Wsabie."
            ],
            "keywords": [
                "large-scale",
                "classification",
                "multiclass",
                "online learning",
                "stochastic gradient"
            ],
            "author": [
                "Maya R Gupta",
                "Jason Weston"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/gupta14a/gupta14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming",
            "abstract": [
                "We present NrSample, a framework for program synthesis in inductive logic programming. NrSample uses propositional logic constraints to exclude undesirable candidates from the search. This is achieved by representing constraints as propositional formulae and solving the associated constraint satisfaction problem. We present a variety of such constraints: pruning, input-output, functional (arithmetic), and variable splitting. NrSample is also capable of detecting search space exhaustion, leading to further speedups in clause induction and optimality. We benchmark NrSample against enumeration search (Aleph's default) and Progol's A * search in the context of program synthesis. The results show that, on large program synthesis problems, NrSample induces between 1 and 1358 times faster than enumeration (236 times faster on average), always with similar or better accuracy. Compared to Progol A * , NrSample is 18 times faster on average with similar or better accuracy except for two problems: one in which Progol A * substantially sacrificed accuracy to induce faster, and one in which Progol A * was a clear winner. Functional constraints provide a speedup of up to 53 times (21 times on average) with similar or better accuracy. We also benchmark using a few concept learning (non-program synthesis) problems. The results indicate that without strong constraints, the overhead of solving constraints is not compensated for."
            ],
            "keywords": [
                "inductive logic programming",
                "program synthesis",
                "theory induction",
                "constraint satisfaction",
                "Boolean satisfiability problem"
            ],
            "author": [
                "John Ahlgren",
                "Yin Shiu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/ahlgren13a/ahlgren13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An efficient distributed learning algorithm based on effective local functional approximations",
            "abstract": [
                "Scalable machine learning over big data is an important problem that is receiving a lot of attention in recent years. On popular distributed environments such as Hadoop running on a cluster of commodity machines, communication costs are substantial and algorithms need to be designed suitably considering those costs. In this paper we give a novel approach to the distributed training of linear classifiers (involving smooth losses and L 2 regularization) that is designed to reduce the total communication costs. At each iteration, the nodes minimize locally formed approximate objective functions; then the resulting minimizers are combined to form a descent direction to move. Our approach gives a lot of freedom in the formation of the approximate objective function as well as in the choice of methods to solve them. The method is shown to have O(log(1/)) time convergence. The method can be viewed as an iterative parameter mixing method. A special instantiation yields a parallel stochastic gradient descent method with strong convergence. When communication times between nodes are large, our method is much faster than the Terascale method (Agarwal et al., 2011), which is a state of the art distributed solver based on the statistical query model (Chu et al., 2006) that computes function and gradient values in a distributed fashion. We also evaluate against other recent distributed methods and demonstrate superior performance of our method."
            ],
            "keywords": [
                "Distributed learning",
                "Example partitioning",
                "L 2 regularization"
            ],
            "author": [
                "Dhruv Mahajan",
                "Nikunj Agrawal",
                "S Sathiya Keerthi",
                "Sundararajan Sellamanickam",
                "Léon Bottou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/15-020/15-020.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized SURE for optimal shrinkage of singular values in low-rank matrix denoising",
            "abstract": [
                "We consider the problem of estimating a low-rank signal matrix from noisy measurements under the assumption that the distribution of the data matrix belongs to an exponential family. In this setting, we derive generalized Stein's unbiased risk estimation (SURE) formulas that hold for any spectral estimators which shrink or threshold the singular values of the data matrix. This leads to new data-driven spectral estimators, whose optimality is discussed using tools from random matrix theory and through numerical experiments. Under the spiked population model and in the asymptotic setting where the dimensions of the data matrix are let going to infinity, some theoretical properties of our approach are compared to recent results on asymptotically optimal shrinking rules for Gaussian noise. It also leads to new procedures for singular values shrinkage in finite-dimensional matrix denoising for Gamma-distributed and Poisson-distributed measurements."
            ],
            "keywords": [
                "Matrix denoising",
                "singular value decomposition",
                "low-rank model",
                "Gaussian spiked population model",
                "spectral estimator",
                "Stein's unbiased risk estimate",
                "random matrix theory",
                "exponential family",
                "optimal shrinkage rule",
                "degrees of freedom"
            ],
            "author": [
                "Jérémie Bigot",
                "Charles Deledalle",
                "Delphine Féral"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-266/16-266.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Group Lasso Estimation of High-dimensional Covariance Matrices",
            "abstract": [
                "In this paper, we consider the Group Lasso estimator of the covariance matrix of a stochastic process corrupted by an additive noise. We propose to estimate the covariance matrix in a highdimensional setting under the assumption that the process has a sparse representation in a large dictionary of basis functions. Using a matrix regression model, we propose a new methodology for high-dimensional covariance matrix estimation based on empirical contrast regularization by a group Lasso penalty. Using such a penalty, the method selects a sparse set of basis functions in the dictionary used to approximate the process, leading to an approximation of the covariance matrix into a low dimensional space. Consistency of the estimator is studied in Frobenius and operator norms and an application to sparse PCA is proposed."
            ],
            "keywords": [
                "group Lasso",
                "ℓ 1 penalty",
                "high-dimensional covariance estimation",
                "basis expansion",
                "sparsity",
                "oracle inequality",
                "sparse PCA"
            ],
            "author": [
                "Jérémie Bigot",
                "Rolando J Biscay",
                "Jean-Michel Loubes",
                "Lilian Muñiz-Alvarez"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/bigot11a/bigot11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Bounded Treewidth Bayesian Networks",
            "abstract": [
                "With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufficiently expressive for generalization while at the same time allow for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overfitting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modifications and that is polynomial both in the size of the graph and the treewidth bound. At the heart of our method is a dynamic triangulation that we update in a way that facilitates the addition of chain structures that increase the bound on the model's treewidth by at most one. We demonstrate the effectiveness of our \"treewidth-friendly\" method on several real-life data sets and show that it is superior to the greedy approach as soon as the bound on the treewidth is nontrivial. Importantly, we also show that by making use of global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth."
            ],
            "keywords": [
                "Bayesian networks",
                "structure learning",
                "model selection",
                "bounded treewidth"
            ],
            "author": [
                "Gal Elidan",
                "Stephen Gould",
                "Stanford Edu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/elidan08a/elidan08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributional Word Clusters vs. Words for Text Categorization",
            "abstract": [
                "We study an approach to text categorization that combines distributional clustering of words and a Support Vector Machine (SVM) classifier. This word-cluster representation is computed using the recently introduced Information Bottleneck method, which generates a compact and efficient representation of documents. When combined with the classification power of the SVM, this method yields high performance in text categorization. This novel combination of SVM with word-cluster representation is compared with SVM-based categorization using the simpler bag-of-words (BOW) representation. The comparison is performed over three known datasets. On one of these datasets (the 20 Newsgroups) the method based on word clusters significantly outperforms the word-based representation in terms of categorization accuracy or representation efficiency. On the two other sets (Reuters-21578 and WebKB) the word-based representation slightly outperforms the word-cluster representation. We investigate the potential reasons for this behavior and relate it to structural differences between the datasets."
            ],
            "keywords": [],
            "author": [
                "Ron Bekkerman",
                "Naftali Tishby",
                "Yoad Winter",
                "Isabelle Guyon",
                "André Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/bekkerman03a/bekkerman03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "LPmade: Link Prediction Made Easy",
            "abstract": [
                "LPmade is a complete cross-platform software solution for multi-core link prediction and related tasks and analysis. Its first principal contribution is a scalable network library supporting highperformance implementations of the most commonly employed unsupervised link prediction methods. Link prediction in longitudinal data requires a sophisticated and disciplined procedure for correct results and fair evaluation, so the second principle contribution of LPmade is a sophisticated GNU make architecture that completely automates link prediction, prediction evaluation, and network analysis. Finally, LPmade streamlines and automates the procedure for creating multivariate supervised link prediction models with a version of WEKA modified to operate effectively on extremely large data sets. With mere minutes of manual work, one may start with a raw stream of records representing a network and progress through hundreds of steps to generate plots, gigabytes or terabytes of output, and actionable or publishable results."
            ],
            "keywords": [
                "link prediction",
                "network analysis",
                "multicore",
                "GNU make",
                "PropFlow",
                "HPLP"
            ],
            "author": [
                "Ryan N Lichtenwalter",
                "Nchawla @ Nd"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/lichtenwalter11a/lichtenwalter11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "VC Theory of Large Margin Multi-Category Classifiers",
            "abstract": [
                "In the context of discriminant analysis, Vapnik's statistical learning theory has mainly been developed in three directions: the computation of dichotomies with binary-valued functions, the computation of dichotomies with real-valued functions, and the computation of polytomies with functions taking their values in finite sets, typically the set of categories itself. The case of classes of vectorvalued functions used to compute polytomies has seldom been considered independently, which is unsatisfactory, for three main reasons. First, this case encompasses the other ones. Second, it cannot be treated appropriately through a naïve extension of the results devoted to the computation of dichotomies. Third, most of the classification problems met in practice involve multiple categories. In this paper, a VC theory of large margin multi-category classifiers is introduced. Central in this theory are generalized VC dimensions called the γ-Ψ-dimensions. First, a uniform convergence bound on the risk of the classifiers of interest is derived. The capacity measure involved in this bound is a covering number. This covering number can be upper bounded in terms of the γ-Ψdimensions thanks to generalizations of Sauer's lemma, as is illustrated in the specific case of the scale-sensitive Natarajan dimension. A bound on this latter dimension is then computed for the class of functions on which multi-class SVMs are based. This makes it possible to apply the structural risk minimization inductive principle to those machines."
            ],
            "keywords": [
                "multi-class discriminant analysis",
                "large margin classifiers",
                "uniform strong laws of large numbers",
                "generalized VC dimensions",
                "multi-class SVMs",
                "structural risk minimization inductive principle",
                "model selection"
            ],
            "author": [
                "Yann Guermeur",
                "Isabelle Guyon",
                "Amir Saffari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/guermeur07a/guermeur07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analysis of spectral clustering algorithms for community detection: the general bipartite setting",
            "abstract": [
                "We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a k-means type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly."
            ],
            "keywords": [
                "Spectral clustering",
                "bipartite networks",
                "stochastic block model",
                "regularization of random graphs",
                "community detection",
                "sub-Gaussian biclustering",
                "graphon clustering"
            ],
            "author": [
                "Zhixin Zhou",
                "Arash A Amini"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-170/18-170.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Blending Learning and Inference in Conditional Random Fields",
            "abstract": [
                "Conditional random fields maximize the log-likelihood of training labels given the training data, e.g., objects given images. In many cases the training labels are structures that consist of a set of variables and the computational complexity for estimating their likelihood is exponential in the number of the variables. Learning algorithms relax this computational burden using approximate inference that is nested as a sub-procedure. In this paper we describe the objective function for nested learning and inference in conditional random fields. The devised objective maximizes the log-beliefs-probability distributions over subsets of training variables that agree on their marginal probabilities. This objective is concave and consists of two types of variables that are related to the learning and inference tasks respectively. Importantly, we afterwards show how to blend the learning and inference procedure and effectively get to the identical optimum much faster. The proposed algorithm currently achieves the state-of-the-art in various computer vision applications."
            ],
            "keywords": [],
            "author": [
                "Tamir Hazan",
                "Alexander G Schwing",
                "Raquel Urtasun"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/13-260/13-260.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization",
            "abstract": [
                "We consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as ℓ 1-norm for promoting sparsity. We develop extensions of Nesterov's dual averaging method, that can exploit the regularization structure in an online setting. At each iteration of these methods, the learning variables are adjusted by solving a simple minimization problem that involves the running average of all past subgradients of the loss function and the whole regularization term, not just its subgradient. In the case of ℓ 1-regularization, our method is particularly effective in obtaining sparse solutions. We show that these methods achieve the optimal convergence rates or regret bounds that are standard in the literature on stochastic and online convex optimization. For stochastic learning problems in which the loss functions have Lipschitz continuous gradients, we also present an accelerated version of the dual averaging method."
            ],
            "keywords": [
                "stochastic learning",
                "online optimization",
                "ℓ -regularization",
                "structural convex optimization",
                "dual averaging methods",
                "accelerated gradient methods"
            ],
            "author": [
                "Lin Xiao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/xiao10a/xiao10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local and Global Scaling Reduce Hubs in Space",
            "abstract": [
                "'Hubness' has recently been identified as a general problem of high dimensional data spaces, manifesting itself in the emergence of objects, so-called hubs, which tend to be among the k nearest neighbors of a large number of data items. As a consequence many nearest neighbor relations in the distance space are asymmetric, that is, object y is amongst the nearest neighbors of x but not vice versa. The work presented here discusses two classes of methods that try to symmetrize nearest neighbor relations and investigates to what extent they can mitigate the negative effects of hubs. We evaluate local distance scaling and propose a global variant which has the advantage of being easy to approximate for large data sets and of having a probabilistic interpretation. Both local and global approaches are shown to be effective especially for high-dimensional data sets, which are affected by high hubness. Both methods lead to a strong decrease of hubness in these data sets, while at the same time improving properties like classification accuracy. We evaluate the methods on a large number of public machine learning data sets and synthetic data. Finally we present a real-world application where we are able to achieve significantly higher retrieval quality."
            ],
            "keywords": [
                "local and global scaling",
                "shared near neighbors",
                "hubness",
                "classification",
                "curse of dimensionality",
                "nearest neighbor relation"
            ],
            "author": [
                "Dominik Schnitzer",
                "Arthur Flexer",
                "Markus Schedl",
                "Gerhard Widmer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/schnitzer12a/schnitzer12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Continuous-Time Birth-Death MCMC for Bayesian Regression Tree Models",
            "abstract": [
                "Decision trees are flexible models that are well suited for many statistical regression problems. In the Bayesian framework for regression trees, Markov Chain Monte Carlo (MCMC) search algorithms are required to generate samples of tree models according to their posterior probabilities. The critical component of such MCMC algorithms is to construct \"good\" Metropolis-Hastings steps to update the tree topology. Such algorithms frequently suffer from poor mixing and local mode stickiness; therefore, the algorithms are slow to converge. Hitherto, authors have primarily used discrete-time birth/death mechanisms for Bayesian (sums of) regression tree models to explore the tree-model space. These algorithms are efficient, in terms of computation and convergence, only if the rejection rate is low which is not always the case. We overcome this issue by developing a novel search algorithm which is based on a continuous-time birth-death Markov process. The search algorithm explores the tree-model space by jumping between parameter spaces corresponding to different tree structures. The jumps occur in continuous time corresponding to the birth-death events which are modeled as independent Poisson processes. In the proposed algorithm, the moves between models are always accepted which can dramatically improve the convergence and mixing properties of the search algorithm. We provide theoretical support of the algorithm for Bayesian regression tree models and demonstrate its performance in a simulated example."
            ],
            "keywords": [
                "Bayesian Regression trees",
                "Decision trees",
                "Continuous-time MCMC",
                "Bayesian structure learning",
                "Birth-death process",
                "Bayesian model averaging",
                "Bayesian model selection"
            ],
            "author": [
                "Reza Mohammadi",
                "Matthew Pratola",
                "Maurits Kaptein",
                "Reza ©2020",
                "Matthew Mohammadi",
                "Maurits Pratola"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-307/19-307.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parsimonious Online Learning with Kernels via Sparse Projections in Function Space *",
            "abstract": [
                "Despite their attractiveness, popular perception is that techniques for nonparametric function approximation do not scale to streaming data due to an intractable growth in the amount of storage they require. To solve this problem in a memory-affordable way, we propose an online technique based on functional stochastic gradient descent in tandem with supervised sparsification based on greedy function subspace projections. The method, called parsimonious online learning with kernels (POLK), provides a controllable tradeoff between its solution accuracy and the amount of memory it requires. We derive conditions under which the generated function sequence converges almost surely to the optimal function, and we establish that the memory requirement remains finite. We evaluate POLK for kernel multi-class logistic regression and kernel hinge-loss classification on three canonical data sets: a synthetic Gaussian mixture model, the MNIST handwritten digits, and the Brodatz texture database. On all three tasks, we observe a favorable trade-off of objective function evaluation, classification performance, and complexity of the nonparametric regressor extracted by the proposed method."
            ],
            "keywords": [
                "kernel methods",
                "online learning",
                "stochastic optimization",
                "supervised learning",
                "orthogonal matching pursuit",
                "nonparametric regression Koppel",
                "Warnell",
                "Stump",
                "and Ribeiro"
            ],
            "author": [
                "Alec Koppel",
                "Garrett Warnell",
                "Ethan Stump",
                "Alejandro Ribeiro"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/16-585/16-585.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A New Algorithm for Estimating the Effective Dimension-Reduction Subspace",
            "abstract": [
                "The statistical problem of estimating the effective dimension-reduction (EDR) subspace in the multi-index regression model with deterministic design and additive noise is considered. A new procedure for recovering the directions of the EDR subspace is proposed. Many methods for estimating the EDR subspace perform principal component analysis on a family of vectors, saŷ β 1 ,. .. ,β L , nearly lying in the EDR subspace. This is in particular the case for the structure-adaptive approach proposed by Hristache et al. (2001a). In the present work, we propose to estimate the projector onto the EDR subspace by the solution to the optimization problem minimize max =1,...,Lβ (I − A)β subject to A ∈ A m * , where A m * is the set of all symmetric matrices with eigenvalues in [0, 1] and trace less than or equal to m * , with m * being the true structural dimension. Under mild assumptions, √ n-consistency of the proposed procedure is proved (up to a logarithmic factor) in the case when the structural dimension is not larger than 4. Moreover, the stochastic error of the estimator of the projector onto the EDR subspace is shown to depend on L logarithmically. This enables us to use a large number of vectorŝ β for estimating the EDR subspace. The empirical behavior of the algorithm is studied through numerical simulations."
            ],
            "keywords": [
                "dimension-reduction",
                "multi-index regression model",
                "structure-adaptive approach",
                "central subspace"
            ],
            "author": [
                "Arnak S Dalalyan",
                "Vladimir Spokoiny"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/dalalyan08a/dalalyan08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exploiting Best-Match Equations for Efficient Reinforcement Learning Harm van Seijen",
            "abstract": [
                "This article presents and evaluates best-match learning, a new approach to reinforcement learning that trades off the sample efficiency of model-based methods with the space efficiency of modelfree methods. Best-match learning works by approximating the solution to a set of best-match equations, which combine a sparse model with a model-free Q-value function constructed from samples not used by the model. We prove that, unlike regular sparse model-based methods, bestmatch learning is guaranteed to converge to the optimal Q-values in the tabular case. Empirical results demonstrate that best-match learning can substantially outperform regular sparse modelbased methods, as well as several model-free methods that strive to improve the sample efficiency of temporal-difference methods. In addition, we demonstrate that best-match learning can be successfully combined with function approximation."
            ],
            "keywords": [
                "reinforcement learning",
                "on-line learning",
                "temporal-difference methods",
                "function approximation",
                "data reuse"
            ],
            "author": [
                "Shimon Whiteson",
                "H Van Hasselt",
                "@ Cwi",
                "Marco Wiering"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/vanseijen11a/vanseijen11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel",
            "abstract": [
                "Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classification analyses in a systematic yet flexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. ML-Flex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net."
            ],
            "keywords": [
                "toolbox",
                "classification",
                "parallel",
                "ensemble",
                "reproducible research"
            ],
            "author": [
                "Stephen R Piccolo",
                "Lewis J Frey"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/piccolo12a/piccolo12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cost-Sensitive Learning with Noisy Labels",
            "abstract": [
                "We study binary classification in the presence of class-conditional random noise, where the learner gets to see labels that are flipped independently with some probability, and where the flip probability depends on the class. Our goal is to devise learning algorithms that are efficient and statistically consistent with respect to commonly used utility measures."
            ],
            "keywords": [
                "class-conditional label noise",
                "statistical consistency",
                "cost-sensitive learning"
            ],
            "author": [
                "Nagarajan Natarajan",
                "Inderjit S Dhillon",
                "Pradeep Ravikumar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-226/15-226.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches",
            "abstract": [
                "We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets."
            ],
            "keywords": [
                "stereo",
                "matching cost",
                "similarity learning",
                "supervised learning",
                "convolutional neural networks"
            ],
            "author": [
                "Yann Lecun",
                "† Yann Lecun"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-535/15-535.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters",
            "abstract": [
                "While the model parameters of a kernel machine are typically given by the solution of a convex optimisation problem, with a single global optimum, the selection of good values for the regularisation and kernel parameters is much less straightforward. Fortunately the leave-one-out cross-validation procedure can be performed or a least approximated very efficiently in closed form for a wide variety of kernel learning methods, providing a convenient means for model selection. Leave-one-out cross-validation based estimates of performance, however, generally exhibit a relatively high variance and are therefore prone to over-fitting. In this paper, we investigate the novel use of Bayesian regularisation at the second level of inference, adding a regularisation term to the model selection criterion corresponding to a prior over the hyper-parameter values, where the additional regularisation parameters are integrated out analytically. Results obtained on a suite of thirteen real-world and synthetic benchmark data sets clearly demonstrate the benefit of this approach."
            ],
            "keywords": [
                "model selection",
                "kernel methods",
                "Bayesian regularisation"
            ],
            "author": [
                "Gavin C Cawley",
                "Nicola L C Talbot",
                "Isabelle Guyon",
                "Amir Saffari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/cawley07a/cawley07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates",
            "abstract": [
                "The problem of learning forest-structured discrete graphical models from i.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu tree through adaptive thresholding is proposed. It is shown that this algorithm is both structurally consistent and risk consistent and the error probability of structure learning decays faster than any polynomial in the number of samples under fixed model size. For the high-dimensional scenario where the size of the model d and the number of edges k scale with the number of samples n, sufficient conditions on (n, d, k) are given for the algorithm to satisfy structural and risk consistencies. In addition, the extremal structures for learning are identified; we prove that the independent (resp., tree) model is the hardest (resp., easiest) to learn using the proposed algorithm in terms of error rates for structure learning."
            ],
            "keywords": [
                "graphical models",
                "forest distributions",
                "structural consistency",
                "risk consistency",
                "method of types"
            ],
            "author": [
                "Vincent Y F Tan",
                "Wisc Edu",
                "Alan S Willsky"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/tan11a/tan11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ensemble Learning for Relational Data",
            "abstract": [
                "We present a theoretical analysis framework for relational ensemble models. We show that ensembles of collective classifiers can improve predictions for graph data by reducing errors due to variance in both learning and inference. In addition, we propose a relational ensemble framework that combines a relational ensemble learning approach with a relational ensemble inference approach for collective classification. The proposed ensemble techniques are applicable for both single and multiple graph settings. Experiments on both synthetic and real-world data demonstrate the effectiveness of the proposed framework. Finally, our experimental results support the theoretical analysis and confirm that ensemble algorithms that explicitly focus on both learning and inference processes and aim at reducing errors associated with both, are the best performers."
            ],
            "keywords": [
                "Ensemble learning",
                "relational ensemble",
                "collective classification",
                "collective inference",
                "bias-variance decomposition",
                "relational machine learning",
                "theoretical framework"
            ],
            "author": [
                "Hoda Eldardiry",
                "Jennifer Neville",
                "Ryan A Rossi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/14-368/14-368.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Manopt, a Matlab Toolbox for Optimization on Manifolds",
            "abstract": [
                "Optimization on manifolds is a rapidly developing branch of nonlinear optimization. Its focus is on problems where the smooth geometry of the search space can be leveraged to design efficient numerical algorithms. In particular, optimization on manifolds is wellsuited to deal with rank and orthogonality constraints. Such structured constraints appear pervasively in machine learning applications, including low-rank matrix completion, sensor network localization, camera network registration, independent component analysis, metric learning, dimensionality reduction and so on. The Manopt toolbox, available at www.manopt.org, is a user-friendly, documented piece of software dedicated to simplify experimenting with state of the art Riemannian optimization algorithms. By dealing internally with most of the differential geometry, the package aims particularly at lowering the entrance barrier."
            ],
            "keywords": [
                "Riemannian optimization",
                "nonlinear programming",
                "non convex",
                "orthogonality constraints",
                "rank constraints",
                "optimization with symmetries",
                "rotation matrices"
            ],
            "author": [
                "Nicolas Boumal",
                "Bamdev Mishra",
                "Rodolphe Sepulchre"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/boumal14a/boumal14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Risk Comparison of Ordinary Least Squares vs Ridge Regression",
            "abstract": [
                "We compare the risk of ridge regression to a simple variant of ordinary least squares, in which one simply projects the data onto a finite dimensional subspace (as specified by a principal component analysis) and then performs an ordinary (un-regularized) least squares regression in this subspace. This note shows that the risk of this ordinary least squares method (PCA-OLS) is within a constant factor (namely 4) of the risk of ridge regression (RR)."
            ],
            "keywords": [
                "risk inflation",
                "ridge regression",
                "pca"
            ],
            "author": [
                "Paramveer S Dhillon",
                "Dean P Foster",
                "Sham M Kakade",
                "Lyle H Ungar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/dhillon13a/dhillon13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Change Point Estimation in a Dynamic Stochastic Block Model",
            "abstract": [
                "We consider the problem of estimating the location of a single change point in a network generated by a dynamic stochastic block model mechanism. This model produces community structure in the network that exhibits change at a single time epoch. We propose two methods of estimating the change point, together with the model parameters, before and after its occurrence. The first employs a least-squares criterion function and takes into consideration the full structure of the stochastic block model and is evaluated at each point in time. Hence, as an intermediate step, it requires estimating the community structure based on a clustering algorithm at every time point. The second method comprises the following two steps: in the first one, a least-squares function is used and evaluated at each time point, but ignoring the community structure and only considering a random graph generating mechanism exhibiting a change point. Once the change point is identified, in the second step, all network data before and after it are used together with a clustering algorithm to obtain the corresponding community structures and subsequently estimate the generating stochastic block model parameters. The first method, since it requires knowledge of the community structure and hence clustering at every point in time, is significantly more computationally expensive than the second one. On the other hand, it requires a significantly less stringent identifiability condition for consistent estimation of the change point and the model parameters than the second method; however, it also requires a condition on the misclassification rate of misallocating network nodes to their respective communities that may fail to hold in many realistic settings. Despite the apparent stringency of the identifiability condition for the second method, we show that networks generated by a stochastic block mechanism exhibiting a change in their structure can easily satisfy this"
            ],
            "keywords": [],
            "author": [
                "Monika Bhattacharjee",
                "Moulinath Banerjee",
                "George Michailidis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-814/18-814.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data *",
            "abstract": [
                "Different aspects of the curse of dimensionality are known to present serious challenges to various machine-learning methods and tasks. This paper explores a new aspect of the dimensionality curse, referred to as hubness, that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theoretical and empirical analysis involving synthetic and real data sets we show that under commonly used assumptions this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent \"popular\" nearest neighbors. We examine the origins of this phenomenon, showing that it is an inherent property of data distributions in high-dimensional vector space, discuss its interaction with dimensionality reduction, and explore its influence on a wide range of machine-learning tasks directly or indirectly based on measuring distances, belonging to supervised, semi-supervised, and unsupervised learning families."
            ],
            "keywords": [
                "nearest neighbors",
                "curse of dimensionality",
                "classification",
                "semi-supervised learning",
                "clustering"
            ],
            "author": [
                "Miloš Radovanović"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/radovanovic10a/radovanovic10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Principled Selection of Hyperparameters in the Latent Dirichlet Allocation Model",
            "abstract": [
                "Latent Dirichlet Allocation (LDA) is a well known topic model that is often used to make inference regarding the properties of collections of text documents. LDA is a hierarchical Bayesian model, and involves a prior distribution on a set of latent topic variables. The prior is indexed by certain hyperparameters, and even though these have a large impact on inference, they are usually chosen either in an ad-hoc manner, or by applying an algorithm whose theoretical basis has not been firmly established. We present a method, based on a combination of Markov chain Monte Carlo and importance sampling, for estimating the maximum likelihood estimate of the hyperparameters. The method may be viewed as a computational scheme for implementation of an empirical Bayes analysis. It comes with theoretical guarantees, and a key feature of our approach is that we provide theoretically-valid error margins for our estimates. Experiments on both synthetic and real data show good performance of our methodology."
            ],
            "keywords": [
                "Empirical Bayes inference",
                "latent Dirichlet allocation",
                "Markov chain Monte Carlo",
                "model selection",
                "topic modelling"
            ],
            "author": [
                "Clint P George",
                "Hani Doss"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-595/15-595.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Multi-Stage Framework for Dantzig Selector and LASSO",
            "abstract": [
                "We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ R n×m (m ≫ n) and a noisy observation vector y ∈ R n satisfying y = Xβ * + ε where ε is the noise vector following a Gaussian distribution N(0, σ 2 I), how to recover the signal (or parameter vector) β * when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively refines the target signal β *. We show that if X obeys a certain condition, then with a large probability the difference between the solutionβ estimated by the proposed method and the true solution β * measured in terms of the ℓ p norm (p ≥ 1) is bounded as β − β * p ≤ C(s − N) 1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β * , the risk of the oracle estimator ∆ is independent of m and is much smaller than the first term, and N is the number of entries of β * larger than a certain value in the order of O(σ √ log m). The proposed method improves the estimation bound of the standard Dantzig selector approximately from Cs 1/p √ log mσ to C(s − N) 1/p √ log mσ where the value N depends on the number of large entries in β *. When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case."
            ],
            "keywords": [
                "multi-stage",
                "Dantzig selector",
                "LASSO",
                "sparse signal recovery"
            ],
            "author": [
                "Ji Liu",
                "Peter Wonka"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/liu12a/liu12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MLPs (Mono-Layer Polynomials and Multi-Layer Perceptrons) for Nonlinear Modeling",
            "abstract": [
                "This paper presents a model selection procedure which stresses the importance of the classic polynomial models as tools for evaluating the complexity of a given modeling problem, and for removing non-significant input variables. If the complexity of the problem makes a neural network necessary, the selection among neural candidates can be performed in two phases. In an additive phase, the most important one, candidate neural networks with an increasing number of hidden neurons are trained. The addition of hidden neurons is stopped when the effect of the round-off errors becomes significant, so that, for instance, confidence intervals cannot be accurately estimated. This phase leads to a set of approved candidate networks. In a subsequent subtractive phase, a selection among approved networks is performed using statistical Fisher tests. The series of tests starts from a possibly too large unbiased network (the full network), and ends with the smallest unbiased network whose input variables and hidden neurons all have a significant contribution to the regression estimate. This method was successfully tested against the real-world regression problems proposed at the NIPS2000 Unlabeled Data Supervised Learning Competition; two of them are included here as illustrative examples."
            ],
            "keywords": [
                "additive procedure",
                "approximate leave-one-out scores",
                "confidence intervals",
                "input variable selection",
                "Jacobian matrix conditioning",
                "model approval",
                "model selection",
                "neural networks",
                "orthogonalization procedure",
                "overfitting avoidance",
                "polynomials",
                "statistical tests"
            ],
            "author": [
                "Isabelle Rivals",
                "Isabelle Guyon",
                "André Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/rivals03a/rivals03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning",
            "abstract": [
                "Revealing latent structure in data is an active field of research, having introduced exciting technologies such as variational autoencoders and adversarial networks, and is essential to push machine learning towards unsupervised knowledge discovery. However, a major challenge is the lack of suitable benchmarks for an objective and quantitative evaluation of learned representations. To address this issue we introduce Morpho-MNIST, a framework that aims to answer: \"to what extent has my model learned to represent specific factors of variation in the data?\" We extend the popular MNIST dataset by adding a morphometric analysis enabling quantitative comparison of trained models, identification of the roles of latent variables, and characterisation of sample diversity. We further propose a set of quantifiable perturbations to assess the performance of unsupervised and supervised methods on challenging tasks such as outlier detection and domain adaptation. Data and code are available at https://github.com/dccastro/Morpho-MNIST."
            ],
            "keywords": [
                "representation learning",
                "generative models",
                "empirical evaluation",
                "disentanglement",
                "morphometrics"
            ],
            "author": [
                "Daniel C Castro",
                "Bernhard Kainz",
                "Ender Konukoglu",
                "Ben Glocker"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-033/19-033.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Practical Scheme and Fast Algorithm to Tune the Lasso With Optimality Guarantees",
            "abstract": [
                "We introduce a novel scheme for choosing the regularization parameter in high-dimensional linear regression with Lasso. This scheme, inspired by Lepski's method for bandwidth selection in non-parametric regression, is equipped with both optimal finite-sample guarantees and a fast algorithm. In particular, for any design matrix such that the Lasso has low sup-norm error under an \"oracle choice\" of the regularization parameter, we show that our method matches the oracle performance up to a small constant factor, and show that it can be implemented by performing simple tests along a single Lasso path. By applying the Lasso to simulated and real data, we find that our novel scheme can be faster and more accurate than standard schemes such as Cross-Validation."
            ],
            "keywords": [
                "Lasso",
                "regularization parameter",
                "tuning parameter",
                "high-dimensional regression",
                "oracle inequalities"
            ],
            "author": [
                "Michaël Chichignoud",
                "Johannes Lederer",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-605/15-605.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Efficient and Effective Generic Agglomerative Hierarchical Clustering Approach",
            "abstract": [
                "We introduce an agglomerative hierarchical clustering (AHC) framework which is generic, efficient and effective. Our approach embeds a sub-family of Lance-Williams (LW) clusterings and relies on inner-products instead of squared Euclidean distances. We carry out a constrained bottom-up merging procedure on a sparsified normalized inner-product matrix. Our method is named SNK-AHC for Sparsified Normalized Kernel matrix based AHC. SNK-AHC is more scalable than the classic dissimilarity matrix based AHC. It can also produce better results when clusters have arbitrary shapes. Artificial and real-world benchmarks are used to exemplify these points. From a theoretical standpoint, SNK-AHC provides another interpretation of the classic techniques which relies on the concept of weighted penalized similarities. The differences between group average, Mcquitty, centroid, median and Ward, can be explained by their distinct averaging strategies for aggregating clusters inter-similarities and intra-similarities. Other features of SNK-AHC are examined. We provide sufficient conditions in order to have monotonic dendrograms, we elaborate a stored data matrix approach for centroid and median, we underline the diagonal translation invariance property of group average, Mcquitty and Ward and we show to what extent SNK-AHC can determine the number of clusters."
            ],
            "keywords": [
                "Agglomerative hierarchical clustering",
                "Lance-Williams formula",
                "Kernel methods",
                "Scalability",
                "Manifold learning"
            ],
            "author": [],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-117/18-117.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Algorithm for Reading Dependencies from the Minimal Undirected Independence Map of a Graphoid that Satisfies Weak Transitivity",
            "abstract": [
                "We present a sound and complete graphical criterion for reading dependencies from the minimal undirected independence map G of a graphoid M that satisfies weak transitivity. Here, complete means that it is able to read all the dependencies in M that can be derived by applying the graphoid properties and weak transitivity to the dependencies used in the construction of G and the independencies obtained from G by vertex separation. We argue that assuming weak transitivity is not too restrictive. As an intermediate step in the derivation of the graphical criterion, we prove that for any undirected graph G there exists a strictly positive discrete probability distribution with the prescribed sample spaces that is faithful to G. We also report an algorithm that implements the graphical criterion and whose running time is considered to be at most O(n 2 (e + n)) for n nodes and e edges. Finally, we illustrate how the graphical criterion can be used within bioinformatics to identify biologically meaningful gene dependencies."
            ],
            "keywords": [
                "graphical models",
                "vertex separation",
                "graphoids",
                "weak transitivity",
                "bioinformatics"
            ],
            "author": [
                "Jose M Peña",
                "Roland Nilsson",
                "Nilsson @ Chgr",
                "Harvard Mgh",
                "Johan Björkegren",
                "Johan Bjorkegren",
                "K I Se",
                "Jesper Tegnér"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/pena09a/pena09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "spark-crowd: A Spark Package for Learning from Crowdsourced Big Data",
            "abstract": [
                "As the data sets increase in size, the process of manually labeling data becomes unfeasible by small groups of experts. Thus, it is common to rely on crowdsourcing platforms which provide inexpensive, but noisy, labels. Although implementations of algorithms to tackle this problem exist, none of them focus on scalability, limiting the area of application to relatively small data sets. In this paper, we present spark-crowd, an Apache Spark package for learning from crowdsourced data with scalability in mind."
            ],
            "keywords": [
                "crowdsourcing",
                "crowdsourced data",
                "learning from crowds",
                "big data",
                "multiple noisy labeling",
                "spark"
            ],
            "author": [
                "Enrique G Rodrigo",
                "Juan A Aledo"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-743/17-743.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM",
            "abstract": [
                "In this paper we propose a new multilayer classifier architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classification module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classification module we use support vector machines (SVMs)-here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classification module classifies the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classification module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classification module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been verified with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed."
            ],
            "keywords": [
                "feature extraction",
                "neural networks (NNs)",
                "support vector machines (SVMs)",
                "hybrid system",
                "majority voting",
                "averaging"
            ],
            "author": [
                "Pradip Ghanty"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/ghanty09a/ghanty09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Permutations with Exponential Weights *",
            "abstract": [
                "We give an algorithm for the on-line learning of permutations. The algorithm maintains its uncertainty about the target permutation as a doubly stochastic weight matrix, and makes predictions using an efficient method for decomposing the weight matrix into a convex combination of permutations. The weight matrix is updated by multiplying the current matrix entries by exponential factors, and an iterative procedure is needed to restore double stochasticity. Even though the result of this procedure does not have a closed form, a new analysis approach allows us to prove an optimal (up to small constant factors) bound on the regret of our algorithm. This regret bound is significantly better than that of either Kalai and Vempala's more efficient Follow the Perturbed Leader algorithm or the computationally expensive method of explicitly representing each permutation as an expert."
            ],
            "keywords": [
                "permutation",
                "ranking",
                "on-line learning",
                "Hedge algorithm",
                "doubly stochastic matrix",
                "relative entropy projection",
                "Sinkhorn balancing"
            ],
            "author": [
                "David P Helmbold",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/helmbold09a/helmbold09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ground Metric Learning",
            "abstract": [
                "Optimal transport distances have been used for more than a decade in machine learning to compare histograms of features. They have one parameter: the ground metric, which can be any metric between the features themselves. As is the case for all parameterized distances, optimal transport distances can only prove useful in practice when this parameter is carefully chosen. To date, the only option available to practitioners to set the ground metric parameter was to rely on a priori knowledge of the features, which limited considerably the scope of application of optimal transport distances. We propose to lift this limitation and consider instead algorithms that can learn the ground metric using only a training set of labeled histograms. We call this approach ground metric learning. We formulate the problem of learning the ground metric as the minimization of the difference of two convex polyhedral functions over a convex set of metric matrices. We follow the presentation of our algorithms with promising experimental results which show that this approach is useful both for retrieval and binary/multiclass classification tasks."
            ],
            "keywords": [
                "optimal transport distance",
                "earth mover's distance",
                "metric learning",
                "metric nearness"
            ],
            "author": [
                "Marco Cuturi",
                "David Avis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/cuturi14a/cuturi14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analysis of Variance of Cross-Validation Estimators of the Generalization Error",
            "abstract": [
                "This paper brings together methods from two different disciplines: statistics and machine learning. We address the problem of estimating the variance of cross-validation (CV) estimators of the generalization error. In particular, we approach the problem of variance estimation of the CV estimators of generalization error as a problem in approximating the moments of a statistic. The approximation illustrates the role of training and test sets in the performance of the algorithm. It provides a unifying approach to evaluation of various methods used in obtaining training and test sets and it takes into account the variability due to different training and test sets. For the simple problem of predicting the sample mean and in the case of smooth loss functions, we show that the variance of the CV estimator of the generalization error is a function of the moments of the random variables Y = Card(S j T S j) and Y * = Card(S c j T S c j), where S j , S j are two training sets, and S c j , S c j are the corresponding test sets. We prove that the distribution of Y and Y* is hypergeometric and we compare our estimator with the one proposed by Nadeau and Bengio (2003). We extend these results in the regression case and the case of absolute error loss, and indicate how the methods can be extended to the classification case. We illustrate the results through simulation."
            ],
            "keywords": [
                "cross-validation",
                "generalization error",
                "moment approximation",
                "prediction",
                "variance estimation"
            ],
            "author": [
                "Marianthi Markatou",
                "Hong Tian",
                "George Hripcsak"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/markatou05a/markatou05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Using Markov Blankets for Causal Structure Learning",
            "abstract": [
                "We show how a generic feature-selection algorithm returning strongly relevant variables can be turned into a causal structure-learning algorithm. We prove this under the Faithfulness assumption for the data distribution. In a causal graph, the strongly relevant variables for a node X are its parents, children, and children's parents (or spouses), also known as the Markov blanket of X. Identifying the spouses leads to the detection of the V-structure patterns and thus to causal orientations. Repeating the task for all variables yields a valid partially oriented causal graph. We first show an efficient way to identify the spouse links. We then perform several experiments in the continuous domain using the Recursive Feature Elimination feature-selection algorithm with Support Vector Regression and empirically verify the intuition of this direct (but computationally expensive) approach. Within the same framework, we then devise a fast and consistent algorithm, Total Conditioning (TC), and a variant, TC bw , with an explicit backward feature-selection heuristics, for Gaussian data. After running a series of comparative experiments on five artificial networks, we argue that Markov blanket algorithms such as TC/TC bw or Grow-Shrink scale better than the reference PC algorithm and provides higher structural accuracy."
            ],
            "keywords": [
                "causal structure learning",
                "feature selection",
                "Markov blanket",
                "partial correlation",
                "statistical test of conditional independence"
            ],
            "author": [
                "Jean-Philippe Pellet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/pellet08a/pellet08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ivanov-Regularised Least-Squares Estimators over Large RKHSs and Their Interpolation Spaces",
            "abstract": [
                "We study kernel least-squares estimation under a norm constraint. This form of regularisation is known as Ivanov regularisation and it provides better control of the norm of the estimator than the well-established Tikhonov regularisation. Ivanov regularisation can be studied under minimal assumptions. In particular, we assume only that the RKHS is separable with a bounded and measurable kernel. We provide rates of convergence for the expected squared L 2 error of our estimator under the weak assumption that the variance of the response variables is bounded and the unknown regression function lies in an interpolation space between L 2 and the RKHS. We then obtain faster rates of convergence when the regression function is bounded by clipping the estimator. In fact, we attain the optimal rate of convergence. Furthermore, we provide a high-probability bound under the stronger assumption that the response variables have subgaussian errors and that the regression function lies in an interpolation space between L ∞ and the RKHS. Finally, we derive adaptive results for the settings in which the regression function is bounded."
            ],
            "keywords": [
                "Interpolation Space",
                "Ivanov Regularisation",
                "Regression",
                "RKHS",
                "Training and Validation"
            ],
            "author": [
                "Steffen Grünewälder"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-672/17-672.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Methods for Measuring Independence",
            "abstract": [
                "We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces (RKHSs). We prove that when the RKHSs are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is verified in the context of independent component analysis."
            ],
            "keywords": [
                "independence",
                "covariance operator",
                "mutual information",
                "kernel",
                "Parzen window estimate",
                "independent component analysis"
            ],
            "author": [
                "Arthur Gretton",
                "Ralf Herbrich",
                "Alexander Smola",
                "Olivier Bousquet",
                "Bernhard Schölkopf",
                "HERBRICH, SMOLA, BOUSQUET AND SCHÖLKOPF Gretton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/gretton05a/gretton05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse/Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory *",
            "abstract": [
                "*. The authors would like to thank Bradley M. Bell for insightful discussions and helpful suggestions. The research leading to these results has received funding from the European Union Seventh Framework Programme [FP7/2007-2013] under grant agreement no 257462 HYCON2 Network of excellence, by the MIUR FIRB project RBFR12M3AC-Learning meets time: a new computational approach to learning in dynamic systems."
            ],
            "keywords": [
                "statistical modeling",
                "convex analysis",
                "nonsmooth optimization",
                "robust inference",
                "sparsity optimization",
                "Kalman smoothing",
                "interior point methods"
            ],
            "author": [
                "Aleksandr Y Aravkin",
                "Ibm T J Watson",
                "James V Burke",
                "Gianluigi Pillonetto"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/aravkin13a/aravkin13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Rates for Q-learning Eyal Even-Dar",
            "abstract": [
                "In this paper we derive convergence rates for Q-learning. We show an interesting relationship between the convergence rate and the learning rate used in Q-learning. For a polynomial learning rate, one which is 1/t ω at time t where ω ∈ (1/2, 1), we show that the convergence rate is polynomial in 1/(1 − γ), where γ is the discount factor. In contrast we show that for a linear learning rate, one which is 1/t at time t, the convergence rate has an exponential dependence on 1/(1 − γ). In addition we show a simple example that proves this exponential behavior is inherent for linear learning rates."
            ],
            "keywords": [
                "Reinforcement Learning",
                "Q-Learning",
                "Stochastic Processes",
                "Convergence Bounds",
                "Learning Rates"
            ],
            "author": [],
            "ref": "http://www.jmlr.org/papers/volume5/evendar03a/evendar03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management",
            "abstract": [
                "We consider the problem of estimating the value of a multiattribute resource, where the attributes are categorical or discrete in nature and the number of potential attribute vectors is very large. The problem arises in approximate dynamic programming when we need to estimate the value of a multiattribute resource from estimates based on Monte-Carlo simulation. These problems have been traditionally solved using aggregation, but choosing the right level of aggregation requires resolving the classic tradeoff between aggregation error and sampling error. We propose a method that estimates the value of a resource at different levels of aggregation simultaneously, and then uses a weighted combination of the estimates. Using the optimal weights, which minimizes the variance of the estimate while accounting for correlations between the estimates, is computationally too expensive for practical applications. We have found that a simple inverse variance formula (adjusted for bias), which effectively assumes the estimates are independent, produces near-optimal estimates. We use the setting of two levels of aggregation to explain why this approximation works so well."
            ],
            "keywords": [
                "hierarchical statistics",
                "approximate dynamic programming",
                "mixture models",
                "adaptive learning",
                "multiattribute resources"
            ],
            "author": [
                "Abraham George",
                "Warren B Powell",
                "Sanjeev R Kulkarni"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/george08a/george08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Data-dependent margin-based generalization bounds for classification",
            "abstract": [
                "We derive new margin-based inequalities for the probability of error of classifiers. The main feature of these bounds is that they can be calculated using the training data and therefore may be effectively used for model selection purposes. In particular, the bounds involve empirical complexities measured on the training data (such as the empirical fatshattering dimension) as opposed to their worst-case counterparts traditionally used in such analyses. Also, our bounds appear to be sharper and more general than recent results involving empirical complexity measures. In addition, we develop an alternative data-based bound for the generalization error of classes of convex combinations of classifiers involving an empirical complexity measure that is easier to compute than the empirical covering number or fat-shattering dimension. We also show examples of efficient computation of the new bounds."
            ],
            "keywords": [
                "classification",
                "margin-based bounds",
                "error estimation",
                "fat-shattering dimension"
            ],
            "author": [
                "András Antos",
                "Tamás Linder"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/antos02a/antos02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation",
            "abstract": [
                "We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efficiently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about specific groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy."
            ],
            "keywords": [
                "group feature selection",
                "generalized spike-and-slab priors",
                "expectation propagation",
                "sparse linear model",
                "approximate inference",
                "sequential experimental design",
                "signal reconstruction"
            ],
            "author": [
                "Daniel Hernández-Lobato",
                "José Miguel Hernández-Lobato",
                "Pierre Dupont"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/hernandez-lobato13a/hernandez-lobato13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Wide Neural Networks with Bottlenecks are Deep Gaussian Processes",
            "abstract": [
                "There has recently been much work on the \"wide limit\" of neural networks, where Bayesian neural networks (BNNs) are shown to converge to a Gaussian process (GP) as all hidden layers are sent to infinite width. However, these results do not apply to architectures that require one or more of the hidden layers to remain narrow. In this paper, we consider the wide limit of BNNs where some hidden layers, called \"bottlenecks\", are held at finite width. The result is a composition of GPs that we term a \"bottleneck neural network Gaussian process\" (bottleneck NNGP). Although intuitive, the subtlety of the proof is in showing that the wide limit of a composition of networks is in fact the composition of the limiting GPs. We also analyze theoretically a single-bottleneck NNGP, finding that the bottleneck induces dependence between the outputs of a multi-output network that persists through extreme post-bottleneck depths, and prevents the kernel of the network from losing discriminative power at extreme post-bottleneck depths."
            ],
            "keywords": [
                "Bayesian neural networks",
                "deep learning",
                "Gaussian processes",
                "kernels",
                "phase transitions"
            ],
            "author": [
                "Devanshu Agrawal",
                "Theodore Papamarkou",
                "Jacob Hinkle"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-017/20-017.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Margin Maximizing with Boosting *",
            "abstract": [
                "AdaBoost produces a linear combination of base hypotheses and predicts with the sign of this linear combination. The linear combination may be viewed as a hyperplane in feature space where the base hypotheses form the features. It has been observed that the generalization error of the algorithm continues to improve even after all examples are on the correct side of the current hyperplane. The improvement is attributed to the experimental observation that the distances (margins) of the examples to the separating hyperplane are increasing even after all examples are on the correct side. We introduce a new version of AdaBoost, called AdaBoost * ν , that explicitly maximizes the minimum margin of the examples up to a given precision. The algorithm incorporates a current estimate of the achievable margin into its calculation of the linear coefficients of the base hypotheses. The bound on the number of iterations needed by the new algorithms is the same as the number needed by a known version of AdaBoost that must have an explicit estimate of the achievable margin as a parameter. We also illustrate experimentally that our algorithm requires considerably fewer iterations than other algorithms that aim to maximize the margin."
            ],
            "keywords": [],
            "author": [
                "Gunnar Rätsch",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/ratsch05a/ratsch05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust high dimensional learning for Lipschitz and convex losses",
            "abstract": [
                "We establish risk bounds for Regularized Empirical Risk Minimizers (RERM) when the loss is Lipschitz and convex and the regularization function is a norm. In a first part, we obtain these results in the i.i.d. setup under subgaussian assumptions on the design. In a second part, a more general framework where the design might have heavier tails and data may be corrupted by outliers both in the design and the response variables is considered. In this situation, RERM performs poorly in general. We analyse an alternative procedure based on median-of-means principles and called \"minmax MOM\". We show optimal subgaussian deviation rates for these estimators in the relaxed setting. The main results are meta-theorems allowing a wide-range of applications to various problems in learning theory. To show a non-exhaustive sample of these potential applications, it is applied to classification problems with logistic loss functions regularized by LASSO and SLOPE, to regression problems with Huber loss regularized by Group LASSO and Total Variation. Another advantage of the minmax MOM formulation is that it suggests a systematic way to slightly modify descent based algorithms used in high-dimensional statistics to make them robust to outliers Lecué and Lerasle (2017b). We illustrate this principle in a Simulations section where a \" minmax MOM\" version of classical proximal descent algorithms are turned into robust to outliers algorithms."
            ],
            "keywords": [
                "Robust Learning",
                "Lipschtiz and convex loss functions",
                "sparsity bounds",
                "Rademacher complexity bounds",
                "LASSO",
                "SLOPE",
                "Group LASSO",
                "Total Variation"
            ],
            "author": [
                "Chinot Geoffrey",
                "Lecué Guillaume",
                "Lerasle Matthieu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-585/19-585.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DESlib: A Dynamic ensemble selection library in Python",
            "abstract": [
                "DESlib is an open-source python library providing the implementation of several dynamic selection techniques. The library is divided into three modules: (i) dcs, containing the implementation of dynamic classifier selection methods (DCS); (ii) des, containing the implementation of dynamic ensemble selection methods (DES); (iii) static, with the implementation of static ensemble techniques. The library is fully documented (documentation available online on Read the Docs), has a high test coverage (codecov.io) and is part of the scikit-learn-contrib supported projects. Documentation, code and examples can be found on its GitHub page: https://github.com/scikit-learn-contrib/DESlib."
            ],
            "keywords": [
                "Multiple classifier systems",
                "Ensemble of Classifiers",
                "Dynamic classifier selection",
                "Dynamic ensemble selection",
                "Machine learning",
                "Python"
            ],
            "author": [
                "Rafael M O Cruz",
                "Luiz G Hafemann",
                "Robert Sabourin",
                "George D C Cavalcanti"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-144/18-144.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Knowledge Graph Completion via Complex Tensor Factorization",
            "abstract": [
                "In statistical relational learning, knowledge graph completion deals with automatically understanding the structure of large knowledge graphs-labeled directed graphsand predicting missing relationships-labeled edges. State-of-the-art embedding models propose different trade-offs between modeling expressiveness, and time and space complexity. We reconcile both expressiveness and complexity through the use of complex-valued embeddings and explore the link between such complex-valued embeddings and unitary diagonalization. We corroborate our approach theoretically and show that all real square matrices-thus all possible relation/adjacency matrices-are the real part of some unitarily diagonalizable matrix. This results opens the door to a lot of other applications of square matrices factorization. Our approach based on complex embeddings is arguably simple, as it only involves a Hermitian dot product, the complex counterpart of the standard dot product between real vectors, whereas other methods resort to more and more complicated composition functions to increase their expressiveness. The proposed complex embeddings are scalable to large data sets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks."
            ],
            "keywords": [
                "complex embeddings",
                "tensor factorization",
                "knowledge graph",
                "matrix completion",
                "statistical relational learning"
            ],
            "author": [
                "Théo Trouillon",
                "Eric Gaussier",
                "Johannes Welbl",
                "Sebastian Riedel",
                "Guillaume Bouchard"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-563/16-563.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient SVM Training Using Low-Rank Kernel Representations",
            "abstract": [
                "SVM training is a convex optimization problem which scales with the training set size rather than the feature space dimension. While this is usually considered to be a desired quality, in large scale problems it may cause training to be impractical. The common techniques to handle this difficulty basically build a solution by solving a sequence of small scale subproblems. Our current effort is concentrated on the rank of the kernel matrix as a source for further enhancement of the training procedure. We first show that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity. We then suggest an efficient use of a known factorization technique to approximate a given kernel matrix by a low rank matrix, which in turn will be used to feed the optimizer. Finally, we derive an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors). This bound is general in the sense that it holds regardless of the approximation method."
            ],
            "keywords": [
                "Support Vector Machine",
                "Interior Point Method",
                "Cholesky Product Form",
                "Cholesky Factorization",
                "Approximate Solution"
            ],
            "author": [
                "Shai Fine",
                "Katya Scheinberg",
                "Nello Cristianini",
                "Bob Williamson"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/fine01a/fine01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Using Anti-Training with Sacrificial Data",
            "abstract": [
                "\\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{\\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ 17 (2016) 1-42 \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{\\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ 12/13; \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ 10/15; \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ \\mathrm{ 4/16"
            ],
            "keywords": [
                "Machine Learning",
                "Optimization",
                "Meta Optimization",
                "No Free Lunch",
                "Anti-Training",
                "Sacrificial Data"
            ],
            "author": [
                "Michael L Valenzuela",
                "Jerzy W Rozenblit"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/13-589/13-589.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A New and Flexible Approach to the Analysis of Paired Comparison Data",
            "abstract": [
                "We consider the situation where I items are ranked by paired comparisons. It is usually assumed that the probability that item i is preferred over item j is p ij = F (µ i −µ j) where F is a symmetric distribution function, which we refer to as the comparison function, and µ i and µ j are the merits or scores of the compared items. This modelling framework, which is ubiquitous in the paired comparison literature, strongly depends on the assumption that the comparison function F is known. In practice, however, this assumption is often unrealistic and may result in poor fit and erroneous inferences. This limitation has motivated us to relax the assumption that F is fully known and simultaneously estimate the merits of the objects and the underlying comparison function. Our formulation yields a flexible semi-definite programming problem that we use as a refinement step for estimating the paired comparison probability matrix. We provide a detailed sensitivity analysis and, as a result, we establish the consistency of the resulting estimators and provide bounds on the estimation and approximation errors. Some statistical properties of the resulting estimators as well as model selection criteria are investigated. Finally, using a large data-set of computer chess matches, we estimate the comparison function and find that the model used by the International Chess Federation does not seem to apply to computer chess."
            ],
            "keywords": [
                "linear stochastic transitivity",
                "statistical ranking",
                "semi-definite programming",
                "model selection",
                "sensitivity analysis",
                "chess"
            ],
            "author": [
                "Ivo F D Oliveira",
                "Nir Ailon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-179/17-179.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Conjugate Gradients for Kernel Machines",
            "abstract": [
                "Regularized least-squares (kernel-ridge / Gaussian process) regression is a fundamental algorithm of statistics and machine learning. Because generic algorithms for the exact solution have cubic complexity in the number of datapoints, large datasets require to resort to approximations. In this work, the computation of the least-squares prediction is itself treated as a probabilistic inference problem. We propose a structured Gaussian regression model on the kernel function that uses projections of the kernel matrix to obtain a low-rank approximation of the kernel and the matrix. A central result is an enhanced way to use the method of conjugate gradients for the specific setting of least-squares regression as encountered in machine learning."
            ],
            "keywords": [
                "Gaussian processes",
                "kernel methods",
                "low-rank approximation",
                "conjugate gradients",
                "probabilistic numerics"
            ],
            "author": [
                "Simon Bartels"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-473/18-473.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Wavelet decompositions of Random Forests -smoothness analysis, sparse approximation and applications",
            "abstract": [
                "In this paper we introduce, in the setting of machine learning, a generalization of wavelet analysis which is a popular approach to low dimensional structured signal analysis. The wavelet decomposition of a Random Forest provides a sparse approximation of any regression or classification high dimensional function at various levels of detail, with a concrete ordering of the Random Forest nodes: from 'significant' elements to nodes capturing only 'insignificant' noise. Motivated by function space theory, we use the wavelet decomposition to compute numerically a 'weak-type' smoothness index that captures the complexity of the underlying function. As we show through extensive experimentation, this sparse representation facilitates a variety of applications such as improved regression for difficult datasets, a novel approach to feature importance, resilience to noisy or irrelevant features, compression of ensembles, etc."
            ],
            "keywords": [
                "Random Forest",
                "Wavelets",
                "Besov spaces",
                "adaptive approximation",
                "feature importance"
            ],
            "author": [
                "Oren Elisha",
                "Lawrence Carin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-203/15-203.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Finding Optimal Bayesian Networks Using Precedence Constraints *",
            "abstract": [
                "We consider the problem of finding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space 2 n , to within a factor polynomial in the number of nodes n. In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: first, the user may trade space against time; second, the proposed algorithms easily and efficiently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order P on the nodes, an optimal DAG compatible with P can be found in time and space roughly proportional to the number of ideals of P , which can be significantly less than 2 n. Considering sufficiently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders."
            ],
            "keywords": [
                "exact algorithm",
                "parallelization",
                "partial order",
                "space-time tradeoff",
                "structure learning"
            ],
            "author": [
                "Pekka Parviainen",
                "Mikko Koivisto"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/parviainen13a/parviainen13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Learning in the Embedded Manifold of Low-rank Matrices",
            "abstract": [
                "When learning models that are represented in matrix forms, enforcing a low-rank constraint can dramatically improve the memory and run time complexity, while providing a natural regularization of the model. However, naive approaches to minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low-rank matrix). We build on recent advances in optimization over manifolds, and describe an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold. While the ideal retraction is costly to compute, and so is the projection operator that approximates it, we describe another retraction that can be computed efficiently. It has run time and memory complexity of O ((n + m)k) for a rank-k matrix of dimension m × n, when using an online procedure with rank-one gradients. We use this algorithm, LORETA, to learn a matrix-form similarity measure over pairs of documents represented as high dimensional vectors. LORETA improves the mean average precision over a passive-aggressive approach in a factorized model, and also improves over a full model trained on pre-selected features using the same memory requirements. We further adapt LORETA to learn positive semi-definite low-rank matrices, providing an online algorithm for low-rank metric learning. LORETA also shows consistent improvement over standard weakly supervised methods in a large (1600 classes and 1 million images, using ImageNet) multi-label image classification task."
            ],
            "keywords": [
                "low rank",
                "Riemannian manifolds",
                "metric learning",
                "retractions",
                "multitask learning",
                "online learning"
            ],
            "author": [
                "Uri Shalit",
                "Daphna Weinshall"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/shalit12a/shalit12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classification with Incomplete Data Using Dirichlet Process Priors",
            "abstract": [
                "A non-parametric hierarchical Bayesian framework is developed for designing a classifier, based on a mixture of simple (linear) classifiers. Each simple classifier is termed a local \"expert\", and the number of experts and their construction are manifested via a Dirichlet process formulation. The simple form of the \"experts\" allows analytical handling of incomplete data. The model is extended to allow simultaneous design of classifiers on multiple data sets, termed multi-task learning, with this also performed non-parametrically via the Dirichlet process. Fast inference is performed using variational Bayesian (VB) analysis, and example results are presented for several data sets. We also perform inference via Gibbs sampling, to which we compare the VB results."
            ],
            "keywords": [
                "classification",
                "incomplete data",
                "expert",
                "Dirichlet process",
                "variational Bayesian",
                "multitask learning"
            ],
            "author": [
                "Chunping Wang",
                "Xuejun Liao",
                "Lawrence Carin",
                "David B Dunson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/wang10a/wang10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Algorithms for Second-Price Auctions with Reserve",
            "abstract": [
                "Second-price auctions with reserve play a critical role in the revenue of modern search engine and popular online sites since the revenue of these companies often directly depends on the outcome of such auctions. The choice of the reserve price is the main mechanism through which the auction revenue can be influenced in these electronic markets. We cast the problem of selecting the reserve price to optimize revenue as a learning problem and present a full theoretical analysis dealing with the complex properties of the corresponding loss function. We further give novel algorithms for solving this problem and report the results of several experiments in both synthetic and real-world data demonstrating their effectiveness."
            ],
            "keywords": [
                "Learning Theory",
                "Auctions",
                "Revenue Optimization"
            ],
            "author": [
                "Mehryar Mohri",
                "Andrés Muñoz Medina"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-499/14-499.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GFA: Exploratory Analysis of Multiple Data Sources with Group Factor Analysis",
            "abstract": [
                "The R package GFA provides a full pipeline for factor analysis of multiple data sources that are represented as matrices with co-occurring samples. It allows learning dependencies between subsets of the data sources, decomposed into latent factors. The package also implements sparse priors for the factorization, providing interpretable biclusters of the multi-source data."
            ],
            "keywords": [
                "Bayesian latent variable modelling",
                "biclustering",
                "data integration",
                "factor analysis",
                "multi-view learning"
            ],
            "author": [
                "Eemeli Leppäaho",
                "Samuel Kaski"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-509/16-509.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "BudgetedSVM: A Toolbox for Scalable SVM Approximations",
            "abstract": [
                "We present BudgetedSVM, an open-source C++ toolbox comprising highly-optimized implementations of recently proposed algorithms for scalable training of Support Vector Machine (SVM) approximators: Adaptive Multi-hyperplane Machines, Low-rank Linearization SVM, and Budgeted Stochastic Gradient Descent. BudgetedSVM trains models with accuracy comparable to LibSVM in time comparable to LibLinear, solving non-linear problems with millions of high-dimensional examples within minutes on a regular computer. We provide command-line and Matlab interfaces to BudgetedSVM, an efficient API for handling large-scale, high-dimensional data sets, as well as detailed documentation to help developers use and further extend the toolbox."
            ],
            "keywords": [
                "non-linear classification",
                "large-scale learning",
                "SVM",
                "machine learning toolbox"
            ],
            "author": [
                "Nemanja Djuric",
                "Liang Lan",
                "Zhuang Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/djuric13a/djuric13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Representing and Generating Kernels by Fuzzy Equivalence Relations",
            "abstract": [
                "Kernels are two-placed functions that can be interpreted as inner products in some Hilbert space. It is this property which makes kernels predestinated to carry linear models of learning, optimization or classification strategies over to non-linear variants. Following this idea, various kernel-based methods like support vector machines or kernel principal component analysis have been conceived which prove to be successful for machine learning, data mining and computer vision applications. When applying a kernel-based method a central question is the choice and the design of the kernel function. This paper provides a novel view on kernels based on fuzzy-logical concepts which allows to incorporate prior knowledge in the design process. It is demonstrated that kernels mapping to the unit interval with constant one in its diagonal can be represented by a commonly used fuzzylogical formula for representing fuzzy rule bases. This means that a great class of kernels can be represented by fuzzy-logical concepts. Apart from this result, which only guarantees the existence of such a representation, constructive examples are presented and the relation to unlabeled learning is pointed out."
            ],
            "keywords": [
                "kernel",
                "triangular norm",
                "T -transitivity",
                "fuzzy relation",
                "residuum"
            ],
            "author": [
                "Bernhard Moser"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/moser06a/moser06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Extracting PICO Sentences from Clinical Trial Reports using Supervised Distant Supervision",
            "abstract": [
                "Systematic reviews underpin Evidence Based Medicine (EBM) by addressing precise clinical questions via comprehensive synthesis of all relevant published evidence. Authors of systematic reviews typically define a Population/Problem, Intervention, Comparator, and Outcome (a PICO criteria) of interest, and then retrieve, appraise and synthesize results from all reports of clinical trials that meet these criteria. Identifying PICO elements in the full-texts of trial reports is thus a critical yet time-consuming step in the systematic review process. We seek to expedite evidence synthesis by developing machine learning models to automatically extract sentences from articles relevant to PICO elements. Collecting a large corpus of training data for this task would be prohibitively expensive. Therefore, we derive distant supervision (DS) with which to train models using previously conducted reviews. DS entails heuristically deriving 'soft' labels from an available structured resource. However, we have access only to unstructured, free-text summaries of PICO elements for corresponding articles; we must derive from these the desired sentence-level annotations."
            ],
            "keywords": [
                "Evidence-based medicine",
                "distant supervision",
                "data extraction",
                "text mining",
                "natural language processing"
            ],
            "author": [
                "Byron C Wallace",
                "Joël Kuiper",
                "Aakash Sharma",
                "Iain J Marshall",
                "Joel Kuiper",
                "Mingxi Zhu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-404/15-404.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Two New Approaches to Compressed Sensing Exhibiting Both Robust Sparse Recovery and the Grouping Effect",
            "abstract": [
                "In this paper we introduce a new optimization formulation for sparse regression and compressed sensing, called CLOT (Combined LOne and Two), wherein the regularizer is a convex combination of the 1-and 2-norms. This formulation differs from the Elastic Net (EN) formulation, in which the regularizer is a convex combination of the 1-and 2-norm squared. It is shown that, in the context of compressed sensing, the EN formulation does not achieve robust recovery of sparse vectors, whereas the new CLOT formulation achieves robust recovery. Also, like EN but unlike LASSO, the CLOT formulation achieves the grouping effect, wherein coefficients of highly correlated columns of the measurement (or design) matrix are assigned roughly comparable values. It is already known LASSO does not have the grouping effect. Therefore the CLOT formulation combines the best features of both LASSO (robust sparse recovery) and EN (grouping effect). The CLOT formulation is a special case of another one called SGL (Sparse Group LASSO) which was introduced into the literature previously, but without any analysis of either the grouping effect or robust sparse recovery. It is shown here that SGL achieves robust sparse recovery, and also achieves a version of the grouping effect in that coefficients of highly correlated columns belonging to the same group of the measurement (or design) matrix are assigned roughly comparable values."
            ],
            "keywords": [
                "Sparse regression",
                "compressed sensing",
                "LASSO",
                "Sparse Group LASSO",
                "Elastic Net"
            ],
            "author": [
                "Eren Mehmet",
                "Niharika Challapalli",
                "Mathukumalli Vidyasagar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-453/14-453.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Frequent Directions with Application in Online Learning",
            "abstract": [
                "The frequent directions (FD) technique is a deterministic approach for online sketching that has many applications in machine learning. The conventional FD is a heuristic procedure that often outputs rank deficient matrices. To overcome the rank deficiency problem, we propose a new sketching strategy called robust frequent directions (RFD) by introducing a regularization term. RFD can be derived from an optimization problem. It updates the sketch matrix and the regularization term adaptively and jointly. RFD reduces the approximation error of FD without increasing the computational cost. We also apply RFD to online learning and propose an effective hyperparameterfree online Newton algorithm. We derive a regret bound for our online Newton algorithm based on RFD, which guarantees the robustness of the algorithm. The experimental studies demonstrate that the proposed method outperforms state-of-the-art second order online learning algorithms."
            ],
            "keywords": [
                "Matrix approximation",
                "sketching",
                "frequent directions",
                "online convex optimization",
                "online Newton algorithm"
            ],
            "author": [
                "Luo Luo",
                "Cheng Chen",
                "Zhihua Zhang",
                "Wu-Jun Li",
                "Tong Zhang",
                "Hong Kong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-773/17-773.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tracking a Small Set of Experts by Mixing Past Posteriors",
            "abstract": [
                "In this paper, we examine on-line learning problems in which the target concept is allowed to change over time. In each trial a master algorithm receives predictions from a large set of n experts. Its goal is to predict almost as well as the best sequence of such experts chosen off-line by partitioning the training sequence into k + 1 sections and then choosing the best expert for each section. We build on methods developed by Herbster and Warmuth and consider an open problem posed by Freund where the experts in the best partition are from a small pool of size m. Since k m, the best expert shifts back and forth between the experts of the small pool. We propose algorithms that solve this open problem by mixing the past posteriors maintained by the master algorithm. We relate the number of bits needed for encoding the best partition to the loss bounds of the algorithms. Instead of paying log n for choosing the best expert in each section we first pay log n m bits in the bounds for identifying the pool of m experts and then log m bits per new section. In the bounds we also pay twice for encoding the boundaries of the sections."
            ],
            "keywords": [
                "On-line learning",
                "loss bounds",
                "shifting experts",
                "share updates"
            ],
            "author": [
                "Olivier Bousquet",
                "Manfred K Warmuth"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/bousquet02b/bousquet02b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A General Framework for Fast Stagewise Algorithms",
            "abstract": [
                "Forward stagewise regression follows a very simple strategy for constructing a sequence of sparse regression estimates: it starts with all coefficients equal to zero, and iteratively updates the coefficient (by a small amount) of the variable that achieves the maximal absolute inner product with the current residual. This procedure has an interesting connection to the lasso: under some conditions, it is known that the sequence of forward stagewise estimates exactly coincides with the lasso path, as the step size goes to zero. Furthermore, essentially the same equivalence holds outside of least squares regression, with the minimization of a differentiable convex loss function subject to an 1 norm constraint (the stagewise algorithm now updates the coefficient corresponding to the maximal absolute component of the gradient). Even when they do not match their 1-constrained analogues, stagewise estimates provide a useful approximation, and are computationally appealing. Their success in sparse modeling motivates the question: can a simple, effective strategy like forward stagewise be applied more broadly in other regularization settings, beyond the 1 norm and sparsity? The current paper is an attempt to do just this. We present a general framework for stagewise estimation, which yields fast algorithms for problems such as group-structured learning, matrix completion, image denoising, and more."
            ],
            "keywords": [
                "forward stagewise regression",
                "lasso",
                "-boosting",
                "regularization paths"
            ],
            "author": [
                "Ryan J Tibshirani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/tibshirani15a/tibshirani15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study *",
            "abstract": [
                "Multi Agent Reinforcement Learning (MARL) has received continually growing attention in the past decade. Many algorithms that vary in their approaches to the different subtasks of MARL have been developed. However, the theoretical convergence results for these algorithms do not give a clue as to their practical performance nor supply insights to the dynamics of the learning process itself. This work is a comprehensive empirical study conducted on MGS, a simulation system developed for this purpose. It surveys the important algorithms in the field, demonstrates the strengths and weaknesses of the different approaches to MARL through application of FriendQ, OAL, WoLF, FoeQ, Rmax, and other algorithms to a variety of fully cooperative and fully competitive domains in self and heterogeneous play, and supplies an informal analysis of the resulting learning processes. The results can aid in the design of new learning algorithms, in matching existing algorithms to specific tasks, and may guide further research and formal analysis of the learning processes."
            ],
            "keywords": [
                "reinforcement learning",
                "multi-agent reinforcement learning",
                "stochastic games"
            ],
            "author": [
                "Avraham Bab",
                "Ronen I Brafman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/bab08a/bab08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum-Gain Working Set Selection for SVMs",
            "abstract": [
                "Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is significantly faster."
            ],
            "keywords": [
                "working set selection",
                "sequential minimal optimization",
                "quadratic programming",
                "support vector machines",
                "large scale optimization"
            ],
            "author": [
                "Tobias Glasmachers",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/glasmachers06a/glasmachers06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gradients Weights improve Regression and Classification",
            "abstract": [
                "In regression problems over R d , the unknown function f often varies more in some coordinates than in others. We show that weighting each coordinate i according to an estimate of the variation of f along coordinate i-e.g. the L 1 norm of the ith-directional derivative of f-is an efficient way to significantly improve the performance of distance-based regressors such as kernel and k-NN regressors. The approach, termed Gradient Weighting (GW), consists of a first pass regression estimate f n which serves to evaluate the directional derivatives of f , and a second-pass regression estimate on the re-weighted data. The GW approach can be instantiated for both regression and classification, and is grounded in strong theoretical principles having to do with the way regression bias and variance are affected by a generic feature-weighting scheme. These theoretical principles provide further technical foundation for some existing feature-weighting heuristics that have proved successful in practice. We propose a simple estimator of these derivative norms and prove its consistency. The proposed estimator computes efficiently and easily extends to run online. We then derive a classification version of the GW approach which evaluates on real-worlds datasets with as much success as its regression counterpart."
            ],
            "keywords": [
                "Nonparametric learning",
                "feature selection",
                "feature weighting",
                "nonparametric sparsity",
                "metric learning"
            ],
            "author": [
                "Samory Kpotufe",
                "Thomas Schultz",
                "Kyoungok Kim"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/13-351/13-351.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Representational Power of Discrete Bayesian Networks",
            "abstract": [
                "One of the most important fundamental properties of Bayesian networks is the representational power, reflecting what kind of functions they can or cannot represent. In this paper, we establish an association between the structural complexity of Bayesian networks and their representational power. We use the maximum number of nodes' parents as the measure for the structural complexity of Bayesian networks, and the maximum XOR contained in a target function as the measure for the function complexity. A representational upper bound is established and proved. Roughly speaking, discrete Bayesian networks with each node having at most k parents cannot represent any function containing (k + 1)-XORs. Our theoretical results help us to gain a deeper understanding on the capacities and limitations of Bayesian networks."
            ],
            "keywords": [
                "Bayesian Networks",
                "Representational Power",
                "Learning Algorithms"
            ],
            "author": [
                "Charles X Ling",
                "Huajie Zhang",
                "Carla E Brodley",
                "Andrea Danyluk"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/ling02a/ling02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Walk-Sums and Belief Propagation in Gaussian Graphical Models *",
            "abstract": [
                "We present a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose the correlation between each pair of variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlation coefficients. This representation holds for a large class of Gaussian graphical models which we call walk-summable. We give a precise characterization of this class of models, and relate it to other classes including diagonally dominant, attractive, nonfrustrated, and pairwise-normalizable. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. The walk-sum perspective leads to a better understanding of Gaussian belief propagation and to stronger results for its convergence in loopy graphs."
            ],
            "keywords": [
                "Gaussian graphical models",
                "walk-sum analysis",
                "convergence of loopy belief propagation"
            ],
            "author": [
                "Dmitry M Malioutov",
                "Jason K Johnson",
                "Alan S Willsky"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/malioutov06a/malioutov06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Hierarchy of Support Vector Machines for Pattern Detection",
            "abstract": [
                "We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly finer subsets. The hierarchy is traversed coarse-to-fine and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This \"free network\" is then perturbed, cell by cell, into another network, which is \"graded\" in two ways: first, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a predetermined , increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-specific SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network."
            ],
            "keywords": [
                "statistical learning",
                "hierarchy of classifiers",
                "coarse-to-fine computation",
                "support vector machines",
                "face detection"
            ],
            "author": [
                "Hichem Sahbi",
                "Donald Geman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/sahbi06a/sahbi06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Randomized Causation Coefficient",
            "abstract": [
                "We are interested in learning causal relationships between pairs of random variables, purely from observational data. To effectively address this task, the state-of-the-art relies on strong assumptions on the mechanisms mapping causes to effects, such as invertibility or the existence of additive noise, which only hold in limited situations. On the contrary, this short paper proposes to learn how to perform causal inference directly from data, without the need of feature engineering. In particular, we pose causality as a kernel mean embedding classification problem, where inputs are samples from arbitrary probability distributions on pairs of random variables, and labels are types of causal relationships. We validate the performance of our method on synthetic and real-world data against the state-of-the-art. Moreover, we submitted our algorithm to the ChaLearn's \"Fast Causation Coefficient Challenge\" competition, with which we won the fastest code prize and ranked third in the overall leaderboard."
            ],
            "keywords": [
                "causality",
                "cause-effect inference",
                "kernel mean embeddings",
                "random features"
            ],
            "author": [
                "David Lopez-Paz",
                "Krikamol Muandet",
                "Benjamin Recht",
                "Isabelle Guyon",
                "Alexander Statnikov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/lopezpaz15a/lopezpaz15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tslearn, A Machine Learning Toolkit for Time Series Data",
            "abstract": [
                "tslearn is a general-purpose Python machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression. It follows scikit-learn's Application Programming Interface for transformers and estimators, allowing the use of standard pipelines and model selection tools on top of tslearn objects. It is distributed under the BSD-2-Clause license, and its source code is available at https://github.com/tslearn-team/tslearn."
            ],
            "keywords": [
                "time series",
                "clustering",
                "classification",
                "pre-processing",
                "data mining"
            ],
            "author": [
                "Romain Tavenard",
                "Johann Faouzi",
                "Gilles Vandewiele",
                "Felix Divo",
                "Guillaume Androz",
                "Chester Holtz",
                "Marie Payne",
                "Roman Yurchak",
                "Marc Rußwurm",
                "Kushal Kolar",
                "Eli Woods"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-091/20-091.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Bayesian Passive-Aggressive Learning *",
            "abstract": [
                "We present online Bayesian Passive-Aggressive (BayesPA) learning, a generic online learning framework for hierarchical Bayesian models with max-margin posterior regularization. We show that BayesPA subsumes the standard online Passive-Aggressive (PA) learning and extends naturally to incorporate latent variables for both parametric and nonparametric Bayesian inference, therefore providing great flexibility for explorative analysis. As an important example, we apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric BayesPA topic models to infer the unknown number of topics in an online manner. Experimental results on 20newsgroups and a large Wikipedia multi-label dataset (with 1.1 millions of training documents and 0.9 million of unique terms in the vocabulary) show that our approaches significantly improve time efficiency while achieving comparable accuracy with the corresponding batch algorithms."
            ],
            "keywords": [],
            "author": [
                "Tianlin Shi",
                "Jun Zhu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-188/14-188.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Certifiably Optimal Low Rank Factor Analysis",
            "abstract": [
                "Factor Analysis (FA) is a technique of fundamental importance that is widely used in classical and modern multivariate statistics, psychometrics, and econometrics. In this paper, we revisit the classical rank-constrained FA problem which seeks to approximate an observed covariance matrix (Σ) by the sum of a Positive Semidefinite (PSD) low-rank component (Θ) and a diagonal matrix (Φ) (with nonnegative entries) subject to Σ − Φ being PSD. We propose a flexible family of rank-constrained, nonlinear Semidefinite Optimization based formulations for this task. We introduce a reformulation of the problem as a smooth optimization problem with convex, compact constraints and propose a unified algorithmic framework, utilizing state of the art techniques in nonlinear optimization to obtain high-quality feasible solutions for our proposed formulation. At the same time, by using a variety of techniques from discrete and global optimization, we show that these solutions are certifiably optimal in many cases, even for problems with thousands of variables. Our techniques are general and make no assumption on the underlying problem data. The estimator proposed herein aids statistical interpretability and provides computational scalability and significantly improved accuracy when compared to current, publicly available popular methods for rank-constrained FA. We demonstrate the effectiveness of our proposal on an array of synthetic and real-life datasets. To our knowledge, this is the first paper that demonstrates how a previously intractable rank-constrained optimization problem can be solved to provable optimality by coupling developments in convex analysis and in global and discrete optimization."
            ],
            "keywords": [
                "factor analysis",
                "rank minimization",
                "semidefinite optimization",
                "first order methods",
                "nonlinear optimization",
                "global optimization",
                "discrete optimization"
            ],
            "author": [
                "Dimitris Bertsimas",
                "Martin S Copenhaver",
                "Rahul Mazumder"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-613/15-613.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ranking Categorical Features Using Generalization Properties *",
            "abstract": [
                "Feature ranking is a fundamental machine learning task with various applications, including feature selection and decision tree learning. We describe and analyze a new feature ranking method that supports categorical features with a large number of possible values. We show that existing ranking criteria rank a feature according to the training error of a predictor based on the feature. This approach can fail when ranking categorical features with many values. We propose the Ginger ranking criterion, that estimates the generalization error of the predictor associated with the Gini index. We show that for almost all training sets, the Ginger criterion produces an accurate estimation of the true generalization error, regardless of the number of values in a categorical feature. We also address the question of finding the optimal predictor that is based on a single categorical feature. It is shown that the predictor associated with the misclassification error criterion has the minimal expected generalization error. We bound the bias of this predictor with respect to the generalization error of the Bayes optimal predictor, and analyze its concentration properties. We demonstrate the efficiency of our approach for feature selection and for learning decision trees in a series of experiments with synthetic and natural data sets."
            ],
            "keywords": [
                "feature ranking",
                "categorical features",
                "generalization bounds",
                "Gini index",
                "decision trees"
            ],
            "author": [
                "Sivan Sabato"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/sabato08a/sabato08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Some Greedy Learning Algorithms for Sparse Regression and Classification with Mercer Kernels",
            "abstract": [
                "We present some greedy learning algorithms for building sparse nonlinear regression and classification models from observational data using Mercer kernels. Our objective is to develop efficient numerical schemes for reducing the training and runtime complexities of kernel-based algorithms applied to large datasets. In the spirit of Natarajan's greedy algorithm (Natarajan, 1995), we iteratively minimize the L 2 loss function subject to a specified constraint on the degree of sparsity required of the final model until a specified stopping criterion is reached. We discuss various greedy criteria for basis selection and numerical schemes for improving the robustness and computational efficiency. Subsequently, algorithms based on residual minimization and thin QR factorization are presented for constructing sparse regression and classification models. During the course of the incremental model construction, the algorithms are terminated using model selection principles such as the minimum descriptive length (MDL) and Akaike's information criterion (AIC). Finally, experimental results on benchmark data are presented to demonstrate the competitiveness of the algorithms developed in this paper."
            ],
            "keywords": [
                "Sparse Learning Machines",
                "Kernel Methods",
                "Greedy Algorithms",
                "Regression",
                "Classification",
                "Model Selection"
            ],
            "author": [
                "Prasanth B Nair",
                "Arindam Choudhury",
                "Andy J Keane",
                "Carla Brodley",
                "Andrea Danyluk"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/nair02a/nair02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MOCCA: Mirrored Convex/Concave Optimization for Nonconvex Composite Functions",
            "abstract": [
                "Many optimization problems arising in high-dimensional statistics decompose naturally into a sum of several terms, where the individual terms are relatively simple but the composite objective function can only be optimized with iterative algorithms. In this paper, we are interested in optimization problems of the form F(Kx) + G(x), where K is a fixed linear transformation, while F and G are functions that may be nonconvex and/or nondifferentiable. In particular, if either of the terms are nonconvex, existing alternating minimization techniques may fail to converge; other types of existing approaches may instead be unable to handle nondifferentiability. We propose the mocca (mirrored convex/concave) algorithm, a primal/dual optimization approach that takes a local convex approximation to each term at every iteration. Inspired by optimization problems arising in computed tomography (CT) imaging, this algorithm can handle a range of nonconvex composite optimization problems, and offers theoretical guarantees for convergence when the overall problem is approximately convex (that is, any concavity in one term is balanced out by convexity in the other term). Empirical results show fast convergence for several structured signal recovery problems."
            ],
            "keywords": [
                "MOCCA",
                "ADMM",
                "nonconvex",
                "penalized likelihood",
                "total variation",
                "computed tomography"
            ],
            "author": [
                "Rina Foygel Barber",
                "Emil Y Sidky"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-583/15-583.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Information Theoretical Estimators Toolbox",
            "abstract": [
                "We present ITE (information theoretical estimators) a free and open source, multi-platform, Matlab/Octave toolbox that is capable of estimating many different variants of entropy, mutual information, divergence, association measures, cross quantities, and kernels on distributions. Thanks to its highly modular design, ITE supports additionally (i) the combinations of the estimation techniques, (ii) the easy construction and embedding of novel information theoretical estimators, and (iii) their immediate application in information theoretical optimization problems. ITE also includes a prototype application in a central problem class of signal processing, independent subspace analysis and its extensions."
            ],
            "keywords": [
                "entropy-",
                "mutual information-",
                "association-",
                "divergence-",
                "distribution kernel estimation",
                "independent subspace analysis and its extensions",
                "modularity",
                "Matlab/Octave",
                "multi-platform",
                "GNU GPLv3 (≥)"
            ],
            "author": [
                "Zoltán Szabó"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/szabo14a/szabo14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes",
            "abstract": [
                "Variational Bayesian (VB) methods are typically only applied to models in the conjugate-exponential family using the variational Bayesian expectation maximisation (VB EM) algorithm or one of its variants. In this paper we present an efficient algorithm for applying VB to more general models. The method is based on specifying the functional form of the approximation, such as multivariate Gaussian. The parameters of the approximation are optimised using a conjugate gradient algorithm that utilises the Riemannian geometry of the space of the approximations. This leads to a very efficient algorithm for suitably structured approximations. It is shown empirically that the proposed method is comparable or superior in efficiency to the VB EM in a case where both are applicable. We also apply the algorithm to learning a nonlinear state-space model and a nonlinear factor analysis model for which the VB EM is not applicable. For these models, the proposed algorithm outperforms alternative gradient-based methods by a significant margin."
            ],
            "keywords": [
                "variational inference",
                "approximate Riemannian conjugate gradient",
                "fixed-form approximation",
                "Gaussian approximation"
            ],
            "author": [
                "Antti Honkela",
                "Mikael Kuusela",
                "Matti Tornio",
                "Juha Karhunen",
                "Tkk Fi",
                "Tapani Raiko",
                "RAIKO, KUUSELA, TORNIO AND KARHUNEN Honkela"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/honkela10a/honkela10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Inference with Unnormalized Discrete Models and Localized Homogeneous Divergences",
            "abstract": [
                "In this paper, we focus on parameters estimation of probabilistic models in discrete space. A naive calculation of the normalization constant of the probabilistic model on discrete space is often infeasible and statistical inference based on such probabilistic models has difficulty. In this paper, we propose a novel estimator for probabilistic models on discrete space, which is derived from an empirically localized homogeneous divergence. The idea of the empirical localization makes it possible to ignore an unobserved domain on sample space, and the homogeneous divergence is a discrepancy measure between two positive measures and has a weak coincidence axiom. The proposed estimator can be constructed without calculating the normalization constant and is asymptotically consistent and Fisher efficient. We investigate statistical properties of the proposed estimator and reveal a relationship between the empirically localized homogeneous divergence and a mixture of the α-divergence. The α-divergence is a non-homogeneous discrepancy measure that is frequently discussed in the context of information geometry. Using the relationship, we also propose an asymptotically consistent estimator of the normalization constant. Experiments showed that the proposed estimator comparably performs to the maximum likelihood estimator but with drastically lower computational cost."
            ],
            "keywords": [
                "unnormalized model",
                "homogeneous divergence",
                "empirical localization",
                "discrete model"
            ],
            "author": [
                "Takashi Takenouchi",
                "Takafumi Kanamori"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-596/15-596.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Semi-Supervised Interpolation in an Anticausal Learning Scenario",
            "abstract": [
                "According to a recently stated 'independence postulate', the distribution P cause contains no information about the conditional P effect|cause while P effect may contain information about P cause|effect. Since semi-supervised learning (SSL) attempts to exploit information from P X to assist in predicting Y from X, it should only work in anticausal direction, i.e., when Y is the cause and X is the effect. In causal direction, when X is the cause and Y the effect, unlabelled x-values should be useless. To shed light on this asymmetry, we study a deterministic causal relation Y = f (X) as recently assayed in Information-Geometric Causal Inference (IGCI). Within this model, we discuss two options to formalize the independence of P X and f as an orthogonality of vectors in appropriate inner product spaces. We prove that unlabelled data help for the problem of interpolating a monotonically increasing function if and only if the orthogonality conditions are violated-which we only expect for the anticausal direction. Here, performance of SSL and its supervised baseline analogue is measured in terms of two different loss functions: first, the mean squared error and second the surprise in a Bayesian prediction scenario."
            ],
            "keywords": [
                "semi-supervised learning",
                "anticausal learning",
                "independence of cause and mechanism",
                "information geometry",
                "causality"
            ],
            "author": [
                "Dominik Janzing",
                "Bernhard Schölkopf",
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/janzing15a/janzing15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simplifying Probabilistic Expressions in Causal Inference",
            "abstract": [
                "Obtaining a non-parametric expression for an interventional distribution is one of the most fundamental tasks in causal inference. Such an expression can be obtained for an identifiable causal effect by an algorithm or by manual application of do-calculus. Often we are left with a complicated expression which can lead to biased or inefficient estimates when missing data or measurement errors are involved. We present an automatic simplification algorithm that seeks to eliminate symbolically unnecessary variables from these expressions by taking advantage of the structure of the underlying graphical model. Our method is applicable to all causal effect formulas and is readily available in the R package causaleffect."
            ],
            "keywords": [
                "simplification",
                "probabilistic expression",
                "causal inference",
                "graphical model",
                "graph theory"
            ],
            "author": [
                "Santtu Tikka"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-166/16-166.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Expectation Propagation as a Way of Life: A Framework for Bayesian Inference on Partitioned Data",
            "abstract": [
                "A common divide-and-conquer approach for Bayesian computation with big data is to partition the data, perform local inference for each piece separately, and combine the results to obtain a global posterior approximation. While being conceptually and computationally appealing, this method involves the problematic need to also split the prior for the local inferences; these weakened priors may not provide enough regularization for each separate computation, thus eliminating one of the key advantages of Bayesian methods. To resolve this dilemma while still retaining the generalizability of the underlying local inference method, we apply the idea of expectation propagation (EP) as a framework for distributed Bayesian inference. The central idea is to iteratively update approximations to the local likelihoods given the state of the other approximations and the prior."
            ],
            "keywords": [
                "Bayesian computation",
                "data partitioning",
                "expectation propagation",
                "hierarchical models",
                "statistical computing"
            ],
            "author": [
                "Aki Vehtari",
                "Andrew Gelman",
                "Dustin Tran",
                "Paul Blomstedt",
                "John P Cunningham",
                "David Schiminovich",
                "Christian P Robert"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-817/18-817.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Branch and Bound for Piecewise Linear Neural Network Verification",
            "abstract": [
                "The success of Deep Learning and its potential use in many safety-critical applications has motivated research on formal verification of Neural Network (NN) models. In this context, verification involves proving or disproving that an NN model satisfies certain input-output properties. Despite the reputation of learned NN models as black boxes, and the theoretical hardness of proving useful properties about them, researchers have been successful in verifying some classes of models by exploiting their piecewise linear structure and taking insights from formal methods such as Satisifiability Modulo Theory. However, these methods are still far from scaling to realistic neural networks. To facilitate progress on this crucial area, we exploit the Mixed Integer Linear Programming (MIP) formulation of verification to propose a family of algorithms based on Branch-and-Bound (BaB). We show that our family contains previous verification methods as special cases. With the help of the BaB framework, we make three key contributions. Firstly, we identify new methods that combine the strengths of multiple existing approaches, accomplishing significant performance improvements over previous state of the art. Secondly, we introduce an effective branching strategy on ReLU non-linearities. This branching strategy allows us to efficiently and successfully deal with high input dimensional problems with convolutional network architecture, on which previous methods fail frequently. Finally, we propose comprehensive test data sets and benchmarks which includes a collection of previously released testcases. We use the data sets to conduct a thorough experimental comparison of existing and new algorithms and to provide an inclusive analysis of the factors impacting the hardness of verification problems."
            ],
            "keywords": [
                "Formal Verification",
                "Branch and Bound",
                "ReLU Branching * . equal contribution"
            ],
            "author": [
                "Rudy Bunel",
                "Philip H S Torr",
                "M Pawan Kumar",
                "Jingyue Lu",
                "Pushmeet Kohli",
                "London N1c 4ag",
                "Ilker Turkaslan",
                "M Pawan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-468/19-468.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scikit-network: Graph Analysis in Python",
            "abstract": [
                "Scikit-network is a Python package inspired by scikit-learn for the analysis of large graphs. Graphs are represented by their adjacency matrix in the sparse CSR format of SciPy. The package provides state-of-the-art algorithms for ranking, clustering, classifying, embedding and visualizing the nodes of a graph. High performance is achieved through a mix of fast matrix-vector products (using SciPy), compiled code (using Cython) and parallel processing. The package is distributed under the BSD license, with dependencies limited to NumPy and SciPy. It is compatible with Python 3.6 and newer. Source code, documentation and installation instructions are available online 1 ."
            ],
            "keywords": [
                "graph analysis",
                "sparse matrices",
                "python",
                "cython",
                "scipy"
            ],
            "author": [
                "Thomas Bonald",
                "Nathan De Lara",
                "Quentin Lutz",
                "Télécom Paris",
                "Bertrand Charpentier"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-412/20-412.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Clustering Hidden Markov Models with Variational HEM",
            "abstract": [
                "The hidden Markov model (HMM) is a widely-used generative model that copes with sequential data, assuming that each observation is conditioned on the state of a hidden Markov chain. In this paper, we derive a novel algorithm to cluster HMMs based on the hierarchical EM (HEM) algorithm. The proposed algorithm i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a \"cluster center\", that is, a novel HMM that is representative for the group, in a manner that is consistent with the underlying generative model of the HMM. To cope with intractable inference in the E-step, the HEM algorithm is formulated as a variational optimization problem, and efficiently solved for the HMM case by leveraging an appropriate variational approximation. The benefits of the proposed algorithm, which we call variational HEM (VHEM), are demonstrated on several tasks involving time-series data, such as hierarchical clustering of motion capture sequences, and automatic annotation and retrieval of music and of online handwriting data, showing improvements over current methods. In particular, our variational HEM algorithm effectively leverages large amounts of data when learning annotation models by using an efficient hierarchical estimation procedure, which reduces learning times and memory requirements, while improving model robustness through better regularization."
            ],
            "keywords": [
                "Hierarchical EM algorithm",
                "clustering",
                "hidden Markov model",
                "hidden Markov mixture model",
                "variational approximation",
                "time-series classification"
            ],
            "author": [
                "Emanuele Coviello",
                "Antoni B Chan",
                "Gert R G Lanckriet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/coviello14a/coviello14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Semiparametric Exponential Family Graphical Models",
            "abstract": [
                "We propose a new class of semiparametric exponential family graphical models for the analysis of high dimensional mixed data. Different from the existing mixed graphical models, we allow the nodewise conditional distributions to be semiparametric generalized linear models with unspecified base measure functions. Thus, one advantage of our method is that it is unnecessary to specify the type of each node and the method is more convenient to apply in practice. Under the proposed model, we consider both problems of parameter estimation and hypothesis testing in high dimensions. In particular, we propose a symmetric pairwise score test for the presence of a single edge in the graph. Compared to the existing methods for hypothesis tests, our approach takes into account of the symmetry of the parameters, such that the inferential results are invariant with respect to the different parametrizations of the same edge. Thorough numerical simulations and a real data example are provided to back up our theoretical results."
            ],
            "keywords": [
                "Graphical Models",
                "Exponential Family",
                "High Dimensional Inference"
            ],
            "author": [
                "Zhuoran Yang",
                "Yang Ning",
                "Han Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/15-493/15-493.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Follow the Leader If You Can, Hedge If You Must",
            "abstract": [
                "Follow-the-Leader (FTL) is an intuitive sequential prediction strategy that guarantees constant regret in the stochastic setting, but has poor performance for worst-case data. Other hedging strategies have better worst-case guarantees but may perform much worse than FTL if the data are not maximally adversarial. We introduce the FlipFlop algorithm, which is the first method that provably combines the best of both worlds. As a stepping stone for our analysis, we develop AdaHedge, which is a new way of dynamically tuning the learning rate in Hedge without using the doubling trick. AdaHedge refines a method by Cesa-Bianchi, Mansour, and Stoltz (2007), yielding improved worst-case guarantees. By interleaving AdaHedge and FTL, FlipFlop achieves regret within a constant factor of the FTL regret, without sacrificing AdaHedge's worst-case guarantees. AdaHedge and FlipFlop do not need to know the range of the losses in advance; moreover, unlike earlier methods, both have the intuitive property that the issued weights are invariant under rescaling and translation of the losses. The losses are also allowed to be negative, in which case they may be interpreted as gains."
            ],
            "keywords": [
                "Hedge",
                "learning rate",
                "mixability",
                "online learning",
                "prediction with expert advice"
            ],
            "author": [
                "Steven De Rooij",
                "Peter D Grünwald",
                "Wouter M Koolen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/rooij14a/rooij14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "L p -Nested Symmetric Distributions",
            "abstract": [
                "In this paper, we introduce a new family of probability densities called L p-nested symmetric distributions. The common property, shared by all members of the new class, is the same functional form ρ(x x x) =ρ(f (x x x)), where f is a nested cascade of L p-norms x x x p = (∑ |x i | p) 1/p. L p-nested symmetric distributions thereby are a special case of ν-spherical distributions for which f is only required to be positively homogeneous of degree one. While both, ν-spherical and L p-nested symmetric distributions, contain many widely used families of probability models such as the Gaussian, spherically and elliptically symmetric distributions, L p-spherically symmetric distributions, and certain types of independent component analysis (ICA) and independent subspace analysis (ISA) models, ν-spherical distributions are usually computationally intractable. Here we demonstrate that L pnested symmetric distributions are still computationally feasible by deriving an analytic expression for its normalization constant, gradients for maximum likelihood estimation, analytic expressions for certain types of marginals, as well as an exact and efficient sampling algorithm. We discuss the tight links of L p-nested symmetric distributions to well known machine learning methods such as ICA, ISA and mixed norm regularizers, and introduce the nested radial factorization algorithm (NRF), which is a form of non-linear ICA that transforms any linearly mixed, non-factorial L pnested symmetric source into statistically independent signals. As a corollary, we also introduce the uniform distribution on the L p-nested unit sphere."
            ],
            "keywords": [
                "parametric density model",
                "symmetric distribution",
                "ν-spherical distributions",
                "non-linear independent component analysis",
                "independent subspace analysis",
                "robust Bayesian inference",
                "mixed norm density model",
                "uniform distributions on mixed norm spheres",
                "nested radial factorization"
            ],
            "author": [
                "Fabian Sinz",
                "Matthias Bethge",
                "Werner Reichardt"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/sinz10a/sinz10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PyStruct -Learning Structured Prediction in Python",
            "abstract": [
                "Structured prediction methods have become a central tool for many machine learning applications. While more and more algorithms are developed, only very few implementations are available. PyStruct aims at providing a general purpose implementation of standard structured prediction methods, both for practitioners and as a baseline for researchers. It is written in Python and adapts paradigms and types from the scientific Python community for seamless integration with other projects."
            ],
            "keywords": [
                "structured prediction",
                "structural support vector machines",
                "conditional random fields",
                "Python"
            ],
            "author": [
                "Andreas C Müller",
                "Sven Behnke"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/mueller14a/mueller14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
            "abstract": [
                "Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also show how to tune SGD with momentum for approximate sampling. (4) We analyze stochastic-gradient MCMC algorithms. For Stochastic-Gradient Langevin Dynamics and Stochastic-Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler."
            ],
            "keywords": [
                "approximate Bayesian inference",
                "variational inference",
                "stochastic optimization",
                "stochastic gradient MCMC",
                "stochastic differential equations"
            ],
            "author": [
                "Stephan Mandt",
                "Matthew D Hoffman",
                "David M Blei"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-214/17-214.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tunability: Importance of Hyperparameters of Machine Learning Algorithms",
            "abstract": [
                "Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is twofold. Firstly, we formalize the problem of tuning from a statistical point of view, define data-based defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to choose adequate hyperparameter spaces for tuning."
            ],
            "keywords": [
                "machine learning",
                "supervised learning",
                "classification",
                "hyperparameters",
                "tuning",
                "meta-learning"
            ],
            "author": [
                "Philipp Probst",
                "Anne-Laure Boulesteix",
                "Bernd Bischl"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-444/18-444.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Derivative Estimation Based on Difference Sequence via Locally Weighted Least Squares Regression",
            "abstract": [
                "A new method is proposed for estimating derivatives of a nonparametric regression function. By applying Taylor expansion technique to a derived symmetric difference sequence, we obtain a sequence of approximate linear regression representation in which the derivative is just the intercept term. Using locally weighted least squares, we estimate the derivative in the linear regression model. The estimator has less bias in both valleys and peaks of the true derivative function. For the special case of a domain with equispaced design points, the asymptotic bias and variance are derived; consistency and asymptotic normality are established. In simulations our estimators have less bias and mean square error than its main competitors, especially second order derivative estimator."
            ],
            "keywords": [
                "nonparametric derivative estimation",
                "locally weighted least squares",
                "biascorrection",
                "symmetric difference sequence",
                "Taylor expansion"
            ],
            "author": [
                "Wenwu Wang",
                "Lu Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/wang15b/wang15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Computational Limits of A Distributed Algorithm for Smoothing Spline",
            "abstract": [
                "In this paper, we explore statistical versus computational trade-off to address a basic question in the application of a distributed algorithm: what is the minimal computational cost in obtaining statistical optimality? In smoothing spline setup, we observe a phase transition phenomenon for the number of deployed machines that ends up being a simple proxy for computing cost. Specifically, a sharp upper bound for the number of machines is established: when the number is below this bound, statistical optimality (in terms of nonparametric estimation or testing) is achievable; otherwise, statistical optimality becomes impossible. These sharp bounds partly capture intrinsic computational limits of the distributed algorithm considered in this paper, and turn out to be fully determined by the smoothness of the regression function. We name the asymptotic analysis on such split-andaggregation estimation/inference as \"splitotic\" theory. As a side remark, we argue that sample splitting may be viewed as an alternative form of regularization, playing a similar role as smoothing parameter."
            ],
            "keywords": [
                "divide-and-conquer",
                "computational limits",
                "smoothing spline",
                "splitotic theory"
            ],
            "author": [
                "Zuofeng Shang",
                "Guang Cheng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-289/16-289.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data",
            "abstract": [
                "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various stateof-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack."
            ],
            "keywords": [],
            "author": [
                "Puyudi Yang",
                "Jianbo Chen",
                "Cho-Jui Hsieh",
                "Jane-Ling Wang",
                "Michael I Jordan",
                "Jianbo Yang",
                "Cho-Jui Chen",
                "Jane-Ling Hsieh",
                "Michael I Jordan Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-569/19-569.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Sparse Low-Threshold Linear Classifiers",
            "abstract": [
                "We consider the problem of learning a non-negative linear classifier with a-norm of at most k, and a fixed threshold, under the hinge-loss. This problem generalizes the problem of learning a k-monotone disjunction. We prove that we can learn efficiently in this setting, at a rate which is linear in both k and the size of the threshold, and that this is the best possible rate. We provide an efficient online learning algorithm that achieves the optimal rate, and show that in the batch case, empirical risk minimization achieves this rate as well. The rates we show are tighter than the uniform convergence rate, which grows with k 2 ."
            ],
            "keywords": [
                "linear classifiers",
                "monotone disjunctions",
                "online learning",
                "empirical risk minimization",
                "uniform convergence"
            ],
            "author": [
                "Sivan Sabato",
                "Shai Shalev-Shwartz",
                "Nathan Srebro",
                "Daniel Hsu",
                "Tong Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/sabato15a/sabato15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Models of Cooperative Teaching and Learning",
            "abstract": [
                "While most supervised machine learning models assume that training examples are sampled at random or adversarially, this article is concerned with models of learning from a cooperative teacher that selects \"helpful\" training examples. The number of training examples a learner needs for identifying a concept in a given class C of possible target concepts (sample complexity of C) is lower in models assuming such teachers, that is, \"helpful\" examples can speed up the learning process."
            ],
            "keywords": [],
            "author": [
                "Sandra Zilles",
                "Steffen Lange",
                "Robert Holte",
                "Martin Zinkevich"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/zilles11a/zilles11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Relationship Between Agnostic Selective Classification, Active Learning and the Disagreement Coefficient",
            "abstract": [
                "A selective classifier (f, g) comprises a classification function f and a binary selection function g, which determines if the classifier abstains from prediction, or uses f to predict. The classifier is called pointwise-competitive if it classifies each point identically to the best classifier in hindsight (from the same class), whenever it does not abstain. The quality of such a classifier is quantified by its rejection mass, defined to be the probability mass of the points it rejects. A \"fast\" rejection rate is achieved if the rejection mass is bounded from above byÕ(1/m) where m is the number of labeled examples used to train the classifier (andÕ hides logarithmic factors). Pointwise-competitive selective (PCS) classifiers are intimately related to disagreement-based active learning and it is known that in the realizable case, a fast rejection rate of a known PCS algorithm (called Consistent Selective Strategy) is equivalent to an exponential speedup of the well-known CAL active algorithm. We focus on the agnostic setting, for which there is a known algorithm called LESS that learns a PCS classifier and achieves a fast rejection rate (depending on Hanneke's disagreement coefficient) under strong assumptions. We present an improved PCS learning algorithm called ILESS for which we show a fast rate (depending on Hanneke's disagreement coefficient) without any assumptions. Our rejection bound smoothly interpolates the realizable and agnostic settings. The main result of this paper is an equivalence between the following three entities: (i) the existence of a fast rejection rate for any PCS learning algorithm (such as ILESS); (ii) a poly-logarithmic bound for Hanneke's disagreement coefficient; and (iii) an exponential speedup for a new disagreement-based active learner called Active-ILESS."
            ],
            "keywords": [
                "active learning",
                "selective prediction",
                "disagreement coefficient",
                "selective sampling",
                "selective classification",
                "reject option",
                "pointwise-competitive",
                "selective classification",
                "statistical learning theory",
                "PAC learning",
                "sample complexity",
                "agnostic case"
            ],
            "author": [
                "Roei Gelbhart"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-147/17-147.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Loopy Belief Propagation: Convergence and Effects of Message Errors",
            "abstract": [
                "Belief propagation (BP) is an increasingly popular method of performing approximate inference on arbitrary graphical models. At times, even further approximations are required, whether due to quantization of the messages or model parameters, from other simplified message or model representations, or from stochastic approximation methods. The introduction of such errors into the BP message computations has the potential to affect the solution obtained adversely. We analyze the effect resulting from message approximation under two particular measures of error, and show bounds on the accumulation of errors in the system. This analysis leads to convergence conditions for traditional BP message passing, and both strict bounds and estimates of the resulting error in systems of approximate BP message passing."
            ],
            "keywords": [
                "belief propagation",
                "sum-product",
                "convergence",
                "approximate inference",
                "quantization"
            ],
            "author": [
                "Alexander T Ihler",
                "Donald Bren",
                "John W Fisher III",
                "Alan S Willsky"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/ihler05a/ihler05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Introduction to the Special Topic on Grammar Induction, Representation of Language and Language Learning",
            "abstract": [
                "Grammar induction refers to the process of learning grammars and languages from data; this finds a variety of applications in syntactic pattern recognition, the modeling of natural language acquisition, data mining and machine translation. This special topic contains several papers presenting some of recent developments in the area of grammar induction and language learning, as applied to various problems in Natural Language Processing, including supervised and unsupervised parsing and statistical machine translation."
            ],
            "keywords": [
                "machine translation",
                "Bayesian inference",
                "grammar induction",
                "natural language parsing"
            ],
            "author": [
                "Dorota Głowacka",
                "Alexander Clark",
                "Mark Johnson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/glowacka11a/glowacka11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Memory-Based Shallow Parsing",
            "abstract": [
                "We present memory-based learning approaches to shallow parsing and apply these to five tasks: base noun phrase identification, arbitrary base phrase recognition, clause detection, noun phrase parsing and full parsing. We use feature selection techniques and system combination methods for improving the performance of the memory-based learner. Our approach is evaluated on standard data sets and the results are compared with that of other systems. This reveals that our approach works well for base phrase identification while its application towards recognizing embedded structures leaves some room for improvement."
            ],
            "keywords": [
                "shallow parsing",
                "memory-based learning",
                "feature selection",
                "system combination"
            ],
            "author": [
                "Erik F Tjong",
                "Kim Sang",
                "James Hammerton",
                "Miles Osborne",
                "Susan Armstrong",
                "Walter Daelemans"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/tks02a/tks02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Generalized Kernel Approach to Dissimilarity-based Classification",
            "abstract": [
                "Usually, objects to be classified are represented by features. In this paper, we discuss an alternative object representation based on dissimilarity values. If such distances separate the classes well, the nearest neighbor method offers a good solution. However, dissimilarities used in practice are usually far from ideal and the performance of the nearest neighbor rule suffers from its sensitivity to noisy examples. We show that other, more global classification techniques are preferable to the nearest neighbor rule, in such cases. For classification purposes, two different ways of using generalized dissimilarity kernels are considered. In the first one, distances are isometrically embedded in a pseudo-Euclidean space and the classification task is performed there. In the second approach, classifiers are built directly on distance kernels. Both approaches are described theoretically and then compared using experiments with different dissimilarity measures and datasets including degraded data simulating the problem of missing values."
            ],
            "keywords": [
                "dissimilarity",
                "embedding",
                "pseudo-Euclidean space",
                "nearest mean classifier",
                "support vector classifier",
                "Fisher linear discriminant"
            ],
            "author": [
                "Pavel Paclík",
                "Robert P W Duin",
                "Nello Cristianini",
                "Robert Williamson"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/pekalska01a/pekalska01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Semi-Supervised Eigenvectors for Large-Scale Locally-Biased Learning",
            "abstract": [
                "In many applications, one has side information, e.g., labels that are provided in a semisupervised manner, about a specific target region of a large data set, and one wants to perform machine learning and data analysis tasks \"nearby\" that prespecified target region. For example, one might be interested in the clustering structure of a data graph near a prespecified \"seed set\" of nodes, or one might be interested in finding partitions in an image that are near a prespecified \"ground truth\" set of pixels. Locally-biased problems of this sort are particularly challenging for popular eigenvector-based machine learning and data analysis tools. At root, the reason is that eigenvectors are inherently global quantities, thus limiting the applicability of eigenvector-based methods in situations where one is interested in very local properties of the data. In this paper, we address this issue by providing a methodology to construct semisupervised eigenvectors of a graph Laplacian, and we illustrate how these locally-biased eigenvectors can be used to perform locally-biased machine learning. These semi-supervised eigenvectors capture successively-orthogonalized directions of maximum variance, conditioned on being well-correlated with an input seed set of nodes that is assumed to be provided in a semi-supervised manner. We show that these semi-supervised eigenvectors can be computed quickly as the solution to a system of linear equations; and we also describe several variants of our basic method that have improved scaling properties. We provide several empirical examples demonstrating how these semi-supervised eigenvectors can be used to perform locally-biased learning; and we discuss the relationship between our results and recent machine learning algorithms that use global eigenvectors of the graph Laplacian."
            ],
            "keywords": [
                "semi-supervised learning",
                "spectral clustering",
                "kernel methods",
                "large-scale machine learning",
                "local spectral methods",
                "locally-biased learning"
            ],
            "author": [
                "Toke J Hansen",
                "Michael W Mahoney"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/hansen14a/hansen14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Unlabeled Compression Schemes for Maximum Classes *",
            "abstract": [
                "Maximum concept classes of VC dimension d over n domain points have size n ≤d , and this is an upper bound on the size of any concept class of VC dimension d over n points. We give a compression scheme for any maximum class that represents each concept by a subset of up to d unlabeled domain points and has the property that for any sample of a concept in the class, the representative of exactly one of the concepts consistent with the sample is a subset of the domain of the sample. This allows us to compress any sample of a concept in the class to a subset of up to d unlabeled sample points such that this subset represents a concept consistent with the entire original sample. Unlike the previously known compression scheme for maximum classes (Floyd and Warmuth, 1995) which compresses to labeled subsets of the sample of size equal d, our new scheme is tight in the sense that the number of possible unlabeled compression sets of size at most d equals the number of concepts in the class."
            ],
            "keywords": [
                "compression schemes",
                "VC dimension",
                "maximum classes",
                "one-inclusion graph"
            ],
            "author": [
                "Dima Kuzmin",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/kuzmin07a/kuzmin07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Prioritization Methods for Accelerating MDP Solvers",
            "abstract": [
                "The performance of value and policy iteration can be dramatically improved by eliminating redundant or useless backups, and by backing up states in the right order. We study several methods designed to accelerate these iterative solvers, including prioritization, partitioning, and variable reordering. We generate a family of algorithms by combining several of the methods discussed, and present extensive empirical evidence demonstrating that performance can improve by several orders of magnitude for many problems, while preserving accuracy and convergence guarantees."
            ],
            "keywords": [
                "Markov Decision Processes",
                "value iteration",
                "policy iteration",
                "prioritized sweeping",
                "dynamic programming"
            ],
            "author": [
                "David Wingate",
                "Kevin D Seppi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/wingate05a/wingate05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics",
            "abstract": [
                "Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data sets is computationally expensive. Both the calculation of the acceptance probability and the creation of informed proposals usually require an iteration through the whole data set. The recently proposed stochastic gradient Langevin dynamics (SGLD) method circumvents this problem by generating proposals which are only based on a subset of the data, by skipping the accept-reject step and by using decreasing step-sizes sequence (δ m) m≥0. We provide in this article a rigorous mathematical framework for analysing this algorithm. We prove that, under verifiable assumptions, the algorithm is consistent, satisfies a central limit theorem (CLT) and its asymptotic bias-variance decomposition can be characterized by an explicit functional of the step-sizes sequence (δ m) m≥0. We leverage this analysis to give practical recommendations for the notoriously difficult tuning of this algorithm: it is asymptotically optimal to use a step-size sequence of the type δ m m −1/3 , leading to an algorithm whose mean squared error (MSE) decreases at rate O(m −1/3)."
            ],
            "keywords": [
                "Markov chain Monte Carlo",
                "Langevin dynamics",
                "big data"
            ],
            "author": [
                "Yee Whye Teh",
                "Alexandre H Thiery"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/teh16a/teh16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric Bayesian Aggregation for Massive Data",
            "abstract": [
                "We develop a set of scalable Bayesian inference procedures for a general class of nonparametric regression models. Specifically, nonparametric Bayesian inferences are separately performed on each subset randomly split from a massive dataset, and then the obtained local results are aggregated into global counterparts. This aggregation step is explicit without involving any additional computation cost. By a careful partition, we show that our aggregated inference results obtain an oracle rule in the sense that they are equivalent to those obtained directly from the entire data (which are computationally prohibitive). For example, an aggregated credible ball achieves desirable credibility level and also frequentist coverage while possessing the same radius as the oracle ball."
            ],
            "keywords": [
                "Credible region",
                "divide-and-conquer",
                "Gaussian process prior",
                "linear functional",
                "nonparametric Bayesian inference"
            ],
            "author": [
                "Zuofeng Shang",
                "Botao Hao",
                "Guang Cheng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-641/17-641.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Profile-Based Bandit with Unknown Profiles",
            "abstract": [
                "Stochastic bandits have been widely studied since decades. A very large panel of settings have been introduced, some of them for the inclusion of some structure between actions. If actions are associated with feature vectors that underlie their usefulness, the discovery of a mapping parameter between such profiles and rewards can help the exploration process of the bandit strategies. This is the setting studied in this paper, but in our case the action profiles (constant feature vectors) are unknown beforehand. Instead, the agent is only given sample vectors, with mean centered on the true profiles, for a subset of actions at each step of the process. In this new bandit instance, policies have thus to deal with a doubled uncertainty, both on the profile estimators and the reward mapping parameters learned so far. We propose a new algorithm, called SampLinUCB, specifically designed for this case. Theoretical convergence guarantees are given for this strategy, according to various profile samples delivery scenarios. Finally, experiments are conducted on both artificial data and a task of focused data capture from online social networks. Obtained results demonstrate the relevance of the approach in various settings."
            ],
            "keywords": [
                "Stochastic Linear Bandits",
                "Profile-based Exploration",
                "Upper Confidence Bounds"
            ],
            "author": [
                "Sylvain Lamprier",
                "Thibault Gisselbrecht",
                "Patrick Gallinari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-693/17-693.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analysis of Langevin Monte Carlo via Convex Optimization",
            "abstract": [
                "In this paper, we provide new insights on the Unadjusted Langevin Algorithm. We show that this method can be formulated as the first order optimization algorithm for an objective functional defined on the Wasserstein space of order 2. Using this interpretation and techniques borrowed from convex optimization, we give a non-asymptotic analysis of this method to sample from log-concave smooth target distribution on R d. Based on this interpretation, we propose two new methods for sampling from a non-smooth target distribution. These new algorithms are natural extensions of the Stochastic Gradient Langevin Dynamics (SGLD) algorithm, which is a popular extension of the Unadjusted Langevin Algorithm for largescale Bayesian inference. Using the optimization perspective, we provide non-asymptotic convergence analysis for the newly proposed methods."
            ],
            "keywords": [
                "Unadjasted Langevin Algorithm",
                "convex optimization",
                "Bayesian inference",
                "gradient flow",
                "Wasserstein metric"
            ],
            "author": [
                "Alain Durmus",
                "Szymon Majewski"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-173/18-173.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Coordinate Descent Method for Learning with Big Data",
            "abstract": [
                "In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent method for solving loss minimization problems with big data. We initially partition the coordinates (features) and assign each partition to a different node of a cluster. At every iteration, each node picks a random subset of the coordinates from those it owns, independently from the other computers, and in parallel computes and applies updates to the selected coordinates based on a simple closed-form formula. We give bounds on the number of iterations sufficient to approximately solve the problem with high probability, and show how it depends on the data and on the partitioning. We perform numerical experiments with a LASSO instance described by a 3TB matrix."
            ],
            "keywords": [
                "stochastic methods",
                "parallel coordinate descent",
                "distributed algorithms",
                "boosting"
            ],
            "author": [
                "Peter Richtárik",
                "Martin Takáč"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-001/15-001.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses",
            "abstract": [
                "Manifold regularization (Belkin et al., 2006) is a geometrically motivated framework for machine learning within which several semi-supervised algorithms have been constructed. Here we try to provide some theoretical understanding of this approach. Our main result is to expose the natural structure of a class of problems on which manifold regularization methods are helpful. We show that for such problems, no supervised learner can learn effectively. On the other hand, a manifold based learner (that knows the manifold or \"learns\" it from unlabeled examples) can learn with relatively few labeled examples. Our analysis follows a minimax style with an emphasis on finite sample results (in terms of n: the number of labeled examples). These results allow us to properly interpret manifold regularization and related spectral and geometric algorithms in terms of their potential use in semi-supervised learning."
            ],
            "keywords": [
                "semi-supervised learning",
                "manifold regularization",
                "graph Laplacian",
                "minimax rates"
            ],
            "author": [
                "Partha Niyogi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/niyogi13a/niyogi13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data",
            "abstract": [
                "In this paper, we present an overview of generalized expectation criteria (GE), a simple, robust, scalable method for semi-supervised training using weakly-labeled data. GE fits model parameters by favoring models that match certain expectation constraints, such as marginal label distributions, on the unlabeled data. This paper shows how to apply generalized expectation criteria to two classes of parametric models: maximum entropy models and conditional random fields. Experimental results demonstrate accuracy improvements over supervised training and a number of other stateof-the-art semi-supervised learning methods for these models."
            ],
            "keywords": [
                "generalized expectation criteria",
                "semi-supervised learning",
                "logistic regression",
                "conditional random fields"
            ],
            "author": [
                "Gideon S Mann",
                "Andrew Mccallum",
                "* Gideon",
                "S Mann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/mann10a/mann10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "How Deep Are Deep Gaussian Processes?",
            "abstract": [
                "Recent research has shown the potential utility of deep Gaussian processes. These deep structures are probability distributions, designed through hierarchical construction, which are conditionally Gaussian. In this paper, the current published body of work is placed in a common framework and, through recursion, several classes of deep Gaussian processes are defined. The resulting samples generated from a deep Gaussian process have a Markovian structure with respect to the depth parameter, and the effective depth of the resulting process is interpreted in terms of the ergodicity, or non-ergodicity, of the resulting Markov chain. For the classes of deep Gaussian processes introduced, we provide results concerning their ergodicity and hence their effective depth. We also demonstrate how these processes may be used for inference; in particular we show how a Metropolis-within-Gibbs construction across the levels of the hierarchy can be used to derive sampling tools which are robust to the level of resolution used to represent the functions on a computer. For illustration, we consider the effect of ergodicity in some simple numerical examples."
            ],
            "keywords": [
                "deep learning",
                "deep Gaussian processes",
                "deep kernels"
            ],
            "author": [
                "Matthew M Dunlop",
                "Mark A Girolami",
                "Andrew M Stuart",
                "Aretha L Teckentrup"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-015/18-015.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exact Learning of Lightweight Description Logic Ontologies",
            "abstract": [
                "We study the problem of learning description logic (DL) ontologies in Angluin et al.'s framework of exact learning via queries. We admit membership queries (\"is a given subsumption entailed by the target ontology?\") and equivalence queries (\"is a given ontology equivalent to the target ontology?\"). We present three main results: (1) ontologies formulated in (two relevant versions of) the description logic DL-Lite can be learned with polynomially many queries of polynomial size; (2) this is not the case for ontologies formulated in the description logic EL, even when only acyclic ontologies are admitted; and (3) ontologies formulated in a fragment of EL related to the web ontology language OWL 2 RL can be learned in polynomial time. We also show that neither membership nor equivalence queries alone are sufficient in cases (1) and (3)."
            ],
            "keywords": [
                "Exact Learning",
                "Description Logic",
                "Complexity"
            ],
            "author": [
                "Boris Konev",
                "Carsten Lutz",
                "Ana Ozaki",
                "Frank Wolter"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-256/16-256.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Clustering of Biological Sequences *",
            "abstract": [
                "Given a point set S and an unknown metric d on S, we study the problem of efficiently partitioning S into k clusters while querying few distances between the points. In our model we assume that we have access to one versus all queries that given a point s ∈ S return the distances between s and all other points. We show that given a natural assumption about the structure of the instance, we can efficiently find an accurate clustering using only O(k) distance queries. Our algorithm uses an active selection strategy to choose a small set of points that we call landmarks, and considers only the distances between landmarks and other points to produce a clustering. We use our procedure to cluster proteins by sequence similarity. This setting nicely fits our model because we can use a fast sequence database search program to query a sequence against an entire data set. We conduct an empirical study that shows that even though we query a small fraction of the distances between the points, we produce clusterings that are close to a desired clustering given by manual classification."
            ],
            "keywords": [
                "clustering",
                "active clustering",
                "k-median",
                "approximation algorithms",
                "approximation stability",
                "clustering accuracy",
                "protein sequences"
            ],
            "author": [
                "Konstantin Voevodski",
                "Maria-Florina Balcan",
                "Heiko Röglin",
                "Shang-Hua Teng",
                "Yu Xia"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/voevodski12a/voevodski12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Estimation in Ising Model via Penalized Monte Carlo Methods",
            "abstract": [
                "We consider a model selection problem in high-dimensional binary Markov random fields. The usefulness of the Ising model in studying systems of complex interactions has been confirmed in many papers. The main drawback of this model is the intractable norming constant that makes estimation of parameters very challenging. In the paper we propose a Lasso penalized version of the Monte Carlo maximum likelihood method. We prove that our algorithm, under mild regularity conditions, recognizes the true dependence structure of the graph with high probability. The efficiency of the proposed method is also investigated via numerical studies."
            ],
            "keywords": [
                "Ising model",
                "Monte Carlo Markov chain",
                "Markov random field",
                "model selection",
                "Lasso penalty"
            ],
            "author": [
                "B Lażej Miasojedow",
                "Wojciech Rejchel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-554/16-554.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Shallow Parsing with PoS Taggers and Linguistic Features",
            "abstract": [
                "Three data-driven publicly available part-of-speech taggers are applied to shallow parsing of Swedish texts. The phrase structure is represented by nine types of phrases in a hierarchical structure containing labels for every constituent type the token belongs to in the parse tree. The encoding is based on the concatenation of the phrase tags on the path from lowest to higher nodes. Various linguistic features are used in learning; the taggers are trained on the basis of lexical information only, part-of-speech only, and a combination of both, to predict the phrase structure of the tokens with or without part-of-speech. Special attention is directed to the taggers' sensitivity to different types of linguistic information included in learning, as well as the taggers' sensitivity to the size and the various types of training data sets. The method can be easily transferred to other languages."
            ],
            "keywords": [
                "Chunking",
                "Shallow parsing",
                "Part-of-speech taggers",
                "Hidden Markov models",
                "Maximum entropy learning",
                "Transformation-based learning"
            ],
            "author": [
                "Beáta Megyesi",
                "James Hammerton",
                "Miles Osborne",
                "Susan Armstrong",
                "Walter Daelemans"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/megyesi02a/megyesi02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Theoretical Analysis of Bayesian Matrix Factorization *",
            "abstract": [
                "Recently, variational Bayesian (VB) techniques have been applied to probabilistic matrix factorization and shown to perform very well in experiments. In this paper, we theoretically elucidate properties of the VB matrix factorization (VBMF) method. Through finite-sample analysis of the VBMF estimator, we show that two types of shrinkage factors exist in the VBMF estimator: the positive-part James-Stein (PJS) shrinkage and the trace-norm shrinkage, both acting on each singular component separately for producing low-rank solutions. The trace-norm shrinkage is simply induced by non-flat prior information, similarly to the maximum a posteriori (MAP) approach. Thus, no trace-norm shrinkage remains when priors are non-informative. On the other hand, we show a counter-intuitive fact that the PJS shrinkage factor is kept activated even with flat priors. This is shown to be induced by the non-identifiability of the matrix factorization model, that is, the mapping between the target matrix and factorized matrices is not one-to-one. We call this model-induced regularization. We further extend our analysis to empirical Bayes scenarios where hyperparameters are also learned based on the VB free energy. Throughout the paper, we assume no missing entry in the observed matrix, and therefore collaborative filtering is out of scope."
            ],
            "keywords": [
                "matrix factorization",
                "variational Bayes",
                "empirical Bayes",
                "positive-part James-Stein shrinkage",
                "non-identifiable model",
                "model-induced regularization"
            ],
            "author": [
                "Shinichi Nakajima"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/nakajima11a/nakajima11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Deep Reinforcement Learning for Swarm Systems",
            "abstract": [
                "Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, the observation vector for decentralized decision making is represented by a concatenation of the (local) information an agent gathers about other agents. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions, where we treat the agents as samples and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and neural networks trained end-to-end. We evaluate the representation on two well-known problems from the swarm literature-rendezvous and pursuit evasion-in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents, facilitating the development of complex collective strategies."
            ],
            "keywords": [
                "deep reinforcement learning",
                "swarm systems",
                "mean embeddings",
                "neural networks",
                "multi-agent systems"
            ],
            "author": [
                "Maximilian Hüttenrauch",
                "Adrian Šošić",
                "Gerhard Neumann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-476/18-476.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fair Data Adaptation with Quantile Preservation",
            "abstract": [
                "Fairness of classification and regression has received much attention recently and various, partially non-compatible, criteria have been proposed. The fairness criteria can be enforced for a given classifier or, alternatively, the data can be adapted to ensure that every classifier trained on the data will adhere to desired fairness criteria. We present a practical data adaption method based on quantile preservation in causal structural equation models. The data adaptation is based on a presumed counterfactual model for the data. While the counterfactual model itself cannot be verified experimentally, we show that certain population notions of fairness are still guaranteed even if the counterfactual model is misspecified. The nature of the fulfilled observational non-causal fairness notion (such as demographic parity, separation or sufficiency) depends on the structure of the underlying causal model and the choice of resolving variables. We describe an implementation of the proposed data adaptation procedure based on Random Forests (Breiman, 2001) and demonstrate its practical use on simulated and real-world data."
            ],
            "keywords": [
                "Supervised learning",
                "Fairness in machine learning",
                "Causality",
                "Graphical models",
                "Counterfactual fairness"
            ],
            "author": [
                "Drago Plečko",
                "Nicolai Meinshausen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-966/19-966.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simultaneous Phase Retrieval and Blind Deconvolution via Convex Programming",
            "abstract": [
                "We consider the task of recovering two real or complex m-vectors from phaseless Fourier measurements of their circular convolution. Our method is a novel convex relaxation that is based on a lifted matrix recovery formulation that allows a non-trivial convex relaxation of the bilinear measurements from convolution. We prove that if the two signals belong to known random subspaces of dimensions k and n, then they can be recovered up to the inherent scaling ambiguity with m (k + n) log 2 m phaseless measurements. Our method provides the first theoretical recovery guarantee for this problem by a computationally efficient algorithm and does not require a solution estimate to be computed for initialization. Our proof is based on Rademacher complexity estimates. Additionally, we provide an alternating direction method of multipliers (ADMM) implementation and provide numerical experiments that verify the theory."
            ],
            "keywords": [
                "Hyperbolic Constraints",
                "Blind Deconvolution",
                "Phase Retrieval",
                "Convex Analysis",
                "Rademacher Complexity"
            ],
            "author": [
                "Ali Ahmed",
                "Alireza Aghasi",
                "Paul Hand"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-332/19-332.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Learning of Clusters of Undercomplete Nonsymmetric Independent Components",
            "abstract": [
                "We apply a variational method to automatically determine the number of mixtures of independent components in high-dimensional datasets, in which the sources may be nonsymmetrically distributed. The data are modeled by clusters where each cluster is described as a linear mixture of independent factors. The variational Bayesian method yields an accurate density model for the observed data without overfitting problems. This allows the dimensionality of the data to be identified for each cluster. The new method was successfully applied to a difficult real-world medical dataset for diagnosing glaucoma."
            ],
            "keywords": [
                "Density Estimations",
                "Mixture Models",
                "Bayesian Learning",
                "ICA"
            ],
            "author": [
                "Kwokleung Chan",
                "Terrence J Sejnowski"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/chan02a/chan02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiclass Learnability and the ERM Principle",
            "abstract": [
                "We study the sample complexity of multiclass prediction in several learning settings. For the PAC setting our analysis reveals a surprising phenomenon: In sharp contrast to binary classification, we show that there exist multiclass hypothesis classes for which some Empirical Risk Minimizers (ERM learners) have lower sample complexity than others. Furthermore, there are classes that are learnable by some ERM learners, while other ERM learners will fail to learn them. We propose a principle for designing good ERM learners, and use this principle to prove tight bounds on the sample complexity of learning symmetric multiclass hypothesis classes-classes that are invariant under permutations of label names. We further provide a characterization of mistake and regret bounds for multiclass learning in the online setting and the bandit setting, using new generalizations of Littlestone's dimension."
            ],
            "keywords": [
                "multiclass",
                "sample complexity",
                "ERM"
            ],
            "author": [
                "Amit Daniely",
                "Sivan Sabato",
                "Shai Ben-David",
                "David R Cheriton",
                "Shai Shalev-Shwartz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/daniely15a/daniely15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Image Components for Object Recognition",
            "abstract": [
                "In order to perform object recognition it is necessary to learn representations of the underlying components of images. Such components correspond to objects, object-parts, or features. Nonnegative matrix factorisation is a generative model that has been specifically proposed for finding such meaningful representations of image data, through the use of non-negativity constraints on the factors. This article reports on an empirical investigation of the performance of non-negative matrix factorisation algorithms. It is found that such algorithms need to impose additional constraints on the sparseness of the factors in order to successfully deal with occlusion. However, these constraints can themselves result in these algorithms failing to identify image components under certain conditions. In contrast, a recognition model (a competitive learning neural network algorithm) reliably and accurately learns representations of elementary image features without such constraints."
            ],
            "keywords": [
                "non-negative matrix factorisation",
                "competitive learning",
                "dendritic inhibition",
                "object recognition"
            ],
            "author": [
                "Michael W Spratling"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/spratling06a/spratling06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization",
            "abstract": [
                "Nonlinear dimensionality reduction methods are often used to visualize high-dimensional data, although the existing methods have been designed for other related tasks such as manifold learning. It has been difficult to assess the quality of visualizations since the task has not been well-defined. We give a rigorous definition for a specific visualization task, resulting in quantifiable goodness measures and new visualization methods. The task is information retrieval given the visualization: to find similar data based on the similarities shown on the display. The fundamental tradeoff between precision and recall of information retrieval can then be quantified in visualizations as well. The user needs to give the relative cost of missing similar points vs. retrieving dissimilar points, after which the total cost can be measured. We then introduce a new method NeRV (neighbor retrieval visualizer) which produces an optimal visualization by minimizing the cost. We further derive a variant for supervised visualization; class information is taken rigorously into account when computing the similarity relationships. We show empirically that the unsupervised version outperforms existing unsupervised dimensionality reduction methods in the visualization task, and the supervised version outperforms existing supervised methods."
            ],
            "keywords": [
                "information retrieval",
                "manifold learning",
                "multidimensional scaling",
                "nonlinear dimensionality reduction",
                "visualization"
            ],
            "author": [
                "Jarkko Venna",
                "Kristian Nybo",
                "Helena Aidos",
                "Samuel Kaski"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/venna10a/venna10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Co-Boosting for Multi-modal Gesture Recognition",
            "abstract": [
                "With the development of data acquisition equipment, more and more modalities become available for gesture recognition. However, there still exist two critical issues for multimodal gesture recognition: how to select discriminative features for recognition and how to fuse features from different modalities. In this paper, we propose a novel Bayesian Co-Boosting framework for multi-modal gesture recognition. Inspired by boosting learning and co-training method, our proposed framework combines multiple collaboratively trained weak classifiers to construct the final strong classifier for the recognition task. During each iteration round, we randomly sample a number of feature subsets and estimate weak classifier's parameters for each subset. The optimal weak classifier and its corresponding feature subset are retained for strong classifier construction. Furthermore, we define an upper bound of training error and derive the update rule of instance's weight, which guarantees the error upper bound to be minimized through iterations. For demonstration, we present an implementation of our framework using hidden Markov models as weak classifiers. We perform extensive experiments using the ChaLearn MMGR and ChAirGest data sets, in which our approach achieves 97.63% and 96.53% accuracy respectively on each publicly available data set."
            ],
            "keywords": [
                "gesture recognition",
                "Bayesian co-boosting",
                "hidden Markov model",
                "multimodal fusion",
                "feature selection"
            ],
            "author": [
                "Jiaxiang Wu",
                "Jian Cheng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/wu14a/wu14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "RLPy: A Value-Function-Based Reinforcement Learning Framework for Education and Research",
            "abstract": [
                "RLPy is an object-oriented reinforcement learning software package with a focus on valuefunction-based methods using linear function approximation and discrete actions. The framework was designed for both educational and research purposes. It provides a rich library of fine-grained, easily exchangeable components for learning agents (e.g., policies or representations of value functions), facilitating recently increased specialization in reinforcement learning. RLPy is written in Python to allow fast prototyping, but is also suitable for large-scale experiments through its built-in support for optimized numerical libraries and parallelization. Code profiling, domain visualizations, and data analysis are integrated in a self-contained package available under the Modified BSD License at http://github.com/rlpy/rlpy. All of these properties allow users to compare various reinforcement learning algorithms with little effort."
            ],
            "keywords": [
                "reinforcement learning",
                "value-function",
                "empirical evaluation",
                "open source"
            ],
            "author": [
                "Alborz Geramifard",
                "Christoph Dann",
                "Robert H Klein",
                "William Dabney",
                "Jonathan P How"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/geramifard15a/geramifard15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Plug-and-Play Dual-Tree Algorithm Runtime Analysis",
            "abstract": [
                "Numerous machine learning algorithms contain pairwise statistical problems at their corethat is, tasks that require computations over all pairs of input points if implemented naively. Often, tree structures are used to solve these problems efficiently. Dual-tree algorithms can efficiently solve or approximate many of these problems. Using cover trees, rigorous worstcase runtime guarantees have been proven for some of these algorithms. In this paper, we present a problem-independent runtime guarantee for any dual-tree algorithm using the cover tree, separating out the problem-dependent and the problem-independent elements. This allows us to just plug in bounds for the problem-dependent elements to get runtime guarantees for dual-tree algorithms for any pairwise statistical problem without re-deriving the entire proof. We demonstrate this plug-and-play procedure for nearest-neighbor search and approximate kernel density estimation to get improved runtime guarantees. Under mild assumptions, we also present the first linear runtime guarantee for dual-tree based range search."
            ],
            "keywords": [
                "dual-tree algorithms",
                "adaptive runtime analysis",
                "cover tree",
                "expansion constant",
                "nearest neighbor search",
                "kernel density estimation",
                "range search"
            ],
            "author": [
                "Ryan R Curtin",
                "William B March"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/curtin15a/curtin15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Using Side Information to Reliably Learn Low-Rank Matrices from Missing and Corrupted Observations",
            "abstract": [
                "Learning a low-rank matrix from missing and corrupted observations is a fundamental problem in many machine learning applications. However, the role of side information in low-rank matrix learning has received little attention, and most current approaches are either ad-hoc or only applicable in certain restrictive cases. In this paper, we propose a general model that exploits side information to better learn low-rank matrices from missing and corrupted observations, and show that the proposed model can be further applied to several popular scenarios such as matrix completion and robust PCA. Furthermore, we study the effect of side information on sample complexity and show that by using our model, the efficiency for learning can be improved given sufficiently informative side information. This result thus provides theoretical insight into the usefulness of side information in our model. Finally, we conduct comprehensive experiments in three real-world applicationsrelationship prediction, semi-supervised clustering and noisy image classification, showing that our proposed model is able to properly exploit side information for more effective learning both in theory and practice."
            ],
            "keywords": [
                "Side information",
                "low-rank matrix learning",
                "learning from missing and corrupted observations",
                "matrix completion",
                "robust PCA"
            ],
            "author": [
                "Kai-Yang Chiang",
                "Inderjit S Dhillon",
                "Cho-Jui Hsieh",
                "Yang Chiang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-112/17-112.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simultaneous Inference for Pairwise Graphical Models with Generalized Score Matching",
            "abstract": [
                "Probabilistic graphical models provide a flexible yet parsimonious framework for modeling dependencies among nodes in networks. There is a vast literature on parameter estimation and consistent model selection for graphical models. However, in many of the applications, scientists are also interested in quantifying the uncertainty associated with the estimated parameters and selected models, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for statistical inference on edge parameters in pairwise graphical models based on generalized Hyvärinen scoring rule. Hyvärinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form, which is a common problem for graphical models, including Ising models and truncated Gaussian graphical models. Our estimator allows us to perform statistical inference for general graphical models whereas the existing works mostly focus on statistical inference for Gaussian graphical models where finding normalizing constant is computationally tractable. Under mild conditions that are typically assumed in the literature for consistent estimation, we prove that our proposed estimator is √ n-consistent and asymptotically normal, which allows us to construct confidence intervals and build hypothesis tests for edge parameters. Moreover, we show how our proposed method can be applied to test hypotheses that involve a large number of model parameters simultaneously. We illustrate validity of our estimator through extensive simulation studies on a diverse collection of data-generating processes."
            ],
            "keywords": [
                "generalized score matching",
                "high-dimensional inference",
                "probabilistic graphical models",
                "simultaneous inference"
            ],
            "author": [
                "Ming Yu",
                "Varun Gupta"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-383/19-383.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A General Distributed Dual Coordinate Optimization Framework for Regularized Loss Minimization",
            "abstract": [
                "In modern large-scale machine learning applications, the training data are often partitioned and stored on multiple machines. It is customary to employ the \"data parallelism\" approach, where the aggregated training loss is minimized without moving data across machines. In this paper, we introduce a novel distributed dual formulation for regularized loss minimization problems that can directly handle data parallelism in the distributed setting. This formulation allows us to systematically derive dual coordinate optimization procedures, which we refer to as Distributed Alternating Dual Maximization (DADM). The framework extends earlier studies described in (Boyd et al., 2011; Ma et al., 2017; Jaggi et al., 2014; Yang, 2013) and has rigorous theoretical analyses. Moreover, with the help of the new formulation, we develop the accelerated version of DADM (Acc-DADM) by generalizing the acceleration technique from (Shalev-Shwartz and Zhang, 2014) to the distributed setting. We also provide theoretical results for the proposed accelerated version, and the new result improves previous ones (Yang, 2013; Ma et al., 2017) whose iteration complexities grow linearly on the condition number. Our empirical studies validate our theory and show that our accelerated approach significantly improves the previous state-of-the-art distributed dual coordinate optimization algorithms. *. Most of the work was done during the internship of Shun Zheng at Baidu Big Data Lab in Beijing."
            ],
            "keywords": [
                "distributed optimization",
                "stochastic dual coordinate ascent",
                "acceleration",
                "regularized loss minimization",
                "computational complexity"
            ],
            "author": [
                "Jialei Wang",
                "Fen Xia",
                "Wei Xu",
                "Tong Zhang",
                "Xia, Xu Zhang Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-463/16-463.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Convergence of Gaussian Belief Propagation with Nodes of Arbitrary Size",
            "abstract": [
                "This paper is concerned with a multivariate extension of Gaussian message passing applied to pairwise Markov graphs (MGs). Gaussian message passing applied to pairwise MGs is often labeled Gaussian belief propagation (GaBP) and can be used to approximate the marginal of each variable contained in the pairwise MG. We propose a multivariate extension of GaBP (we label this GaBP-m) that can be used to estimate higher-dimensional marginals. Beyond the ability to estimate higher-dimensional marginals, GaBP-m exhibits better convergence behavior than GaBP, and can also provide more accurate univariate marginals. The theoretical results of this paper are based on an extension of the computation tree analysis conducted on univariate nodes to the multivariate case. The main contribution of this paper is the development of a convergence condition for GaBP-m that moves beyond the walk-summability of the precision matrix. Based on this convergence condition, we derived an upper bound for the number of iterations required for convergence of the GaBP-m algorithm. An upper bound on the dissimilarity between the approximate and exact marginal covariance matrices was established. We argue that GaBP-m is robust towards a certain change in variables, a property not shared by iterative solvers of linear systems, such as the conjugate gradient (CG) and preconditioned conjugate gradient (PCG) methods. The advantages of using GaBP-m over GaBP are also illustrated empirically."
            ],
            "keywords": [
                "belief propagation",
                "Gaussian distributions",
                "higher-dimensional marginals",
                "preconditioning",
                "inference"
            ],
            "author": [
                "Francois Kamper",
                "Johan A Du Preez"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-040/18-040.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Overfitting in Making Comparisons Between Variable Selection Methods",
            "abstract": [
                "This paper addresses a common methodological flaw in the comparison of variable selection methods. A practical approach to guide the search or the selection process is to compute cross-validation performance estimates of the different variable subsets. Used with computationally intensive search algorithms, these estimates may overfit and yield biased predictions. Therefore, they cannot be used reliably to compare two selection methods, as is shown by the empirical results of this paper. Instead, like in other instances of the model selection problem, independent test sets should be used for determining the final performance. The claims made in the literature about the superiority of more exhaustive search algorithms over simpler ones are also revisited, and some of them infirmed."
            ],
            "keywords": [
                "Variable selection",
                "Algorithm comparison",
                "Overfitting",
                "Cross-validation",
                "k nearest neighbors"
            ],
            "author": [
                "Juha Reunanen",
                "Isabelle Guyon",
                "André Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/reunanen03a/reunanen03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Online Learning via Truncated Gradient",
            "abstract": [
                "We propose a general method called truncated gradient to induce sparsity in the weights of onlinelearning algorithms with convex loss functions. This method has several essential properties: 1. The degree of sparsity is continuous-a parameter controls the rate of sparsification from no sparsification to total sparsification. 2. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L 1-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online-learning guarantees. 3. The approach works well empirically. We apply the approach to several data sets and find for data sets with large numbers of features, substantial sparsity is discoverable."
            ],
            "keywords": [
                "truncated gradient",
                "stochastic gradient descent",
                "online learning",
                "sparsity",
                "regularization",
                "Lasso"
            ],
            "author": [
                "John Langford",
                "Lihong Li",
                "Tong Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/langford09a/langford09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "CEKA: A Tool for Mining the Wisdom of Crowds",
            "abstract": [
                "CEKA is a software package for developers and researchers to mine the wisdom of crowds. It makes the entire knowledge discovery procedure much easier, including analyzing qualities of workers, simulating labeling behaviors, inferring true class labels of instances, filtering and correcting mislabeled instances (noise), building learning models and evaluating them. It integrates a set of state-of-the-art inference algorithms, a set of general noise handling algorithms, and abundant functions for model training and evaluation. CEKA is written in Java with core classes being compatible with the well-known machine learning tool WEKA, which makes the utilization of the functions in WEKA much easier."
            ],
            "keywords": [
                "crowdsourcing",
                "learning from crowds",
                "multiple noisy labeling",
                "inference",
                "noise handling",
                "repeated labeling simulation"
            ],
            "author": [
                "Jing Zhang",
                "Victor S Sheng",
                "Bryce A Nicholson",
                "Xindong Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/zhang15a/zhang15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimal Kernel Classifiers",
            "abstract": [
                "A finite concave minimization algorithm is proposed for constructing kernel classifiers that use a minimal number of data points both in generating and characterizing a classifier. The algorithm is theoretically justified on the basis of linear programming perturbation theory and a leave-one-out error bound as well as effective computational results on seven real world datasets. A nonlinear rectangular kernel is generated by systematically utilizing as few of the data as possible both in training and in characterizing a nonlinear separating surface. This can result in substantial reduction in kernel data-dependence (over 94% in six of the seven public datasets tested on) and with test set correctness equal to that obtained by using a conventional support vector machine classifier that depends on many more data points. This reduction in data dependence results in a much faster classifier that requires less storage. To eliminate data points, the proposed approach makes use of a novel loss function, the \"pound\" function (•) # , which is a linear combination of the 1-norm and the step function that measures both the magnitude and the presence of any error."
            ],
            "keywords": [
                "Support Vector Machines",
                "Sparse Kernels",
                "Data Reduction",
                "Concave Minimization"
            ],
            "author": [
                "Glenn M Fung",
                "Olvi L Mangasarian",
                "Alexander J Smola"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/fung02a/fung02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Plug-in Approach to Active Learning",
            "abstract": [
                "We present a new active learning algorithm based on nonparametric estimators of the regression function. Our investigation provides probabilistic bounds for the rates of convergence of the generalization error achievable by proposed method over a broad class of underlying distributions. We also prove minimax lower bounds which show that the obtained rates are almost tight."
            ],
            "keywords": [
                "active learning",
                "selective sampling",
                "model selection",
                "classification",
                "confidence bands"
            ],
            "author": [
                "Stanislav Minsker"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/minsker12a/minsker12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning",
            "abstract": [
                "imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of overand under-sampling, and (iv) ensemble learning methods. The proposed toolbox depends only on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. Source code, binaries, and documentation can be downloaded from https://github.com/scikit-learn-contrib/imbalanced-learn."
            ],
            "keywords": [
                "Imbalanced Dataset",
                "Over-Sampling",
                "Under-Sampling",
                "Ensemble Learning",
                "Machine Learning",
                "Python"
            ],
            "author": [
                "Guillaume Lemaître",
                "Fernando Nogueira",
                "Christos K Aridas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-365/16-365.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Factorized Self-Controlled Case Series Method: An Approach for Estimating the Effects of Many Drugs on Many Outcomes",
            "abstract": [
                "We provide a hierarchical Bayesian model for estimating the effects of transient drug exposures on a collection of health outcomes, where the effects of all drugs on all outcomes are estimated simultaneously. The method possesses properties that allow it to handle important challenges of dealing with large-scale longitudinal observational databases. In particular, this model is a generalization of the self-controlled case series (SCCS) method, meaning that certain patient specific baseline rates never need to be estimated. Further, this model is formulated with layers of latent factors, which substantially reduces the number of parameters and helps with interpretability by illuminating latent classes of drugs and outcomes. We believe our work is the first to consider multivariate SCCS (in the sense of multiple outcomes) and is the first to couple latent factor analysis with SCCS. We demonstrate the approach by estimating the effects of various time-sensitive insulin treatments for diabetes."
            ],
            "keywords": [
                "Bayesian Analysis",
                "Drug Safety",
                "Self-Controlled Case Series",
                "Matrix Factorization",
                "Effect Size Estimation"
            ],
            "author": [
                "Ramin Moghaddass",
                "Cynthia Rudin",
                "David Madigan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-405/15-405.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition",
            "abstract": [
                "In multi-response regression, pursuit of two different types of structures is essential to battle the curse of dimensionality. In this paper, we seek a sparsest decomposition representation of a parameter matrix in terms of a sum of sparse and low rank matrices, among many overcomplete decompositions. On this basis, we propose a constrained method subject to two nonconvex constraints, respectively for sparseness and low-rank properties. Computationally, obtaining an exact global optimizer is rather challenging. To overcome the difficulty, we use an alternating directions method solving a low-rank subproblem and a sparseness subproblem alternatively, where we derive an exact solution to the low-rank subproblem, as well as an exact solution in a special case and an approximated solution generally through a surrogate of the L 0-constraint and difference convex programming, for the sparse subproblem. Theoretically, we establish convergence rates of a global minimizer in the Hellinger-distance, providing an insight into why pursuit of two different types of decomposed structures is expected to deliver higher estimation accuracy than its counterparts based on either sparseness alone or low-rank approximation alone. Numerical examples are given to illustrate these aspects, in addition to an application to facial imagine recognition and multiple time series analysis."
            ],
            "keywords": [
                "blockwise decent",
                "nonconvex minimization",
                "matrix decomposition",
                "structure pursuit"
            ],
            "author": [
                "Qi Yan",
                "Xiaotong Shen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/yan15a/yan15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Discriminant Wavelet Packet Coordinates for Face Recognition",
            "abstract": [
                "Face recognition is a challenging problem due to variations in pose, illumination, and expression. Techniques that can provide effective feature representation with enhanced discriminability are crucial. Wavelets have played an important role in image processing for its ability to capture localized spatial-frequency information of images. In this paper, we propose a novel local discriminant coordinates method based on wavelet packet for face recognition to compensate for these variations. Traditional wavelet-based methods for face recognition select or operate on the most discriminant subband, and neglect the scattered characteristic of discriminant features. The proposed method selects the most discriminant coordinates uniformly from all spatial frequency subbands to overcome the deficiency of traditional wavelet-based methods. To measure the discriminability of coordinates, a new dilation invariant entropy and a maximum a posterior logistic model are put forward. Moreover, a new triangle square ratio criterion is used to improve classification using the Euclidean distance and the cosine criterion. Experimental results show that the proposed method is robust for face recognition under variations in illumination, pose and expression."
            ],
            "keywords": [
                "local discriminant coordinates",
                "invariant entropy",
                "logistic model",
                "wavelet packet",
                "face recognition",
                "illumination",
                "pose and expression variations"
            ],
            "author": [
                "Chao-Chun Liu",
                "Dao-Qing Dai",
                "Hong Yan",
                "H Yan@",
                "Cityu Edu",
                "Chun Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/liu07a/liu07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal Data: Learning and Inference",
            "abstract": [
                "Modeling continuous-time physiological processes that manifest a patient's evolving clinical states is a key step in approaching many problems in healthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model (HASMM): a versatile probabilistic model that is capable of capturing the modern electronic health record (EHR) data. Unlike existing models, the HASMM accommodates irregularly sampled, temporally correlated, and informatively censored physiological data, and can describe non-stationary clinical state transitions. Learning the HASMM parameters from the EHR data is achieved via a novel forward-filtering backward-sampling Monte-Carlo EM algorithm that exploits the knowledge of the end-point clinical outcomes (informative censoring) in the EHR data, and implements the E-step by sequentially sampling the patients' clinical states in the reversetime direction while conditioning on the future states. Real-time inferences are drawn via a forward-filtering algorithm that operates on a virtually constructed discrete-time embedded Markov chain that mirrors the patient's continuous-time state trajectory. We demonstrate the prognostic utility of the HASMM in a critical care prognosis setting using a real-world dataset for patients admitted to the Ronald Reagan UCLA Medical Center. In particular, we show that using HASMMs, a patient's clinical deterioration can be predicted 8-9 hours prior to intensive care unit admission, with a 22% AUC gain compared to the Rothman index, which is the state-of-the-art critical care risk scoring technology."
            ],
            "keywords": [
                "Hidden Semi-Markov Models",
                "Medical Informatics",
                "Monte Carlo methods"
            ],
            "author": [
                "Ahmed M Alaa",
                "Edoardo M Airoldi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-656/16-656.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric Sparsity and Regularization",
            "abstract": [
                "In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose a new notion of nonparametric sparsity and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art methods."
            ],
            "keywords": [
                "sparsity",
                "nonparametric",
                "variable selection",
                "regularization",
                "proximal methods",
                "RKHS"
            ],
            "author": [
                "Lorenzo Rosasco",
                "Sofia Mosci",
                "Matteo Santoro",
                "Alessandro Verri",
                "Silvia Villa"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/rosasco13a/rosasco13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information",
            "abstract": [
                "We study the misclassification error for community detection in general heterogeneous stochastic block models (SBM) with noisy or partial label information. We establish a connection between the misclassification rate and the notion of minimum energy on the local neighborhood of the SBM. We develop an optimally weighted message passing algorithm to reconstruct labels for SBM based on the minimum energy flow and the eigenvectors of a certain Markov transition matrix. The general SBM considered in this paper allows for unequal-size communities, degree heterogeneity, and different connection probabilities among blocks. We focus on how to optimally weigh the message passing to improve misclassification."
            ],
            "keywords": [
                "semi-supervised learning",
                "general stochastic block models",
                "misclassification",
                "weighted message passing",
                "minimum energy flow",
                "statistical inference"
            ],
            "author": [
                "T Tony Cai",
                "Tengyuan Liang",
                "Alexander Rakhlin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-573/18-573.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes",
            "abstract": [
                "Bayesian learning methods have recently been shown to provide an elegant solution to the explorationexploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be finitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent's return improve as a function of experience."
            ],
            "keywords": [
                "reinforcement learning",
                "Bayesian inference",
                "partially observable Markov decision processes"
            ],
            "author": [
                "Stéphane Ross",
                "Joelle Pineau",
                "Pierre Kreitmann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/ross11a/ross11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Community-Based Group Graphical Lasso",
            "abstract": [
                "A new strategy for probabilistic graphical modeling is developed that draws parallels to community detection analysis. The method jointly estimates an undirected graph and homogenous communities of nodes. The structure of the communities is taken into account when estimating the graph and at the same time, the structure of the graph is accounted for when estimating communities of nodes. The procedure uses a joint group graphical lasso approach with community detection-based grouping, such that some groups of edges cooccur in the estimated graph. The grouping structure is unknown and is estimated based on community detection algorithms. Theoretical derivations regarding graph convergence and sparsistency, as well as accuracy of community recovery are included, while the method's empirical performance is illustrated in an fMRI context, as well as with simulated examples."
            ],
            "keywords": [
                "community detection",
                "graphical model",
                "group penalty",
                "joint graphical lasso"
            ],
            "author": [
                "Eugen Pircalabelu",
                "Gerda Claeskens"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-181/19-181.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice",
            "abstract": [
                "We introduce a generic scheme for accelerating gradient-based optimization methods in the sense of Nesterov. The approach, called Catalyst, builds upon the inexact accelerated proximal point algorithm for minimizing a convex objective function, and consists of approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. One of the keys to achieve acceleration in theory and in practice is to solve these sub-problems with appropriate accuracy by using the right stopping criterion and the right warm-start strategy. We give practical guidelines to use Catalyst and present a comprehensive analysis of its global complexity. We show that Catalyst applies to a large class of algorithms, including gradient descent, block coordinate descent, incremental algorithms such as SAG, SAGA, SDCA, SVRG, MISO/Finito, and their proximal variants. For all of these methods, we establish faster rates using the Catalyst acceleration, for strongly convex and non-strongly convex objectives. We conclude with extensive experiments showing that acceleration is useful in practice, especially for ill-conditioned problems."
            ],
            "keywords": [
                "convex optimization",
                "first-order methods",
                "large-scale machine learning"
            ],
            "author": [
                "Hongzhou Lin",
                "Julien Mairal",
                "Zaid Harchaoui"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-748/17-748.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Polynomial Identification in the Limit of Substitutable Context-free Languages",
            "abstract": [
                "This paper formalises the idea of substitutability introduced by Zellig Harris in the 1950s and makes it the basis for a learning algorithm from positive data only for a subclass of context-free languages. We show that there is a polynomial characteristic set, and thus prove polynomial identification in the limit of this class. We discuss the relationship of this class of languages to other common classes discussed in grammatical inference. It transpires that it is not necessary to identify constituents in order to learn a context-free language-it is sufficient to identify the syntactic congruence, and the operations of the syntactic monoid can be converted into a context-free grammar. We also discuss modifications to the algorithm that produces a reduction system rather than a context-free grammar, that will be much more compact. We discuss the relationship to Angluin's notion of reversibility for regular languages. We also demonstrate that an implementation of this algorithm is capable of learning a classic example of structure dependent syntax in English: this constitutes a refutation of an argument that has been used in support of nativist theories of language."
            ],
            "keywords": [
                "grammatical inference",
                "context-free languages",
                "positive data only",
                "reduction system",
                "natural languages"
            ],
            "author": [
                "Alexander Clark"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/clark07a/clark07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ranking a Random Feature for Variable and Feature Selection",
            "abstract": [
                "We describe a feature selection method that can be applied directly to models that are linear with respect to their parameters, and indirectly to others. It is independent of the target machine. It is closely related to classical statistical hypothesis tests, but it is more intuitive, hence more suitable for use by engineers who are not statistics experts. Furthermore, some assumptions of classical tests are relaxed. The method has been used successfully in a number of applications that are briefly described."
            ],
            "keywords": [
                "Model selection",
                "variable selection",
                "feature selection",
                "kernel",
                "classification",
                "neural networks",
                "leave-one-out",
                "Gram-Schmidt orthogonalization",
                "statistical tests",
                "information filtering"
            ],
            "author": [
                "Hervé Stoppiglia",
                "Herve Stoppiglia",
                "Pechiney Com",
                "Rémi Dubois",
                "Isabelle Guyon",
                "Andrè Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/stoppiglia03a/stoppiglia03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Integrative Analysis using Coupled Latent Variable Models for Individualizing Prognoses",
            "abstract": [
                "Complex chronic diseases (e.g., autism, lupus, and Parkinson's) are remarkably heterogeneous across individuals. This heterogeneity makes treatment difficult for caregivers because they cannot accurately predict the way in which the disease will progress in order to guide treatment decisions. Therefore, tools that help to predict the trajectory of these complex chronic diseases can help to improve the quality of health care. To build such tools, we can leverage clinical markers that are collected at baseline when a patient first presents and longitudinally over time during follow-up visits. Because complex chronic diseases are typically systemic, the longitudinal markers often track disease progression in multiple organ systems. In this paper, our goal is to predict a function of time that models the future trajectory of a single target clinical marker tracking a disease process of interest. We want to make these predictions using the histories of many related clinical markers as input. Our proposed solution tackles several key challenges. First, we can easily handle irregularly and sparsely sampled markers, which are standard in clinical data. Second, the number of parameters and the computational complexity of learning our model grows linearly in the number of marker types included in the model. This makes our approach applicable to diseases where many different markers are recorded over time. Finally, our model accounts for latent factors influencing disease expression, whereas standard regression models rely on observed features alone to explain variability. Moreover, our approach can be applied dynamically in continous-time and updates its predictions as soon as any new data is available. We apply our approach to the problem of predicting lung disease trajectories in scleroderma, a complex autoimmune disease. We show that our model improves over state-of-the-art baselines in predictive accuracy and we provide a qualitative analysis of our model's output. Finally, the variability of disease presentation in scleroderma makes clinical trial recruitment challenging. We show that a prognostic tool that integrates multiple types of routinely collected longitudinal data can be used to identify individuals at greatest risk of rapid progression and to target trial recruitment."
            ],
            "keywords": [
                "gaussian processes",
                "conditional random fields",
                "prediction of functional targets",
                "latent variable models",
                "disease trajectories",
                "precision medicine"
            ],
            "author": [
                "Peter Schulam",
                "Suchi Saria"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-436/15-436.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Control Knowledge for Forward Search Planning",
            "abstract": [
                "A number of today's state-of-the-art planners are based on forward state-space search. The impressive performance can be attributed to progress in computing domain independent heuristics that perform well across many domains. However, it is easy to find domains where such heuristics provide poor guidance, leading to planning failure. Motivated by such failures, the focus of this paper is to investigate mechanisms for learning domain-specific knowledge to better control forward search in a given domain. While there has been a large body of work on inductive learning of control knowledge for AI planning, there is a void of work aimed at forward-state-space search. One reason for this may be that it is challenging to specify a knowledge representation for compactly representing important concepts across a wide range of domains. One of the main contributions of this work is to introduce a novel feature space for representing such control knowledge. The key idea is to define features in terms of information computed via relaxed plan extraction, which has been a major source of success for non-learning planners. This gives a new way of leveraging relaxed planning techniques in the context of learning. Using this feature space, we describe three forms of control knowledge-reactive policies (decision list rules and measures of progress) and linear heuristics-and show how to learn them and incorporate them into forward state-space search. Our empirical results show that our approaches are able to surpass state-of-the-art nonlearning planners across a wide range of planning competition domains."
            ],
            "keywords": [
                "planning",
                "machine learning",
                "knowledge representation",
                "search"
            ],
            "author": [
                "Sungwook Yoon",
                "Asu Edu",
                "Robert Givan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/yoon08a/yoon08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Transformation Models for Ranking and Survival Analysis",
            "abstract": NaN,
            "keywords": [
                "support vector machines",
                "preference learning",
                "ranking models",
                "ordinal regression",
                "survival analysis"
            ],
            "author": [
                "Vanya Van Belle",
                "Johan A K Suykens",
                "Sabine Van Huffel",
                "Van Belle",
                "Kristiaan Pelckmans",
                "Sabine Van"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/vanbelle11a/vanbelle11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Unfaithful K-separable Gaussian Graphical Models",
            "abstract": [
                "The global Markov property for Gaussian graphical models ensures graph separation implies conditional independence. Specifically if a node set S graph separates nodes u and v then X u is conditionally independent of X v given X S. The opposite direction need not be true, that is, X u ⊥ X v | X S need not imply S is a node separator of u and v. When it does, the relation X u ⊥ X v | X S is called faithful. In this paper we provide a characterization of faithful relations and then provide an algorithm to test faithfulness based only on knowledge of other conditional relations of the form X i ⊥ X j | X S. We study two classes of separable Gaussian graphical models, namely, weakly K-separable and strongly K-separable Gaussian graphical models. Using the above test for faithfulness, we introduce algorithms to learn the topologies of weakly K-separable and strongly K-separable Gaussian graphical models with Ω(K log p) sample complexity. For strongly K-separable Gaussian graphical models, we additionally provide a method with error bounds for learning the off-diagonal precision matrix entries."
            ],
            "keywords": [
                "Gaussian graphical model selection",
                "separable graphs",
                "high-dimensional statistical learning",
                "faithful conditional independence relations",
                "structural consistency"
            ],
            "author": [
                "Wen De"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-329/18-329.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Importance Sampling Techniques for Policy Optimization",
            "abstract": [
                "How can we effectively exploit the collected samples when solving a continuous control task with Reinforcement Learning? Recent results have empirically demonstrated that multiple policy optimization steps can be performed with the same batch by using off-distribution techniques based on importance sampling. However, when dealing with off-distribution optimization, it is essential to take into account the uncertainty introduced by the importance sampling process. In this paper, we propose and analyze a class of model-free, policy search algorithms that extend the recent Policy Optimization via Importance Sampling (Metelli et al., 2018) by incorporating two advanced variance reduction techniques: per-decision and multiple importance sampling. For both of them, we derive a high-probability bound, of independent interest, and then we show how to employ it to define a suitable surrogate objective function that can be used for both action-based and parameter-based settings. The resulting algorithms are finally evaluated on a set of continuous control tasks, using both linear and deep policies, and compared with modern policy optimization methods."
            ],
            "keywords": [
                "Reinforcement Learning",
                "Policy Optimization",
                "Importance Sampling",
                "Per-Decision Importance Sampling",
                "Multiple Importance Sampling"
            ],
            "author": [
                "Alberto Maria Metelli",
                "Matteo Papini",
                "Marcello Restelli",
                "Nico Montali"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-124/20-124.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Consistency of the Likelihood Maximization Vertex Nomination Scheme: Bridging the Gap Between Maximum Likelihood Estimation and Graph Matching",
            "abstract": [
                "Given a graph in which a few vertices are deemed interesting a priori, the vertex nomination task is to order the remaining vertices into a nomination list such that there is a concentration of interesting vertices at the top of the list. Previous work has yielded several approaches to this problem, with theoretical results in the setting where the graph is drawn from a stochastic block model (SBM), including a vertex nomination analogue of the Bayes optimal classifier. In this paper, we prove that maximum likelihood (ML)-based vertex nomination is consistent, in the sense that the performance of the ML-based scheme asymptotically matches that of the Bayes optimal scheme. We prove theorems of this form both when model parameters are known and unknown. Additionally, we introduce and prove consistency of a related, more scalable restricted-focus ML vertex nomination scheme. Finally, we incorporate vertex and edge features into ML-based vertex nomination and briefly explore the empirical effectiveness of this approach."
            ],
            "keywords": [
                "vertex nomination",
                "graph matching",
                "graph inference",
                "stochastic block model",
                "graph mining"
            ],
            "author": [
                "Vince Lyzinski",
                "Keith Levin",
                "Donniell E Fishkind"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-343/16-343.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Incorporating Functional Knowledge in Neural Networks",
            "abstract": [
                "Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in its two arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of Lipschitz 1 functions with these and other properties. We apply this new class of functions to the task of modelling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints."
            ],
            "keywords": [
                "neural networks",
                "universal approximation",
                "monotonicity",
                "convexity",
                "call options"
            ],
            "author": [
                "Charles Dugas",
                "Yoshua Bengio",
                "Claude Nadeau"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/dugas09a/dugas09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multitask Sparsity via Maximum Entropy Discrimination",
            "abstract": [
                "A multitask learning framework is developed for discriminative classification and regression where multiple large-margin linear classifiers are estimated for different prediction problems. These classifiers operate in a common input space but are coupled as they recover an unknown shared representation. A maximum entropy discrimination (MED) framework is used to derive the multitask algorithm which involves only convex optimization problems that are straightforward to implement. Three multitask scenarios are described. The first multitask method produces multiple support vector machines that learn a shared sparse feature selection over the input space. The second multitask method produces multiple support vector machines that learn a shared conic kernel combination. The third multitask method produces a pooled classifier as well as adaptively specialized individual classifiers. Furthermore, extensions to regression, graphical model structure estimation and other sparse methods are discussed. The maximum entropy optimization problems are implemented via a sequential quadratic programming method which leverages recent progress in fast SVM solvers. Fast monotonic convergence bounds are provided by bounding the MED sparsifying cost function with a quadratic function and ensuring only a constant factor runtime increase above standard independent SVM solvers. Results are shown on multitask data sets and favor multitask learning over single-task or tabula rasa methods."
            ],
            "keywords": [
                "meta-learning",
                "support vector machines",
                "feature selection",
                "kernel selection",
                "maximum entropy",
                "large margin",
                "Bayesian methods",
                "variational bounds",
                "classification",
                "regression",
                "Lasso",
                "graphical model structure estimation",
                "quadratic programming",
                "convex programming"
            ],
            "author": [
                "Tony Jebara"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/jebara11a/jebara11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Using Conceptors to Manage Neural Long-Term Memories for Temporal Patterns",
            "abstract": [
                "Biological brains can learn, recognize, organize, and regenerate large repertoires of temporal patterns. Here I propose a mechanism of neurodynamical pattern learning and representation, called conceptors, which offers an integrated account of a number of such phenomena and functionalities. It becomes possible to store a large number of temporal patterns in a single recurrent neural network. In the recall process, stored patterns can be morphed and \"focussed\". Parametric families of patterns can be learnt from a very small number of examples. Stored temporal patterns can be content-addressed in ways that are analog to recalling static patterns in Hopfield networks."
            ],
            "keywords": [
                "Recurrent neural network",
                "temporal pattern learning",
                "neural long-term memory",
                "neural dynamics"
            ],
            "author": [
                "Herbert Jaeger"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-449/15-449.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Loss Minimization and Parameter Estimation with Heavy Tails",
            "abstract": [
                "This work studies applications and generalizations of a simple estimation technique that provides exponential concentration under heavy-tailed distributions, assuming only bounded low-order moments. We show that the technique can be used for approximate minimization of smooth and strongly convex losses, and specifically for least squares linear regression. For instance, our d-dimensional estimator requires justÕ(d log(1/δ)) random samples to obtain a constant factor approximation to the optimal least squares loss with probability 1 − δ, without requiring the covariates or noise to be bounded or subgaussian. We provide further applications to sparse linear regression and low-rank covariance matrix estimation with similar allowances on the noise and covariate distributions. The core technique is a generalization of the median-of-means estimator to arbitrary metric spaces."
            ],
            "keywords": [
                "Heavy-tailed distributions",
                "unbounded losses",
                "linear regression",
                "least squares"
            ],
            "author": [
                "Daniel Hsu",
                "Sivan Sabato"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-273/14-273.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Pyramid Match Kernel: Efficient Learning with Sets of Features",
            "abstract": [
                "In numerous domains it is useful to represent a single example by the set of the local features or parts that comprise it. However, this representation poses a challenge to many conventional machine learning techniques, since sets may vary in cardinality and elements lack a meaningful ordering. Kernel methods can learn complex functions, but a kernel over unordered set inputs must somehow solve for correspondences-generally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function called the pyramid match that measures partial match similarity in time linear in the number of features. The pyramid match maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in order to find implicit correspondences based on the finest resolution histogram cell where a matched pair first appears. We show the pyramid match yields a Mercer kernel, and we prove bounds on its error relative to the optimal partial matching cost. We demonstrate our algorithm on both classification and regression tasks, including object recognition, 3-D human pose inference, and time of publication estimation for documents, and we show that the proposed method is accurate and significantly more efficient than current approaches."
            ],
            "keywords": [
                "kernel",
                "sets of features",
                "histogram intersection",
                "multi-resolution histogram pyramid",
                "approximate matching",
                "object recognition"
            ],
            "author": [
                "Kristen Grauman",
                "Trevor Darrell"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/grauman07a/grauman07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convex vs Non-Convex Estimators for Regression and Sparse Estimation: the Mean Squared Error Properties of ARD and GLasso",
            "abstract": [
                "We study a simple linear regression problem for grouped variables; we are interested in methods which jointly perform estimation and variable selection, that is, that automatically set to zero groups of variables in the regression vector. The Group Lasso (GLasso), a well known approach used to tackle this problem which is also a special case of Multiple Kernel Learning (MKL), boils down to solving convex optimization problems. On the other hand, a Bayesian approach commonly known as Sparse Bayesian Learning (SBL), a version of which is the well known Automatic Relevance Determination (ARD), lead to non-convex problems. In this paper we discuss the relation between ARD (and a penalized version which we call PARD) and Glasso, and study their asymptotic properties in terms of the Mean Squared Error in estimating the unknown parameter. The theoretical arguments developed here are independent of the correctness of the prior models and clarify the advantages of PARD over GLasso."
            ],
            "keywords": [
                "Lasso",
                "Group Lasso",
                "Multiple Kernel Learning",
                "Bayesian regularization",
                "marginal likelihood"
            ],
            "author": [
                "Aleksandr Aravkin",
                "James V Burke",
                "Alessandro Chiuso",
                "Gianluigi Pillonetto"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/aravkin14a/aravkin14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Controlling the False Discovery Rate of the Association/Causality Structure Learned with the PC Algorithm",
            "abstract": [
                "In real world applications, graphical statistical models are not only a tool for operations such as classification or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level, and a heuristic modification of the method is able to control the FDR more accurately around the user-specified level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models."
            ],
            "keywords": [
                "Bayesian networks",
                "false discovery rate",
                "PC algorithm",
                "directed acyclic graph",
                "skeleton"
            ],
            "author": [
                "Junning Li",
                "Z Jane Wang",
                "Paolo Frasconi",
                "Kristian Kersting",
                "Hannu Toivonen",
                "Koji Tsuda"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/li09a/li09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spurious Valleys in One-hidden-layer Neural Network Optimization Landscapes",
            "abstract": [
                "Neural networks provide a rich class of high-dimensional, non-convex optimization problems. Despite their non-convexity, gradient-descent methods often successfully optimize these models. This has motivated a recent spur in research attempting to characterize properties of their loss surface that may explain such success. In this paper, we address this phenomenon by studying a key topological property of the loss: the presence or absence of spurious valleys, defined as connected components of sub-level sets that do not include a global minimum. Focusing on a class of one-hidden-layer neural networks defined by smooth (but generally non-linear) activation functions, we identify a notion of intrinsic dimension and show that it provides necessary and sufficient conditions for the absence of spurious valleys. More concretely, finite intrinsic dimension guarantees that for sufficiently overparametrised models no spurious valleys exist, independently of the data distribution. Conversely, infinite intrinsic dimension implies that spurious valleys do exist for certain data distributions, independently of model overparametrisation. Besides these positive and negative results, we show that, although spurious valleys may exist in general, they are confined to low risk levels and avoided with high probability on overparametrised models."
            ],
            "keywords": [],
            "author": [
                "Luca Venturi",
                "Joan Bruna"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-674/18-674.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Constructive Approach to L 0 Penalized Regression",
            "abstract": [
                "We propose a constructive approach to estimating sparse, high-dimensional linear regression models. The approach is a computational algorithm motivated from the KKT conditions for the 0-penalized least squares solutions. It generates a sequence of solutions iteratively, based on support detection using primal and dual information and root finding. We refer to the algorithm as SDAR for brevity. Under a sparse Riesz condition on the design matrix and certain other conditions, we show that with high probability, the estimation error of the solution sequence decays exponentially to the minimax error bound in O(log(R √ J)) iterations, where J is the number of important predictors and R is the relative magnitude of the nonzero target coefficients; and under a mutual coherence condition and certain other conditions, the ∞ estimation error decays to the optimal error bound in O(log(R)) iterations. Moreover the SDAR solution recovers the oracle least squares estimator within a finite number of iterations with high probability if the sparsity level is known. Computational complexity analysis shows that the cost of SDAR is O(np) per iteration. We also consider an adaptive version of SDAR for use in practical applications where the true sparsity level is unknown. Simulation studies demonstrate that SDAR outperforms Lasso, MCP and two greedy methods in accuracy and efficiency."
            ],
            "keywords": [
                "Geometrical convergence",
                "KKT conditions",
                "nonasymptotic error bounds",
                "oracle property",
                "root finding",
                "support detection"
            ],
            "author": [
                "Jian Huang",
                "Yuling Jiao",
                "Yanyan Liu",
                "Xiliang Lu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-194/17-194.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Solving the OSCAR and SLOPE Models Using a Semismooth Newton-Based Augmented Lagrangian Method",
            "abstract": [
                "The octagonal shrinkage and clustering algorithm for regression (OSCAR), equipped with the 1-norm and a pair-wise ∞-norm regularizer, is a useful tool for feature selection and grouping in high-dimensional data analysis. The computational challenge posed by OSCAR, for high dimensional and/or large sample size data, has not yet been well resolved due to the non-smoothness and non-separability of the regularizer involved. In this paper, we successfully resolve this numerical challenge by proposing a sparse semismooth Newtonbased augmented Lagrangian method to solve the more general SLOPE (the sorted Lone penalized estimation) model. By appropriately exploiting the inherent sparse and low-rank property of the generalized Jacobian of the semismooth Newton system in the augmented Lagrangian subproblem, we show how the computational complexity can be substantially reduced. Our algorithm offers a notable computational advantage in the high-dimensional statistical regression settings. Numerical experiments are conducted on real data sets, and the results demonstrate that our algorithm is far superior, in both speed and robustness, to the existing state-of-the-art algorithms based on first-order iterative schemes, including the widely used accelerated proximal gradient (APG) method and the alternating direction method of multipliers (ADMM)."
            ],
            "keywords": [
                "Linear Regression",
                "OSCAR",
                "Sparsity",
                "Augmented Lagrangian Method",
                "Semismooth Newton method"
            ],
            "author": [
                "Ziyan Luo",
                "Hong Kong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-172/18-172.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Weak Convergence Properties of Constrained Emphatic Temporal-difference Learning with Constant and Slowly Diminishing Stepsize",
            "abstract": [
                "We consider the emphatic temporal-difference (TD) algorithm, ETD(λ), for learning the value functions of stationary policies in a discounted, finite state and action Markov decision process. The ETD(λ) algorithm was recently proposed by Sutton, Mahmood, and White (2016) to solve a long-standing divergence problem of the standard TD algorithm when it is applied to off-policy training, where data from an exploratory policy are used to evaluate other policies of interest. The almost sure convergence of ETD(λ) has been proved in our recent work under general off-policy training conditions, but for a narrow range of diminishing stepsize. In this paper we present convergence results for constrained versions of ETD(λ) with constant stepsize and with diminishing stepsize from a broad range. Our results characterize the asymptotic behavior of the trajectory of iterates produced by those algorithms, and are derived by combining key properties of ETD(λ) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory. For the case of constant stepsize, in addition to analyzing the behavior of the algorithms in the limit as the stepsize parameter approaches zero, we also analyze their behavior for a fixed stepsize and bound the deviations of their averaged iterates from the desired solution. These results are obtained by exploiting the weak Feller property of the Markov chains associated with the algorithms, and by using ergodic theorems for weak Feller Markov chains, in conjunction with the convergence results we get from the weak convergence methods. Besides ETD(λ), our analysis also applies to the off-policy TD(λ) algorithm, when the divergence issue is avoided by setting λ sufficiently large. It yields, for that case, new results on the asymptotic convergence properties of constrained off-policy TD(λ) with constant or slowly diminishing stepsize."
            ],
            "keywords": [
                "Markov decision processes",
                "approximate policy evaluation",
                "reinforcement learning",
                "temporal-difference methods",
                "importance sampling",
                "stochastic approximation",
                "convergence"
            ],
            "author": [
                "Huizhen Yu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-242/16-242.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ℓ p -Norm Multiple Kernel Learning",
            "abstract": [
                "Learning linear combinations of multiple kernels is an appealing strategy when the right choice of features is unknown. Previous approaches to multiple kernel learning (MKL) promote sparse kernel combinations to support interpretability and scalability. Unfortunately, this ℓ 1-norm MKL is rarely observed to outperform trivial baselines in practical applications. To allow for robust kernel mixtures that generalize well, we extend MKL to arbitrary norms. We devise new insights on the connection between several existing MKL formulations and develop two efficient interleaved optimization strategies for arbitrary norms, that is ℓ p-norms with p ≥ 1. This interleaved optimization is much faster than the commonly used wrapper approaches, as demonstrated on several data sets. A theoretical analysis and an experiment on controlled artificial data shed light on the appropriateness of sparse, non-sparse and ℓ ∞-norm MKL in various scenarios. Importantly, empirical applications of ℓ p-norm MKL to three real-world problems from computational biology show that non-sparse MKL achieves accuracies that surpass the state-of-the-art."
            ],
            "keywords": [
                "multiple kernel learning",
                "learning kernels",
                "non-sparse",
                "support vector machine",
                "convex conjugate",
                "block coordinate descent",
                "large scale optimization",
                "bioinformatics",
                "generalization bounds",
                "Rademacher complexity"
            ],
            "author": [
                "Marius Kloft",
                "Ulf Brefeld",
                "Alexander Zien"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/kloft11a/kloft11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Algebraic Geometric Comparison of Probability Distributions",
            "abstract": [
                "We propose a novel algebraic algorithmic framework for dealing with probability distributions represented by their cumulants such as the mean and covariance matrix. As an example, we consider the unsupervised learning problem of finding the subspace on which several probability distributions agree. Instead of minimizing an objective function involving the estimated cumulants, we show that by treating the cumulants as elements of the polynomial ring we can directly solve the problem, at a lower computational cost and with higher accuracy. Moreover, the algebraic viewpoint on probability distributions allows us to invoke the theory of algebraic geometry, which we demonstrate in a compact proof for an identifiability criterion."
            ],
            "keywords": [
                "computational algebraic geometry",
                "approximate algebra",
                "unsupervised Learning"
            ],
            "author": [
                "Franz J Király",
                "Frank C Meinecke",
                "Duncan A J Blythe",
                "Klaus-Robert Müller",
                "Klaus-Robert Mueller@tu-Berlin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/kiraly12a/kiraly12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A determinantal point process for column subset selection",
            "abstract": [
                "Two popular approaches to dimensionality reduction are principal component analysis, which projects onto a small number of well-chosen but non-interpretable directions, and feature selection, which selects a small number of the original features. Feature selection can be abstracted as selecting the subset of columns of a matrix X ∈ R N ×d which minimize the approximation error, i.e., the norm of the residual after projecting X onto the space spanned by the selected columns. Such a combinatorial optimization is usually impractical, and there has been interest in polynomial-cost, random subset selection algorithms that favour small values of this approximation error. We propose sampling from a projection determinantal point process, a repulsive distribution over column indices that favours diversity among the selected columns. We bound the ratio of the expected approximation error over the optimal error of PCA. These bounds improve over the state-of-the-art bounds of volume sampling when some realistic structural assumptions are satisfied for X. Numerical experiments suggest that our bounds are tight, and that our algorithms have comparable performance with the double phase algorithm, often considered the practical state-of-the-art."
            ],
            "keywords": [
                "column subset selection",
                "determinantal point process",
                "volume sampling",
                "leverage score sampling",
                "low-rank approximation"
            ],
            "author": [
                "Ayoub Belhadji"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-080/19-080.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Transformations for Clustering and Classification",
            "abstract": [
                "A low-rank transformation learning framework for subspace clustering and classification is proposed here. Many high-dimensional data, such as face images and motion sequences, approximately lie in a union of low-dimensional subspaces. The corresponding subspace clustering problem has been extensively studied in the literature to partition such highdimensional data into clusters corresponding to their underlying low-dimensional subspaces. Low-dimensional intrinsic structures are often violated for real-world observations, as they can be corrupted by errors or deviate from ideal models. We propose to address this by learning a linear transformation on subspaces using nuclear norm as the modeling and optimization criteria. The learned linear transformation restores a low-rank structure for data from the same subspace, and, at the same time, forces a maximally separated structure for data from different subspaces. In this way, we reduce variations within the subspaces, and increase separation between the subspaces for a more robust subspace clustering. This proposed learned robust subspace clustering framework significantly enhances the performance of existing subspace clustering methods. Basic theoretical results presented here help to further support the underlying framework. To exploit the low-rank structures of the transformed subspaces, we further introduce a fast subspace clustering technique, which efficiently combines robust PCA with sparse modeling. When class labels are present at the training stage, we show this low-rank transformation framework also significantly enhances classification performance. Extensive experiments using public data sets are presented, showing that the proposed approach significantly outperforms state-of-the-art methods for subspace clustering and classification. The learned low cost transform is also applicable to other classification frameworks."
            ],
            "keywords": [
                "subspace clustering",
                "classification",
                "low-rank transformation",
                "nuclear norm",
                "feature learning"
            ],
            "author": [
                "Qiang Qiu",
                "Guillermo Sapiro"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/qiu15a/qiu15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Policy Evaluation with Temporal Differences: A Survey and Comparison",
            "abstract": [
                "Policy evaluation is an essential step in most reinforcement learning approaches. It yields a value function, the quality assessment of states for a given policy, which can be used in a policy improvement step. Since the late 1980s, this research area has been dominated by temporal-difference (TD) methods due to their data-efficiency. However, core issues such as stability guarantees in the off-policy scenario, improved sample efficiency and probabilistic treatment of the uncertainty in the estimates have only been tackled recently, which has led to a large number of new approaches. This paper aims at making these new developments accessible in a concise overview, with foci on underlying cost functions, the off-policy scenario as well as on regularization in high dimensional feature spaces. By presenting the first extensive, systematic comparative evaluations comparing TD, LSTD, LSPE, FPKF, the residual-gradient algorithm, Bellman residual minimization, GTD, GTD2 and TDC, we shed light on the strengths and weaknesses of the methods. Moreover, we present alternative versions of LSTD and LSPE with drastically improved off-policy performance."
            ],
            "keywords": [
                "temporal differences",
                "policy evaluation",
                "value function estimation",
                "reinforcement learning"
            ],
            "author": [
                "Christoph Dann",
                "Gerhard Neumann",
                "Jan Peters"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/dann14a/dann14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features",
            "abstract": [
                "This paper proposes several novel methods, based on machine learning, to detect malware in executable files without any need for preprocessing, such as unpacking or disassembling. The basic method (Mal-ID) is a new static (form-based) analysis methodology that uses common segment analysis in order to detect malware files. By using common segment analysis, Mal-ID is able to discard malware parts that originate from benign code. In addition, Mal-ID uses a new kind of feature, termed meta-feature, to better capture the properties of the analyzed segments. Rather than using the entire file, as is usually the case with machine learning based techniques, the new approach detects malware on the segment level. This study also introduces two Mal-ID extensions that improve the Mal-ID basic method in various aspects. We rigorously evaluated Mal-ID and its two extensions with more than ten performance measures, and compared them to the highly rated boosted decision tree method under identical settings. The evaluation demonstrated that Mal-ID and the two Mal-ID extensions outperformed the boosted decision tree method in almost all respects. In addition, the results indicated that by extracting meaningful features, it is sufficient to employ one simple detection rule for classifying executable files."
            ],
            "keywords": [
                "computer security",
                "malware detection",
                "common segment analysis",
                "supervised learning"
            ],
            "author": [
                "Gil Tahan",
                "Lior Rokach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/tahan12a/tahan12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Kernel Regression with Coefficient-based q −regularization",
            "abstract": [
                "In this paper, we consider the q −regularized kernel regression with 0 < q ≤ 1. In form, the algorithm minimizes a least-square loss functional adding a coefficient-based q −penalty term over a linear span of features generated by a kernel function. We study the asymptotic behavior of the algorithm under the framework of learning theory. The contribution of this paper is twofold. First, we derive a tight bound on the 2 −empirical covering numbers of the related function space involved in the error analysis. Based on this result, we obtain the convergence rates for the 1 −regularized kernel regression which is the best so far. Second, for the case 0 < q < 1, we show that the regularization parameter plays a role as a trade-off between sparsity and convergence rates. Under some mild conditions, the fraction of non-zero coefficients in a local minimizer of the algorithm will tend to 0 at a polynomial decay rate when the sample size m becomes large. As the concerned algorithm is non-convex, we also discuss how to generate a minimizing sequence iteratively, which can help us to search a local minimizer around any initial point."
            ],
            "keywords": [
                "Learning Theory",
                "Kernel Regression",
                "Coefficient-based q -regularization (0 < q ≤ 1)",
                "Sparsity",
                "2 -empirical Covering Number"
            ],
            "author": [
                "Lei Shi",
                "Xiaolin Huang",
                "Johan A K Suykens"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/13-124/13-124.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Activized Learning: Transforming Passive to Active with Improved Label Complexity *",
            "abstract": [
                "We study the theoretical advantages of active learning over passive learning. Specifically, we prove that, in noise-free classifier learning for VC classes, any passive learning algorithm can be transformed into an active learning algorithm with asymptotically strictly superior label complexity for all nontrivial target functions and distributions. We further provide a general characterization of the magnitudes of these improvements in terms of a novel generalization of the disagreement coefficient. We also extend these results to active learning in the presence of label noise, and find that even under broad classes of noise distributions, we can typically guarantee strict improvements over the known results for passive learning."
            ],
            "keywords": [
                "active learning",
                "selective sampling",
                "sequential design",
                "statistical learning theory",
                "PAC learning",
                "sample complexity"
            ],
            "author": [
                "Steve Hanneke"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/hanneke12a/hanneke12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimal Learning Machine: Theoretical Results and Clustering-Based Reference Point Selection",
            "abstract": [
                "The Minimal Learning Machine (MLM) is a nonlinear, supervised approach based on learning linear mapping between distance matrices computed in input and output data spaces, where distances are calculated using a subset of points called reference points. Its simple formulation has attracted several recent works on extensions and applications. In this paper, we aim to address some open questions related to the MLM. First, we detail the theoretical aspects that assure the MLM's interpolation and universal approximation capabilities, which had previously only been empirically verified. Second, we identify the major importance of the task of selecting reference points for the MLM's generalization capability. Several clustering-based methods for reference point selection in regression scenarios are then proposed and analyzed. Based on an extensive empirical evaluation, we conclude that the evaluated methods are both scalable and useful. Specifically, for a small number of reference points, the clustering-based methods outperform the standard random selection of the original MLM formulation."
            ],
            "keywords": [
                "Minimal Learning Machine",
                "Interpolation",
                "Universal Approximation",
                "Clustering",
                "Reference Point Selection"
            ],
            "author": [
                "Joonas Hämäläinen",
                "Alisson S C Alencar",
                "Tommi Kärkkäinen",
                "César L C Mattos",
                "Amauri H Souza Júnior",
                "João P P Gomes",
                "C Alencar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-786/19-786.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Segmental Hidden Markov Models with Random Effects for Waveform Modeling",
            "abstract": [
                "This paper proposes a general probabilistic framework for shape-based modeling and classification of waveform data. A segmental hidden Markov model (HMM) is used to characterize waveform shape and shape variation is captured by adding random effects to the segmental model. The resulting probabilistic framework provides a basis for learning of waveform models from data as well as parsing and recognition of new waveforms. Expectation-maximization (EM) algorithms are derived and investigated for fitting such models to data. In particular, the \"expectation conditional maximization either\" (ECME) algorithm is shown to provide significantly faster convergence than a standard EM procedure. Experimental results on two real-world data sets demonstrate that the proposed approach leads to improved accuracy in classification and segmentation when compared to alternatives such as Euclidean distance matching, dynamic time warping, and segmental HMMs without random effects."
            ],
            "keywords": [
                "waveform recognition",
                "random effects",
                "segmental hidden Markov models",
                "EM algorithm",
                "ECME"
            ],
            "author": [
                "Seyoung Kim",
                "Padhraic Smyth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/kim06a/kim06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GenSVM: A Generalized Multiclass Support Vector Machine",
            "abstract": [
                "Traditional extensions of the binary support vector machine (SVM) to multiclass problems are either heuristics or require solving a large dual optimization problem. Here, a generalized multiclass SVM is proposed called GenSVM. In this method classification boundaries for a K-class problem are constructed in a (K − 1)-dimensional space using a simplex encoding. Additionally, several different weightings of the misclassification errors are incorporated in the loss function, such that it generalizes three existing multiclass SVMs through a single optimization problem. An iterative majorization algorithm is derived that solves the optimization problem without the need of a dual formulation. This algorithm has the advantage that it can use warm starts during cross validation and during a grid search, which significantly speeds up the training phase. Rigorous numerical experiments compare linear GenSVM with seven existing multiclass SVMs on both small and large data sets. These comparisons show that the proposed method is competitive with existing methods in both predictive accuracy and training time, and that it significantly outperforms several existing methods on these criteria."
            ],
            "keywords": [
                "support vector machines",
                "SVM",
                "multiclass classification",
                "iterative majorization",
                "MM algorithm",
                "classifier comparison"
            ],
            "author": [
                "Gerrit J J Van Den Burg",
                "Patrick J F Groenen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-526/14-526.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Canonical Correlation Analysis",
            "abstract": [
                "We study the sample complexity of canonical correlation analysis (CCA), i.e., the number of samples needed to estimate the population canonical correlation and directions up to arbitrarily small error. With mild assumptions on the data distribution, we show that in order to achieve-suboptimality in a properly defined measure of alignment between the estimated canonical directions and the population solution, we can solve the empirical objective exactly with N (, ∆, γ) samples, where ∆ is the singular value gap of the whitened cross-covariance matrix and 1/γ is an upper bound of the condition number of auto-covariance matrices. Moreover, we can achieve the same learning accuracy by drawing the same level of samples and solving the empirical objective approximately with a stochastic optimization algorithm; this algorithm is based on the shift-and-invert power iterations and only needs to process the dataset for O log 1 passes. Finally, we show that, given an estimate of the canonical correlation, the streaming version of the shift-and-invert power iterations achieves the same learning accuracy with the same level of sample complexity, by processing the data only once."
            ],
            "keywords": [
                "Canonical correlation analysis",
                "sample complexity",
                "shift-and-invert preconditioning",
                "streaming CCA"
            ],
            "author": [
                "Chao Gao",
                "Dan Garber",
                "Nathan Srebro",
                "Jialei Wang",
                "Weiran Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-095/18-095.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Automated Scalable Bayesian Inference via Hilbert Coresets",
            "abstract": [
                "The automation of posterior inference in Bayesian data analysis has enabled experts and nonexperts alike to use more sophisticated models, engage in faster exploratory modeling and analysis, and ensure experimental reproducibility. However, standard automated posterior inference algorithms are not tractable at the scale of massive modern data sets, and modifications to make them so are typically model-specific, require expert tuning, and can break theoretical guarantees on inferential quality. Building on the Bayesian coresets framework, this work instead takes advantage of data redundancy to shrink the data set itself as a preprocessing step, providing fully-automated, scalable Bayesian inference with theoretical guarantees. We begin with an intuitive reformulation of Bayesian coreset construction as sparse vector sum approximation, and demonstrate that its automation and performance-based shortcomings arise from the use of the supremum norm. To address these shortcomings we develop Hilbert coresets, i.e., Bayesian coresets constructed under a norm induced by an inner-product on the log-likelihood function space. We propose two Hilbert coreset construction algorithms-one based on importance sampling, and one based on the Frank-Wolfe algorithm-along with theoretical guarantees on approximation quality as a function of coreset size. Since the exact computation of the proposed inner-products is model-specific, we automate the construction with a random finite-dimensional projection of the log-likelihood functions. The resulting automated coreset construction algorithm is simple to implement, and experiments on a variety of models with real and synthetic data sets show that it provides high-quality posterior approximations and a significant reduction in the computational cost of inference."
            ],
            "keywords": [
                "Bayesian inference",
                "scalable",
                "automated",
                "coreset",
                "Hilbert",
                "Frank-Wolfe"
            ],
            "author": [
                "Trevor Campbell",
                "Tamara Broderick"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-613/17-613.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model-Free Trajectory-based Policy Optimization with Monotonic Improvement",
            "abstract": [
                "Many of the recent trajectory optimization algorithms alternate between linear approximation of the system dynamics around the mean trajectory and conservative policy update. One way of constraining the policy change is by bounding the Kullback-Leibler (KL) divergence between successive policies. These approaches already demonstrated great experimental success in challenging problems such as end-to-end control of physical systems. However, the linear approximation of the system dynamics can introduce a bias in the policy update and prevent convergence to the optimal policy. In this article, we propose a new model-free trajectory-based policy optimization algorithm with guaranteed monotonic improvement. The algorithm backpropagates a local, quadratic and time-dependent Q-Function learned from trajectory data instead of a model of the system dynamics. Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics. We experimentally demonstrate on highly non-linear control tasks the improvement in performance of our algorithm in comparison to approaches linearizing the system dynamics. In order to show the monotonic improvement of our algorithm, we additionally conduct a theoretical analysis of our policy update scheme to derive a lower bound of the change in policy return between successive iterations."
            ],
            "keywords": [
                "Reinforcement Learning",
                "Policy Optimization",
                "Trajectory Optimization",
                "Robotics"
            ],
            "author": [
                "Riad Akrour",
                "Jan Peters",
                "Gerhard Neumann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-329/17-329.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Differentiable Game Mechanics",
            "abstract": [
                "Deep learning is built on the foundational guarantee that gradient descent on an objective function converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, that exhibit multiple interacting losses. The behavior of gradient-based methods in games is not well understood-and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new tools to understand and control the dynamics in n-player differentiable games. The key result is to decompose the game Jacobian into two components. The first, symmetric component, is related to potential games, which reduce to gradient descent on an implicit function. The second, antisymmetric component, relates to Hamiltonian games, a new class of games that obey a conservation law akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in differentiable games. Basic experiments show SGA is competitive with recently proposed algorithms for finding stable fixed points in GANs-while at the same time being applicable to, and having guarantees in, much more general cases."
            ],
            "keywords": [
                "game theory",
                "generative adversarial networks",
                "deep learning",
                "classical mechanics",
                "hamiltonian mechanics",
                "gradient descent",
                "dynamical systems"
            ],
            "author": [
                "Alistair Letcher",
                "David Balduzzi",
                "Sébastien Racanière",
                "James Martens",
                "Jakob Foerster",
                "Karl Tuyls"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-008/19-008.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unified Continuous Optimization Framework for Center-Based Clustering Methods",
            "abstract": [
                "Center-based partitioning clustering algorithms rely on minimizing an appropriately formulated objective function, and different formulations suggest different possible algorithms. In this paper, we start with the standard nonconvex and nonsmooth formulation of the partitioning clustering problem. We demonstrate that within this elementary formulation, convex analysis tools and optimization theory provide a unifying language and framework to design, analyze and extend hard and soft center-based clustering algorithms, through a generic algorithm which retains the computational simplicity of the popular k-means scheme. We show that several well known and more recent center-based clustering algorithms, which have been derived either heuristically, or/and have emerged from intuitive analogies in physics, statistical techniques and information theoretic perspectives can be recovered as special cases of the proposed analysis and we streamline their relationships."
            ],
            "keywords": [
                "clustering",
                "k-means algorithm",
                "convex analysis",
                "support and asymptotic functions",
                "distance-like functions",
                "Bregman and Csiszar divergences",
                "nonlinear means",
                "nonsmooth optimization",
                "smoothing algorithms",
                "fixed point methods",
                "deterministic annealing",
                "expectation maximization",
                "information theory and entropy methods"
            ],
            "author": [
                "Marc Teboulle"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/teboulle07a/teboulle07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The flare Package for High Dimensional Linear Regression and Precision Matrix Estimation in R *",
            "abstract": [
                "This paper describes an R package named flare, which implements a family of new high dimensional regression methods (LAD Lasso, SQRT Lasso, q Lasso, and Dantzig selector) and their extensions to sparse precision matrix estimation (TIGER and CLIME). These methods exploit different nonsmooth loss functions to gain modeling flexibility, estimation robustness, and tuning insensitiveness. The developed solver is based on the alternating direction method of multipliers (ADMM). The package flare is coded in double precision C, and called from R by a user-friendly interface. The memory usage is optimized by using the sparse matrix output. The experiments show that flare is efficient and can scale up to large problems."
            ],
            "keywords": [
                "sparse linear regression",
                "sparse precision matrix estimation",
                "alternating direction method of multipliers",
                "robustness",
                "tuning insensitiveness"
            ],
            "author": [
                "Xingguo Li",
                "Tuo Zhao",
                "Xiaoming Yuan",
                "Han Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/li15a/li15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Well-Tempered Landscape for Non-convex Robust Subspace Recovery",
            "abstract": [
                "We present a mathematical analysis of a non-convex energy landscape for robust subspace recovery. We prove that an underlying subspace is the only stationary point and local minimizer in a specified neighborhood under a deterministic condition on a dataset. If the deterministic condition is satisfied, we further show that a geodesic gradient descent method over the Grassmannian manifold can exactly recover the underlying subspace when the method is properly initialized. Proper initialization by principal component analysis is guaranteed with a simple deterministic condition. Under slightly stronger assumptions, the gradient descent method with a piecewise constant step-size scheme achieves linear convergence. The practicality of the deterministic condition is demonstrated on some statistical models of data, and the method achieves almost state-of-the-art recovery guarantees on the Haystack Model for different regimes of sample size and ambient dimension. In particular, when the ambient dimension is fixed and the sample size is large enough, we show that our gradient method can exactly recover the underlying subspace for any fixed fraction of outliers (less than 1)."
            ],
            "keywords": [
                "robust subspace recovery",
                "non-convex optimization",
                "dimension reduction",
                "optimization on the Grassmannian"
            ],
            "author": [
                "Tyler Maunu",
                "Teng Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-324/17-324.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Geometry and Expressive Power of Conditional Restricted Boltzmann Machines",
            "abstract": [
                "Conditional restricted Boltzmann machines are undirected stochastic neural networks with a layer of input and output units connected bipartitely to a layer of hidden units. These networks define models of conditional probability distributions on the states of the output units given the states of the input units, parameterized by interaction weights and biases. We address the representational power of these models, proving results on their ability to represent conditional Markov random fields and conditional distributions with restricted supports, the minimal size of universal approximators, the maximal model approximation errors, and on the dimension of the set of representable conditional distributions. We contribute new tools for investigating conditional probability models, which allow us to improve the results that can be derived from existing work on restricted Boltzmann machine probability models."
            ],
            "keywords": [
                "conditional restricted Boltzmann machine",
                "universal approximation",
                "Kullback-Leibler approximation error",
                "expected dimension"
            ],
            "author": [
                "Guido Montúfar",
                "Nihat Ay"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/montufar15b/montufar15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SVDFeature: A Toolkit for Feature-based Collaborative Filtering",
            "abstract": [
                "In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative filtering. SVDFeature is designed to efficiently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efficient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years."
            ],
            "keywords": [
                "large-scale collaborative filtering",
                "context-aware recommendation",
                "ranking"
            ],
            "author": [
                "Tianqi Chen",
                "Weinan Zhang",
                "Qiuxia Lu",
                "Kailong Chen",
                "Zhao Zheng",
                "Yong Yu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/chen12a/chen12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization *",
            "abstract": [
                "We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate (SPDC) method, which alternates between maximizing over a randomly chosen dual variable and minimizing over the primal variables. An extrapolation step on the primal variables is performed to obtain accelerated convergence rate. We also develop a minibatch version of the SPDC method which facilitates parallel computing, and an extension with weighted sampling probabilities on the dual variables, which has a better complexity than uniform sampling on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods."
            ],
            "keywords": [
                "empirical risk minimization",
                "randomized algorithms",
                "convex-concave saddle point problems",
                "primal-dual algorithms",
                "computational complexity"
            ],
            "author": [
                "Yuchen Zhang",
                "Lin Xiao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-568/16-568.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Neyman-Pearson Classification, Convexity and Stochastic Constraints",
            "abstract": [
                "Motivated by problems of anomaly detection, this paper implements the Neyman-Pearson paradigm to deal with asymmetric errors in binary classification with a convex loss ϕ. Given a finite collection of classifiers, we combine them and obtain a new classifier that satisfies simultaneously the two following properties with high probability: (i) its ϕ-type I error is below a pre-specified level and (ii), it has ϕ-type II error close to the minimum possible. The proposed classifier is obtained by minimizing an empirical convex objective with an empirical convex constraint. The novelty of the method is that the classifier output by this computationally feasible program is shown to satisfy the original constraint on type I error. New techniques to handle such problems are developed and they have consequences on chance constrained programming. We also evaluate the price to pay in terms of type II error for being conservative on type I error."
            ],
            "keywords": [
                "binary classification",
                "Neyman-Pearson paradigm",
                "anomaly detection",
                "empirical constraint",
                "empirical risk minimization",
                "chance constrained optimization"
            ],
            "author": [
                "Philippe Rigollet",
                "Xin Tong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/rigollet11a/rigollet11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning a Hidden Hypergraph",
            "abstract": [
                "We consider the problem of learning a hypergraph using edge-detecting queries. In this model, the learner may query whether a set of vertices induces an edge of the hidden hypergraph or not. We show that an r-uniform hypergraph with m edges and n vertices is learnable with O(2 4r m • poly(r, log n)) queries with high probability. The queries can be made in O(min(2 r (log m + r) 2 , (log m + r) 3)) rounds. We also give an algorithm that learns an almost uniform hypergraph of dimension r using O(2 O((1+ ∆ 2)r) • m 1+ ∆ 2 • poly(log n)) queries with high probability, where ∆ is the difference between the maximum and the minimum edge sizes. This upper bound matches our lower bound of Ω((m 1+ ∆ 2) 1+ ∆ 2) for this class of hypergraphs in terms of dependence on m. The queries can also be made in O((1 + ∆) • min(2 r (log m + r) 2 , (log m + r) 3)) rounds."
            ],
            "keywords": [
                "query learning",
                "hypergraph",
                "multiple round algorithm",
                "sampling",
                "chemical reaction network"
            ],
            "author": [
                "Dana Angluin",
                "Jiang Chen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/angluin06a/angluin06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization",
            "abstract": [
                "A popular approach for solving stochastic optimization problems is the stochastic gradient descent (SGD) method. Although the SGD iteration is computationally cheap and its practical performance may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate choice of parameters. To avoid some of the drawbacks of SGD, stochastic proximal point (SPP) algorithms have been recently considered. We introduce a new variant of the SPP method for solving stochastic convex problems subject to (in)finite intersection of constraints satisfying a linear regularity condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex Lipschitz continuous objective functions, we prove nonasymptotic convergence rates in terms of the expected value function gap of order O 1 k 1/2 , where k is the iteration counter. We also derive better nonasymptotic convergence rates in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order O 1 k. Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real problems."
            ],
            "keywords": [
                "Stochastic convex optimization",
                "intersection of convex constraints",
                "stochastic proximal point",
                "nonasymptotic convergence analysis",
                "rates of convergence"
            ],
            "author": [
                "Andrei Patrascu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-347/17-347.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Logical Explanations for Deep Relational Machines Using Relevance Information",
            "abstract": [
                "Our interest in this paper is in the construction of symbolic explanations for predictions made by a deep neural network. We will focus attention on deep relational machines (DRMs: a term introduced in Lodhi (2013)). A DRM is a deep network in which the input layer consists of Boolean-valued functions (features) that are defined in terms of relations provided as domain, or background, knowledge. Our DRMs differ from those in Lodhi (2013), which uses an Inductive Logic Programming (ILP) engine to first select features (we use random selections from a space of features that satisfies some approximate constraints on logical relevance and non-redundancy). But why do the DRMs predict what they do? One way of answering this was provided in recent work Ribeiro et al. (2016), by constructing readable proxies for a black-box predictor. The proxies are intended only to model the predictions of the black-box in local regions of the instance-space. But readability alone may not be enough: to be understandable, the local models must use relevant concepts in an meaningful manner. We investigate the use of a Bayes-like approach to identify logical proxies for local predictions of a DRM. As a preliminary step, we show that DRM's with our randomised propositionalization method achieve predictive performance that is comparable to the best reports in the ILP literature. Our principal results on logical explanations show: (a) Models in first-order logic can approximate the DRM's prediction closely in a small local region; and (b) Expert-provided relevance information can play the role of a prior to distinguish between logical explanations that perform equivalently on prediction alone."
            ],
            "keywords": [
                "Explainable AI",
                "Inductive Logic Programming",
                "Deep Relational Machines"
            ],
            "author": [
                "Ashwin Srinivasan",
                "Michael Bain"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-517/18-517.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hierarchically Compositional Kernels for Scalable Nonparametric Learning",
            "abstract": [
                "We propose a novel class of kernels to alleviate the high computational cost of large-scale nonparametric learning with kernel methods. The proposed kernel is defined based on a hierarchical partitioning of the underlying data domain, where the Nyström method (a globally low-rank approximation) is married with a locally lossless approximation in a hierarchical fashion. The kernel maintains (strict) positive-definiteness. The corresponding kernel matrix admits a recursively off-diagonal low-rank structure, which allows for fast linear algebra computations. Suppressing the factor of data dimension, the memory and arithmetic complexities for training a regression or a classifier are reduced from O(n 2) and O(n 3) to O(nr) and O(nr 2), respectively, where n is the number of training examples and r is the rank on each level of the hierarchy. Although other randomized approximate kernels entail a similar complexity, empirical results show that the proposed kernel achieves a matching performance with a smaller r. We demonstrate comprehensive experiments to show the effective use of the proposed kernel on data sizes up to the order of millions."
            ],
            "keywords": [],
            "author": [
                "Jie Chen",
                "Ibm Thomas",
                "J Watson",
                "Research Center"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-376/15-376.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Denoising Source Separation",
            "abstract": [
                "A new algorithmic framework called denoising source separation (DSS) is introduced. The main benefit of this framework is that it allows for the easy development of new source separation algorithms which can be optimised for specific problems. In this framework, source separation algorithms are constructed around denoising procedures. The resulting algorithms can range from almost blind to highly specialised source separation algorithms. Both simple linear and more complex nonlinear or adaptive denoising schemes are considered. Some existing independent component analysis algorithms are reinterpreted within the DSS framework and new, robust blind source separation algorithms are suggested. The framework is derived as a one-unit equivalent to an EM algorithm for source separation. However, in the DSS framework it is easy to utilise various kinds of denoising procedures which need not be based on generative models. In the experimental section, various DSS schemes are applied extensively to artificial data, to real magnetoencephalograms and to simulated CDMA mobile network signals. Finally, various extensions to the proposed DSS algorithms are considered. These include nonlinear observation mappings, hierarchical models and over-complete, nonorthogonal feature spaces. With these extensions, DSS appears to have relevance to many existing models of neural information processing."
            ],
            "keywords": [
                "blind source separation",
                "BSS",
                "prior information",
                "denoising",
                "denoising source separation",
                "DSS",
                "independent component analysis",
                "ICA",
                "magnetoencephalograms",
                "MEG",
                "CDMA"
            ],
            "author": [
                "Jaakko Särelä",
                "Harri Valpola"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/sarela05a/sarela05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with Two-Point Feedback",
            "abstract": [
                "We consider the closely related problems of bandit convex optimization with two-point feedback, and zero-order stochastic convex optimization with two function evaluations per round. We provide a simple algorithm and analysis which is optimal for convex Lipschitz functions. This improves on Duchi et al. (2015), which only provides an optimal result for smooth functions; Moreover, the algorithm and analysis are simpler, and readily extend to non-Euclidean problems. The algorithm is based on a small but surprisingly powerful modification of the gradient estimator."
            ],
            "keywords": [
                "zero-order optimization",
                "bandit optimization",
                "stochastic optimization",
                "gradient estimator"
            ],
            "author": [
                "Ohad Shamir"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-632/16-632.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Nonparametric Comorbidity Analysis of Psychiatric Disorders",
            "abstract": [
                "The analysis of comorbidity is an open and complex research field in the branch of psychiatry, where clinical experience and several studies suggest that the relation among the psychiatric disorders may have etiological and treatment implications. In this paper, we are interested in applying latent feature modeling to find the latent structure behind the psychiatric disorders that can help to examine and explain the relationships among them. To this end, we use the large amount of information collected in the National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database and propose to model these data using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the discrete nature of the data, we first need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. We also provide a variational inference algorithm for this model, which provides a complementary (and less expensive in terms of computational complexity) alternative to the Gibbs sampler allowing us to deal with a larger number of data. Finally, we use the model to analyze comorbidity among the psychiatric disorders diagnosed by experts from the NESARC database."
            ],
            "keywords": [
                "Bayesian nonparametrics",
                "Indian buffet process",
                "categorical observations",
                "multinomial-logit function",
                "Laplace approximation",
                "variational inference"
            ],
            "author": [
                "Francisco J R Ruiz",
                "Isabel Valera",
                "Carlos Blanco",
                "Fernando Perez-Cruz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/ruiz14a/ruiz14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Minimum Error Entropy Algorithms",
            "abstract": [
                "Minimum Error Entropy (MEE) principle is an important approach in Information Theoretical Learning (ITL). It is widely applied and studied in various fields for its robustness to noise. In this paper, we study a reproducing kernel-based distributed MEE algorithm, DMEE, which is designed to work with both fully supervised data and semi-supervised data. The divide-andconquer approach is employed, so there is no inter-node communication overhead. Similar as other distributed algorithms, DMEE significantly reduces the computational complexity and memory requirement on single computing nodes. With fully supervised data, our proved learning rates equal the minimax optimal learning rates of the classical pointwise kernel-based regressions. Under the semi-supervised learning scenarios, we show that DMEE exploits unlabeled data effectively, in the sense that first, under the settings with weak regularity assumptions, additional unlabeled data significantly improves the learning rates of DMEE. Second, with sufficient unlabeled data, labeled data can be distributed to many more computing nodes, that each node takes only O(1) labels, without spoiling the learning rates in terms of the number of labels. This conclusion overcomes the saturation phenomenon in unlabeled data size. It parallels a recent results for regularized least squares (Lin and Zhou, 2018), and suggests that an inflation of unlabeled data is a solution to the MEE learning problems with decentralized data source for the concerns of privacy protection. Our work refers to pairwise learning and non-convex loss. The theoretical analysis is achieved by distributed U-statistics and error decomposition techniques in integral operators."
            ],
            "keywords": [
                "Information theoretic learning",
                "minimum error entropy",
                "distributed method",
                "semisupervised data",
                "reproducing kernel Hilbert space"
            ],
            "author": [
                "Xin Guo",
                "Ting Hu",
                "Qiang Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-696/18-696.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graph Estimation From Multi-Attribute Data",
            "abstract": [
                "Undirected graphical models are important in a number of modern applications that involve exploring or exploiting dependency structures underlying the data. For example, they are often used to explore complex systems where connections between entities are not well understood, such as in functional brain networks or genetic networks. Existing methods for estimating structure of undirected graphical models focus on scenarios where each node represents a scalar random variable, such as a binary neural activation state or a continuous mRNA abundance measurement, even though in many real world problems, nodes can represent multivariate variables with much richer meanings, such as whole images, text documents, or multi-view feature vectors. In this paper, we propose a new principled framework for estimating the structure of undirected graphical models from such multivariate (or multi-attribute) nodal data. The structure of a graph is inferred through estimation of non-zero partial canonical correlation between nodes. Under a Gaussian model, this strategy is equivalent to estimating conditional independencies between random vectors represented by the nodes and it generalizes the classical problem of covariance selection (Dempster, 1972). We relate the problem of estimating non-zero partial canonical correlations to maximizing a penalized Gaussian likelihood objective and develop a method that efficiently maximizes this objective. Extensive simulation studies demonstrate the effectiveness of the method under various conditions. We provide illustrative applications to uncovering gene regulatory networks from gene and protein profiles, and uncovering brain connectivity graph from positron emission tomography data. Finally, we provide sufficient conditions under which the true graphical structure can be recovered correctly."
            ],
            "keywords": [
                "graphical model selection",
                "multi-attribute data",
                "network analysis",
                "partial canonical correlation"
            ],
            "author": [
                "Mladen Kolar",
                "Han Liu",
                "Eric P Xing"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/kolar14a/kolar14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Rates for Multi-pass Stochastic Gradient Methods",
            "abstract": [
                "We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. We study how regularization properties are controlled by the step-size, the number of passes and the mini-batch size. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases. As a byproduct, we derive optimal convergence results for batch gradient methods (even in the non-attainable cases)."
            ],
            "keywords": [],
            "author": [
                "Junhong Lin",
                "Lorenzo Rosasco"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-176/17-176.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Compression-Based Averaging of Selective Naive Bayes Classifiers",
            "abstract": [
                "The naive Bayes classifier has proved to be very effective on many real data applications. Its performance usually benefits from an accurate estimation of univariate conditional probabilities and from variable selection. However, although variable selection is a desirable feature, it is prone to overfitting. In this paper, we introduce a Bayesian regularization technique to select the most probable subset of variables compliant with the naive Bayes assumption. We also study the limits of Bayesian model averaging in the case of the naive Bayes assumption and introduce a new weighting scheme based on the ability of the models to conditionally compress the class labels. The weighting scheme on the models reduces to a weighting scheme on the variables, and finally results in a naive Bayes classifier with \"soft variable selection\". Extensive experiments show that the compressionbased averaged classifier outperforms the Bayesian model averaging scheme."
            ],
            "keywords": [
                "naive Bayes",
                "Bayesian",
                "model selection",
                "model averaging"
            ],
            "author": [
                "Marc Boullé",
                "France Telecom",
                "Isabelle Guyon",
                "Amir Saffari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/boulle07a/boulle07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning From Crowds",
            "abstract": [
                "For many supervised learning tasks it may be infeasible (or very expensive) to obtain objective and reliable labels. Instead, we can collect subjective (possibly noisy) labels from multiple experts or annotators. In practice, there is a substantial amount of disagreement among the annotators, and hence it is of great practical interest to address conventional supervised learning problems in this scenario. In this paper we describe a probabilistic approach for supervised learning when we have multiple annotators providing (possibly noisy) labels but no absolute gold standard. The proposed algorithm evaluates the different experts and also gives an estimate of the actual hidden labels. Experimental results indicate that the proposed method is superior to the commonly used majority voting baseline."
            ],
            "keywords": [
                "multiple annotators",
                "multiple experts",
                "multiple teachers",
                "crowdsourcing"
            ],
            "author": [
                "Vikas C Raykar",
                "Shipeng Yu",
                "Linda H Zhao",
                "Gerardo Hermosillo Valadez",
                "Charles Florin",
                "Linda Moy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/raykar10a/raykar10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems",
            "abstract": [
                "There are two main approaches to binary classification problems: the loss function approach and the uncertainty set approach. The loss function approach is widely used in real-world data analysis. Statistical decision theory has been used to elucidate its properties such as statistical consistency. Conditional probabilities can also be estimated by using the minimum solution of the loss function. In the uncertainty set approach, an uncertainty set is defined for each binary label from training samples. The best separating hyperplane between the two uncertainty sets is used as the decision function. Although the uncertainty set approach provides an intuitive understanding of learning algorithms, its statistical properties have not been sufficiently studied. In this paper, we show that the uncertainty set is deeply connected with the convex conjugate of a loss function. On the basis of the conjugate relation, we propose a way of revising the uncertainty set approach so that it will have good statistical properties such as statistical consistency. We also introduce statistical models corresponding to uncertainty sets in order to estimate conditional probabilities. Finally, we present numerical experiments, verifying that the learning with revised uncertainty sets improves the prediction accuracy."
            ],
            "keywords": [
                "loss function",
                "uncertainty set",
                "convex conjugate",
                "consistency"
            ],
            "author": [
                "Takafumi Kanamori",
                "Akiko Takeda",
                "Taiji Suzuki"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/kanamori13a/kanamori13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gaussian Lower Bound for the Information Bottleneck Limit",
            "abstract": [
                "The Information Bottleneck (IB) is a conceptual method for extracting the most compact, yet informative, representation of a set of variables, with respect to the target. It generalizes the notion of minimal sufficient statistics from classical parametric statistics to a broader information-theoretic sense. The IB curve defines the optimal trade-off between representation complexity and its predictive power. Specifically, it is achieved by minimizing the level of mutual information (MI) between the representation and the original variables, subject to a minimal level of MI between the representation and the target. This problem is shown to be in general NP hard. One important exception is the multivariate Gaussian case, for which the Gaussian IB (GIB) is known to obtain an analytical closed form solution, similar to Canonical Correlation Analysis (CCA). In this work we introduce a Gaussian lower bound to the IB curve; we find an embedding of the data which maximizes its \"Gaussian part\", on which we apply the GIB. This embedding provides an efficient (and practical) representation of any arbitrary data-set (in the IB sense), which in addition holds the favorable properties of a Gaussian distribution. Importantly, we show that the optimal Gaussian embedding is bounded from above by non-linear CCA. This allows a fundamental limit for our ability to Gaussianize arbitrary data-sets and solve complex problems by linear methods."
            ],
            "keywords": [
                "Information Bottleneck",
                "Canonical Correlations",
                "ACE",
                "Gaussianization",
                "Mutual Information Maximization",
                "Infomax"
            ],
            "author": [
                "Amichai Painsky",
                "Naftali Tishby"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-398/17-398.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Surrogate Regret Bounds for Bipartite Ranking via Strongly Proper Losses",
            "abstract": [
                "The problem of bipartite ranking, where instances are labeled positive or negative and the goal is to learn a scoring function that minimizes the probability of mis-ranking a pair of positive and negative instances (or equivalently, that maximizes the area under the ROC curve), has been widely studied in recent years. A dominant theoretical and algorithmic framework for the problem has been to reduce bipartite ranking to pairwise classification; in particular, it is well known that the bipartite ranking regret can be formulated as a pairwise classification regret, which in turn can be upper bounded using usual regret bounds for classification problems. Recently, Kotlowski et al. (2011) showed regret bounds for bipartite ranking in terms of the regret associated with balanced versions of the standard (non-pairwise) logistic and exponential losses. In this paper, we show that such (nonpairwise) surrogate regret bounds for bipartite ranking can be obtained in terms of a broad class of proper (composite) losses that we term as strongly proper. Our proof technique is much simpler than that of Kotlowski et al. (2011), and relies on properties of proper (composite) losses as elucidated recently by Reid and Williamson (2010, 2011) and others. Our result yields explicit surrogate bounds (with no hidden balancing terms) in terms of a variety of strongly proper losses, including for example logistic, exponential, squared and squared hinge losses as special cases. An important consequence is that standard algorithms minimizing a (non-pairwise) strongly proper loss, such as logistic regression and boosting algorithms (assuming a universal function class and appropriate regularization), are in fact consistent for bipartite ranking; moreover, our results allow us to quantify the bipartite ranking regret in terms of the corresponding surrogate regret. We also obtain tighter surrogate bounds under certain low-noise conditions via a recent result of Clémençon and Robbiano (2011)."
            ],
            "keywords": [
                "bipartite ranking",
                "area under ROC curve (AUC)",
                "statistical consistency",
                "regret bounds",
                "proper losses",
                "strongly proper losses"
            ],
            "author": [
                "Shivani Agarwal"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/agarwal14b/agarwal14b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Error Bound for L 1 -norm Support Vector Machine Coefficients in Ultra-high Dimension",
            "abstract": [
                "Comparing with the standard L 2-norm support vector machine (SVM), the L 1-norm SVM enjoys the nice property of simultaneously preforming classification and feature selection. In this paper, we investigate the statistical performance of L 1-norm SVM in ultra-high dimension, where the number of features p grows at an exponential rate of the sample size n. Different from existing theory for SVM which has been mainly focused on the generalization error rates and empirical risk, we study the asymptotic behavior of the coefficients of L 1norm SVM. Our analysis reveals that the estimated L 1-norm SVM coefficients achieve near oracle rate, that is, with high probability, the L 2 error bound of the estimated L 1norm SVM coefficients is of order O p (q log p/n), where q is the number of features with nonzero coefficients. Furthermore, we show that if the L 1-norm SVM is used as an initial value for a recently proposed algorithm for solving non-convex penalized SVM (Zhang et al., 2016b), then in two iterative steps it is guaranteed to produce an estimator that possesses the oracle property in ultra-high dimension, which in particular implies that with probability approaching one the zero coefficients are estimated as exactly zero. Simulation studies demonstrate the fine performance of L 1-norm SVM as a sparse classifier and its effectiveness to be utilized to solve non-convex penalized SVM problems in high dimension."
            ],
            "keywords": [
                "feature selection",
                "L 1 -norm SVM; non-convex penalty",
                "oracle property",
                "error bound",
                "support vector machine",
                "ulta-high dimension"
            ],
            "author": [
                "Bo Peng",
                "Lan Wang",
                "Yichao Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-654/15-654.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Time for a Change: a Tutorial for Comparing Multiple Classifiers Through Bayesian Analysis",
            "abstract": [
                "The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better-more sound and useful-alternatives for it."
            ],
            "keywords": [
                "comparing classifiers",
                "null hypothesis significance testing",
                "pitfalls of p-values",
                "Bayesian hypothesis tests",
                "Bayesian correlated t-test",
                "Bayesian hierarchical correlated ttest",
                "Bayesian signed-rank test"
            ],
            "author": [
                "Alessio Benavoli",
                "Giorgio Corani",
                "Marco Zaffalon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-305/16-305.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Coordinate Covariances via Gradients",
            "abstract": [
                "We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efficient implementation with respect to both memory and time."
            ],
            "keywords": [
                "Tikhnonov regularization",
                "variable selection",
                "reproducing kernel Hilbert space",
                "generalization bounds"
            ],
            "author": [
                "Sayan Mukherjee",
                "Ding-Xuan Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/mukherjee06a/mukherjee06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares",
            "abstract": [
                "The matrix-completion problem has attracted a lot of attention, largely as a result of the celebrated Netflix competition. Two popular approaches for solving the problem are nuclear-norm-regularized matrix approximation (Candès and Tao, 2009; Mazumder et al., 2010), and maximum-margin matrix factorization (Srebro et al., 2005). These two procedures are in some cases solving equivalent problems, but with quite different algorithms. In this article we bring the two approaches together, leading to an efficient algorithm for large matrix factorization and completion that outperforms both of these. We develop a software package softImpute in R for implementing our approaches, and a distributed version for very large matrices using the Spark cluster programming environment"
            ],
            "keywords": [
                "matrix completion",
                "alternating least squares",
                "svd",
                "nuclear norm"
            ],
            "author": [
                "Trevor Hastie",
                "Rahul Mazumder",
                "Jason D Lee",
                "Reza Zadeh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/hastie15a/hastie15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unified Framework of Online Learning Algorithms for Training Recurrent Neural Networks",
            "abstract": [
                "We present a framework for compactly summarizing many recent results in efficient and/or biologically plausible online training of recurrent neural networks (RNN). The framework organizes algorithms according to several criteria: (a) past vs. future facing, (b) tensor structure, (c) stochastic vs. deterministic, and (d) closed form vs. numerical. These axes reveal latent conceptual connections among several recent advances in online learning. Furthermore, we provide novel mathematical intuitions for their degree of success. Testing these algorithms on two parametric task families shows that performances cluster according to our criteria. Although a similar clustering is also observed for pairwise gradient alignment, alignment with exact methods does not explain ultimate performance. This suggests the need for better comparison metrics."
            ],
            "keywords": [
                "real-time recurrent learning",
                "backpropagation through time",
                "approximation",
                "biologically plausible learning",
                "local",
                "online"
            ],
            "author": [
                "Owen Marschall",
                "Kyunghyun Cho",
                "Cristina Savin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-562/19-562.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Structure Learning of Bayesian Networks using Constraints",
            "abstract": [
                "This paper addresses the problem of learning Bayesian network structures from data based on score functions that are decomposable. It describes properties that strongly reduce the time and memory costs of many known methods without losing global optimality guarantees. These properties are derived for different score criteria such as Minimum Description Length (or Bayesian Information Criterion), Akaike Information Criterion and Bayesian Dirichlet Criterion. Then a branch-andbound algorithm is presented that integrates structural constraints with data in a way to guarantee global optimality. As an example, structural constraints are used to map the problem of structure learning in Dynamic Bayesian networks into a corresponding augmented Bayesian network. Finally, we show empirically the benefits of using the properties with state-of-the-art methods and with the new algorithm, which is able to handle larger data sets than before."
            ],
            "keywords": [
                "Bayesian networks",
                "structure learning",
                "properties of decomposable scores",
                "structural constraints",
                "branch-and-bound technique"
            ],
            "author": [
                "Cassio P De Campos",
                "Qiang Ji"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/decampos11a/decampos11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Building Blocks for Variational Bayesian Learning of Latent Variable Models",
            "abstract": [
                "We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to fit together and to yield efficient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a specific model structure has been fixed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method."
            ],
            "keywords": [
                "latent variable models",
                "variational Bayesian learning",
                "graphical models",
                "building blocks",
                "Bayesian modelling",
                "local computation"
            ],
            "author": [
                "Tapani Raiko",
                "Harri Valpola",
                "Markus Harva",
                "Juha Karhunen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/raiko07a/raiko07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ORCA: A Matlab/Octave Toolbox for Ordinal Regression",
            "abstract": [
                "Ordinal regression, also named ordinal classification, studies classification problems where there exist a natural order between class labels. This structured order of the labels is crucial in all steps of the learning process in order to take full advantage of the data. ORCA (Ordinal Regression and Classification Algorithms) is a Matlab/Octave framework that implements and integrates different ordinal classification algorithms and specifically designed performance metrics. The framework simplifies the task of experimental comparison to a great extent, allowing the user to: (i) describe experiments by simple configuration files; (ii) automatically run different data partitions; (iii) parallelize the executions; (iv) generate a variety of performance reports and (v) include new algorithms by using its intuitive interface. Source code, binaries, documentation, descriptions and links to data sets and tutorials (including examples of educational purpose) are available at https://github.com/ayrna/orca."
            ],
            "keywords": [
                "Ordinal regression",
                "ordinal classification",
                "Matlab",
                "Octave",
                "threshold models"
            ],
            "author": [
                "Javier Sánchez-Monedero",
                "Pedro A Gutiérrez",
                "María Pérez-Ortiz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-349/18-349.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Discovery with Continuous Additive Noise Models",
            "abstract": [
                "We consider the problem of learning causal directed acyclic graphs from an observational joint distribution. One can use these graphs to predict the outcome of interventional experiments, from which data are often not available. We show that if the observational distribution follows a structural equation model with an additive noise structure, the directed acyclic graph becomes identifiable from the distribution under mild conditions. This constitutes an interesting alternative to traditional methods that assume faithfulness and identify only the Markov equivalence class of the graph, thus leaving some edges undirected. We provide practical algorithms for finitely many samples, RESIT (regression with subsequent independence test) and two methods based on an independence score. We prove that RESIT is correct in the population setting and provide an empirical evaluation."
            ],
            "keywords": [
                "causal inference",
                "structural equation models",
                "additive noise",
                "identifiability",
                "causal minimality",
                "Bayesian networks"
            ],
            "author": [
                "Jonas Peters",
                "Joris M Mooij",
                "Bernhard Schölkopf"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/peters14a/peters14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Approximate Sequential Patterns for Classification",
            "abstract": [
                "In this paper, we present an automated approach to discover patterns that can distinguish between sequences belonging to different labeled groups. Our method searches for approximately conserved motifs that occur with varying statistical properties in positive and negative training examples. We propose a two-step process to discover such patterns. Using locality sensitive hashing (LSH), we first estimate the frequency of all subsequences and their approximate matches within a given Hamming radius in labeled examples. The discriminative ability of each pattern is then assessed from the estimated frequencies by concordance and rank sum testing. The use of LSH to identify approximate matches for each candidate pattern helps reduce the runtime of our method. Space requirements are reduced by decomposing the search problem into an iterative method that uses a single LSH table in memory. We propose two further optimizations to the search for discriminative patterns. Clustering with redundancy based on a 2-approximate solution of the k-center problem decreases the number of overlapping approximate groups while providing exhaustive coverage of the search space. Sequential statistical methods allow the search process to use data from only as many training examples as are needed to assess significance. We evaluated our algorithm on data sets from different applications to discover sequential patterns for classification. On nucleotide sequences from the Drosophila genome compared with random background sequences, our method was able to discover approximate binding sites that were preserved upstream of genes. We observed a similar result in experiments on ChIP-on-chip data. For cardiovascular data from patients admitted with acute coronary syndromes, our pattern discovery approach identified approximately conserved sequences of morphology variations that were predictive of future death in a test population. Our data showed that the use of LSH, clustering, and sequential statistics improved the running time of the search algorithm by an order of magnitude without any noticeable effect on accuracy. These results suggest that our methods may allow for an unsupervised approach to efficiently learn interesting dissimilarities between positive and negative examples that may have a functional role."
            ],
            "keywords": [
                "pattern discovery",
                "motif discovery",
                "locality sensitive hashing",
                "classification"
            ],
            "author": [
                "Zeeshan Syed",
                "Piotr Indyk",
                "John Guttag"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/syed09a/syed09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Significance Tests for Neural Networks",
            "abstract": [
                "We develop a pivotal test to assess the statistical significance of the feature variables in a single-layer feedforward neural network regression model. We propose a gradient-based test statistic and study its asymptotics using nonparametric techniques. Under technical conditions, the limiting distribution is given by a mixture of chi-square distributions. The tests enable one to discern the impact of individual variables on the prediction of a neural network. The test statistic can be used to rank variables according to their influence. Simulation results illustrate the computational efficiency and the performance of the test. An empirical application to house price valuation highlights the behavior of the test using actual data."
            ],
            "keywords": [
                "significance test",
                "neural network",
                "model interpretability",
                "nonparametric regression",
                "nonlinear regression",
                "feature selection"
            ],
            "author": [
                "Enguerrand Horel",
                "Kay Giesecke"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-264/19-264.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Post-Regularization Inference for Time-Varying Nonparanormal Graphical Models",
            "abstract": [
                "We propose a novel class of time-varying nonparanormal graphical models, which allows us to model high dimensional heavy-tailed systems and the evolution of their latent network structures. Under this model we develop statistical tests for presence of edges both locally at a fixed index value and globally over a range of values. The tests are developed for a high-dimensional regime, are robust to model selection mistakes and do not require commonly assumed minimum signal strength. The testing procedures are based on a high dimensional, debiasing-free moment estimator, which uses a novel kernel smoothed Kendall's tau correlation matrix as an input statistic. The estimator consistently estimates the latent inverse Pearson correlation matrix uniformly in both the index variable and kernel bandwidth. Its rate of convergence is shown to be minimax optimal. Our method is supported by thorough numerical simulations and an application to a neural imaging data set."
            ],
            "keywords": [
                "graphical model selection",
                "nonparanormal graph",
                "time-varying network analysis",
                "hypothesis test",
                "regularized rank-based estimator"
            ],
            "author": [
                "Junwei Lu",
                "Mladen Kolar",
                "Han Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-145/17-145.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A New Approximate Maximal Margin Classification Algorithm",
            "abstract": [
                "A new incremental learning algorithm is described which approximates the maximal margin hyperplane w.r.t. norm p ≥ 2 for a set of linearly separable data. Our algorithm, called alma p (Approximate Large Margin algorithm w.r.t. norm p), takes O (p−1) α 2 γ 2 corrections to separate the data with p-norm margin larger than (1 − α) γ, where γ is the (normalized) p-norm margin of the data. alma p avoids quadratic (or higher-order) programming methods. It is very easy to implement and is as fast as on-line algorithms, such as Rosenblatt's Perceptron algorithm. We performed extensive experiments on both real-world and artificial datasets. We compared alma 2 (i.e., alma p with p = 2) to standard Support vector Machines (SVM) and to two incremental algorithms: the Perceptron algorithm and Li and Long's ROMMA. The accuracy levels achieved by alma 2 are superior to those achieved by the Perceptron algorithm and ROMMA, but slightly inferior to SVM's. On the other hand, alma 2 is quite faster and easier to implement than standard SVM training algorithms. When learning sparse target vectors, alma p with p > 2 largely outperforms Perceptron-like algorithms, such as alma 2 ."
            ],
            "keywords": [
                "Binary Classification",
                "Large Margin",
                "Support Vector Machines",
                "On-line Learning"
            ],
            "author": [
                "Claudio Gentile",
                "Nello Cristianini",
                "Bob Williamson"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/gentile01a/gentile01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Theoretical Analysis of Cross-Validation for Estimating the Risk of the k-Nearest Neighbor Classifier",
            "abstract": [
                "The present work aims at deriving theoretical guaranties on the behavior of some crossvalidation procedures applied to the k-nearest neighbors (kNN) rule in the context of binary classification. Here we focus on the leave-pout cross-validation (LpO) used to assess the performance of the kNN classifier. Remarkably this LpO estimator can be efficiently computed in this context using closed-form formulas derived by Celisse and Mary-Huard (2011). We describe a general strategy to derive moment and exponential concentration inequalities for the LpO estimator applied to the kNN classifier. Such results are obtained first by exploiting the connection between the LpO estimator and U-statistics, and second by making an intensive use of the generalized Efron-Stein inequality applied to the L1O estimator. One other important contribution is made by deriving new quantifications of the discrepancy between the LpO estimator and the classification error/risk of the kNN classifier. The optimality of these bounds is discussed by means of several lower bounds as well as simulation experiments."
            ],
            "keywords": [
                "Classification",
                "Cross-validation",
                "Risk estimation"
            ],
            "author": [
                "Alain Celisse",
                "Tristan Mary-Huard"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/15-498/15-498.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Covering Number Bounds of Certain Regularized Linear Function Classes",
            "abstract": [
                "Recently, sample complexity bounds have been derived for problems involving linear functions such as neural networks and support vector machines. In many of these theoretical studies, the concept of covering numbers played an important role. It is thus useful to study covering numbers for linear function classes. In this paper, we investigate two closely related methods to derive upper bounds on these covering numbers. The first method, already employed in some earlier studies, relies on the so-called Maurey's lemma; the second method uses techniques from the mistake bound framework in online learning. We compare results from these two methods, as well as their consequences in some learning formulations."
            ],
            "keywords": [
                "Covering Numbers",
                "Learning Sample Complexity",
                "Sparse Approximation",
                "Mistake Bounds"
            ],
            "author": [
                "Tong Zhang",
                "Peter L Bartlett"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/zhang02b/zhang02b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiclass Classification with Multi-Prototype Support Vector Machines",
            "abstract": [
                "Winner-take-all multiclass classifiers are built on the top of a set of prototypes each representing one of the available classes. A pattern is then classified with the label associated to the most 'similar' prototype. Recent proposal of SVM extensions to multiclass can be considered instances of the same strategy with one prototype per class. The multi-prototype SVM proposed in this paper extends multiclass SVM to multiple prototypes per class. It allows to combine several vectors in a principled way to obtain large margin decision functions. For this problem, we give a compact constrained quadratic formulation and we propose a greedy optimization algorithm able to find locally optimal solutions for the non convex objective function. This algorithm proceeds by reducing the overall problem into a series of simpler convex problems. For the solution of these reduced problems an efficient optimization algorithm is proposed. A number of pattern selection strategies are then discussed to speed-up the optimization process. In addition, given the combinatorial nature of the overall problem, stochastic search strategies are suggested to escape from local minima which are not globally optimal. Finally, we report experiments on a number of datasets. The performance obtained using few simple linear prototypes is comparable to that obtained by state-of-the-art kernel-based methods but with a significant reduction (of one or two orders) in response time."
            ],
            "keywords": [
                "multiclass classification",
                "multi-prototype support vector machines",
                "kernel machines",
                "stochastic search optimization",
                "large margin classifiers"
            ],
            "author": [
                "Fabio Aiolli",
                "Alessandro Sperduti"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/aiolli05a/aiolli05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Bootstrap Confidence Intervals for the Stochastic Gradient Descent Estimator",
            "abstract": [
                "In many applications involving large dataset or online learning, stochastic gradient descent (SGD) is a scalable algorithm to compute parameter estimates and has gained increasing popularity due to its numerical convenience and memory efficiency. While the asymptotic properties of SGD-based estimators have been well established, statistical inference such as interval estimation remains much unexplored. The classical bootstrap is not directly applicable if the data are not stored in memory. The plug-in method is not applicable when there is no explicit formula for the covariance matrix of the estimator. In this paper, we propose an online bootstrap procedure for the estimation of confidence intervals, which, upon the arrival of each observation, updates the SGD estimate as well as a number of randomly perturbed SGD estimates. The proposed method is easy to implement in practice. We establish its theoretical properties for a general class of models that includes linear regressions, generalized linear models, M-estimators and quantile regressions as special cases. The finite-sample performance and numerical utility is evaluated by simulation studies and real data applications."
            ],
            "keywords": [
                "Bootstrap",
                "Interval estimation",
                "Generalized linear models",
                "Large datasets",
                "M-estimators",
                "Quantile regression",
                "Resampling methods",
                "Stochastic gradient descent"
            ],
            "author": [
                "Jinfeng Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-370/17-370.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Discriminative Learning Under Covariate Shift",
            "abstract": [
                "We address classification problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution-problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. The problem of learning under covariate shift can be written as an integrated optimization problem. Instantiating the general optimization problem leads to a kernel logistic regression and an exponential model classifier for covariate shift. The optimization problem is convex under certain conditions; our findings also clarify the relationship to the known kernel mean matching procedure. We report on experiments on problems of spam filtering, text classification, and landmine detection."
            ],
            "keywords": [
                "covariate shift",
                "discriminative learning",
                "transfer learning"
            ],
            "author": [
                "Steffen Bickel",
                "Michael Brückner",
                "Tobias Scheffer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/bickel09a/bickel09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikit-learn",
            "abstract": [
                "scikit-survival is an open-source Python package for time-to-event analysis fully compatible with scikit-learn. It provides implementations of many popular machine learning techniques for time-to-event analysis, including penalized Cox model, Random Survival Forest, and Survival Support Vector Machine. In addition, the library includes tools to evaluate model performance on censored time-to-event data. The documentation contains installation instructions, interactive notebooks, and a full description of the API. scikit-survival is distributed under the GPL-3 license with the source code and detailed instructions available at https://github.com/sebp/scikit-survival"
            ],
            "keywords": [
                "Time-to-event Analysis",
                "Survival Analysis",
                "Censored Data",
                "Python"
            ],
            "author": [
                "Sebastian Pölsterl"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-729/20-729.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Semi-supervised Learning with Kernel Ridge Regression",
            "abstract": [
                "This paper provides error analysis for distributed semi-supervised learning with kernel ridge regression (DSKRR) based on a divide-and-conquer strategy. DSKRR applies kernel ridge regression (KRR) to data subsets that are distributively stored on multiple servers to produce individual output functions, and then takes a weighted average of the individual output functions as a final estimator. Using a novel error decomposition which divides the generalization error of DSKRR into the approximation error, sample error and distributed error, we find that the sample error and distributed error reflect the power and limitation of DSKRR, compared with KRR processing the whole data. Thus a small distributed error provides a large range of the number of data subsets to guarantee a small generalization error. Our results show that unlabeled data play important roles in reducing the distributed error and enlarging the number of data subsets in DSKRR. Our analysis also applies to the case when the regression function is out of the reproducing kernel Hilbert space. Numerical experiments including toy simulations and a music-prediction task are employed to demonstrate our theoretical statements and show the power of unlabeled data in distributed learning."
            ],
            "keywords": [
                "learning theory",
                "distributed learning",
                "kernel ridge regression",
                "semi-supervised learning",
                "unlabeled data",
                "error decomposition"
            ],
            "author": [
                "Xiangyu Chang",
                "Shao-Bo Lin",
                "Ding-Xuan Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-601/16-601.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "One-class classification of point patterns of extremes",
            "abstract": [
                "Novelty detection or one-class classification starts from a model describing some type of 'normal behaviour' and aims to classify deviations from this model as being either novelties or anomalies. In this paper the problem of novelty detection for point patterns S = {x 1 ,. .. , x k } ⊂ R d is treated where examples of anomalies are very sparse, or even absent. The latter complicates the tuning of hyperparameters in models commonly used for novelty detection, such as one-class support vector machines and hidden Markov models. To this end, the use of extreme value statistics is introduced to estimate explicitly a model for the abnormal class by means of extrapolation from a statistical model X for the normal class. We show how multiple types of information obtained from any available extreme instances of S can be combined to reduce the high false-alarm rate that is typically encountered when classes are strongly imbalanced, as often occurs in the one-class setting (whereby 'abnormal' data are often scarce). The approach is illustrated using simulated data and then a real-life application is used as an exemplar, whereby accelerometry data from epileptic seizures are analysed-these are known to be extreme and rare with respect to normal accelerometer data."
            ],
            "keywords": [
                "Sequence classification",
                "novelty detection",
                "extreme value theory",
                "class imbalance",
                "asymptotic theory"
            ],
            "author": [
                "Stijn Luca",
                "David A Clifton",
                "Bart Vanrumste"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-112/16-112.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Automatic Differentiation Variational Inference Alp Kucukelbir",
            "abstract": [
                "Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference (). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. supports a broad class of models-no conjugacy assumptions are required. We study across ten modern probabilistic models and apply it to a dataset with millions of observations. We deploy as part of Stan, a probabilistic programming system."
            ],
            "keywords": [
                "Bayesian inference",
                "approximate inference",
                "probabilistic programming"
            ],
            "author": [
                "Dustin Tran",
                "Rajesh Ranganath",
                "Andrew Gelman",
                "David M Blei"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-107/16-107.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Faithfulness of Probability Distributions and Graphs",
            "abstract": [
                "A main question in graphical models and causal inference is whether, given a probability distribution P (which is usually an underlying distribution of data), there is a graph (or graphs) to which P is faithful. The main goal of this paper is to provide a theoretical answer to this problem. We work with general independence models, which contain probabilistic independence models as a special case. We exploit a generalization of ordering, called preordering, of the nodes of (mixed) graphs. This allows us to provide sufficient conditions for a given independence model to be Markov to a graph with the minimum possible number of edges, and more importantly, necessary and sufficient conditions for a given probability distribution to be faithful to a graph. We present our results for the general case of mixed graphs, but specialize the definitions and results to the better-known subclasses of undirected (concentration) and bidirected (covariance) graphs as well as directed acyclic graphs."
            ],
            "keywords": [
                "causal discovery",
                "compositional graphoid",
                "directed acyclic graph",
                "faithfulness",
                "graphical model selection",
                "independence model",
                "Markov property",
                "mixed graph",
                "structural learning"
            ],
            "author": [
                "Kayvan Sadeghi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-275/17-275.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized probabilistic principal component analysis of correlated data",
            "abstract": [
                "Principal component analysis (PCA) is a well-established tool in machine learning and data processing. The principal axes in PCA were shown to be equivalent to the maximum marginal likelihood estimator of the factor loading matrix in a latent factor model for the observed data, assuming that the latent factors are independently distributed as standard normal distributions. However, the independence assumption may be unrealistic for many scenarios such as modeling multiple time series, spatial processes, and functional data, where the outcomes are correlated. In this paper, we introduce the generalized probabilistic principal component analysis (GPPCA) to study the latent factor model for multiple correlated outcomes, where each factor is modeled by a Gaussian process. Our method generalizes the previous probabilistic formulation of PCA (PPCA) by providing the closedform maximum marginal likelihood estimator of the factor loadings and other parameters. Based on the explicit expression of the precision matrix in the marginal likelihood that we derived, the number of the computational operations is linear to the number of output variables. Furthermore, we also provide the closed-form expression of the marginal likelihood when other covariates are included in the mean structure. We highlight the advantage of GPPCA in terms of the practical relevance, estimation accuracy and computational convenience. Numerical studies of simulated and real data confirm the excellent finite-sample performance of the proposed approach."
            ],
            "keywords": [
                "Gaussian process",
                "maximum marginal likelihood estimator",
                "kernel method",
                "principal component analysis",
                "Stiefel manifold"
            ],
            "author": [
                "Mengyang Gu",
                "Weining Shen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-595/18-595.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Learning with Samples Drawn from Non-identical Distributions",
            "abstract": [
                "Learning algorithms are based on samples which are often drawn independently from an identical distribution (i.i.d.). In this paper we consider a different setting with samples drawn according to a non-identical sequence of probability distributions. Each time a sample is drawn from a different distribution. In this setting we investigate a fully online learning algorithm associated with a general convex loss function and a reproducing kernel Hilbert space (RKHS). Error analysis is conducted under the assumption that the sequence of marginal distributions converges polynomially in the dual of a Hölder space. For regression with least square or insensitive loss, learning rates are given in both the RKHS norm and the L 2 norm. For classification with hinge loss and support vector machine q-norm loss, rates are explicitly stated with respect to the excess misclassification error."
            ],
            "keywords": [
                "sampling with non-identical distributions",
                "online learning",
                "classification with a general convex loss",
                "regression with insensitive loss and least square loss",
                "reproducing kernel Hilbert space"
            ],
            "author": [
                "Ting Hu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/hu09a/hu09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Transport: Fast Probabilistic Approximation with Exact Solvers",
            "abstract": [
                "We propose a simple subsampling scheme for fast randomized approximate computation of optimal transport distances on finite spaces. This scheme operates on a random subset of the full data and can use any exact algorithm as a black-box back-end, including state-of-the-art solvers and entropically penalized versions. It is based on averaging the exact distances between empirical measures generated from independent samples from the original measures and can easily be tuned towards higher accuracy or shorter computation times. To this end, we give non-asymptotic deviation bounds for its accuracy in the case of discrete optimal transport problems. In particular, we show that in many important instances, including images (2D-histograms), the approximation error is independent of the size of the full problem. We present numerical experiments that demonstrate that a very good approximation in typical applications can be obtained in a computation time that is several orders of magnitude smaller than what is required for exact computation of the full problem."
            ],
            "keywords": [
                "computational vs statistical accuracy",
                "covering numbers",
                "empirical optimal transport",
                "resampling",
                "risk bounds",
                "spanning tree",
                "Wasserstein distance"
            ],
            "author": [
                "Max Sommerfeld",
                "Jörn Schrieber",
                "Yoav Zemel",
                "Axel Munk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-079/18-079.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Markov Blanket and Markov Boundary of Multiple Variables",
            "abstract": [
                "Markov blanket (Mb) and Markov boundary (MB) are two key concepts in Bayesian networks (BNs). In this paper, we study the problem of Mb and MB for multiple variables. First, we show that Mb possesses the additivity property under the local intersection assumption, that is, an Mb of multiple targets can be constructed by simply taking the union of Mbs of the individual targets and removing the targets themselves. MB is also proven to have additivity under the local intersection assumption. Second, we analyze the cases of violating additivity of Mb and MB and then put forward the notions of Markov blanket supplementary (MbS) and Markov boundary supplementary (MBS). The properties of MbS and MBS are studied in detail. Third, we build two MB discovery algorithms and prove their correctness under the local composition assumption. We also discuss the ways of practically doing conditional independence tests and analyze the complexities of the algorithms. Finally, we make a benchmarking study based on six synthetic BNs and then apply MB discovery to multi-class prediction based on a real data set. The experimental results reveal our algorithms have higher accuracies and lower complexities than existing algorithms."
            ],
            "keywords": [
                "Markov blanket",
                "Markov boundary",
                "Markov blanket supplementary",
                "Markov boundary supplementary",
                "Bayesian network"
            ],
            "author": [
                "Xu-Qing Liu",
                "Xin-Sheng Liu",
                "Marina Meila",
                "Kevin Murphy",
                "Joris Mooij"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/14-033/14-033.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Differentiable reservoir computing",
            "abstract": [
                "Numerous results in learning and approximation theory have evidenced the importance of differentiability at the time of countering the curse of dimensionality. In the context of reservoir computing, much effort has been devoted in the last two decades to characterize the situations in which systems of this type exhibit the so-called echo state (ESP) and fading memory (FMP) properties. These important features amount, in mathematical terms, to the existence and continuity of global reservoir system solutions. That research is complemented in this paper with the characterization of the differentiability of reservoir filters for very general classes of discrete-time deterministic inputs. This constitutes a novel strong contribution to the long line of research on the ESP and the FMP and, in particular, links to existing research on the input-dependence of the ESP. Differentiability has been shown in the literature to be a key feature in the learning of attractors of chaotic dynamical systems. A Volterra-type series representation for reservoir filters with semi-infinite discrete-time inputs is constructed in the analytic case using Taylor's theorem and corresponding approximation bounds are provided. Finally, it is shown as a corollary of these results that any fading memory filter can be uniformly approximated by a finite Volterra series with finite memory."
            ],
            "keywords": [
                "reservoir computing",
                "fading memory property",
                "finite memory",
                "echo state property",
                "differentiable reservoir filter",
                "Volterra series representation",
                "state-space systems",
                "system identification",
                "machine learning"
            ],
            "author": [
                "Lyudmila Grigoryeva"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-150/19-150.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regret Bounds and Minimax Policies under Partial Monitoring",
            "abstract": [
                "This work deals with four classical prediction settings, namely full information, bandit, label efficient and bandit label efficient as well as four different notions of regret: pseudo-regret, expected regret, high probability regret and tracking the best expert regret. We introduce a new forecaster, INF (Implicitly Normalized Forecaster) based on an arbitrary function ψ for which we propose a unified analysis of its pseudo-regret in the four games we consider. In particular, for ψ(x) = exp(ηx) + γ K , INF reduces to the classical exponentially weighted average forecaster and our analysis of the pseudo-regret recovers known results while for the expected regret we slightly tighten the bounds. On the other hand with ψ(x) = η −x q + γ K , which defines a new forecaster, we are able to remove the extraneous logarithmic factor in the pseudo-regret bounds for bandits games, and thus fill in a long open gap in the characterization of the minimax rate for the pseudo-regret in the bandit game. We also provide high probability bounds depending on the cumulative reward of the optimal action. Finally, we consider the stochastic bandit game, and prove that an appropriate modification of the upper confidence bound policy UCB1 (Auer et al., 2002a) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays."
            ],
            "keywords": [
                "Bandits (adversarial and stochastic)",
                "regret bound",
                "minimax rate",
                "label efficient",
                "upper confidence bound (UCB) policy",
                "online learning",
                "prediction with limited feedback"
            ],
            "author": [
                "Jean-Yves Audibert",
                "Sébastien Bubeck"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/audibert10a/audibert10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Passive-Aggressive Algorithms",
            "abstract": [
                "We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This unified view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any fixed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets."
            ],
            "keywords": [],
            "author": [
                "Koby Crammer",
                "Joseph Keshet",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/crammer06a/crammer06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Equivalence of Graphical Lasso and Thresholding for Sparse Graphs",
            "abstract": [
                "This paper is concerned with the problem of finding a sparse graph capturing the conditional dependence between the entries of a Gaussian random vector, where the only available information is a sample correlation matrix. A popular approach to address this problem is the graphical lasso technique, which employs a sparsity-promoting regularization term. This paper derives a simple condition under which the computationally-expensive graphical lasso behaves the same as the simple heuristic method of thresholding. This condition depends only on the solution of graphical lasso and makes no direct use of the sample correlation matrix or the regularization coefficient. It is proved that this condition is always satisfied if the solution of graphical lasso is close to its first-order Taylor approximation or equivalently the regularization term is relatively large. This condition is tested on several random problems, and it is shown that graphical lasso and the thresholding method lead to highly similar results in the case where a sparse graph is sought. We also conduct two case studies on brain connectivity networks of twenty subjects based on fMRI data and the topology identification of electrical circuits to support the findings of this work on the similarity of graphical lasso and thresholding."
            ],
            "keywords": [
                "Graphical Lasso",
                "Graphical Models",
                "Sparse Graphs",
                "Brain Connectivity Networks",
                "Electrical Circuits"
            ],
            "author": [
                "Somayeh Sojoudi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-013/16-013.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized M -estimators with Nonconvexity: Statistical and Algorithmic Theory for Local Optima",
            "abstract": [
                "We provide novel theoretical results regarding local optima of regularized M-estimators, allowing for nonconvexity in both loss and penalty functions. Under restricted strong convexity on the loss and suitable regularity conditions on the penalty, we prove that any stationary point of the composite objective function will lie within statistical precision of the underlying parameter vector. Our theory covers many nonconvex objective functions of interest, including the corrected Lasso for errors-in-variables linear models; regression for generalized linear models with nonconvex penalties such as SCAD, MCP, and capped-1 ; and high-dimensional graphical model estimation. We quantify statistical accuracy by providing bounds on the- ,-, and prediction error between stationary points and the population-level optimum. We also propose a simple modification of composite gradient descent that may be used to obtain a near-global optimum within statistical precision stat in log(1/ stat) steps, which is the fastest possible rate of any first-order method. We provide simulation studies illustrating the sharpness of our theoretical results."
            ],
            "keywords": [
                "high-dimensional statistics",
                "M -estimation",
                "model selection",
                "nonconvex optimization",
                "nonconvex regularization"
            ],
            "author": [
                "Po-Ling Loh",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/loh15a/loh15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Weighted SGD for p Regression with Randomized Preconditioning *",
            "abstract": [
                "In recent years, stochastic gradient descent (SGD) methods and randomized linear algebra (RLA) algorithms have been applied to many large-scale problems in machine learning and data analysis. SGD methods are easy to implement and applicable to a wide range of convex optimization problems. In contrast, RLA algorithms provide much stronger performance guarantees but are applicable to a narrower class of problems. We aim to bridge the gap between these two methods in solving constrained overdetermined linear regression problems-e.g., 2 and 1 regression problems."
            ],
            "keywords": [],
            "author": [
                "Jiyan Yang",
                "Stanford Edu",
                "Christopher Ré",
                "Michael W Mahoney"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-044/17-044.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Indian Buffet Process: An Introduction and Review",
            "abstract": [
                "The Indian buffet process is a stochastic process defining a probability distribution over equivalence classes of sparse binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an infinite latent feature model. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes."
            ],
            "keywords": [
                "nonparametric Bayes",
                "Markov chain Monte Carlo",
                "latent variable models",
                "Chinese restaurant processes",
                "beta process",
                "exchangeable distributions",
                "sparse binary matrices"
            ],
            "author": [
                "Thomas L Griffiths",
                "Tom Griffiths",
                "Berkeley Edu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/griffiths11a/griffiths11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized TD Learning",
            "abstract": [
                "Since the invention of temporal difference (TD) learning (Sutton, 1988), many new algorithms for model-free policy evaluation have been proposed. Although they have brought much progress in practical applications of reinforcement learning (RL), there still remain fundamental problems concerning statistical properties of the value function estimation. To solve these problems, we introduce a new framework, semiparametric statistical inference, to model-free policy evaluation. This framework generalizes TD learning and its extensions, and allows us to investigate statistical properties of both of batch and online learning procedures for the value function estimation in a unified way in terms of estimating functions. Furthermore, based on this framework, we derive an optimal estimating function with the minimum asymptotic variance and propose batch and online learning algorithms which achieve the optimality."
            ],
            "keywords": [
                "reinforcement learning",
                "model-free policy evaluation",
                "TD learning",
                "semiparametirc model",
                "estimating function"
            ],
            "author": [
                "Tsuyoshi Ueno",
                "Shin Ishii"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/ueno11a/ueno11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs",
            "abstract": [
                "Recent methods for estimating sparse undirected graphs for real-valued data in high dimensional problems rely heavily on the assumption of normality. We show how to use a semiparametric Gaussian copula-or \"nonparanormal\"-for high dimensional inference. Just as additive models extend linear models by replacing linear functions with a set of one-dimensional smooth functions, the nonparanormal extends the normal by transforming the variables by smooth functions. We derive a method for estimating the nonparanormal, study the method's theoretical properties, and show that it works well in many examples."
            ],
            "keywords": [
                "graphical models",
                "Gaussian copula",
                "high dimensional inference",
                "sparsity",
                "ℓ 1 regularization",
                "graphical lasso",
                "paranormal",
                "occult"
            ],
            "author": [
                "Han Liu",
                "John Lafferty",
                "Larry Wasserman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/liu09a/liu09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Translation Invariant Kernels for Classification",
            "abstract": [
                "Appropriate selection of the kernel function, which implicitly defines the feature space of an algorithm, has a crucial role in the success of kernel methods. In this paper, we consider the problem of optimizing a kernel function over the class of translation invariant kernels for the task of binary classification. The learning capacity of this class is invariant with respect to rotation and scaling of the features and it encompasses the set of radial kernels. We show that how translation invariant kernel functions can be embedded in a nested set of sub-classes and consider the kernel learning problem over one of these sub-classes. This allows the choice of an appropriate sub-class based on the problem at hand. We use the criterion proposed by Lanckriet et al. (2004) to obtain a functional formulation for the problem. It will be proven that the optimal kernel is a finite mixture of cosine functions. The kernel learning problem is then formulated as a semi-infinite programming (SIP) problem which is solved by a sequence of quadratically constrained quadratic programming (QCQP) sub-problems. Using the fact that the cosine kernel is of rank two, we propose a formulation of a QCQP sub-problem which does not require the kernel matrices to be loaded into memory, making the method applicable to large-scale problems. We also address the issue of including other classes of kernels, such as individual kernels and isotropic Gaussian kernels, in the learning process. Another interesting feature of the proposed method is that the optimal classifier has an expansion in terms of the number of cosine kernels, instead of support vectors, leading to a remarkable speedup at run-time. As a by-product, we also generalize the kernel trick to complex-valued kernel functions. Our experiments on artificial and real-world benchmark data sets, including the USPS and the MNIST digit recognition data sets, show the usefulness of the proposed method."
            ],
            "keywords": [
                "kernel learning",
                "translation invariant kernels",
                "capacity control",
                "support vector machines",
                "classification",
                "semi-infinite programming"
            ],
            "author": [
                "Kamaledin Ghiasi-Shirazi",
                "Reza Safabakhsh",
                "Mostafa Shamsi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/ghiasi-shirazi10a/ghiasi-shirazi10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Closed Sets for Labeled Data",
            "abstract": [
                "Closed sets have been proven successful in the context of compacted data representation for association rule learning. However, their use is mainly descriptive, dealing only with unlabeled data. This paper shows that when considering labeled data, closed sets can be adapted for classification and discrimination purposes by conveniently contrasting covering properties on positive and negative examples. We formally prove that these sets characterize the space of relevant combinations of features for discriminating the target class. In practice, identifying relevant/irrelevant combinations of features through closed sets is useful in many applications: to compact emerging patterns of typical descriptive mining applications, to reduce the number of essential rules in classification, and to efficiently learn subgroup descriptions, as demonstrated in real-life subgroup discovery experiments on a high dimensional microarray data set."
            ],
            "keywords": [
                "rule relevancy",
                "closed sets",
                "ROC space",
                "emerging patterns",
                "essential rules",
                "subgroup discovery"
            ],
            "author": [
                "Gemma C Garriga",
                "Petra Kralj",
                "Nada Lavrač"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/garriga08a/garriga08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improved spectral community detection in large heterogeneous networks",
            "abstract": [
                "In this article, we propose and study the performance of spectral community detection for a family of \"α-normalized\" adjacency matrices A, of the type D −α AD −α with D the degree matrix, in heterogeneous dense graph models. We show that the previously used normalization methods based on A or D −1 AD −1 are in general suboptimal in terms of correct recovery rates and, relying on advanced random matrix methods, we prove instead the existence of an optimal value α opt of the parameter α in our generic model; we further provide an online estimation of α opt only based on the node degrees in the graph. Numerical simulations show that the proposed method outperforms state-of-the-art spectral approaches on moderately dense to dense heterogeneous graphs."
            ],
            "keywords": [
                "community detection",
                "random networks",
                "heterogeneous graphs",
                "random matrix theory",
                "spectral clustering"
            ],
            "author": [
                "Tiomoko Hafiz",
                "Romain Couillet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-247/17-247.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistent Nonparametric Tests of Independence",
            "abstract": [
                "Three simple and explicit procedures for testing the independence of two multi-dimensional random variables are described. Two of the associated test statistics (L 1 , log-likelihood) are defined when the empirical distribution of the variables is restricted to finite partitions. A third test statistic is defined as a kernel-based independence measure. Two kinds of tests are provided. Distributionfree strong consistent tests are derived on the basis of large deviation bounds on the test statistics: these tests make almost surely no Type I or Type II error after a random sample size. Asymptotically α-level tests are obtained from the limiting distribution of the test statistics. For the latter tests, the Type I error converges to a fixed non-zero value α, and the Type II error drops to zero, for increasing sample size. All tests reject the null hypothesis of independence if the test statistics become large. The performance of the tests is evaluated experimentally on benchmark data."
            ],
            "keywords": [
                "hypothesis test",
                "independence",
                "L1",
                "log-likelihood",
                "kernel methods",
                "distribution-free consistent test"
            ],
            "author": [
                "Arthur Gretton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/gretton10a/gretton10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Strategy for Stratified Monte Carlo Sampling",
            "abstract": [
                "We consider the problem of stratified sampling for Monte Carlo integration of a random variable. We model this problem in a K-armed bandit, where the arms represent the K strata. The goal is to estimate the integral mean, that is a weighted average of the mean values of the arms. The learner is allowed to sample the variable n times, but it can decide on-line which stratum to sample next. We propose an UCB-type strategy that samples the arms according to an upper bound on their estimated standard deviations. We compare its performance to an ideal sample allocation that knows the standard deviations of the arms. For sub-Gaussian arm distributions, we provide bounds on the total regret: a distributiondependent bound of order poly(λ −1 min) O(n −3/2) 1 that depends on a measure of the disparity λ min of the per stratum variances and a distribution-free bound poly(K) O(n −7/6) that does not. We give similar, but somewhat sharper bounds on a proxy of the regret. The problemindependent bound for this proxy matches its recent minimax lower bound in terms of n up to a log n factor."
            ],
            "keywords": [
                "adaptive sampling",
                "bandit theory",
                "stratified Monte Carlo",
                "minimax strategies",
                "active learning"
            ],
            "author": [
                "Alexandra Carpentier",
                "Remi Munos",
                "Deepmind Google",
                "UK London"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/carpentier15a/carpentier15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting *",
            "abstract": [
                "We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets."
            ],
            "keywords": [
                "kernel methods",
                "inductive logic programming",
                "Prolog",
                "learning from program traces"
            ],
            "author": [
                "Andrea Passerini",
                "Paolo Frasconi",
                "Luc De Raedt",
                "Roland Olsson",
                "Ute Schmid"
            ],
            "ref": "http://www.jmlr.org/papers/volume7/passerini06a/passerini06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Expectation Correction for Smoothed Inference in Switching Linear Dynamical Systems",
            "abstract": [
                "We introduce a method for approximate smoothed inference in a class of switching linear dynamical systems, based on a novel form of Gaussian Sum smoother. This class includes the switching Kalman 'Filter' and the more general case of switch transitions dependent on the continuous latent state. The method improves on the standard Kim smoothing approach by dispensing with one of the key approximations, thus making fuller use of the available future information. Whilst the central assumption required is projection to a mixture of Gaussians, we show that an additional conditional independence assumption results in a simpler but accurate alternative. Our method consists of a single Forward and Backward Pass and is reminiscent of the standard smoothing 'correction' recursions in the simpler linear dynamical system. The method is numerically stable and compares favourably against alternative approximations, both in cases where a single mixture component provides a good posterior approximation, and where a multimodal approximation is required."
            ],
            "keywords": [
                "Gaussian sum smoother",
                "switching Kalman filter",
                "switching linear dynamical system",
                "expectation propagation",
                "expectation correction"
            ],
            "author": [
                "David Barber"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/barber06a/barber06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Margin-based Ranking and an Equivalence between AdaBoost and RankBoost",
            "abstract": [
                "We study boosting algorithms for learning to rank. We give a general margin-based bound for ranking based on covering numbers for the hypothesis space. Our bound suggests that algorithms that maximize the ranking margin will generalize well. We then describe a new algorithm, smooth margin ranking, that precisely converges to a maximum ranking-margin solution. The algorithm is a modification of RankBoost, analogous to \"approximate coordinate ascent boosting.\" Finally, we prove that AdaBoost and RankBoost are equally good for the problems of bipartite ranking and classification in terms of their asymptotic behavior on the training set. Under natural conditions, AdaBoost achieves an area under the ROC curve that is equally as good as RankBoost's; furthermore, RankBoost, when given a specific intercept, achieves a misclassification error that is as good as AdaBoost's. This may help to explain the empirical observations made by Cortes and Mohri, and Caruana and Niculescu-Mizil, about the excellent performance of AdaBoost as a bipartite ranking algorithm, as measured by the area under the ROC curve."
            ],
            "keywords": [
                "ranking",
                "RankBoost",
                "generalization bounds",
                "AdaBoost",
                "area under the ROC curve"
            ],
            "author": [
                "Cynthia Rudin",
                "Robert E Schapire"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/rudin09a/rudin09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hitting and Commute Times in Large Random Neighborhood Graphs",
            "abstract": [
                "In machine learning, a popular tool to analyze the structure of graphs is the hitting time and the commute distance (resistance distance). For two vertices u and v, the hitting time H uv is the expected time it takes a random walk to travel from u to v. The commute distance is its symmetrized version C uv = H uv + H vu. In our paper we study the behavior of hitting times and commute distances when the number n of vertices in the graph tends to infinity. We focus on random geometric graphs (ε-graphs, kNN graphs and Gaussian similarity graphs), but our results also extend to graphs with a given expected degree distribution or Erdős-Rényi graphs with planted partitions. We prove that in these graph families, the suitably rescaled hitting time H uv converges to 1/d v and the rescaled commute time to 1/d u + 1/d v where d u and d v denote the degrees of vertices u and v. In these cases, hitting and commute times do not provide information about the structure of the graph, and their use is discouraged in many machine learning applications."
            ],
            "keywords": [
                "commute distance",
                "resistance",
                "random graph",
                "k-nearest neighbor graph",
                "spectral gap"
            ],
            "author": [
                "Ulrike Von Luxburg",
                "Agnes Radl",
                "Matthias Hein"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/vonluxburg14a/vonluxburg14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Machine Learning with Operational Costs",
            "abstract": [
                "This work proposes a way to align statistical modeling with decision making. We provide a method that propagates the uncertainty in predictive modeling to the uncertainty in operational cost, where operational cost is the amount spent by the practitioner in solving the problem. The method allows us to explore the range of operational costs associated with the set of reasonable statistical models, so as to provide a useful way for practitioners to understand uncertainty. To do this, the operational cost is cast as a regularization term in a learning algorithm's objective function, allowing either an optimistic or pessimistic view of possible costs, depending on the regularization parameter. From another perspective, if we have prior knowledge about the operational cost, for instance that it should be low, this knowledge can help to restrict the hypothesis space, and can help with generalization. We provide a theoretical generalization bound for this scenario. We also show that learning with operational costs is related to robust optimization."
            ],
            "keywords": [
                "statistical learning theory",
                "optimization",
                "covering numbers",
                "decision theory"
            ],
            "author": [
                "Theja Tulabandhula",
                "Cynthia Rudin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/tulabandhula13a/tulabandhula13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks *",
            "abstract": [
                "We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting W be the number of weights and L be the number of layers, we prove that the VC-dimension is O(W L log(W)), and provide examples with VC-dimension Ω(W L log(W/L)). This improves both the previously known upper bounds and lower bounds. In terms of the number U of non-linear units, we prove a tight bound Θ(W U) on the VC-dimension. All of these bounds generalize to arbitrary piecewise linear activation functions, and also hold for the pseudodimensions of these function classes. Combined with previous results, this gives an intriguing range of dependencies of the VC-dimension on depth for networks with different non-linearities: there is no dependence for piecewise-constant, linear dependence for piecewise-linear, and no more than quadratic dependence for general piecewise-polynomial."
            ],
            "keywords": [
                "VC-dimension",
                "pseudodimension",
                "neural networks",
                "ReLU activation function",
                "statistical learning theory"
            ],
            "author": [
                "Peter L Bartlett",
                "Nick Harvey",
                "Christopher Liaw"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-612/17-612.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Measuring Dependence Powerfully and Equitably",
            "abstract": [
                "Given a high-dimensional data set, we often wish to find the strongest relationships within it. A common strategy is to evaluate a measure of dependence on every variable pair and retain the highest-scoring pairs for follow-up. This strategy works well if the statistic used (a) has good power to detect non-trivial relationships, and (b) is equitable, meaning that for some measure of noise it assigns similar scores to equally noisy relationships regardless of relationship type (e.g., linear, exponential, periodic). In this paper, we define and theoretically characterize two new statistics that together yield an efficient approach for obtaining both power and equitability. To do this, we first introduce a new population measure of dependence and show three equivalent ways that it can be viewed, including as a canonical \"smoothing\" of mutual information. We then introduce an efficiently computable consistent estimator of our population measure of dependence, and we empirically establish its equitability on a large class of noisy functional relationships. This new statistic has better bias/variance properties and better runtime complexity than a previous heuristic approach. Next, we derive a second, related statistic whose computation is a trivial side"
            ],
            "keywords": [
                "maximal information coefficient",
                "total information coefficient",
                "equitability",
                "statistical power",
                "mutual information"
            ],
            "author": [
                "Yakir A Reshef",
                "David N Reshef",
                "Hilary K Finucane",
                "Pardis C Sabeti",
                "Michael M Mitzenmacher",
                "Reshef, Finucane, Sabeti Mitzenmacher Reshef"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-308/15-308.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Unsupervised Evaluation and Weighted Aggregation of Ranked Classification Predictions",
            "abstract": [
                "Ensemble methods that aggregate predictions from a set of diverse base learners consistently outperform individual classifiers. Many such popular strategies have been developed in a supervised setting, where the sample labels have been provided to the ensemble algorithm. However, with the rising interest in unsupervised algorithms for machine learning and growing amounts of uncurated data, the reliance on labeled data precludes the application of ensemble algorithms to many real world problems. To this end we develop a new theoretical framework for ensemble learning, the Strategy for Unsupervised Multiple Method Aggregation (SUMMA), that estimates the performances of base classifiers and uses these estimates to form an ensemble classifier. SUMMA also generates an ensemble ranking of samples based on the confidence score it assigns to each sample. We illustrate the performance of SUMMA using a synthetic example as well as two real world problems."
            ],
            "keywords": [
                "Ensemble learning",
                "Ensemble classifier",
                "Unsupervised Learning",
                "AUC",
                "Spectral Decomposition"
            ],
            "author": [
                "Mehmet Eren Ahsen",
                "Robert M Vogel",
                "C Mehmet",
                "Eren Ahsen",
                "Gustavo Stolovitzky"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-094/18-094.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations",
            "abstract": [
                "We put forth a deep learning approach for discovering nonlinear partial differential equations from scattered and potentially noisy observations in space and time. Specifically, we approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. The first network acts as a prior on the unknown solution and essentially enables us to avoid numerical differentiations which are inherently ill-conditioned and unstable. The second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal data-set. We test the effectiveness of our approach for several benchmark problems spanning a number of scientific domains and demonstrate how the proposed framework can help us accurately learn the underlying dynamics and forecast future states of the system. In particular, we study the Burgers', Korteweg-de Vries (KdV), Kuramoto-Sivashinsky, nonlinear Schrödinger, and Navier-Stokes equations."
            ],
            "keywords": [
                "Systems Identification",
                "Data-driven Scientific Discovery",
                "Physics Informed Machine Learning",
                "Predictive Modeling",
                "Nonlinear Dynamics",
                "Big Data"
            ],
            "author": [
                "Maziar Raissi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-046/18-046.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Finite Sample Analysis of the Naive Bayes Classifier *",
            "abstract": [
                "We revisit, from a statistical learning perspective, the classical decision-theoretic problem of weighted expert voting. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Naive Bayes weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. We derive optimality results for our estimates and also establish some structural characterizations. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. Several challenging open problems are posed, and experimental results are provided to illustrate the theory."
            ],
            "keywords": [
                "experts",
                "hypothesis testing",
                "Chernoff-Stein lemma",
                "Neyman-Pearson lemma",
                "naive Bayes",
                "measure concentration"
            ],
            "author": [
                "Daniel Berend",
                "Aryeh Kontorovich"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/berend15a/berend15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization",
            "abstract": [
                "We show that the average stability notion introduced by Kearns and Ron (1999); Bousquet and Elisseeff (2002) is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses. In other words, when analyzing the stability rate of a given algorithm, we may assume the optimal preconditioning of the data. This implies that, at least from a statistical perspective, explicit regularization is not required in order to compensate for ill-conditioned data, which stands in contrast to a widely common approach that includes a regularization for analyzing the sample complexity of generalized linear models. Several important implications of our findings include: a) We demonstrate that the excess risk of empirical risk minimization (ERM) is controlled by the preconditioned stability rate. This immediately yields a relatively short and elegant proof for the fast rates attained by ERM in our context. b) We complement the recent bounds of Hardt et al. (2015) on the stability rate of the Stochastic Gradient Descent algorithm."
            ],
            "keywords": [],
            "author": [
                "Alon Gonen",
                "Shai Shalev-Shwartz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-503/16-503.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Achievability of Asymptotic Minimax Regret by Horizon-Dependent and Horizon-Independent Strategies",
            "abstract": [
                "The normalized maximum likelihood distribution achieves minimax coding (log-loss) regret given a fixed sample size, or horizon, n. It generally requires that n be known in advance. Furthermore, extracting the sequential predictions from the normalized maximum likelihood distribution is computationally infeasible for most statistical models. Several computationally feasible alternative strategies have been devised. We characterize the achievability of asymptotic minimaxity by horizon-dependent and horizon-independent strategies. We prove that no horizon-independent strategy can be asymptotically minimax in the multinomial case. A weaker result is given in the general case subject to a condition on the horizon-dependence of the normalized maximum likelihood. Motivated by these negative results, we demonstrate that an easily implementable Bayes mixture based on a conjugate Dirichlet prior with a simple dependency on n achieves asymptotic minimaxity for all sequences, simplifying earlier similar proposals. Our numerical experiments for the Bernoulli model demonstrate improved finite-sample performance by a number of novel horizon-dependent and horizon-independent algorithms."
            ],
            "keywords": [
                "on-line learning",
                "prediction of individual sequences",
                "normalized maximum likelihood",
                "asymptotic minimax regret",
                "Bayes mixture"
            ],
            "author": [
                "Kazuho Watanabe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/watanabe15a/watanabe15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Universal Kernels",
            "abstract": [
                "In this paper we investigate conditions on the features of a continuous kernel so that it may approximate an arbitrary continuous target function uniformly on any compact subset of the input space. A number of concrete examples are given of kernels with this universal approximating property."
            ],
            "keywords": [
                "density",
                "translation invariant kernels",
                "radial kernels"
            ],
            "author": [
                "Charles A Micchelli",
                "Yuesheng Xu",
                "Haizhang Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/micchelli06a/micchelli06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization *",
            "abstract": [
                "Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-specific and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two significant problems in natural language processing-synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets confirm improved performance compared to the bag of words document representation."
            ],
            "keywords": [
                "feature generation",
                "text classification",
                "background knowledge"
            ],
            "author": [
                "Evgeniy Gabrilovich"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/gabrilovich07a/gabrilovich07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Composite Multiclass Losses",
            "abstract": [
                "We consider loss functions for multiclass prediction problems. We show when a multiclass loss can be expressed as a \"proper composite loss\", which is the composition of a proper loss and a link function. We extend existing results for binary losses to multiclass losses. We subsume results on \"classification calibration\" by relating it to properness. We determine the stationarity condition, Bregman representation, order-sensitivity, and quasi-convexity of multiclass proper losses. We then characterise the existence and uniqueness of the composite representation for multiclass losses. We show how the composite representation is related to other core properties of a loss: mixability, admissibility and (strong) convexity of multiclass losses which we characterise in terms of the Hessian of the Bayes risk. We show that the simple integral representation for binary proper losses can not be extended to multiclass losses but offer concrete guidance regarding how to design different loss functions. The conclusion drawn from these results is that the proper composite representation is a natural and convenient tool for the design of multiclass loss functions."
            ],
            "keywords": [
                "Proper losses",
                "Multiclass losses",
                "Link Functions",
                "Convexity and quasi-convexity of losses",
                "Margin losses",
                "Classification calibration",
                "Parametrisations and representations of loss functions",
                "Admissibility",
                "Mixability",
                "Minimaxity",
                "Superprediction set"
            ],
            "author": [
                "Robert C Williamson",
                "Elodie Vernet",
                "Mark D Reid"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-294/14-294.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Refined Error Bounds for Several Learning Algorithms",
            "abstract": [
                "This article studies the achievable guarantees on the error rates of certain learning algorithms, with particular focus on refining logarithmic factors. Many of the results are based on a general technique for obtaining bounds on the error rates of sample-consistent classifiers with monotonic error regions, in the realizable case. We prove bounds of this type expressed in terms of either the VC dimension or the sample compression size. This general technique also enables us to derive several new bounds on the error rates of general sample-consistent learning algorithms, as well as refined bounds on the label complexity of the CAL active learning algorithm. Additionally, we establish a simple necessary and sufficient condition for the existence of a distribution-free bound on the error rates of all sample-consistent learning rules, converging at a rate inversely proportional to the sample size. We also study learning in the presence of classification noise, deriving a new excess error rate guarantee for general VC classes under Tsybakov's noise condition, and establishing a simple and general necessary and sufficient condition for the minimax excess risk under bounded noise to converge at a rate inversely proportional to the sample size."
            ],
            "keywords": [
                "sample complexity",
                "PAC learning",
                "statistical learning theory",
                "active learning",
                "minimax analysis"
            ],
            "author": [
                "Steve Hanneke"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-655/15-655.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion",
            "abstract": [
                "Recently, Ye (2005) suggested yet another optimization criterion for discriminant analysis and proposed a characterization of the family of solutions to this objective. The characterization, however, merely describes a part of the full solution set, that is, it is not complete and therefore not at all a characterization. This correspondence first gives the correct characterization and afterwards compares it to Ye's."
            ],
            "keywords": [
                "linear discriminant analysis",
                "Fisher criterion",
                "small sample",
                "characterization"
            ],
            "author": [
                "Marco Loog"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/loog07a/loog07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Unsupervised Supervised Learning II: Margin-Based Classification Without Labels",
            "abstract": [
                "Many popular linear classifiers, such as logistic regression, boosting, or SVM, are trained by optimizing a margin-based risk function. Traditionally, these risk functions are computed based on a labeled data set. We develop a novel technique for estimating such risks using only unlabeled data and the marginal label distribution. We prove that the proposed risk estimator is consistent on high-dimensional data sets and demonstrate it on synthetic and real-world data. In particular, we show how the estimate is used for evaluating classifiers in transfer learning, and for training classifiers with no labeled data whatsoever."
            ],
            "keywords": [
                "classification",
                "large margin",
                "maximum likelihood"
            ],
            "author": [
                "Krishnakumar Balasubramanian",
                "Pinar Donmez",
                "Guy Lebanon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/balasubramanian11a/balasubramanian11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On-Line Sequential Bin Packing",
            "abstract": [
                "We consider a sequential version of the classical bin packing problem in which items are received one by one. Before the size of the next item is revealed, the decision maker needs to decide whether the next item is packed in the currently open bin or the bin is closed and a new bin is opened. If the new item does not fit, it is lost. If a bin is closed, the remaining free space in the bin accounts for a loss. The goal of the decision maker is to minimize the loss accumulated over n periods. We present an algorithm that has a cumulative loss not much larger than any strategy in a finite class of reference strategies for any sequence of items. Special attention is payed to reference strategies that use a fixed threshold at each step to decide whether a new bin is opened. Some positive and negative results are presented for this case."
            ],
            "keywords": [
                "bin packing",
                "on-line learning",
                "prediction with expert advice"
            ],
            "author": [
                "András György"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/gyorgy10a/gyorgy10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Proximal Gradient Algorithm for Partially Asynchronous Computer Clusters *",
            "abstract": [
                "With ever growing data volume and model size, an error-tolerant, communication efficient, yet versatile distributed algorithm has become vital for the success of many large-scale machine learning applications. In this work we propose m-PAPG, an implementation of the flexible proximal gradient algorithm in model parallel systems equipped with the partially asynchronous communication protocol. The worker machines communicate asynchronously with a controlled staleness bound s and operate at different frequencies. We characterize various convergence properties of m-PAPG: 1) Under a general non-smooth and non-convex setting, we prove that every limit point of the sequence generated by m-PAPG is a critical point of the objective function; 2) Under an error bound condition of convex objective functions , we prove that the optimality gap decays linearly for every s steps; 3) Under the Kurdyka-Lojasiewicz inequality and a sufficient decrease assumption , we prove that the sequences generated by m-PAPG converge to the same critical point, provided that a proximal Lipschitz condition is satisfied."
            ],
            "keywords": [
                "proximal gradient",
                "distributed system",
                "model parallel",
                "partially asynchronous",
                "machine learning"
            ],
            "author": [
                "Yi Zhou",
                "Yingbin Liang",
                "Yaoliang Yu",
                "Wei Dai",
                "Eric P Xing"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-444/17-444.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximations for Binary Gaussian Process Classification",
            "abstract": [
                "We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches."
            ],
            "keywords": [
                "Gaussian process priors",
                "probabilistic classification",
                "Laplaces's approximation",
                "expectation propagation",
                "variational bounding",
                "mean field methods",
                "marginal likelihood evidence",
                "MCMC"
            ],
            "author": [
                "Hannes Nickisch",
                "Carl Edward Rasmussen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/nickisch08a/nickisch08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Minimax Regression Estimation over Sparse q -Hulls",
            "abstract": [
                "Given a dictionary of M n predictors, in a random design regression setting with n observations, we construct estimators that target the best performance among all the linear combinations of the predictors under a sparse q-norm (0 ≤ q ≤ 1) constraint on the linear coefficients. Besides identifying the optimal rates of convergence, our universal aggregation strategies by model mixing achieve the optimal rates simultaneously over the full range of ≤ q ≤ 1 for any M n and without knowledge of the q-norm of the best linear coefficients to represent the regression function. selection theory that deals with a large number of models."
            ],
            "keywords": [
                "high-dimensional sparse learning",
                "minimax rate of convergence",
                "model selection",
                "optimal aggregation",
                "sparse q -constraint"
            ],
            "author": [
                "Zhan Wang",
                "Sandra Paterlini",
                "Fuchang Gao",
                "Yuhong Yang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/wang14b/wang14b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Weisfeiler-Lehman Graph Kernels",
            "abstract": [
                "In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis."
            ],
            "keywords": [
                "graph kernels",
                "graph classification",
                "similarity measures for graphs",
                "Weisfeiler-Lehman algorithm"
            ],
            "author": [
                "Nino Shervashidze",
                "Pascal Schweitzer",
                "Erik Jan Van Leeuwen",
                "Kurt Mehlhorn",
                "Karsten M Borgwardt",
                "SCHWEITZER, VAN LEEUWEN, MEHLHORN AND BORGWARDT Shervashidze"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/shervashidze11a/shervashidze11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Learning with Integral Operators",
            "abstract": [
                "A large number of learning algorithms, for example, spectral clustering, kernel Principal Components Analysis and many manifold methods are based on estimating eigenvalues and eigenfunctions of operators defined by a similarity function or a kernel, given empirical data. Thus for the analysis of algorithms, it is an important problem to be able to assess the quality of such approximations. The contribution of our paper is twofold: 1. We use a technique based on a concentration inequality for Hilbert spaces to provide new much simplified proofs for a number of results in spectral approximation. 2. Using these methods we provide several new results for estimating spectral properties of the graph Laplacian operator extending and strengthening results from von Luxburg et al. (2008)."
            ],
            "keywords": [
                "spectral convergence",
                "empirical operators",
                "learning integral operators",
                "perturbation methods"
            ],
            "author": [
                "Lorenzo Rosasco",
                "Mit Edu",
                "Mikhail Belkin",
                "Ernesto De"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/rosasco10a/rosasco10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bayes Optimal Approach for Partitioning the Values of Categorical Attributes",
            "abstract": [
                "In supervised machine learning, the partitioning of the values (also called grouping) of a categorical attribute aims at constructing a new synthetic attribute which keeps the information of the initial attribute and reduces the number of its values. In this paper, we propose a new grouping method MODL 1 founded on a Bayesian approach. The method relies on a model space of grouping models and on a prior distribution defined on this model space. This results in an evaluation criterion of grouping, which is minimal for the most probable grouping given the data, i.e. the Bayes optimal grouping. We propose new super-linear optimization heuristics that yields near-optimal groupings. Extensive comparative experiments demonstrate that the MODL grouping method builds high quality groupings in terms of predictive quality, robustness and small number of groups."
            ],
            "keywords": [
                "data preparation",
                "grouping",
                "Bayesianism",
                "model selection",
                "classification",
                "naïve Bayes"
            ],
            "author": [
                "Marc ©2004",
                "Marc Boullé",
                "France Telecom"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/boulle05a/boulle05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Boosting",
            "abstract": [
                "We propose Sparse Boosting (the SparseL Boost algorithm), a variant on boosting with the squared error loss. SparseL 2 Boost yields sparser solutions than the previously proposed L 2 Boosting by minimizing some penalized L 2-loss functions, the FPE model selection criteria, through smallstep gradient descent. Although boosting may give already relatively sparse solutions, for example corresponding to the soft-thresholding estimator in orthogonal linear models, there is sometimes a desire for more sparseness to increase prediction accuracy and ability for better variable selection: such goals can be achieved with SparseL Boost. We prove an equivalence of SparseL Boost to Breiman's nonnegative garrote estimator for orthogonal linear models and demonstrate the generic nature of SparseL Boost for nonparametric interaction modeling. For an automatic selection of the tuning parameter in SparseL 2 Boost we propose to employ the gMDL model selection criterion which can also be used for early stopping of L 2 Boosting. Consequently, we can select between SparseL 2 Boost and L 2 Boosting by comparing their gMDL scores."
            ],
            "keywords": [
                "lasso",
                "minimum description length (MDL)",
                "model selection",
                "nonnegative garrote",
                "regression"
            ],
            "author": [
                "Peter Bühlmann",
                "Yoram Singer",
                "Larry Wasserman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/buehlmann06a/buehlmann06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Computational and Theoretical Analysis of Null Space and Orthogonal Linear Discriminant Analysis",
            "abstract": [
                "Dimensionality reduction is an important pre-processing step in many applications. Linear discriminant analysis (LDA) is a classical statistical approach for supervised dimensionality reduction. It aims to maximize the ratio of the between-class distance to the within-class distance, thus maximizing the class discrimination. It has been used widely in many applications. However, the classical LDA formulation requires the nonsingularity of the scatter matrices involved. For undersampled problems, where the data dimensionality is much larger than the sample size, all scatter matrices are singular and classical LDA fails. Many extensions, including null space LDA (NLDA) and orthogonal LDA (OLDA), have been proposed in the past to overcome this problem. NLDA aims to maximize the between-class distance in the null space of the within-class scatter matrix, while OLDA computes a set of orthogonal discriminant vectors via the simultaneous diagonalization of the scatter matrices. They have been applied successfully in various applications. In this paper, we present a computational and theoretical analysis of NLDA and OLDA. Our main result shows that under a mild condition which holds in many applications involving highdimensional data, NLDA is equivalent to OLDA. We have performed extensive experiments on various types of data and results are consistent with our theoretical analysis. We further apply the regularization to OLDA. The algorithm is called regularized OLDA (or ROLDA for short). An efficient algorithm is presented to estimate the regularization value in ROLDA. A comparative study on classification shows that ROLDA is very competitive with OLDA. This confirms the effectiveness of the regularization in ROLDA."
            ],
            "keywords": [
                "linear discriminant analysis",
                "dimensionality reduction",
                "null space",
                "orthogonal matrix",
                "regularization"
            ],
            "author": [
                "Jieping Ye",
                "Tao Xiong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/ye06a/ye06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion",
            "abstract": [
                "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations."
            ],
            "keywords": [
                "deep learning",
                "unsupervised feature learning",
                "deep belief networks",
                "autoencoders",
                "denoising"
            ],
            "author": [
                "Pascal Vincent"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/vincent10a/vincent10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiparameter Persistence Landscapes",
            "abstract": [
                "An important problem in the field of Topological Data Analysis is defining topological summaries which can be combined with traditional data analytic tools. In recent work Bubenik introduced the persistence landscape, a stable representation of persistence diagrams amenable to statistical analysis and machine learning tools. In this paper we generalise the persistence landscape to multiparameter persistence modules providing a stable representation of the rank invariant. We show that multiparameter landscapes are stable with respect to the interleaving distance and persistence weighted Wasserstein distance, and that the collection of multiparameter landscapes faithfully represents the rank invariant. Finally we provide example calculations and statistical tests to demonstrate a range of potential applications and how one can interpret the landscapes associated to a multiparameter module."
            ],
            "keywords": [
                "Topological Data Analysis",
                "Multiparameter Persistence",
                "Persistence Landscapes",
                "Machine Learning",
                "Statistical Topology"
            ],
            "author": [
                "Oliver Vipond"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-054/19-054.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rank Determination for Low-Rank Data Completion",
            "abstract": [
                "Recently, fundamental conditions on the sampling patterns have been obtained for finite completability of low-rank matrices or tensors given the corresponding ranks. In this paper, we consider the scenario where the rank is not given and we aim to approximate the unknown rank based on the location of sampled entries and some given completion. We consider a number of data models, including single-view matrix, multi-view matrix, CP tensor, tensor-train tensor and Tucker tensor. For each of these data models, we provide an upper bound on the rank when an arbitrary low-rank completion is given. We characterize these bounds both deterministically, i.e., with probability one given that the sampling pattern satisfies certain combinatorial properties, and probabilistically, i.e., with high probability given that the sampling probability is above some threshold. Moreover, for both single-view matrix and CP tensor, we are able to show that the obtained upper bound is exactly equal to the unknown rank if the lowest-rank completion is given. Furthermore, we provide numerical experiments for the case of single-view matrix, where we use nuclear norm minimization to find a low-rank completion of the sampled data and we observe that in most of the cases the proposed upper bound on the rank is equal to the true rank."
            ],
            "keywords": [
                "Low-rank data completion",
                "rank estimation",
                "tensor",
                "matrix",
                "manifold",
                "Tucker rank",
                "tensor-train rank",
                "CP rank",
                "multi-view matrix"
            ],
            "author": [
                "Morteza Ashraphijuo",
                "Xiaodong Wang",
                "Vaneet Aggarwal"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-375/17-375.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning by Unsupervised Nonlinear Diffusion",
            "abstract": [
                "This paper proposes and analyzes a novel clustering algorithm, called learning by unsupervised nonlinear diffusion (LUND), that combines graph-based diffusion geometry with techniques based on density and mode estimation. LUND is suitable for data generated from mixtures of distributions with densities that are both multimodal and supported near nonlinear sets. A crucial aspect of this algorithm is the use of time of a data-adapted diffusion process, and associated diffusion distances, as a scale parameter that is different from the local spatial scale parameter used in many clustering algorithms. We prove estimates for the behavior of diffusion distances with respect to this time parameter under a flexible nonparametric data model, identifying a range of times in which the mesoscopic equilibria of the underlying process are revealed, corresponding to a gap between within-cluster and between-cluster diffusion distances. These structures may be missed by the top eigenvectors of the graph Laplacian, commonly used in spectral clustering. This analysis is leveraged to prove sufficient conditions guaranteeing the accuracy of LUND. We implement LUND and confirm its theoretical properties on illustrative data sets, demonstrating its theoretical and empirical advantages over both spectral and density-based clustering."
            ],
            "keywords": [
                "unsupervised learning",
                "clustering",
                "spectral graph theory",
                "manifold learning",
                "diffusion geometry"
            ],
            "author": [
                "Mauro Maggioni",
                "James M Murphy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-873/18-873.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stable Regression: On the Power of Optimization over Randomization in Training Regression Problems",
            "abstract": [
                "We investigate and ultimately suggest remediation to the widely held belief that the best way to train regression models is via random assignment of our data to training and validation sets. In particular, we show that taking a robust optimization approach, and optimally selecting such training and validation sets, leads to models that not only perform significantly better than their randomly constructed counterparts in terms of prediction error, but more importantly, are considerably more stable in the sense that the standard deviation of the resulting predictions, as well as of the model coefficients, is greatly reduced. Moreover, we show that this optimization approach to training is far more effective at recovering the true support of a given data set, i.e., correctly identifying important features while simultaneously excluding spurious ones. We further compare the robust optimization approach to cross validation and find that optimization continues to have a performance edge albeit smaller. Finally, we show that this optimization approach to training is equivalent to building models that are robust to all subpopulations in the data, and thus in particular are robust to the hardest subpopulation, which leads to interesting domain specific interpretations through the use of optimal classification trees. The proposed robust optimization algorithm is efficient and scales training to essentially any desired size."
            ],
            "keywords": [
                "stability",
                "randomization",
                "optimization",
                "regression",
                "robustness",
                "interpretability"
            ],
            "author": [
                "Dimitris Bertsimas",
                "Ivan Paskov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-408/19-408.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-scale Classification using Localized Spatial Depth",
            "abstract": [
                "In this article, we develop and investigate a new classifier based on features extracted using spatial depth. Our construction is based on fitting a generalized additive model to posterior probabilities of different competing classes. To cope with possible multi-modal as well as non-elliptic nature of the population distribution, we also develop a localized version of spatial depth and use that with varying degrees of localization to build the classifier. Final classification is done by aggregating several posterior probability estimates, each of which is obtained using this localized spatial depth with a fixed scale of localization. The proposed classifier can be conveniently used even when the dimension of the data is larger than the sample size, and its good discriminatory power for such data has been established using theoretical as well as numerical results."
            ],
            "keywords": [
                "Bayes classifier",
                "elliptic distributions",
                "generalized additive models",
                "HDLSS asymptotics",
                "uniform strong consistency",
                "weighted aggregation of posteriors"
            ],
            "author": [
                "Subhajit Dutta"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-113/16-113.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Emergence of Invariance and Disentanglement in Deep Representations",
            "abstract": [
                "Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error."
            ],
            "keywords": [
                "Representation learning",
                "PAC-Bayes",
                "information bottleneck",
                "flat minima",
                "generalization",
                "invariance",
                "independence"
            ],
            "author": [
                "Alessandro Achille",
                "Stefano Soatto"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-646/17-646.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sufficient Dimensionality Reduction",
            "abstract": [
                "Dimensionality reduction of empirical co-occurrence data is a fundamental problem in unsupervised learning. It is also a well studied problem in statistics known as the analysis of cross-classified data. One principled approach to this problem is to represent the data in low dimension with minimal loss of (mutual) information contained in the original data. In this paper we introduce an information theoretic nonlinear method for finding such a most informative dimension reduction. In contrast with previously introduced clustering based approaches, here we extract continuous feature functions directly from the co-occurrence matrix. In a sense, we automatically extract functions of the variables that serve as approximate sufficient statistics for a sample of one variable about the other one. Our method is different from dimensionality reduction methods which are based on a specific, sometimes arbitrary, metric or embedding. Another interpretation of our method is as generalized-multi-dimensional-non-linear regression, where rather than fitting one regression function through two dimensional data, we extract d-regression functions whose expectation values capture the information among the variables. It thus presents a new learning paradigm that unifies aspects from both supervised and unsupervised learning. The resulting dimension reduction can be described by two conjugate d-dimensional differential manifolds that are coupled through Maximum Entropy I-projections. The Riemannian metrics of these manifolds are determined by the observed expectation values of our extracted features. Following this geometric interpretation we present an iterative information projection algorithm for finding such features and prove its convergence. Our algorithm is similar to the method of \"association analysis\" in statistics, though the feature extraction context as well as the information theoretic and geometric interpretation are new. The algorithm is illustrated by various synthetic co-occurrence data. It is then demonstrated for text categorization and information retrieval and proves effective in selecting a small set of features, often improving performance over the original feature set."
            ],
            "keywords": [
                "Feature Extraction",
                "Maximum Entropy",
                "Information Geometry",
                "Mutual Information",
                "Association Analysis"
            ],
            "author": [
                "Amir Globerson",
                "Naftali Tishby",
                "Isabelle Guyon",
                "André Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/globerson03a/globerson03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Halfspaces with Malicious Noise",
            "abstract": [
                "We give new algorithms for learning halfspaces in the challenging malicious noise model, where an adversary may corrupt both the labels and the underlying distribution of examples. Our algorithms can tolerate malicious noise rates exponentially larger than previous work in terms of the dependence on the dimension n, and succeed for the fairly broad class of all isotropic log-concave distributions. We give poly(n, 1/ε)-time algorithms for solving the following problems to accuracy ε: • Learning origin-centered halfspaces in R n with respect to the uniform distribution on the unit ball with malicious noise rate η = Ω(ε 2 / log(n/ε)). (The best previous result was Ω(ε/(n log(n/ε)) 1/4).) • Learning origin-centered halfspaces with respect to any isotropic logconcave distribution on R n with malicious noise rate η = Ω(ε 3 / log 2 (n/ε)). This is the first efficient algorithm for learning under isotropic log-concave distributions in the presence of malicious noise. We also give a poly(n, 1/ε)-time algorithm for learning origin-centered halfspaces under any isotropic log-concave distribution on R n in the presence of adversarial label noise at rate η = Ω(ε 3 / log(1/ε)). In the adversarial label noise setting (or agnostic model), labels can be noisy, but not example points themselves. Previous results could handle η = Ω(ε) but had running time exponential in an unspecified function of 1/ε. Our analysis crucially exploits both concentration and anti-concentration properties of isotropic log-concave distributions. Our algorithms combine an iterative outlier removal procedure using Principal Component Analysis together with \"smooth\" boosting."
            ],
            "keywords": [
                "PAC learning",
                "noise tolerance",
                "malicious noise",
                "agnostic learning",
                "label noise",
                "halfspace learning",
                "linear classifiers"
            ],
            "author": [
                "Adam R Klivans",
                "Philip M Long",
                "Rocco A Servedio",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/klivans09a/klivans09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Approximation Methods for Speech Recognition",
            "abstract": [
                "We study the performance of kernel methods on the acoustic modeling task for automatic speech recognition, and compare their performance to deep neural networks (DNNs). To scale the kernel methods to large data sets, we use the random Fourier feature method of Rahimi and Recht (2007). We propose two novel techniques for improving the performance of kernel acoustic models. First, we propose a simple but effective feature selection method which reduces the number of random features required to attain a fixed level of performance. Second, we present a number of metrics which correlate strongly with speech recognition performance when computed on the heldout set; we attain improved performance by using these metrics to decide when to stop training. Additionally, we show that the linear bottleneck method of Sainath et al. (2013a) improves the performance of our kernel models significantly, in addition to speeding up training and making the models more compact. Leveraging these three methods, the kernel methods attain token error rates between 0.5% better and 0.1% worse than fully-connected DNNs across four speech recognition data sets, including the TIMIT and Broadcast News benchmark tasks."
            ],
            "keywords": [
                "kernel methods",
                "deep neural networks",
                "acoustic modeling",
                "automatic speech recognition",
                "feature selection",
                "logistic regression"
            ],
            "author": [
                "Avner May",
                "Dong Guo",
                "Kuan Liu",
                "Linxi Fan",
                "Michael Collins",
                "Daniel Hsu",
                "Brian Kingsbury",
                "Sha Fei",
                "Alireza Bagheri Garakani",
                "Zhiyun Lu",
                "Aurélien Bellet",
                "Michael Picheny"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-026/17-026.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nesterov's Acceleration for Approximate Newton",
            "abstract": [
                "Optimization plays a key role in machine learning. Recently, stochastic second-order methods have attracted considerable attention because of their low computational cost in each iteration. However, these methods might suffer from poor performance when the Hessian is hard to be approximate well in a computation-efficient way. To overcome this dilemma, we resort to Nesterov's acceleration to improve the convergence performance of these second-order methods and propose accelerated approximate Newton. We give the theoretical convergence analysis of accelerated approximate Newton and show that Nesterov's acceleration can improve the convergence rate. Accordingly, we propose an accelerated regularized sub-sampled Newton (ARSSN) which performs much better than the conventional regularized sub-sampled Newton empirically and theoretically. Moreover, we show that ARSSN has better performance than classical first-order methods empirically."
            ],
            "keywords": [
                "Nesterov's Acceleration",
                "Approximate Newton",
                "Stochastic Second-order"
            ],
            "author": [
                "Haishan Ye",
                "Luo Luo",
                "Zhihua Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-265/19-265.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Influence Maximization for Multiple Products in Continuous-Time Diffusion Networks",
            "abstract": [
                "A typical viral marketing model identifies influential users in a social network to maximize a single product adoption assuming unlimited user attention, campaign budgets, and time. In reality, multiple products need campaigns, users have limited attention, convincing users incurs costs, and advertisers have limited budgets and expect the adoptions to be maximized soon. Facing these user, monetary, and timing constraints, we formulate the problem as a submodular maximization task in a continuous-time diffusion model under the intersection of one matroid and multiple knapsack constraints. We propose a randomized algorithm estimating the user influence 1 in a network (|V| nodes, |E| edges) to an accuracy of with n = O(1/ 2) randomizations andÕ(n|E| + n|V|) computations. By exploiting the influence estimation algorithm as a subroutine, we develop an adaptive threshold greedy algorithm achieving an approximation factor k a /(2 + 2k) of the optimal when k a out of the k knapsack constraints are active. Extensive experiments on networks of millions of nodes demonstrate that the proposed algorithms achieve the state-of-the-art in terms of effectiveness and scalability."
            ],
            "keywords": [
                "Influence Maximization",
                "Influence Estimation",
                "Continuous-time Diffusion Model",
                "Matroid",
                "Knapsack"
            ],
            "author": [
                "Nan Du",
                "Yingyu Liang",
                "Manuel Gomez-Rodriguez",
                "Le Song",
                "Maria-Florina Balcan",
                "Hongyuan Zha"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-400/14-400.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-dimensional Gaussian graphical models on network-linked data",
            "abstract": [
                "Graphical models are commonly used to represent conditional dependence relationships between variables. There are multiple methods available for exploring them from highdimensional data, but almost all of them rely on the assumption that the observations are independent and identically distributed. At the same time, observations connected by a network are becoming increasingly common, and tend to violate these assumptions. Here we develop a Gaussian graphical model for observations connected by a network with potentially different mean vectors, varying smoothly over the network. We propose an efficient estimation algorithm and demonstrate its effectiveness on both simulated and real data, obtaining meaningful and interpretable results on a statistics coauthorship network. We also prove that our method estimates both the inverse covariance matrix and the corresponding graph structure correctly under the assumption of network \"cohesion\", which refers to the empirically observed phenomenon of network neighbors sharing similar traits."
            ],
            "keywords": [
                "High-dimensional statistics",
                "Gaussian graphical model",
                "network analysis",
                "network cohesion",
                "statistical learning"
            ],
            "author": [
                "Tianxi Li",
                "Cheng Qian",
                "Ji Zhu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-563/19-563.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Regularization of Noisy Point Clouds: Improved Global Geometric Estimates and Data Analysis",
            "abstract": [
                "Several data analysis techniques employ similarity relationships between data points to uncover the intrinsic dimension and geometric structure of the underlying data-generating mechanism. In this paper we work under the model assumption that the data is made of random perturbations of feature vectors lying on a low-dimensional manifold. We study two questions: how to define the similarity relationships over noisy data points, and what is the resulting impact of the choice of similarity in the extraction of global geometric information from the underlying manifold. We provide concrete mathematical evidence that using a local regularization of the noisy data to define the similarity improves the approximation of the hidden Euclidean distance between unperturbed points. Furthermore, graph-based objects constructed with the locally regularized similarity function satisfy better error bounds in their recovery of global geometric ones. Our theory is supported by numerical experiments that demonstrate that the gain in geometric understanding facilitated by local regularization translates into a gain in classification accuracy in simulated and real data."
            ],
            "keywords": [
                "manifold denoising",
                "metric estimation",
                "spectral convergence",
                "graph Laplacian"
            ],
            "author": [
                "García Nicolás",
                "Daniel Sanz-Alonso",
                "Ruiyi Yang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-261/19-261.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse PCA via Covariance Thresholding",
            "abstract": [
                "In sparse principal component analysis we are given noisy observations of a low-rank matrix of dimension n × p and seek to reconstruct it under additional sparsity assumptions. In particular, we assume here each of the principal components v 1 ,. .. , v r has at most s 0 non-zero entries. We are particularly interested in the high dimensional regime wherein p is comparable to, or even much larger than n. In an influential paper, Johnstone and Lu (2004) introduced a simple algorithm that estimates the support of the principal vectors v 1 ,. .. , v r by the largest entries in the diagonal of the empirical covariance. This method can be shown to identify the correct support with high probability if s 0 ≤ K 1 n/ log p, and to fail with high probability if s 0 ≥ K 2 n/ log p for two constants 0 < K 1 , K 2 < ∞. Despite a considerable amount of work over the last ten years, no practical algorithm exists with provably better support recovery guarantees. Here we analyze a covariance thresholding algorithm that was recently proposed by Krauthgamer, Nadler, Vilenchik, et al. (2015). On the basis of numerical simulations (for the rank-one case), these authors conjectured that covariance thresholding correctly recover the support with high probability for s 0 ≤ K √ n (assuming n of the same order as p). We"
            ],
            "keywords": [],
            "author": [
                "Yash Deshpande",
                "Andrea Montanari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-160/15-160.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PC Algorithm for Nonparanormal Graphical Models",
            "abstract": [
                "The PC algorithm uses conditional independence tests for model selection in graphical modeling with acyclic directed graphs. In Gaussian models, tests of conditional independence are typically based on Pearson correlations, and high-dimensional consistency results have been obtained for the PC algorithm in this setting. Analyzing the error propagation from marginal to partial correlations, we prove that high-dimensional consistency carries over to a broader class of Gaussian copula or nonparanormal models when using rank-based measures of correlation. For graph sequences with bounded degree, our consistency result is as strong as prior Gaussian results. In simulations, the 'Rank PC' algorithm works as well as the 'Pearson PC' algorithm for normal data and considerably better for non-normal data, all the while incurring a negligible increase of computation time. While our interest is in the PC algorithm, the presented analysis of error propagation could be applied to other algorithms that test the vanishing of low-order partial correlations."
            ],
            "keywords": [
                "Gaussian copula",
                "graphical model",
                "model selection",
                "multivariate normal distribution",
                "nonparanormal distribution"
            ],
            "author": [
                "Harris Naftali",
                "Stanford Edu",
                "Mathias Drton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/harris13a/harris13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Structure Identification With Greedy Search",
            "abstract": [
                "In this paper we prove the so-called \"Meek Conjecture\". In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G = H. As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a twophase greedy search algorithm that-when applied to a particular sparsely-connected search space-provably identifies a perfect map of the generative distribution if that perfect map is a DAG. We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored efficiently using local functions of the nodes in the domain. Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes."
            ],
            "keywords": [],
            "author": [
                "David Maxwell Chickering"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/chickering02b/chickering02b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Persistence Images: A Stable Vector Representation of Persistent Homology",
            "abstract": [
                "Many data sets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a data set. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finitedimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The"
            ],
            "keywords": [
                "topological data analysis",
                "persistent homology",
                "persistence images",
                "machine learning",
                "dynamical systems"
            ],
            "author": [
                "Henry Adams",
                "Tegan Emerson",
                "Michael Kirby",
                "Rachel Neville",
                "Chris Peterson",
                "Patrick Shipman",
                "Eric Hanson",
                "Francis Motta",
                "Lori Ziegelmeier"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-337/16-337.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Inference via Low-Dimensional Couplings",
            "abstract": [
                "We investigate the low-dimensional structure of deterministic transformations between random variables, i.e., transport maps between probability measures. In the context of statistics and machine learning, these transformations can be used to couple a tractable \"reference\" measure (e.g., a standard Gaussian) with a target measure of interest. Direct simulation from the desired measure can then be achieved by pushing forward reference samples through the map. Yet characterizing such a map-e.g., representing and evaluating it-grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of low-dimensional couplings, induced by transport maps that are sparse and/or decomposable. Our analysis not only facilitates the construction of transformations in high-dimensional settings, but also suggests new inference methodologies for continuous non-Gaussian graphical models. For instance, in the context of nonlinear state-space models, we describe new variational algorithms for filtering, smoothing, and sequential parameter inference. These algorithms can be understood as the natural generalization-to the non-Gaussian case-of the square-root Rauch-Tung-Striebel Gaussian smoother."
            ],
            "keywords": [
                "transport map",
                "variational inference",
                "graphical models",
                "sparsity",
                "state-space models",
                "joint parameter and state estimation"
            ],
            "author": [
                "Alessio Spantini",
                "Daniele Bigoni",
                "Youssef Marzouk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-747/17-747.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines Don Hush",
            "abstract": [
                "We describe polynomial-time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classifiers. These algorithms employ a two-stage process where the first stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an approximate dual solution with accuracy (2 √ 2K n + 8 √ λ) −2 λε 2 p to an approximate primal solution with accuracy ε p where n is the number of data samples, K n is the maximum kernel value over the data and λ > 0 is the SVM regularization parameter. For the first stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for τ-rate certifying decomposition algorithms we establish the optimality of τ = 1/(n − 1). In addition we extend the recent τ = 1/(n − 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the τ = 1/(n − 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the τ-rate certifying property of these algorithms to produce new stopping rules that are computationally efficient and that guarantee a specified accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classification problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n 2) bound on the overall run time for the first stage. Combining the first and second stages gives an overall run time of O(n 2 (c k + 1)) where c k is an upper bound on the computation to perform a kernel evaluation. Pseudocode is presented for a complete algorithm that inputs an accuracy ε p and produces an approximate solution that satisfies this accuracy in low order polynomial time. Experiments are included to illustrate the new stopping rules and to compare the Simon and composite decomposition algorithms."
            ],
            "keywords": [
                "quadratic programming",
                "decomposition algorithms",
                "approximation algorithms",
                "support vector machines"
            ],
            "author": [
                "Patrick Kelly",
                "Clint Scovel",
                "Ingo Steinwart"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/hush06a/hush06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-Dimensional Poisson Structural Equation Model Learning via 1 -Regularized Regression",
            "abstract": [
                "In this paper, we develop a new approach to learning high-dimensional Poisson structural equation models from only observational data without strong assumptions such as faithfulness and a sparse moralized graph. A key component of our method is to decouple the ordering estimation or parent search where the problems can be efficiently addressed using 1regularized regression and the moments relation. We show that sample size n = Ω(d 2 log 9 p) is sufficient for our polynomial time Moments Ratio Scoring (MRS) algorithm to recover the true directed graph, where p is the number of nodes and d is the maximum indegree. We verify through simulations that our algorithm is statistically consistent in the highdimensional p > n setting, and performs well compared to state-of-the-art ODS, GES, and MMHC algorithms. We also demonstrate through multivariate real count data that our MRS algorithm is well-suited to estimating DAG models for multivariate count data in comparison to other methods used for discrete data."
            ],
            "keywords": [
                "Bayesian Networks",
                "Directed Acyclic Graph",
                "Identifiability",
                "Structure Learning",
                "1 -Regularization",
                "Multivariate Count Distribution"
            ],
            "author": [
                "Gunwoong Park",
                "Sion Park"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-819/18-819.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Rotation Ensembles",
            "abstract": [
                "In machine learning, ensemble methods combine the predictions of multiple base learners to construct more accurate aggregate predictions. Established supervised learning algorithms inject randomness into the construction of the individual base learners in an effort to promote diversity within the resulting ensembles. An undesirable side effect of this approach is that it generally also reduces the accuracy of the base learners. In this paper, we introduce a method that is simple to implement yet general and effective in improving ensemble diversity with only modest impact on the accuracy of the individual base learners. By randomly rotating the feature space prior to inducing the base learners, we achieve favorable aggregate predictions on standard data sets compared to state of the art ensemble methods, most notably for tree-based ensembles, which are particularly sensitive to rotation."
            ],
            "keywords": [
                "feature rotation",
                "ensemble diversity",
                "smooth decision boundary"
            ],
            "author": [
                "Rico Blaser",
                "Piotr Fryzlewicz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/blaser16a/blaser16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency, Breakdown Robustness, and Algorithms for Robust Improper Maximum Likelihood Clustering",
            "abstract": [
                "The robust improper maximum likelihood estimator (RIMLE) is a new method for robust multivariate clustering finding approximately Gaussian clusters. It maximizes a pseudolikelihood defined by adding a component with improper constant density for accommodating outliers to a Gaussian mixture. A special case of the RIMLE is MLE for multivariate finite Gaussian mixture models. In this paper we treat existence, consistency, and breakdown theory for the RIMLE comprehensively. RIMLE's existence is proved under non-smooth covariance matrix constraints. It is shown that these can be implemented via a computationally feasible Expectation-Conditional Maximization algorithm."
            ],
            "keywords": [
                "Robustness",
                "Improper density",
                "Mixture models",
                "Model-based clustering",
                "Maximum likelihood",
                "ECM-algorithm"
            ],
            "author": [
                "Pietro Coretto"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-382/16-382.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Representer Theorem for Deep Kernel Learning",
            "abstract": [
                "In this paper we provide a finite-sample and an infinite-sample representer theorem for the concatenation of (linear combinations of) kernel functions of reproducing kernel Hilbert spaces. These results serve as mathematical foundation for the analysis of machine learning algorithms based on compositions of functions. As a direct consequence in the finitesample case, the corresponding infinite-dimensional minimization problems can be recast into (nonlinear) finite-dimensional minimization problems, which can be tackled with nonlinear optimization algorithms. Moreover, we show how concatenated machine learning problems can be reformulated as neural networks and how our representer theorem applies to a broad class of state-of-the-art deep learning methods."
            ],
            "keywords": [
                "deep kernel learning",
                "representer theorem",
                "artificial neural networks",
                "multilayer kernel",
                "regularized least-squares regression"
            ],
            "author": [
                "Bastian Bohn",
                "Michael Griebel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-621/17-621.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generative Adversarial Nets for Robust Scatter Estimation: A Proper Scoring Rule Perspective",
            "abstract": [
                "Robust covariance matrix estimation is a fundamental task in statistics. The recent discovery on the connection between robust estimation and generative adversarial nets (GANs) by Gao et al. (2019) suggests that it is possible to compute depth-like robust estimators using similar techniques that optimize GANs. In this paper, we introduce a general learning via classification framework based on the notion of proper scoring rules. This framework allows us to understand both matrix depth function, a technique of rateoptimal robust estimation, and various GANs through the lens of variational approximations of f-divergences induced by proper scoring rules. We then propose a new class of robust covariance matrix estimators in this framework by carefully constructing discriminators with appropriate neural network structures. These estimators are proved to achieve the minimax rate of covariance matrix estimation under Huber's contamination model. The results are also extended to robust scatter estimation for elliptical distributions. Our numerical results demonstrate the good performance of the proposed procedures under various settings against competitors in the literature."
            ],
            "keywords": [
                "robust statistics",
                "neural networks",
                "minimax rate",
                "data depth",
                "contamination model",
                "GAN"
            ],
            "author": [
                "Chao Gao",
                "Yuan Yao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-462/19-462.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Joint Causal Inference from Multiple Contexts",
            "abstract": [
                "The gold standard for discovering causal relations is by means of experimentation. Over the last decades, alternative methods have been proposed that can infer causal relations between variables from certain statistical patterns in purely observational data. We introduce Joint Causal Inference (JCI), a novel approach to causal discovery from multiple data sets from different contexts that elegantly unifies both approaches. JCI is a causal modeling framework rather than a specific algorithm, and it can be implemented using any causal discovery algorithm that can take into account certain background knowledge. JCI can deal with different types of interventions (e.g., perfect, imperfect, stochastic, etc.) in a unified fashion, and does not require knowledge of intervention targets or types in case of interventional data. We explain how several well-known causal discovery algorithms can be seen as addressing special cases of the JCI framework, and we also propose novel implementations that extend existing causal discovery methods for purely observational data to the JCI setting. We evaluate different JCI implementations on synthetic data and on flow cytometry protein expression data and conclude that JCI implementations can considerably outperform state-of-the-art causal discovery algorithms."
            ],
            "keywords": [
                "causal discovery",
                "causal modeling",
                "causal inference",
                "observational and experimental data",
                "interventions",
                "randomized controlled trials"
            ],
            "author": [
                "Joris M Mooij",
                "Sara Magliacane",
                "Tom Claassen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/17-123/17-123.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Task Learning for Classification with Dirichlet Process Priors",
            "abstract": [
                "Consider the problem of learning logistic-regression models for multiple classification tasks, where the training data set for each task is not drawn from the same statistical distribution. In such a multi-task learning (MTL) scenario, it is necessary to identify groups of similar tasks that should be learned jointly. Relying on a Dirichlet process (DP) based statistical model to learn the extent of similarity between classification tasks, we develop computationally efficient algorithms for two different forms of the MTL problem. First, we consider a symmetric multi-task learning (SMTL) situation in which classifiers for multiple tasks are learned jointly using a variational Bayesian (VB) algorithm. Second, we consider an asymmetric multi-task learning (AMTL) formulation in which the posterior density function from the SMTL model parameters (from previous tasks) is used as a prior for a new task: this approach has the significant advantage of not requiring storage and use of all previous data from prior tasks. The AMTL formulation is solved with a simple Markov Chain Monte Carlo (MCMC) construction. Experimental results on two real life MTL problems indicate that the proposed algorithms: (a) automatically identify subgroups of related tasks whose training data appear to be drawn from similar distributions; and (b) are more accurate than simpler approaches such as single-task learning, pooling of data across all tasks, and simplified approximations to DP."
            ],
            "keywords": [
                "classification",
                "hierarchical Bayesian models",
                "Dirichlet process"
            ],
            "author": [
                "Ya Xue",
                "Xuejun Liao",
                "Lawrence Carin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/xue07a/xue07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large Scale Online Kernel Learning",
            "abstract": [
                "In this paper, we present a new framework for large scale online kernel learning, making kernel methods efficient and scalable for large-scale online learning applications. Unlike the regular budget online kernel learning scheme that usually uses some budget maintenance strategies to bound the number of support vectors, our framework explores a completely different approach of kernel functional approximation techniques to make the subsequent online learning task efficient and scalable. Specifically, we present two different online kernel machine learning algorithms: (i) Fourier Online Gradient Descent (FOGD) algorithm that applies the random Fourier features for approximating kernel functions; and (ii) Nyström Online Gradient Descent (NOGD) algorithm that applies the Nyström method to approximate large kernel matrices. We explore these two approaches to tackle three online learning tasks: binary classification, multi-class classification, and regression. The encouraging results of our experiments on large-scale datasets validate the effectiveness and efficiency of the proposed algorithms, making them potentially more practical than the family of existing budget online kernel learning approaches."
            ],
            "keywords": [
                "online learning",
                "kernel approximation",
                "large scale machine learning"
            ],
            "author": [
                "Jing Lu",
                "Steven C H Hoi",
                "Peilin Zhao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-148/14-148.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximate Submodularity and its Applications: Subset Selection, Sparse Approximation and Dictionary Selection *",
            "abstract": [
                "We introduce the submodularity ratio as a measure of how \"close\" to submodular a set function f is. We show that when f has submodularity ratio γ, the greedy algorithm for maximizing f provides a (1 − e −γ)-approximation. Furthermore, when γ is bounded away from 0, the greedy algorithm for minimum submodular cover also provides essentially an O(log n) approximation for a universe of n elements. As a main application of this framework, we study the problem of selecting a subset of k random variables from a large set, in order to obtain the best linear prediction of another variable of interest. We analyze the performance of widely used greedy heuristics; in particular, by showing that the submodularity ratio is lower-bounded by the smallest 2ksparse eigenvalue of the covariance matrix, we obtain the strongest known approximation guarantees for the Forward Regression and Orthogonal Matching Pursuit algorithms. As a second application, we analyze greedy algorithms for the dictionary selection problem, and significantly improve the previously known guarantees. Our theoretical analysis is complemented by experiments on real-world and synthetic data sets; in particular, we focus on an analysis of how tight various spectral parameters and the submodularity ratio are in terms of predicting the performance of the greedy algorithms."
            ],
            "keywords": [],
            "author": [
                "Abhimanyu Das",
                "David Kempe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-534/16-534.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Optimized Risk Scores",
            "abstract": [
                "Risk scores are simple classification models that let users make quick risk predictions by adding and subtracting a few small numbers. These models are widely used in medicine and criminal justice, but are difficult to learn from data because they need to be calibrated, sparse, use small integer coefficients, and obey application-specific constraints. In this paper, we introduce a machine learning method to learn risk scores. We formulate the risk score problem as a mixed integer nonlinear program, and present a cutting plane algorithm to recover its optimal solution. We improve our algorithm with specialized techniques that generate feasible solutions, narrow the optimality gap, and reduce data-related computation. Our algorithm can train risk scores in a way that scales linearly in the number of samples in a dataset, and that allows practitioners to address application-specific constraints without parameter tuning or post-processing. We benchmark the performance of different methods to learn risk scores on publicly available datasets, comparing risk scores produced by our method to risk scores built using methods that are used in practice. We also discuss the practical benefits of our method through a real-world application where we build a customized risk score for ICU seizure prediction in collaboration with the Massachusetts General Hospital."
            ],
            "keywords": [
                "scoring systems",
                "classification",
                "calibration",
                "customization",
                "interpretability",
                "cutting plane methods",
                "discrete optimization",
                "mixed integer nonlinear programming"
            ],
            "author": [
                "Berk Ustun",
                "Cynthia Rudin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-615/18-615.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Binary Embedding using Circulant Matrices",
            "abstract": [
                "Binary embeddings provide efficient and powerful ways to perform operations on large scale data. However binary embedding typically requires long codes in order to preserve the discriminative power of the input space. Thus binary coding methods traditionally suffer from high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure allows us to use Fast Fourier Transform algorithms to speed up the computation. For obtaining k-bit binary codes from d-dimensional data, our method improves the time complexity from O(dk) to O(d log d), and the space complexity from O(dk) to O(d). We study two settings, which differ in the way we choose the parameters of the circulant matrix. In the first, the parameters are chosen randomly and in the second, the parameters are learned using the data. For randomized CBE, we give a theoretical analysis comparing it with binary embedding using an unstructured random projection matrix. The challenge here is to show that the dependencies in the entries of the circulant matrix do not lead to a loss in performance. In the second setting, we design a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. In both the settings, we show by extensive experiments that the CBE approach gives much better performance than the state-ofthe-art approaches if we fix a running time, and provides much faster computation with negligible performance degradation if we fix the number of bits in the embedding."
            ],
            "keywords": [
                "structured matrix",
                "circulant matrix",
                "dimensionality reduction",
                "binary embedding",
                "FFT"
            ],
            "author": [
                "Felix X Yu",
                "Sanjiv Kumar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-619/15-619.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Learning Rates for Localized SVMs",
            "abstract": [
                "One of the limiting factors of using support vector machines (SVMs) in large scale applications are their super-linear computational requirements in terms of the number of training samples. To address this issue, several approaches that train SVMs on many small chunks separately have been proposed in the literature. With the exception of random chunks, which is also known as divide-and-conquer kernel ridge regression, however, these approaches have only been empirically investigated. In this work we investigate a spatially oriented method to generate the chunks. For the resulting localized SVM that uses Gaussian kernels and the least squares loss we derive an oracle inequality, which in turn is used to deduce learning rates that are essentially minimax optimal under some standard smoothness assumptions on the regression function. In addition, we derive local learning rates that are based on the local smoothness of the regression function. We further introduce a data-dependent parameter selection method for our local SVM approach and show that this method achieves the same almost optimal learning rates. Finally, we present a few larger scale experiments for our localized SVM showing that it achieves essentially the same test error as a global SVM for a fraction of the computational requirements. In addition, it turns out that the computational requirements for the local SVMs are similar to those of a vanilla random chunk approach, while the achieved test errors are significantly better."
            ],
            "keywords": [
                "least squares regression",
                "support vector machines",
                "localization"
            ],
            "author": [
                "Mona Meister",
                "Ingo Steinwart"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-023/14-023.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Information Criterion for Variable Selection in Support Vector Machines",
            "abstract": [
                "Support vector machines for classification have the advantage that the curse of dimensionality is circumvented. It has been shown that a reduction of the dimension of the input space leads to even better results. For this purpose, we propose two information criteria which can be computed directly from the definition of the support vector machine. We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. We arrive at the same findings for comparison on some real-world benchmark data sets."
            ],
            "keywords": [
                "information criterion",
                "supervised classification",
                "support vector machine",
                "variable selection"
            ],
            "author": [
                "Gerda Claeskens",
                "Johan Van Kerckhoven",
                "Isabelle Guyon",
                "Amir Saffari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/claeskens08a/claeskens08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Time-to-Event Prediction with Neural Networks and Cox Regression",
            "abstract": [
                "New methods for time-to-event prediction are proposed by extending the Cox proportional hazards model with neural networks. Building on methodology from nested case-control studies, we propose a loss function that scales well to large data sets and enables fitting of both proportional and non-proportional extensions of the Cox model. Through simulation studies, the proposed loss function is verified to be a good approximation for the Cox partial log-likelihood. The proposed methodology is compared to existing methodologies on real-world data sets and is found to be highly competitive, typically yielding the best performance in terms of Brier score and binomial log-likelihood. A python package for the proposed methods is available at https://github.com/havakv/pycox."
            ],
            "keywords": [
                "Cox regression",
                "customer churn",
                "neural networks",
                "non-proportional hazards",
                "survival prediction"
            ],
            "author": [
                "Håvard Kvamme",
                "Ørnulf Borgan",
                "Ida Scheel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-424/18-424.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency and Convergence Rates of One-Class SVMs and Related Algorithms",
            "abstract": [
                "We determine the asymptotic behaviour of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to infinity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held fixed. Non-asymptotic convergence bounds to this limit in the L 2 sense are provided, together with upper bounds on the classification error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the first time to be a consistent density level set estimator."
            ],
            "keywords": [
                "regularization",
                "Gaussian kernel RKHS",
                "one-class SVM",
                "convex loss functions",
                "kernel density estimation"
            ],
            "author": [
                "Régis Vert",
                "Jean-Philippe Vert"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/vert06a/vert06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin",
            "abstract": [
                "Much attention has been paid to the theoretical explanation of the empirical success of AdaBoost. The most influential work is the margin theory, which is essentially an upper bound for the generalization error of any voting classifier in terms of the margin distribution over the training data. However, important questions were raised about the margin explanation. Breiman (1999) proved a bound in terms of the minimum margin, which is sharper than the margin distribution bound. He argued that the minimum margin would be better in predicting the generalization error. Grove and Schuurmans (1998) developed an algorithm called LP-AdaBoost which maximizes the minimum margin while keeping all other factors the same as AdaBoost. In experiments however, LP-AdaBoost usually performs worse than AdaBoost, putting the margin explanation into serious doubt. In this paper, we make a refined analysis of the margin theory. We prove a bound in terms of a new margin measure called the Equilibrium margin (Emargin). The Emargin bound is uniformly"
            ],
            "keywords": [
                "boosting",
                "margin bounds",
                "voting classifier"
            ],
            "author": [
                "Liwei Wang",
                "Cheng Yang",
                "Liwei ©2011",
                "Masashi Wang",
                "Zhaoxiang Sugiyama",
                "Cheng Jing",
                "Zhi-Hua Yang",
                "Jufu Zhou",
                "SUGIYAMA, JING, YANG, ZHOU AND FENG Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/wang11a/wang11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Control Function Instrumental Variable Estimation of Nonlinear Causal Effect Models",
            "abstract": [
                "The instrumental variable method consistently estimates the effect of a treatment when there is unmeasured confounding and a valid instrumental variable. A valid instrumental variable is a variable that is independent of unmeasured confounders and affects the treatment but does not have a direct effect on the outcome beyond its effect on the treatment. Two commonly used estimators for using an instrumental variable to estimate a treatment effect are the two stage least squares estimator and the control function estimator. For linear causal effect models, these two estimators are equivalent, but for nonlinear causal effect models, the estimators are different. We provide a systematic comparison of these two estimators for nonlinear causal effect models and develop an approach to combing the two estimators that generally performs better than either one alone. We show that the control function estimator is a two stage least squares estimator with an augmented set of instrumental variables. If these augmented instrumental variables are valid, then the control function estimator can be much more efficient than usual two stage least squares without the augmented instrumental variables while if the augmented instrumental variables are not valid, then the control function estimator may be inconsistent while the usual two stage least squares remains consistent. We apply the Hausman test to test whether the augmented instrumental variables are valid and construct a pretest estimator based on this test. The pretest estimator is shown to work well in a simulation study. An application to the effect of exposure to violence on time preference is considered."
            ],
            "keywords": [
                "Causal Inference",
                "Control Function Estimator",
                "Endogenous Variable",
                "Instrumental Variable Method",
                "Two Stage Least Squares Estimator",
                "Pretest Estimator"
            ],
            "author": [
                "Zijian Guo",
                "Dylan S Small",
                "Isabelle Guyon",
                "Alexander Statnikov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-379/14-379.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Refinable Kernels *",
            "abstract": [
                "Motivated by mathematical learning from training data, we introduce the notion of refinable kernels. Various characterizations of refinable kernels are presented. The concept of refinable kernels leads to the introduction of wavelet-like reproducing kernels. We also investigate a refinable kernel that forms a Riesz basis. In particular, we characterize refinable translation invariant kernels, and refinable kernels defined by refinable functions. This study leads to multiresolution analysis of reproducing kernel Hilbert spaces."
            ],
            "keywords": [
                "refinable kernels",
                "refinable feature maps",
                "wavelet-like reproducing kernels",
                "dual kernels",
                "learning with kernels",
                "reproducing kernel Hilbert spaces",
                "Riesz bases"
            ],
            "author": [
                "Yuesheng Xu",
                "Haizhang Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/xu07a/xu07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "No-Regret Bayesian Optimization with Unknown Hyperparameters",
            "abstract": [
                "Bayesian optimization (BO) based on Gaussian process models is a powerful paradigm to optimize black-box functions that are expensive to evaluate. While several BO algorithms provably converge to the global optimum of the unknown function, they assume that the hyperparameters of the kernel are known in advance. This is not the case in practice and misspecification often causes these algorithms to converge to poor local optima. In this paper, we present the first BO algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters. During optimization we slowly adapt the hyperparameters of stationary kernels and thereby expand the associated function class over time, so that the BO algorithm considers more complex function candidates. Based on the theoretical insights, we propose several practical algorithms that achieve the empirical sample efficiency of BO with online hyperparameter estimation, but retain theoretical convergence guarantees. We evaluate our method on several benchmark problems."
            ],
            "keywords": [
                "Bayesian Optimization Special Issue Bayesian optimization",
                "Unknown hyperparameters",
                "Reproducing kernel Hilbert space (RKHS)",
                "Bandits",
                "No regret"
            ],
            "author": [
                "Felix Berkenkamp",
                "Angela P Schoellig",
                "Andreas Krause"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-213/18-213.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distance Preserving Embeddings for General n-Dimensional Manifolds",
            "abstract": [
                "Low dimensional embeddings of manifold data have gained popularity in the last decade. However, a systematic finite sample analysis of manifold embedding algorithms largely eludes researchers. Here we present two algorithms that embed a general n-dimensional manifold into R d (where d only depends on some key manifold properties such as its intrinsic dimension, volume and curvature) that guarantee to approximately preserve all interpoint geodesic distances."
            ],
            "keywords": [
                "manifold learning",
                "isometric embeddings",
                "non-linear dimensionality reduction",
                "Nash's embedding theorem"
            ],
            "author": [
                "Nakul Verma"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/verma13a/verma13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity",
            "abstract": [
                "Given a set V of n elements we wish to linearly order them given pairwise preference labels which may be non-transitive (due to irrationality or arbitrary noise). The goal is to linearly order the elements while disagreeing with as few pairwise preference labels as possible. Our performance is measured by two parameters: The number of disagreements (loss) and the query complexity (number of pairwise preference labels). Our algorithm adaptively queries at most O(ε −6 n log 5 n) preference labels for a regret of ε times the optimal loss. As a function of n, this is asymptotically better than standard (non-adaptive) learning bounds achievable for the same problem. Our main result takes us a step closer toward settling an open problem posed by learning-torank (from pairwise information) theoreticians and practitioners: What is a provably correct way to sample preference labels? To further show the power and practicality of our solution, we analyze a typical test case in which a large margin linear relaxation is used for efficiently solving the simpler learning problems in our decomposition."
            ],
            "keywords": [
                "statistical learning theory",
                "active learning",
                "ranking",
                "pairwise ranking",
                "preferences"
            ],
            "author": [
                "Nir Ailon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/ailon12a/ailon12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Perturbation Bounds for Procrustes, Classical Scaling, and Trilateration, with Applications to Manifold Learning",
            "abstract": [
                "One of the common tasks in unsupervised learning is dimensionality reduction, where the goal is to find meaningful low-dimensional structures hidden in high-dimensional data. Sometimes referred to as manifold learning, this problem is closely related to the problem of localization, which aims at embedding a weighted graph into a low-dimensional Euclidean space. Several methods have been proposed for localization, and also manifold learning. Nonetheless, the robustness property of most of them is little understood. In this paper, we obtain perturbation bounds for classical scaling and trilateration, which are then applied to derive performance bounds for Isomap, Landmark Isomap, and Maximum Variance Unfolding. A new perturbation bound for procrustes analysis plays a key role."
            ],
            "keywords": [],
            "author": [
                "Ery Arias-Castro",
                "Adel Javanmard",
                "Bruno Pelletier"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-720/18-720.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels",
            "abstract": [
                "A kernel method for realizing Bayes' rule is proposed, based on representations of probabilities in reproducing kernel Hilbert spaces. Probabilities are uniquely characterized by the mean of the canonical map to the RKHS. The prior and conditional probabilities are expressed in terms of RKHS functions of an empirical sample: no explicit parametric model is needed for these quantities. The posterior is likewise an RKHS mean of a weighted sample. The estimator for the expectation of a function of the posterior is derived, and rates of consistency are shown. Some representative applications of the kernel Bayes' rule are presented, including Bayesian computation without likelihood and filtering with a nonparametric state-space model."
            ],
            "keywords": [
                "kernel method",
                "Bayes' rule",
                "reproducing kernel Hilbert space"
            ],
            "author": [
                "Kenji Fukumizu",
                "Le Song",
                "Arthur Gretton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/fukumizu13a/fukumizu13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Accelerated Alternating Projections for Robust Principal Component Analysis",
            "abstract": [
                "We study robust PCA for the fully observed setting, which is about separating a low rank matrix L and a sparse matrix S from their sum D = L + S. In this paper, a new algorithm, dubbed accelerated alternating projections, is introduced for robust PCA which significantly improves the computational efficiency of the existing alternating projections proposed in (Netrapalli et al., 2014) when updating the low rank factor. The acceleration is achieved by first projecting a matrix onto some low dimensional subspace before obtaining a new estimate of the low rank matrix via truncated SVD. Exact recovery guarantee has been established which shows linear convergence of the proposed algorithm. Empirical performance evaluations establish the advantage of our algorithm over other state-of-theart algorithms for robust PCA."
            ],
            "keywords": [
                "Robust PCA",
                "Alternating Projections",
                "Matrix Manifold",
                "Tangent Space",
                "Subspace Projection"
            ],
            "author": [
                "Hanqin Cai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-022/18-022.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Universal Well-Calibrated Algorithm for On-line Classification",
            "abstract": [
                "We study the problem of on-line classification in which the prediction algorithm, for each \"significance level\" δ, is required to output as its prediction a range of labels (intuitively, those labels deemed compatible with the available data at the level δ) rather than just one label; as usual, the examples are assumed to be generated independently from the same probability distribution P. The prediction algorithm is said to be \"well-calibrated\" for P and δ if the long-run relative frequency of errors does not exceed δ almost surely w.r. to P. For well-calibrated algorithms we take the number of \"uncertain\" predictions (i.e., those containing more than one label) as the principal measure of predictive performance. The main result of this paper is the construction of a prediction algorithm which, for any (unknown) P and any δ: (a) makes errors independently and with probability δ at every trial (in particular, is well-calibrated for P and δ); (b) makes in the long run no more uncertain predictions than any other prediction algorithm that is well-calibrated for P and δ; (c) processes example n in time O(log n)."
            ],
            "keywords": [],
            "author": [
                "Vladimir Vovk",
                "Kristin Bennett",
                "Nicolò Cesa-Bianchi"
            ],
            "ref": "http://www.jmlr.org/papers/volume5/vovk04a/vovk04a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Efficient Adjustment in Causal Graphs",
            "abstract": [
                "We consider estimation of a total causal effect from observational data via covariate adjustment. Ideally, adjustment sets are selected based on a given causal graph, reflecting knowledge of the underlying causal structure. Valid adjustment sets are, however, not unique. Recent research has introduced a graphical criterion for an 'optimal' valid adjustment set (O-set). For a given graph, adjustment by the O-set yields the smallest asymptotic variance compared to other adjustment sets in certain parametric and non-parametric models. In this paper, we provide three new results on the O-set. First, we give a novel, more intuitive graphical characterisation: We show that the O-set is the parent set of the outcome node(s) in a suitable latent projection graph, which we call the forbidden projection. An important property is that the forbidden projection preserves all information relevant to total causal effect estimation via covariate adjustment, making it a useful methodological tool in its own right. Second, we extend the existing IDA algorithm to use the O-set, and argue that the algorithm remains semi-local. This is implemented in the R-package pcalg. Third, we present assumptions under which the O-set can be viewed as the target set of popular non-graphical variable selection algorithms such as stepwise backward selection."
            ],
            "keywords": [
                "causal discovery",
                "causal inference",
                "confounder selection",
                "confounding",
                "efficiency",
                "graphical models",
                "IDA algorithm",
                "model selection",
                "sufficient adjustment set"
            ],
            "author": [
                "Janine Witte",
                "Leonard Henckel",
                "Marloes H Maathuis",
                "Vanessa Didelez"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-175/20-175.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Joint Structural Estimation of Multiple Graphical Models",
            "abstract": [
                "Gaussian graphical models capture dependence relationships between random variables through the pattern of nonzero elements in the corresponding inverse covariance matrices. To date, there has been a large body of literature on both computational methods and analytical results on the estimation of a single graphical model. However, in many application domains, one has to estimate several related graphical models, a problem that has also received attention in the literature. The available approaches usually assume that all graphical models are globally related. On the other hand, in many settings different relationships between subsets of the node sets exist between different graphical models. We develop methodology that jointly estimates multiple Gaussian graphical models, assuming that there exists prior information on how they are structurally related. For many applications, such information is available from external data sources. The proposed method consists of first applying neighborhood selection with a group lasso penalty to obtain edge sets of the graphs, and a maximum likelihood refit for estimating the nonzero entries in the inverse covariance matrices. We establish consistency of the proposed method for sparse high-dimensional Gaussian graphical models and examine its performance using simulation experiments. Applications to a climate data set and a breast cancer data set are also discussed."
            ],
            "keywords": [
                "Gaussian graphical model",
                "structured sparsity",
                "group lasso penalty",
                "consistency",
                "edge set recovery"
            ],
            "author": [
                "Jing Ma",
                "George Michailidis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-656/15-656.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Using Simulation to Improve Sample-Efficiency of Bayesian Optimization for Bipedal Robots",
            "abstract": [
                "Learning for control can acquire controllers for novel robotic tasks, paving the path for autonomous agents. Such controllers can be expert-designed policies, which typically require tuning of parameters for each task scenario. In this context, Bayesian optimization (BO) has emerged as a promising approach for automatically tuning controllers. However, sample-efficiency can still be an issue for high-dimensional policies on hardware. Here, we develop an approach that utilizes simulation to learn structured feature transforms that map the original parameter space into a domain-informed space. During BO, similarity between controllers is now calculated in this transformed space. Experiments on the ATRIAS robot hardware and simulation show that our approach succeeds at sampleefficiently learning controllers for multiple robots. Another question arises: What if the simulation significantly differs from hardware? To answer this, we create increasingly approximate simulators and study the effect of increasing simulation-hardware mismatch on the performance of Bayesian optimization. We also compare our approach to other approaches from literature, and find it to be more reliable, especially in cases of high mismatch. Our experiments show that our approach succeeds across different controller types, bipedal robot models and simulator fidelity levels, making it applicable to a wide range of bipedal locomotion problems."
            ],
            "keywords": [
                "Bayesian Optimization Special Issue Bayesian Optimization",
                "Bipedal Locomotion",
                "Transfer Learning"
            ],
            "author": [
                "Akshara Rai",
                "Rika Antonova",
                "Franziska Meier",
                "Paul G Allen",
                "Christopher G Atkeson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-196/18-196.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cornac: A Comparative Framework for Multimodal Recommender Systems",
            "abstract": [
                "Cornac is an open-source Python framework for multimodal recommender systems. In addition to core utilities for accessing, building, evaluating, and comparing recommender models, Cornac is distinctive in putting emphasis on recommendation models that leverage auxiliary information in the form of a social network, item textual descriptions, product images, etc. Such multimodal auxiliary data supplement user-item interactions (e.g., ratings, clicks), which tend to be sparse in practice. To facilitate broad adoption and community contribution, Cornac is publicly available at https://github.com/PreferredAI/cornac, and it can be installed via Anaconda or the Python Package Index (pip). Not only is it well-covered by unit tests to ensure code quality, but it is also accompanied with a detailed documentation 1 , tutorials, examples, and several built-in benchmarking data sets."
            ],
            "keywords": [
                "comparison",
                "multimodality",
                "recommendation algorithms",
                "software"
            ],
            "author": [
                "Aghiles Salah",
                "Hady W Lauw",
                "Andreas Mueller"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-805/19-805.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multivariate Convex Regression with Adaptive Partitioning",
            "abstract": [
                "We propose a new, nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function. Convexity constraints are common in economics, statistics, operations research, financial engineering and optimization, but there is currently no multivariate method that is stable and computationally feasible for more than a few thousand observations. We introduce convex adaptive partitioning (CAP), which creates a globally convex regression model from locally linear estimates fit on adaptively selected covariate partitions. CAP is a computationally efficient, consistent method for convex regression. We demonstrate empirical performance by comparing the performance of CAP to other shape-constrained and unconstrained regression methods for predicting weekly wages and value function approximation for pricing American basket options."
            ],
            "keywords": [
                "adaptive partitioning",
                "convex regression",
                "nonparametric regression",
                "shape constraint",
                "treed linear model"
            ],
            "author": [
                "Lauren A Hannah",
                "David B Dunson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/hannah13a/hannah13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency of Cheeger and Ratio Graph Cuts",
            "abstract": [
                "This paper establishes the consistency of a family of graph-cut-based algorithms for clustering of data clouds. We consider point clouds obtained as samples of a ground-truth measure. We investigate approaches to clustering based on minimizing objective functionals defined on proximity graphs of the given sample. Our focus is on functionals based on graph cuts like the Cheeger and ratio cuts. We show that minimizers of these cuts converge as the sample size increases to a minimizer of a corresponding continuum cut (which partitions the ground truth measure). Moreover, we obtain sharp conditions on how the connectivity radius can be scaled with respect to the number of sample points for the consistency to hold. We provide results for two-way and for multiway cuts. Furthermore we provide numerical experiments that illustrate the results and explore the optimality of scaling in dimension two."
            ],
            "keywords": [
                "data clustering",
                "balanced cut",
                "consistency",
                "graph partitioning"
            ],
            "author": [
                "García Nicolás",
                "James Von Brecht",
                "Thomas Laurent",
                "Xavier Bresson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-490/14-490.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Interplay of Optimization and Machine Learning Research",
            "abstract": [
                "The fields of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semidefinite, and semi-infinite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms."
            ],
            "keywords": [
                "machine learning",
                "mathematical programming",
                "convex optimization"
            ],
            "author": [
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "http://www.jmlr.org/papers/volume7/MLOPT-intro06a/MLOPT-intro06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "abstract": [
                "Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code."
            ],
            "keywords": [
                "transfer learning",
                "natural language processing",
                "multi-task learning",
                "attentionbased models",
                "deep learning"
            ],
            "author": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Sharan Narang",
                "Michael Matena",
                "Wei Li",
                "Peter J Liu",
                "Colin ©2020",
                "Noam Raffel",
                "Adam Shazeer",
                "Katherine Roberts",
                "Sharan Lee",
                "Michael Narang",
                "Yanqi Matena",
                "Wei Zhou",
                "Peter J Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-074/20-074.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simple, Robust and Optimal Ranking from Pairwise Comparisons",
            "abstract": [
                "We consider data in the form of pairwise comparisons of n items, with the goal of identifying the top k items for some value of k < n, or alternatively, recovering a ranking of all the items. We analyze the Borda counting algorithm that ranks the items in order of the number of pairwise comparisons won, and show it has three attractive features: (a) it is an optimal method achieving the information-theoretic limits up to constant factors; (b) it is robust in that its optimality holds without imposing conditions on the underlying matrix of pairwise-comparison probabilities, in contrast to some prior work that applies only to the BTL parametric model; and (c) its computational efficiency leads to speed-ups of several orders of magnitude. We address the problem of exact recovery, and for the top-k recovery problem we also extend our results to obtain sharp guarantees for approximate recovery under the Hamming distortion metric, and more generally, to any arbitrary error requirement that satisfies a simple and natural monotonicity condition. In doing so, we introduce a general framework that allows us to treat a variety of problems in the literature in an unified manner."
            ],
            "keywords": [
                "Pairwise comparisons",
                "Ranking",
                "Set recovery",
                "Approximate recovery",
                "Borda count",
                "Permutation-based models",
                "Occam's razor"
            ],
            "author": [
                "Nihar B Shah",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-206/16-206.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Controlling the False Discovery Rate of the Association/Causality Structure Learned with the PC Algorithm",
            "abstract": [
                "In real world applications, graphical statistical models are not only a tool for operations such as classification or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level, and a heuristic modification of the method is able to control the FDR more accurately around the user-specified level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models."
            ],
            "keywords": [
                "Bayesian networks",
                "false discovery rate",
                "PC algorithm",
                "directed acyclic graph",
                "skeleton"
            ],
            "author": [
                "Junning Li",
                "Z Jane Wang",
                "Paolo Frasconi",
                "Kristian Kersting",
                "Hannu Toivonen",
                "Koji Tsuda"
            ],
            "ref": "http://www.jmlr.org/papers/volume10/li09a/li09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Synthetic Control",
            "abstract": [
                "We present a robust generalization of the synthetic control method for comparative case studies. Like the classical method cf. Abadie and Gardeazabal (2003), we present an algorithm to estimate the unobservable counterfactual of a treatment unit. A distinguishing feature of our algorithm is that of de-noising the data matrix via singular value thresholding, which renders our approach robust in multiple facets: it automatically identifies a good subset of donors for the synthetic control, overcomes the challenges of missing data, and continues to work well in settings where covariate information may not be provided. We posit that the setting can be viewed as an instance of the Latent Variable Model and provide the first finite sample analysis (coupled with asymptotic results) for the estimation of the counterfactual. Our algorithm accurately imputes missing entries and filters corrupted observations in producing a consistent estimator of the underlying signal matrix, provided p = Ω(T −1+ζ) for some ζ > 0; here, p is the fraction of observed data and T is the time interval of interest. Under the same proportion of observations, we demonstrate that the mean-squared error in our counterfactual estimation scales as O(σ 2 /p + 1/ √ T), where σ 2 is the variance of the inherent noise. Additionally, we introduce a Bayesian framework to quantify the estimation uncertainty. Our experiments, using both synthetic and real-world datasets, demonstrate that our robust generalization yields an improvement over the classical synthetic control method."
            ],
            "keywords": [
                "Observation Studies",
                "Causal Inference",
                "Matrix Estimation"
            ],
            "author": [
                "Muhammad Amjad",
                "Devavrat Shah",
                "Dennis Shen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-777/17-777.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-Convex Matrix Completion and Related Problems via Strong Duality",
            "abstract": [
                "This work studies the strong duality of non-convex matrix factorization problems: we show that under certain dual conditions, these problems and the dual have the same optimum. This has been well understood for convex optimization, but little was known for non-convex problems. We propose a novel analytical framework and prove that under certain dual conditions, the optimal solution of the matrix factorization program is the same as that of its bi-dual and thus the global optimality of the non-convex program can be achieved by solving its bi-dual which is convex. These dual conditions are satisfied by a wide class of matrix factorization problems, although matrix factorization is hard to solve in full generality. This analytical framework may be of independent interest to non-convex optimization more broadly. We apply our framework to two prototypical matrix factorization problems: matrix completion and robust Principal Component Analysis. These are examples of efficiently recovering a hidden matrix given limited reliable observations. Our framework shows that exact recoverability and strong duality hold with nearly-optimal sample complexity for the two problems."
            ],
            "keywords": [
                "strong duality",
                "non-convex optimization",
                "matrix factorization",
                "matrix completion",
                "robust principal component analysis",
                "sample complexity"
            ],
            "author": [
                "Maria-Florina Balcan",
                "Yingyu Liang",
                "Zhao Song",
                "David P Woodruff",
                "Hongyang Zhang",
                "Florina Balcan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-611/17-611.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Using Contextual Representations to Efficiently Learn Context-Free Languages",
            "abstract": [
                "We present a polynomial update time algorithm for the inductive inference of a large class of context-free languages using the paradigm of positive data and a membership oracle. We achieve this result by moving to a novel representation, called Contextual Binary Feature Grammars (CBFGs), which are capable of representing richly structured context-free languages as well as some context sensitive languages. These representations explicitly model the lattice structure of the distribution of a set of substrings and can be inferred using a generalisation of distributional learning. This formalism is an attempt to bridge the gap between simple learnable classes and the sorts of highly expressive representations necessary for linguistic representation: it allows the learnability of a large class of context-free languages, that includes all regular languages and those context-free languages that satisfy two simple constraints. The formalism and the algorithm seem well suited to natural language and in particular to the modeling of first language acquisition. Preliminary experimental results confirm the effectiveness of this approach."
            ],
            "keywords": [
                "grammatical inference",
                "context-free language",
                "positive data only",
                "membership queries"
            ],
            "author": [
                "Alexander Clark"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/clark10a/clark10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Utilizing Second Order Information in Minibatch Stochastic Variance Reduced Proximal Iterations",
            "abstract": [
                "We present a novel minibatch stochastic optimization method for empirical risk minimization of linear predictors. The method efficiently leverages both sub-sampled first-order and higher-order information, by incorporating variance-reduction and acceleration techniques. We prove improved iteration complexity over state-of-the-art methods under suitable conditions. In particular, the approach enjoys global fast convergence for quadratic convex objectives and local fast convergence for general convex objectives. Experiments are provided to demonstrate the empirical advantage of the proposed method over existing approaches in the literature."
            ],
            "keywords": [],
            "author": [
                "Jialei Wang",
                "Hong Kong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-594/17-594.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Quantile Regression in Reproducing Kernel Hilbert Spaces with the Data Sparsity Constraint",
            "abstract": [
                "For spline regressions, it is well known that the choice of knots is crucial for the performance of the estimator. As a general learning framework covering the smoothing splines, learning in a Reproducing Kernel Hilbert Space (RKHS) has a similar issue. However, the selection of training data points for kernel functions in the RKHS representation has not been carefully studied in the literature. In this paper we study quantile regression as an example of learning in a RKHS. In this case, the regular squared norm penalty does not perform training data selection. We propose a data sparsity constraint that imposes thresholding on the kernel function coefficients to achieve a sparse kernel function representation. We demonstrate that the proposed data sparsity method can have competitive prediction performance for certain situations, and have comparable performance in other cases compared to that of the traditional squared norm penalty. Therefore, the data sparsity method can serve as a competitive alternative to the squared norm penalty method. Some theoretical properties of our proposed method using the data sparsity constraint are obtained. Both simulated and real data sets are used to demonstrate the usefulness of our data sparsity constraint."
            ],
            "keywords": [
                "kernel learning",
                "Rademacher complexity",
                "regression",
                "smoothing",
                "sparsity"
            ],
            "author": [
                "Chong Zhang",
                "Yufeng Liu",
                "Yichao Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/zhang16a/zhang16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Kernel Multiple Change-point Algorithm via Model Selection",
            "abstract": [
                "We consider a general formulation of the multiple change-point problem, in which the data is assumed to belong to a set equipped with a positive semidefinite kernel. We propose a model-selection penalty allowing to select the number of change points in Harchaoui and Cappé's kernel-based change-point detection method. The model-selection penalty generalizes non-asymptotic model-selection penalties for the change-in-mean problem with univariate data. We prove a non-asymptotic oracle inequality for the resulting kernelbased change-point detection method, whatever the unknown number of change points, thanks to a concentration result for Hilbert-space valued random variables which may be of independent interest. Experiments on synthetic and real data illustrate the proposed method, demonstrating its ability to detect subtle changes in the distribution of data."
            ],
            "keywords": [
                "model selection",
                "kernel methods",
                "change-point detection",
                "concentration inequality"
            ],
            "author": [
                "Sylvain Arlot",
                "Alain Celisse",
                "Zaid Harchaoui"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/16-155/16-155.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Measuring Differentiability: Unmasking Pseudonymous Authors",
            "abstract": [
                "In the authorship verification problem, we are given examples of the writing of a single author and are asked to determine if given long texts were or were not written by this author. We present a new learning-based method for adducing the \"depth of difference\" between two example sets and offer evidence that this method solves the authorship verification problem with very high accuracy. The underlying idea is to test the rate of degradation of the accuracy of learned models as the best features are iteratively dropped from the learning process."
            ],
            "keywords": [
                "authorship attribution",
                "one-class learning",
                "unmasking"
            ],
            "author": [
                "Moshe ©2007",
                "Jonathan Koppel",
                "Elisheva Schler",
                "Moshe Koppel",
                "Jonathan Schler",
                "Le7 Bonchek Dokow",
                "Gmail Com"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/koppel07a/koppel07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sequential change-point detection in high-dimensional Gaussian graphical models",
            "abstract": [
                "High dimensional piecewise stationary graphical models represent a versatile class for modelling time varying networks arising in diverse application areas, including biology, economics, and social sciences. There has been recent work in offline detection and estimation of regime changes in the topology of sparse graphical models. However, the online setting remains largely unexplored, despite its high relevance to applications in sensor networks and other engineering monitoring systems, as well as financial markets. To that end, this work introduces a novel scalable online algorithm for detecting an unknown number of abrupt changes in the inverse covariance matrix of sparse Gaussian graphical models with small delay. The proposed algorithm is based upon monitoring the conditional log-likelihood of all nodes in the network and can be extended to a large class of continuous and discrete graphical models. We also investigate asymptotic properties of our procedure under certain mild regularity conditions on the graph size, sparsity level, number of samples, and preand post-changes in the topology of the network. Numerical works on both synthetic and real data illustrate the good performance of the proposed methodology both in terms of computational and statistical efficiency across numerous experimental settings."
            ],
            "keywords": [
                "Sequential change-point detection",
                "Gaussian graphical models",
                "Pseudolikelihood",
                "Mini-batch update",
                "Asymptotic analysis"
            ],
            "author": [
                "Hossein Keshavarz",
                "George Michailidis",
                "Yves Atchadé"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-410/18-410.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Consistent Vertex Nomination Schemes",
            "abstract": [
                "Given a vertex of interest in a network G 1 , the vertex nomination problem seeks to find the corresponding vertex of interest (if it exists) in a second network G 2. A vertex nomination scheme produces a list of the vertices in G 2 , ranked according to how likely they are judged to be the corresponding vertex of interest in G. The vertex nomination problem and related information retrieval tasks have attracted much attention in the machine learning literature, with numerous applications to social and biological networks. However, the current framework has often been confined to a comparatively small class of network models, and the concept of statistically consistent vertex nomination schemes has been only shallowly explored. In this paper, we extend the vertex nomination problem to a very general statistical model of graphs. Further, drawing inspiration from the long-established classification framework in the pattern recognition literature, we provide definitions for the key notions of Bayes optimality and consistency in our extended vertex nomination framework, including a derivation of the Bayes optimal vertex nomination scheme. In addition, we prove that no universally consistent vertex nomination schemes exist. Illustrative examples are provided throughout."
            ],
            "keywords": [
                "Vertex nomination",
                "graph inference",
                "recommender systems"
            ],
            "author": [
                "Vince Lyzinski",
                "Keith Levin",
                "Carey E Priebe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-048/18-048.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "How to Explain Individual Classification Decisions",
            "abstract": [
                "After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method."
            ],
            "keywords": [
                "explaining",
                "nonlinear",
                "black box model",
                "kernel methods",
                "Ames mutagenicity"
            ],
            "author": [
                "David Baehrens",
                "Stefan Harmeling",
                "Katja Hansen",
                "Klaus-Robert Robert Müller",
                "Timon Schroeter",
                "Motoaki Kawanabe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/baehrens10a/baehrens10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "WEKA-Experiences with a Java Open-Source Project",
            "abstract": [
                "WEKA is a popular machine learning workbench with a development life of nearly two decades. This article provides an overview of the factors that we believe to be important to its success. Rather than focussing on the software's functionality, we review aspects of project management and historical development decisions that likely had an impact on the uptake of the project."
            ],
            "keywords": [],
            "author": [
                "Remco R Bouckaert",
                "Mark A Hall",
                "Geoffrey Holmes",
                "Bernhard Pfahringer",
                "Peter Reutemann",
                "Ian H Witten"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/bouckaert10a/bouckaert10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels",
            "abstract": [
                "Estimating the error rates of classifiers or regression models is a fundamental task in machine learning which has thus far been studied exclusively using supervised learning techniques. We propose a novel unsupervised framework for estimating these error rates using only unlabeled data and mild assumptions. We prove consistency results for the framework and demonstrate its practical applicability on both synthetic and real world data."
            ],
            "keywords": [
                "classification and regression",
                "maximum likelihood",
                "latent variable models"
            ],
            "author": [
                "Pinar Donmez",
                "Krishnakumar Balasubramanian"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/donmez10a/donmez10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "partykit: A Modular Toolkit for Recursive Partytioning in R",
            "abstract": [
                "The R package partykit provides a flexible toolkit for learning, representing, summarizing, and visualizing a wide range of tree-structured regression and classification models. The functionality encompasses: (a) basic infrastructure for representing trees (inferred by any algorithm) so that unified print/plot/predict methods are available; (b) dedicated methods for trees with constant fits in the leaves (or terminal nodes) along with suitable coercion functions to create such trees (e.g., by rpart, RWeka, PMML); (c) a reimplementation of conditional inference trees (ctree, originally provided in the party package); (d) an extended reimplementation of model-based recursive partitioning (mob, also originally in party) along with dedicated methods for trees with parametric models in the leaves. Here, a brief overview of the package and its design is given while more detailed discussions of items (a)-(d) are available in vignettes accompanying the package."
            ],
            "keywords": [
                "recursive partitioning",
                "regression trees",
                "classification trees",
                "statistical learning",
                "R"
            ],
            "author": [
                "Torsten Hothorn"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/hothorn15a/hothorn15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Prediction Risk for the Horseshoe Regression",
            "abstract": [
                "We show that prediction performance for global-local shrinkage regression can overcome two major difficulties of global shrinkage regression: (i) the amount of relative shrinkage is monotone in the singular values of the design matrix and (ii) the shrinkage is determined by a single tuning parameter. Specifically, we show that the horseshoe regression, with heavy-tailed component-specific local shrinkage parameters, in conjunction with a global parameter providing shrinkage towards zero, alleviates both these difficulties and consequently, results in an improved risk for prediction. Numerical demonstrations of improved prediction over competing approaches in simulations and in a pharmacogenomics data set confirm our theoretical findings."
            ],
            "keywords": [
                "Global-local Priors",
                "Principal Components",
                "Shrinkage Regression",
                "Stein's Unbiased Risk Rstimate"
            ],
            "author": [
                "Anindya Bhadra",
                "Jyotishka Datta",
                "Yunfan Li",
                "Nicholas G Polson",
                "Brandon Willard"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-321/18-321.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear Regression With Random Projections",
            "abstract": [
                "We investigate a method for regression that makes use of a randomly generated subspace G P ⊂ F (of finite dimension P) of a given large (possibly infinite) dimensional function space F , for example, L 2 ([0, 1] d ; R). G P is defined as the span of P random features that are linear combinations of a basis functions of F weighted by random Gaussian i.i.d. coefficients. We show practical motivation for the use of this approach, detail the link that this random projections method share with RKHS and Gaussian objects theory and prove, both in deterministic and random design, approximation error bounds when searching for the best regression function in G P rather than in F , and derive excess risk bounds for a specific regression algorithm (least squares regression in G P). This paper stresses the motivation to study such methods, thus the analysis developed is kept simple for explanations purpose and leaves room for future developments."
            ],
            "keywords": [
                "regression",
                "random matrices",
                "dimension reduction"
            ],
            "author": [
                "Odalric-Ambrym Maillard"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/maillard12a/maillard12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MSVMpack: A Multi-Class Support Vector Machine Package",
            "abstract": [
                "This paper describes MSVMpack, an open source software package dedicated to our generic model of multi-class support vector machine. All four multi-class support vector machines (M-SVMs) proposed so far in the literature appear as instances of this model. MSVMpack provides for them the first unified implementation and offers a convenient basis to develop other instances. This is also the first parallel implementation for M-SVMs. The package consists in a set of command-line tools with a callable library. The documentation includes a tutorial, a user's guide and a developer's guide."
            ],
            "keywords": [
                "multi-class support vector machines",
                "open source",
                "C"
            ],
            "author": [
                "Fabien Lauer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/lauer11a/lauer11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Suitable Domain for SVM Training in Image Coding Gustavo Camps-Valls",
            "abstract": [
                "Conventional SVM-based image coding methods are founded on independently restricting the distortion in every image coefficient at some particular image representation. Geometrically, this implies allowing arbitrary signal distortions in an n-dimensional rectangle defined by the ε-insensitivity zone in each dimension of the selected image representation domain. Unfortunately, not every image representation domain is well-suited for such a simple, scalar-wise, approach because statistical and/or perceptual interactions between the coefficients may exist. These interactions imply that scalar approaches may induce distortions that do not follow the image statistics and/or are perceptually annoying. Taking into account these relations would imply using non-rectangular εinsensitivity regions (allowing coupled distortions in different coefficients), which is beyond the conventional SVM formulation."
            ],
            "keywords": [],
            "author": [
                "Juan Gutiérrez",
                "Jesús Malo",
                "Dept D'òptica"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/camps-valls08a/camps-valls08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear Programs for Hypotheses Selection in Probabilistic Inference Models",
            "abstract": [
                "We consider an optimization problem in probabilistic inference: Given n hypotheses H j , m possible observations O k , their conditional probabilities p k j , and a particular O k , select a possibly small subset of hypotheses excluding the true target only with some error probability ε. After specifying the optimization goal we show that this problem can be solved through a linear program in mn variables that indicate the probabilities to discard a hypothesis given an observation. Moreover, we can compute optimal strategies where only O(m + n) of these variables get fractional values. The manageable size of the linear programs and the mostly deterministic shape of optimal strategies makes the method practicable. We interpret the dual variables as worst-case distributions of hypotheses, and we point out some counterintuitive nonmonotonic behaviour of the variables as a function of the error bound ε. One of the open problems is the existence of a purely combinatorial algorithm that is faster than generic linear programming."
            ],
            "keywords": [
                "probabilistic inference",
                "error probability",
                "linear programming",
                "cycle-free graphs",
                "network flows"
            ],
            "author": [
                "Anders Bergkvist",
                "Peter Damaschke",
                "Marcel Lüthi",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/bergkvist06a/bergkvist06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Machine Learning in an Auction Environment",
            "abstract": [
                "We consider a model of repeated online auctions in which an ad with an uncertain clickthrough rate faces a random distribution of competing bids in each auction and there is discounting of payoffs. We formulate the optimal solution to this explore/exploit problem as a dynamic programming problem and show that efficiency is maximized by making a bid for each advertiser equal to the advertiser's expected value for the advertising opportunity plus a term proportional to the variance in this value divided by the number of impressions the advertiser has received thus far. We then use this result to illustrate that the value of incorporating active exploration in an auction environment is exceedingly small."
            ],
            "keywords": [
                "Auctions",
                "Explore/exploit",
                "Machine learning",
                "Online advertising"
            ],
            "author": [
                "Patrick Hummel",
                "R Preston Mcafee"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-109/15-109.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Contextual Bandits with Continuous Actions: Smoothing, Zooming, and Adapting",
            "abstract": [
                "We study contextual bandit learning with an abstract policy class and continuous action space. We obtain two qualitatively different regret bounds: one competes with a smoothed version of the policy class under no continuity assumptions, while the other requires standard Lipschitz assumptions. Both bounds exhibit data-dependent \"zooming\" behavior and, with no tuning, yield improved guarantees for benign problems. We also study adapting to unknown smoothness parameters, establishing a price-of-adaptivity and deriving optimal adaptive algorithms that require no additional information."
            ],
            "keywords": [],
            "author": [
                "Akshay Krishnamurthy",
                "John Langford",
                "Aleksandrs Slivkins",
                "Chicheng Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-650/19-650.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Multiple Tasks with Kernel Methods",
            "abstract": [
                "We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task."
            ],
            "keywords": [
                "multi-task learning",
                "kernels",
                "vector-valued functions",
                "regularization",
                "learning algorithms"
            ],
            "author": [
                "Theodoros Evgeniou",
                "Charles A Micchelli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/evgeniou05a/evgeniou05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variable Selection Using SVM-based Criteria",
            "abstract": [
                "We propose new methods to evaluate variable subset relevance with a view to variable selection. Relevance criteria are derived from Support Vector Machines and are based on weight vector w or generalization error bounds sensitivity with respect to a variable. Experiments on linear and non-linear toy problems and real-world datasets have been carried out to assess the effectiveness of these criteria. Results show that the criterion based on weight vector derivative achieves good results and performs consistently well over the datasets we used."
            ],
            "keywords": [
                "support vector machines",
                "kernels",
                "variable selection",
                "sensitivity"
            ],
            "author": [
                "Alain Rakotomamonjy",
                "Isabelle Guyon",
                "André Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/rakotomamonjy03a/rakotomamonjy03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals",
            "abstract": [
                "We show that many machine learning goals can be expressed as \"rate constraints\" on a model's predictions. We study the problem of training non-convex models subject to these rate constraints (or other non-convex or non-differentiable constraints). In the non-convex setting, the standard approach of Lagrange multipliers may fail. Furthermore, if the constraints are non-differentiable, then one cannot optimize the Lagrangian with gradient-based methods. To solve these issues, we introduce a new \"proxy-Lagrangian\" formulation. This leads to an algorithm that, assuming access to an optimization oracle, produces a stochastic classifier by playing a two-player non-zero-sum game solving for what we call a semi-coarse correlated equilibrium, which in turn corresponds to an approximately optimal and feasible solution to the constrained optimization problem. We then give a procedure that shrinks the randomized solution down to a mixture of at most m + 1 deterministic solutions, given m constraints. This culminates in a procedure that can solve non-convex constrained optimization problems with possibly non-differentiable and non-convex constraints, and enjoys theoretical guarantees. We provide extensive experimental results covering a broad range of policy goals, including various fairness metrics, accuracy, coverage, recall, and churn."
            ],
            "keywords": [
                "constrained optimization",
                "non-convex",
                "fairness",
                "churn",
                "swap regret",
                "non-zero-sum game"
            ],
            "author": [
                "Andrew Cotter",
                "Heinrich Jiang",
                "Maya Gupta",
                "Serena Wang",
                "Taman Narayan",
                "Karthik Sridharan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-616/18-616.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Similarity-based Classification: Concepts and Algorithms",
            "abstract": [
                "This paper reviews and extends the field of similarity-based classification, presenting new analyses, algorithms, data sets, and a comprehensive set of experimental results for a rich collection of classification problems. Specifically, the generalizability of using similarities as features is analyzed, design goals and methods for weighting nearest-neighbors for similarity-based learning are proposed, and different methods for consistently converting similarities into kernels are compared. Experiments on eight real data sets compare eight approaches and their variants to similarity-based learning."
            ],
            "keywords": [
                "similarity",
                "dissimilarity",
                "similarity-based learning",
                "indefinite kernels"
            ],
            "author": [
                "Yihua Chen",
                "Eric K Garcia",
                "Maya R Gupta",
                "Ali Rahimi",
                "Luca Cazzanti",
                "Alexander J Smola"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/chen09a/chen09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Horn Expressions with LOGAN-H",
            "abstract": [
                "The paper introduces LOGAN-H-a system for learning first-order function-free Horn expressions from interpretations. The system is based on an algorithm that learns by asking questions and that was proved correct in previous work. The current paper shows how the algorithm can be implemented in a practical system, and introduces a new algorithm based on it that avoids interaction and learns from examples only. The LOGAN-H system implements these algorithms and adds several facilities and optimizations that allow efficient applications in a wide range of problems. As one of the important ingredients, the system includes several fast procedures for solving the subsumption problem, an NP-complete problem that needs to be solved many times during the learning process. We describe qualitative and quantitative experiments in several domains. The experiments demonstrate that the system can deal with varied problems, large amounts of data, and that it achieves good classification accuracy."
            ],
            "keywords": [
                "inductive logic programming",
                "subsumption",
                "bottom-up learning",
                "learning with queries"
            ],
            "author": [
                "Marta Arias",
                "Roni Khardon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/arias07a/arias07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Agnostic Insurability of Model Classes",
            "abstract": [
                "Motivated by problems in insurance, our task is to predict finite upper bounds on a future draw from an unknown distribution p over natural numbers. We can only use past observations generated independently and identically distributed according to p. While p is unknown, it is known to belong to a given collection P of probability distributions on the natural numbers. The support of the distributions p ∈ P may be unbounded, and the prediction game goes on for infinitely many draws. We are allowed to make observations without predicting upper bounds for some time. But we must, with probability 1, start and then continue to predict upper bounds after a finite time irrespective of which p ∈ P governs the data. If it is possible, without knowledge of p and for any prescribed confidence however close to 1, to come up with a sequence of upper bounds that is never violated over an infinite time window with confidence at least as big as prescribed, we say the model class P is insurable. We completely characterize the insurability of any class P of distributions over natural numbers by means of a condition on how the neighborhoods of distributions in P should be, one that is both necessary and sufficient. Keywords: insurance, 1 topology of probability distributions over countable sets, non-parametric approaches, prediction of quantiles of distributions, universal compression"
            ],
            "keywords": [],
            "author": [
                "Narayana Santhanam",
                "Venkat Anantharam",
                "Gabor Lugosi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/santhanam15a/santhanam15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Patchwork Kriging for Large-scale Gaussian Process Regression",
            "abstract": [
                "This paper presents a new approach for Gaussian process (GP) regression for large datasets. The approach involves partitioning the regression input domain into multiple local regions with a different local GP model fitted in each region. Unlike existing local partitioned GP approaches, we introduce a technique for patching together the local GP models nearly seamlessly to ensure that the local GP models for two neighboring regions produce nearly the same response prediction and prediction error variance on the boundary between the two regions. This largely mitigates the well-known discontinuity problem that degrades the prediction accuracy of existing local partitioned GP methods over regional boundaries. Our main innovation is to represent the continuity conditions as additional pseudo-observations that the differences between neighboring GP responses are identically zero at an appropriately chosen set of boundary input locations. To predict the response at any input location, we simply augment the actual response observations with the pseudo-observations and apply standard GP prediction methods to the augmented data. In contrast to heuristic continuity adjustments, this has an advantage of working within a formal GP framework, so that the GP-based predictive uncertainty quantification remains valid. Our approach also inherits a sparse block-like structure for the sample covariance matrix, which results in computationally efficient closed-form expressions for the predictive mean and variance. In addition, we provide a new spatial partitioning scheme based on a recursive space partitioning along local principal component directions, which makes the proposed approach applicable for regression domains having more than two dimensions. Using three spatial datasets and three higher dimensional datasets, we investigate the numerical performance of the approach and compare it to several state-of-the-art approaches."
            ],
            "keywords": [
                "Local Kriging",
                "Model Split and Merge",
                "Pseudo Observations",
                "Spatial Partition"
            ],
            "author": [
                "Chiwoo Park",
                "Daniel Apley"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-042/17-042.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Logistic Stick-Breaking Process",
            "abstract": [
                "A logistic stick-breaking process (LSBP) is proposed for non-parametric clustering of general spatially-or temporally-dependent data, imposing the belief that proximate data are more likely to be clustered together. The sticks in the LSBP are realized via multiple logistic regression functions, with shrinkage priors employed to favor contiguous and spatially localized segments. The LSBP is also extended for the simultaneous processing of multiple data sets, yielding a hierarchical logistic stick-breaking process (H-LSBP). The model parameters (atoms) within the H-LSBP are shared across the multiple learning tasks. Efficient variational Bayesian inference is derived, and comparisons are made to related techniques in the literature. Experimental analysis is performed for audio waveforms and images, and it is demonstrated that for segmentation applications the LSBP yields generally homogeneous segments with sharp boundaries."
            ],
            "keywords": [
                "Bayesian",
                "nonparametric",
                "dependent",
                "hierarchical models",
                "segmentation"
            ],
            "author": [
                "Lu Ren",
                "Lan Du",
                "Lawrence Carin",
                "David B Dunson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/ren11a/ren11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Incremental Identification of Qualitative Models of Biological Systems using Inductive Logic Programming",
            "abstract": [
                "The use of computational models is increasingly expected to play an important role in predicting the behaviour of biological systems. Models are being sought at different scales of biological organisation namely: sub-cellular, cellular, tissue, organ, organism and ecosystem; with a view of identifying how different components are connected together, how they are controlled and how they behave when functioning as a system. Except for very simple biological processes, system identification from first principles can be extremely difficult. This has brought into focus automated techniques for constructing models using data of system behaviour. Such techniques face three principal issues: (1) The model representation language must be rich enough to capture system behaviour; (2) The system identification technique must be powerful enough to identify substantially complex models; and (3) There may not be sufficient data to obtain both the model's structure and precise estimates of all of its parameters. In this paper, we address these issues in the following ways: (1) Models are represented in an expressive subset of first-order logic. Specifically, they are expressed as logic programs; (2) System identification is done using techniques developed in Inductive Logic Programming (ILP). This allows the identification of first-order logic models from data. Specifically, we employ an incremental approach in which increasingly complex models are constructed from simpler ones using snapshots of system behaviour; and (3) We restrict ourselves to \"qualitative\" models. These are non-parametric: thus, usually less data are required than for identifying parametric quantitative models. A further advantage is that the data need not be precise numerical observations (instead, they are abstractions like positive, negative, zero, increasing, decreasing and so on). We describe incremental construction of qualitative models using a simple physical system and demonstrate its application to identification of models at four scales of biological organisation, namely: (a) a predator-prey model at the ecosystem level; (b) a model for the human lung at the organ level; (c) a model for regulation of glucose by insulin in the human body at the extra-cellular level; and (d) a model for the glycolysis metabolic pathway at the cellular level."
            ],
            "keywords": [
                "ILP",
                "qualitative system identification",
                "biology"
            ],
            "author": [
                "Ashwin Srinivasan",
                "Ross D King"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/srinivasan08a/srinivasan08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Causal Networks via Additive Faithfulness",
            "abstract": [
                "In this paper we introduce a statistical model, called additively faithful directed acyclic graph (AFDAG), for causal learning from observational data. Our approach is based on additive conditional independence (ACI), a recently proposed three-way statistical relation that shares many similarities with conditional independence but without resorting to multidimensional kernels. This distinct feature strikes a balance between a parametric model and a fully nonparametric model, which makes the proposed model attractive for handling large networks. We develop an estimator for AFDAG based on a linear operator that characterizes ACI, and establish the consistency and convergence rates of this estimator, as well as the uniform consistency of the estimated DAG. Moreover, we introduce a modified PC-algorithm to implement the estimating procedure efficiently, so that its complexity is determined by the level of sparseness rather than the dimension of the network. Through simulation studies we show that our method outperforms existing methods when commonly assumed conditions such as Gaussian or Gaussian copula distributions do not hold. Finally, the usefulness of AFDAG formulation is demonstrated through an application to a proteomics data set."
            ],
            "keywords": [
                "additive conditional independence",
                "additive reproducing kernel Hilbert space",
                "directed acyclic graph",
                "global Markov property",
                "normalized additive conditional covariance operator",
                "PC-algorithm"
            ],
            "author": [
                "Kuang-Yao Lee",
                "Tianqi Liu",
                "Bing Li",
                "Hongyu Zhao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/16-252/16-252.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Perturbation Corrections in Approximate Inference: Mixture Modelling Applications",
            "abstract": [
                "Bayesian inference is intractable for many interesting models, making deterministic algorithms for approximate inference highly desirable. Unlike stochastic methods, which are exact in the limit, the accuracy of these approaches cannot be reasonably judged. In this paper we show how low order perturbation corrections to an expectation-consistent (EC) approximation can provide the necessary tools to ameliorate inference accuracy, and to give an indication of the quality of approximation without having to resort to Monte Carlo methods. Further comparisons are given with variational Bayes and parallel tempering (PT) combined with thermodynamic integration on a Gaussian mixture model. To obtain practical results we further generalize PT to temper from arbitrary distributions rather than a prior in Bayesian inference."
            ],
            "keywords": [
                "Bayesian inference",
                "mixture models",
                "expectation propagation",
                "expectation consistent",
                "perturbation correction",
                "variational Bayes",
                "parallel tempering",
                "thermodynamic integration"
            ],
            "author": [
                "Ulrich Paquet",
                "Ole Winther",
                "Manfred Opper"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/paquet09a/paquet09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Prior Knowledge and Preferential Structures in Gradient Descent Learning Algorithms *",
            "abstract": [
                "A family of gradient descent algorithms for learning linear functions in an online setting is considered. The family includes the classical LMS algorithm as well as new variants such as the Exponentiated Gradient (EG) algorithm due to Kivinen and Warmuth. The algorithms are based on prior distributions defined on the weight space. Techniques from differential geometry are used to develop the algorithms as gradient descent iterations with respect to the natural gradient in the Riemannian structure induced by the prior distribution. The proposed framework subsumes the notion of \"link-functions\"."
            ],
            "keywords": [
                "Gradient descent",
                "exponentiated gradient algorithm",
                "natural gradient",
                "linkfunctions",
                "Riemannian metric"
            ],
            "author": [
                "Robert E Mahony",
                "Robert C Williamson"
            ],
            "ref": "http://www.jmlr.org/papers/volume1/mahony01a/mahony01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dimensionality Reduction via Sparse Support Vector Machines",
            "abstract": [
                "We describe a methodology for performing variable ranking and selection using support vector machines (SVMs). The method constructs a series of sparse linear SVMs to generate linear models that can generalize well, and uses a subset of nonzero weighted variables found by the linear models to produce a final nonlinear model. The method exploits the fact that a linear SVM (no kernels) with 1-norm regularization inherently performs variable selection as a side-effect of minimizing capacity of the SVM model. The distribution of the linear model weights provides a mechanism for ranking and interpreting the effects of variables. Starplots are used to visualize the magnitude and variance of the weights for each variable. We illustrate the effectiveness of the methodology on synthetic data, benchmark problems, and challenging regression problems in drug design. This method can dramatically reduce the number of variables and outperforms SVMs trained using all attributes and using the attributes selected according to correlation coefficients. The visualization of the resulting models is useful for understanding the role of underlying variables."
            ],
            "keywords": [
                "Variable Selection",
                "Dimensionality Reduction",
                "Support Vector Machines",
                "Regression",
                "Pattern Search",
                "Bootstrap Aggregation",
                "Model Visualization"
            ],
            "author": [
                "Jinbo Bi",
                "Kristin P Bennett",
                "Mark Embrechts",
                "Curt M Breneman",
                "Isabelle Guyon",
                "André Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/bi03a/bi03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Domain-Adversarial Training of Neural Networks",
            "abstract": [
                "We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application."
            ],
            "keywords": [
                "domain adaptation",
                "neural network",
                "representation learning",
                "deep learning",
                "synthetic data",
                "image classification",
                "sentiment analysis",
                "person re-identification"
            ],
            "author": [
                "Yaroslav Ganin",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "François Laviolette",
                "Mario Marchand",
                "Victor Lempitsky",
                "Urun Dogan",
                "Marius Kloft",
                "Francesco Orabona",
                "Tatiana Tommasi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-239/15-239.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cramer-Wold Auto-Encoder",
            "abstract": [
                "The computation of the distance to the true distribution is a key component of most state-ofthe-art generative models. Inspired by prior works on the Sliced-Wasserstein Auto-Encoders (SWAE) and the Wasserstein Auto-Encoders with MMD-based penalty (WAE-MMD), we propose a new generative model-a Cramer-Wold Auto-Encoder (CWAE). A fundamental component of CWAE is the characteristic kernel, the construction of which is one of the goals of this paper, from here on referred to as the Cramer-Wold kernel. Its main distinguishing feature is that it has a closed-form of the kernel product of radial Gaussians. Consequently, CWAE model has a closed-form for the distance between the posterior and the normal prior, which simplifies the optimization procedure by removing the need to sample in order to compute the loss function. At the same time, CWAE performance often improves upon WAE-MMD and SWAE on standard benchmarks."
            ],
            "keywords": [
                "Auto-Encoder",
                "Generative model",
                "Wasserstein Auto-Encoder",
                "Cramer-Wold Theorem",
                "Deep neural network"
            ],
            "author": [
                "Szymon Knop",
                "Jacek Tabor",
                "Igor Podolak",
                "Stanisław Jastrzębski"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-560/19-560.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction",
            "abstract": [
                "Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable features and identify the support vectors. It has achieved great successes in many real-world applications. However, for large-scale problems involving a huge number of samples and ultra-high dimensional features, solving sparse SVMs remains challenging. By noting that sparse SVMs induce sparsities in both feature and sample spaces, we propose a novel approach, which is based on accurate estimations of the primal and dual optima of sparse SVMs, to simultaneously identify the inactive features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identified inactive samples and features from the training phase, leading to substantial savings in the computational cost without sacrificing the accuracy. Moreover, we show that our method can be extended to multi-class sparse support vector machines. To the best of our knowledge, the proposed method is the first static feature and sample reduction method for sparse SVMs and multi-class sparse SVMs. Experiments on both synthetic and real data sets demonstrate that our approach significantly outperforms state-of-the-art methods and the speedup gained by our approach can be orders of magnitude."
            ],
            "keywords": [
                "screening",
                "SVM",
                "dual problem",
                "optimization",
                "classification"
            ],
            "author": [
                "Weizhong Zhang",
                "Wei Liu",
                "Deng Cai",
                "C Bin Hong",
                "Jieping Ye",
                "Xiaofei He",
                "Jie Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-723/17-723.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization *",
            "abstract": [
                "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms."
            ],
            "keywords": [
                "subgradient methods",
                "adaptivity",
                "online learning",
                "stochastic convex optimization"
            ],
            "author": [
                "John Duchi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/duchi11a/duchi11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation",
            "abstract": [
                "Gaussian processes (GPs) are flexible distributions over functions that enable highlevel assumptions about unknown functions to be encoded in a parsimonious, flexible and general way. Although elegant, the application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. In this paper we develop a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. Unlike much of the previous venerable work in this area, the new framework is built on standard methods for approximate inference (variational free-energy, EP and Power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way all of the approximation is performed at 'inference time' rather than at 'modelling time', resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression and classification tasks."
            ],
            "keywords": [
                "Gaussian process",
                "expectation propagation",
                "variational inference",
                "sparse approximation"
            ],
            "author": [
                "Thang D Bui",
                "Josiah Yan",
                "Richard E Turner"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-603/16-603.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Refinery: An Open Source Topic Modeling Web Platform",
            "abstract": [
                "We introduce Refinery, an open source platform for exploring large text document collections with topic models. Refinery is a standalone web application driven by a graphical interface, so it is usable by those without machine learning or programming expertise. Users can interactively organize articles by topic and also refine this organization with phrase-level analysis. Under the hood, we train Bayesian nonparametric topic models that can adapt model complexity to the provided data with scalable learning algorithms. The project website http://daeilkim.github.io/refinery/ contains Python code and further documentation."
            ],
            "keywords": [
                "topic models",
                "visualization",
                "software"
            ],
            "author": [
                "Daeil Kim",
                "Benjamin F Swanson",
                "Michael C Hughes",
                "Erik B Sudderth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-441/15-441.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rate Minimaxity of the Lasso and Dantzig Selector for the ℓ q Loss in ℓ r Balls Fei Ye",
            "abstract": [
                "We consider the estimation of regression coefficients in a high-dimensional linear model. For regression coefficients in ℓ r balls, we provide lower bounds for the minimax ℓ q risk and minimax quantiles of the ℓ q loss for all design matrices. Under an ℓ 0 sparsity condition on a target coefficient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. We derive oracle inequalities for target coefficient vectors with many small elements and smaller threshold levels than the universal threshold. These oracle inequalities provide sufficient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the ℓ q risk and loss in ℓ r balls, 0 ≤ r ≤ 1 ≤ q ≤ ∞. By allowing q = ∞, our risk bounds imply the variable selection consistency of threshold Lasso and Dantzig selectors."
            ],
            "keywords": [
                "variable selection",
                "estimation",
                "oracle inequality",
                "minimax",
                "linear regression",
                "penalized least squares",
                "linear programming"
            ],
            "author": [
                "FYE@DRW Com",
                "Cun-Hui Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/ye10a/ye10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ooDACE Toolbox: A Flexible Object-Oriented Kriging Implementation",
            "abstract": [
                "When analyzing data from computationally expensive simulation codes, surrogate modeling methods are firmly established as facilitators for design space exploration, sensitivity analysis, visualization and optimization. Kriging is a popular surrogate modeling technique used for the Design and Analysis of Computer Experiments (DACE). Hence, the past decade Kriging has been the subject of extensive research and many extensions have been proposed, e.g., co-Kriging, stochastic Kriging, blind Kriging, etc. However, few Kriging implementations are publicly available and tailored towards scientists and engineers. Furthermore, no Kriging toolbox exists that unifies several Kriging flavors. This paper addresses this need by presenting an efficient object-oriented Kriging implementation and several Kriging extensions, providing a flexible and easily extendable framework to test and implement new Kriging flavors while reusing as much code as possible."
            ],
            "keywords": [
                "Kriging",
                "Gaussian process",
                "co-Kriging",
                "blind Kriging",
                "surrogate modeling",
                "metamodeling",
                "DACE"
            ],
            "author": [
                "Ivo Couckuyt",
                "Tom Dhaene",
                "Piet Demeester"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/couckuyt14a/couckuyt14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Kernel Two-Sample Test",
            "abstract": [
                "We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distributionfree tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests."
            ],
            "keywords": [
                "kernel methods",
                "two-sample test",
                "uniform convergence bounds",
                "schema matching",
                "integral probability metric",
                "hypothesis testing"
            ],
            "author": [
                "Arthur Gretton",
                "Karsten M Borgwardt",
                "Tuebingen Mpg De",
                "Malte J Rasch",
                "Xinjiekouwai St",
                "Bernhard Schölkopf",
                "Alexander Smola",
                "BORGWARDT, RASCH, SCHÖLKOPF AND SMOLA Gretton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/gretton12a/gretton12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Convergence Rate of ℓ p -Norm Multiple Kernel Learning *",
            "abstract": [
                "We derive an upper bound on the local Rademacher complexity of ℓ p-norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely fast convergence rates of the order O(n − α 1+α), where α is the minimum eigenvalue decay rate of the individual kernels."
            ],
            "keywords": [
                "multiple kernel learning",
                "learning kernels",
                "generalization bounds",
                "local Rademacher complexity"
            ],
            "author": [
                "Marius Kloft",
                "Gilles Blanchard"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/kloft12a/kloft12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Trans-dimensional von Mises-Fisher Mixture Models for User Profiles",
            "abstract": [
                "The proliferation of online communities has attracted much attention to modelling user behaviour in terms of social interaction, language adoption and contribution activity. Nevertheless, when applied to large-scale and cross-platform behavioural data, existing approaches generally suffer from expressiveness, scalability and generality issues. This paper proposes trans-dimensional von Mises-Fisher (TvMF) mixture models for L 2 normalised behavioural data, which encapsulate: (1) a Bayesian framework for vMF mixtures that enables prior knowledge and information sharing among clusters, (2) an extended version of reversible jump MCMC algorithm that allows adaptive changes in the number of clusters for vMF mixtures when the model parameters are updated, and (3) an online TvMF mixture model that accommodates the dynamics of clusters for time-varying user behavioural data. We develop efficient collapsed Gibbs sampling techniques for posterior inference, which facilitates parallelism for parameter updates. Empirical results on simulated and real-world data show that the proposed TvMF mixture models can discover more interpretable and intuitive clusters than other widely-used models, such as k-means, non-negative matrix factorization (NMF), Dirichlet process Gaussian mixture models (DP-GMM), and dynamic topic models (DTM). We further evaluate the performance of proposed models in real-world applications, such as the churn prediction task, that shows the usefulness of the features generated."
            ],
            "keywords": [
                "Mixture Models",
                "von Mises-Fisher",
                "Bayesian Nonparametric",
                "Temporal Evolution",
                "User Modelling"
            ],
            "author": [
                "Xiangju Qin",
                "Michael Salter-Townshend"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-454/15-454.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Choice of V for V -Fold Cross-Validation in Least-Squares Density Estimation",
            "abstract": [
                "This paper studies V-fold cross-validation for model selection in least-squares density estimation. The goal is to provide theoretical grounds for choosing V in order to minimize the least-squares loss of the selected estimator. We first prove a non-asymptotic oracle inequality for V-fold cross-validation and its bias-corrected version (V-fold penalization). In particular, this result implies that V-fold penalization is asymptotically optimal in the nonparametric case. Then, we compute the variance of V-fold cross-validation and related criteria, as well as the variance of key quantities for model selection performance. We show that these variances depend on V like 1 + 4/(V − 1), at least in some particular cases, suggesting that the performance increases much from V = 2 to V = 5 or 10, and then is almost constant. Overall, this can explain the common advice to take V = 5-at least in our setting and when the computational power is limited-, as supported by some simulation experiments. An oracle inequality and exact formulas for the variance are also proved for Monte-Carlo cross-validation, also known as repeated cross-validation, where the parameter V is replaced by the number B of random splits of the data."
            ],
            "keywords": [
                "V -fold cross-validation",
                "Monte-Carlo cross-validation",
                "leave-one-out",
                "leave-pout",
                "resampling penalties",
                "density estimation",
                "model selection",
                "penalization"
            ],
            "author": [
                "Sylvain Arlot",
                "Matthieu Lerasle"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-296/14-296.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "New Perspectives on k-Support and Cluster Norms",
            "abstract": [
                "We study a regularizer which is defined as a parameterized infimum of quadratics, and which we call the box-norm. We show that the k-support norm, a regularizer proposed by Argyriou et al. (2012) for sparse vector prediction problems, belongs to this family, and the box-norm can be generated as a perturbation of the former. We derive an improved algorithm to compute the proximity operator of the squared box-norm, and we provide a method to compute the norm. We extend the norms to matrices, introducing the spectral k-support norm and spectral box-norm. We note that the spectral box-norm is essentially equivalent to the cluster norm, a multitask learning regularizer introduced by Jacob et al. (2009a), and which in turn can be interpreted as a perturbation of the spectral k-support norm. Centering the norm is important for multitask learning and we also provide a method to use centered versions of the norms as regularizers. Numerical experiments indicate that the spectral k-support and box-norms and their centered variants provide state of the art performance in matrix completion and multitask learning problems respectively."
            ],
            "keywords": [
                "Convex optimization",
                "matrix completion",
                "multitask learning",
                "spectral regularization",
                "structured sparsity"
            ],
            "author": [
                "Andrew M Mcdonald",
                "Massimiliano Pontil",
                "Dimitris Stamos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-151/15-151.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Inductive Bias of Dropout",
            "abstract": [
                "Dropout is a simple but effective technique for learning in neural networks and other settings. A sound theoretical understanding of dropout is needed to determine when dropout should be applied and how to use it most effectively. In this paper we continue the exploration of dropout as a regularizer pioneered by Wager et al. We focus on linear classification where a convex proxy to the misclassification loss (i.e. the logistic loss used in logistic regression) is minimized. We show: • when the dropout-regularized criterion has a unique minimizer, • when the dropout-regularization penalty goes to infinity with the weights, and when it remains bounded, • that the dropout regularization can be non-monotonic as individual weights increase from 0, and • that the dropout regularization penalty may not be convex. This last point is particularly surprising because the combination of dropout regularization with any convex loss proxy is always a convex function. In order to contrast dropout regularization with L 2 regularization, we formalize the notion of when different random sources of data are more compatible with different regularizers. We then exhibit distributions that are provably more compatible with dropout regularization than L 2 regularization, and vice versa. These sources provide additional insight into how the inductive biases of dropout and L 2 regularization differ. We provide some similar results for L 1 regularization."
            ],
            "keywords": [
                "dropout",
                "inductive bias",
                "learning theory",
                "regularization",
                "feature noising"
            ],
            "author": [
                "David P Helmbold",
                "Philip M Long",
                "Samy Bengio"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/helmbold15a/helmbold15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Consistency of Kernel Canonical Correlation Analysis",
            "abstract": [
                "While kernel canonical correlation analysis (CCA) has been applied in many contexts, the convergence of finite sample estimates of the associated functions to their population counterparts has not yet been established. This paper gives a mathematical proof of the statistical convergence of kernel CCA, providing a theoretical justification for the method. The proof uses covariance operators defined on reproducing kernel Hilbert spaces, and analyzes the convergence of their empirical estimates of finite rank to their population counterparts, which can have infinite rank. The result also gives a sufficient condition for convergence on the regularization coefficient involved in kernel CCA: this should decrease as n −1/3 , where n is the number of data."
            ],
            "keywords": [
                "canonical correlation analysis",
                "kernel",
                "consistency",
                "regularization",
                "Hilbert space"
            ],
            "author": [
                "Kenji Fukumizu",
                "Francis R Bach",
                "Arthur Gretton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/fukumizu07a/fukumizu07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptation Based on Generalized Discrepancy",
            "abstract": [
                "We present a new algorithm for domain adaptation improving upon a discrepancy minimization algorithm, (DM), previously shown to outperform a number of algorithms for this problem. Unlike many previously proposed solutions for domain adaptation, our algorithm does not consist of a fixed reweighting of the losses over the training sample. Instead, the reweighting depends on the hypothesis sought. The algorithm is derived from a less conservative notion of discrepancy than the DM algorithm called generalized discrepancy. We present a detailed description of our algorithm and show that it can be formulated as a convex optimization problem. We also give a detailed theoretical analysis of its learning guarantees which helps us select its parameters. Finally, we report the results of experiments demonstrating that it improves upon discrepancy minimization."
            ],
            "keywords": [],
            "author": [
                "Corinna Cortes",
                "Mehryar Mohri",
                "Andrés Muñoz Medina"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/15-192/15-192.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large Margin Semi-supervised Learning",
            "abstract": [
                "In classification, semi-supervised learning occurs when a large amount of unlabeled data is available with only a small number of labeled data. In such a situation, how to enhance predictability of classification through unlabeled data is the focus. In this article, we introduce a novel large margin semi-supervised learning methodology, using grouping information from unlabeled data, together with the concept of margins, in a form of regularization controlling the interplay between labeled and unlabeled data. Based on this methodology, we develop two specific machines involving support vector machines and ψ-learning, denoted as SSVM and SPSI, through difference convex programming. In addition, we estimate the generalization error using both labeled and unlabeled data, for tuning regularizers. Finally, our theoretical and numerical analyses indicate that the proposed methodology achieves the desired objective of delivering high performance in generalization, particularly against some strong performers."
            ],
            "keywords": [
                "generalization",
                "grouping",
                "sequential quadratic programming",
                "support vectors"
            ],
            "author": [
                "Junhui Wang",
                "Xiaotong Shen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/wang07a/wang07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Orlicz Random Fourier Features",
            "abstract": [
                "Kernel techniques are among the most widely-applied and influential tools in machine learning with applications at virtually all areas of the field. To combine this expressive power with computational efficiency numerous randomized schemes have been proposed in the literature, among which probably random Fourier features (RFF) are the simplest and most popular. While RFFs were originally designed for the approximation of kernel values, recently they have been adapted to kernel derivatives, and hence to the solution of large-scale tasks involving function derivatives. Unfortunately, the understanding of the RFF scheme for the approximation of higher-order kernel derivatives is quite limited due to the challenging polynomial growing nature of the underlying function class in the empirical process. To tackle this difficulty, we establish a finite-sample deviation bound for a general class of polynomial-growth functions under α-exponential Orlicz condition on the distribution of the sample. Instantiating this result for RFFs, our finite-sample uniform guarantee implies a.s. convergence with tight rate for arbitrary kernel with α-exponential Orlicz spectrum and any order of derivative."
            ],
            "keywords": [
                "random Fourier features",
                "kernel derivative",
                "polynomial-growth functions",
                "α-exponential Orlicz norm",
                "unbounded empirical processes"
            ],
            "author": [
                "Linda Chamakh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-1031/19-1031.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Computationally Tractable Selection of Experiments in Measurement-Constrained Regression Models",
            "abstract": [
                "We derive computationally tractable methods to select a small subset of experiment settings from a large pool of given design points. The primary focus is on linear regression models, while the technique extends to generalized linear models and Delta's method (estimating functions of linear regression models) as well. The algorithms are based on a continuous relaxation of an otherwise intractable combinatorial optimization problem, with sampling or greedy procedures as post-processing steps. Formal approximation guarantees are established for both algorithms, and numerical results on both synthetic and real-world data confirm the effectiveness of the proposed methods."
            ],
            "keywords": [
                "optimal selection of experiments",
                "A-optimality",
                "computationally tractable methods",
                "minimax analysis"
            ],
            "author": [
                "Yining Wang",
                "Adams Wei Yu",
                "Aarti Singh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-175/17-175.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning from Partial Labels",
            "abstract": [
                "We address the problem of partially-labeled multiclass classification, where instead of a single label per instance, the algorithm is given a candidate set of labels, only one of which is correct. Our setting is motivated by a common scenario in many image and video collections, where only partial access to labels is available. The goal is to learn a classifier that can disambiguate the partiallylabeled training instances, and generalize to unseen data. We define an intuitive property of the data distribution that sharply characterizes the ability to learn in this setting and show that effective learning is possible even when all the data is only partially labeled. Exploiting this property of the data, we propose a convex learning formulation based on minimization of a loss function appropriate for the partial label setting. We analyze the conditions under which our loss function is asymptotically consistent, as well as its generalization and transductive performance. We apply our framework to identifying faces culled from web news sources and to naming characters in TV series and movies; in particular, we annotated and experimented on a very large video data set and achieve 6% error for character naming on 16 episodes of the TV series Lost."
            ],
            "keywords": [
                "weakly supervised learning",
                "multiclass classification",
                "convex learning",
                "generalization bounds",
                "names and faces"
            ],
            "author": [
                "Timothee Cour",
                "Benjamin Sapp",
                "Ben Taskar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/cour11a/cour11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Flexible High-Dimensional Classification Machines and Their Asymptotic Properties",
            "abstract": [
                "Classification is an important topic in statistics and machine learning with great potential in many real applications. In this paper, we investigate two popular large-margin classification methods, Support Vector Machine (SVM) and Distance Weighted Discrimination (DWD), under two contexts: the high-dimensional, low-sample size data and the imbalanced data. A unified family of classification machines, the FLexible Assortment MachinE (FLAME) is proposed, within which DWD and SVM are special cases. The FLAME family helps to identify the similarities and differences between SVM and DWD. It is well known that many classifiers overfit the data in the high-dimensional setting; and others are sensitive to the imbalanced data, that is, the class with a larger sample size overly influences the classifier and pushes the decision boundary towards the minority class. SVM is resistant to the imbalanced data issue, but it overfits high-dimensional data sets by showing the undesired data-piling phenomenon. The DWD method was proposed to improve SVM in the highdimensional setting, but its decision boundary is sensitive to the imbalanced ratio of sample sizes. Our FLAME family helps to understand an intrinsic connection between SVM and DWD, and provides a trade-off between sensitivity to the imbalanced data and overfitting the high-dimensional data. Several asymptotic properties of the FLAME classifiers are studied. Simulations and real data applications are investigated to illustrate theoretical findings."
            ],
            "keywords": [
                "classification",
                "Fisher consistency",
                "high-dimensional low-sample size asymptotics",
                "imbalanced data",
                "support vector machine"
            ],
            "author": [
                "Xingye Qiao",
                "Lingsong Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/qiao15a/qiao15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Training and Testing Low-degree Polynomial Data Mappings via Linear SVM",
            "abstract": [
                "Kernel techniques have long been used in SVM to handle linearly inseparable problems by transforming data to a high dimensional space, but training and testing large data sets is often time consuming. In contrast, we can efficiently train and test much larger data sets using linear SVM without kernels. In this work, we apply fast linear-SVM methods to the explicit form of polynomially mapped data and investigate implementation issues. The approach enjoys fast training and testing, but may sometimes achieve accuracy close to that of using highly nonlinear kernels. Empirical experiments show that the proposed method is useful for certain large-scale data sets. We successfully apply the proposed method to a natural language processing (NLP) application by improving the testing accuracy under some training/testing speed requirements."
            ],
            "keywords": [
                "decomposition methods",
                "low-degree polynomial mapping",
                "kernel functions",
                "support vector machines",
                "dependency parsing",
                "natural language processing"
            ],
            "author": [
                "Yin-Wen Chang",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang",
                "Michael Ringgaard",
                "Chih-Jen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/chang10a/chang10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Least-squares Approach to Direct Importance Estimation *",
            "abstract": [
                "We address the problem of estimating the ratio of two probability density functions, which is often referred to as the importance. The importance values can be used for various succeeding tasks such as covariate shift adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally highly efficient and simple to implement. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bounds. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efficient than competing approaches."
            ],
            "keywords": [
                "importance sampling",
                "covariate shift adaptation",
                "novelty detection",
                "regularization path",
                "leave-one-out cross validation"
            ],
            "author": [
                "Takafumi Kanamori",
                "Shohei Hido"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/kanamori09a/kanamori09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Reinforcement Learning with Bayesian Optimisation and Quadrature",
            "abstract": [
                "Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables: state features that are unobservable and randomly determined by the environment in a physical setting but are controllable in a simulator. This article considers the problem of finding a robust policy while taking into account the impact of environment variables. We present alternating optimisation and quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. We also present transferable ALOQ (TALOQ), for settings where simulator inaccuracies lead to difficulty in transferring the learnt policy to the physical system. We show that our algorithms are robust to the presence of significant rare events, which may not be observable under random sampling but play a substantial role in determining the optimal policy. Experimental results across different domains show that our algorithms learn robust policies efficiently."
            ],
            "keywords": [
                "Bayesian Optimization Special Issue Reinforcement Learning",
                "Bayesian Optimisation",
                "Bayesian Quadrature",
                "Significant rare events",
                "Environment variables"
            ],
            "author": [
                "Supratik Paul",
                "Kamil Ciosek",
                "Jean-Baptiste Mouret",
                "Michael A Osborne",
                "Shimon Whiteson",
                "Konstantinos Paul",
                "Kamil Chatzilygeroudis",
                "Jean-Baptiste Ciosek",
                "Michael A Mouret",
                "Shimon Osborne"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-216/18-216.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Topology of Deep Neural Networks",
            "abstract": [
                "We study how the topology of a data set M = M a ∪M b ⊆ R d , representing two classes a and b in a binary classification problem, changes as it passes through the layers of a well-trained neural network, i.e., one with perfect accuracy on training set and near-zero generalization error (≈ 0.01%). The goal is to shed light on two mysteries in deep neural networks: (i) a nonsmooth activation function like ReLU outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, even though a shallow network can approximate any function arbitrarily well. We performed extensive experiments on the persistent homology of a wide range of point cloud data sets, both real and simulated. The results consistently demonstrate the following: Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of M we begin with, when passed through a well-trained neural network f : R d → R p , there is a vast reduction in the Betti numbers of both components M a and M b ; in fact they nearly always reduce to their lowest possible values: β k f (M i) = 0 for k ≥ 1 and β 0 f (M i) = 1, i = a, b. The reduction in Betti numbers is significantly faster for ReLU activation than for hyperbolic tangent activation as the former defines nonhomeomorphic maps that change topology, whereas the latter defines homeomorphic maps that preserve topology. Shallow and deep networks transform data sets differently-a shallow network operates mainly through changing geometry and changes topology only in its final layers, a deep one spreads topological changes more evenly across all layers."
            ],
            "keywords": [
                "neural networks",
                "topology change",
                "Betti numbers",
                "topological complexity",
                "persistent homology"
            ],
            "author": [
                "Gregory Naitzat",
                "Andrey Zhitnikov",
                "Lek-Heng Lim"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-345/20-345.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Two Distributed-State Models For Generating High-Dimensional Time Series *",
            "abstract": [
                "In this paper we develop a class of nonlinear generative models for high-dimensional time series. We first propose a model based on the restricted Boltzmann machine (RBM) that uses an undirected model with binary latent variables and real-valued \"visible\" variables. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. This \"conditional\" RBM (CRBM) makes on-line inference efficient and allows us to use a simple approximate learning procedure. We demonstrate the power of our approach by synthesizing various sequences from a model trained on motion capture data and by performing on-line filling in of data lost during capture. We extend the CRBM in a way that preserves its most important computational properties and introduces multiplicative three-way interactions that allow the effective interaction weight between two variables to be modulated by the dynamic state of a third variable. We introduce a factoring of the implied three-way weight tensor to permit a more compact parameterization. The resulting model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve its ability to blend motion styles or to transition smoothly among them."
            ],
            "keywords": [
                "unsupervised learning",
                "restricted Boltzmann machines",
                "time series",
                "generative models",
                "motion capture"
            ],
            "author": [
                "Graham W Taylor",
                "Geoffrey E Hinton",
                "Sam T Roweis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/taylor11a/taylor11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Redundancy Techniques for Straggler Mitigation in Distributed Optimization and Learning",
            "abstract": [
                "Performance of distributed optimization and learning systems is bottlenecked by \"straggler\" nodes and slow communication links, which significantly delay computation. We propose a distributed optimization framework where the dataset is \"encoded\" to have an overcomplete representation with built-in redundancy, and the straggling nodes in the system are dynamically treated as missing, or as \"erasures\" at every iteration, whose loss is compensated by the embedded redundancy. For quadratic loss functions, we show that under a simple encoding scheme, many optimization algorithms (gradient descent, L-BFGS, and proximal gradient) operating under data parallelism converge to an approximate solution even when stragglers are ignored. Furthermore, we show a similar result for a wider class of convex loss functions when operating under model parallelism. The applicable classes of objectives covers several popular learning problems such as linear regression, LASSO, support vector machine, collaborative filtering, and generalized linear models including logistic regression. These convergence results are deterministic, i.e., they establish sample path convergence for arbitrary sequences of delay patterns or distributions on the nodes, and are independent of the tail behavior of the delay distribution. We demonstrate that equiangular tight frames have desirable properties as encoding matrices, and propose efficient mechanisms for encoding large-scale data. We implement the proposed technique on Amazon EC2 clusters, and demonstrate its performance over several learning problems, including matrix factorization, LASSO, ridge regression and logistic regression, and compare the proposed method with uncoded, asynchronous, and data replication strategies."
            ],
            "keywords": [
                "Karakus",
                "Sun",
                "Diggavi",
                "Yin Distributed optimization",
                "straggler mitigation",
                "proximal gradient",
                "coordinate descent",
                "restricted isometry property"
            ],
            "author": [
                "Can Karakus",
                "Yifan Sun",
                "Wotao Yin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-148/18-148.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotics in Empirical Risk Minimization",
            "abstract": [
                "In this paper, we study a two-category classification problem. We indicate the categories by labels Y = 1 and Y = −1. We observe a covariate, or feature, X ∈ X ⊂ R d. Consider a collection {h a } of classifiers indexed by a finite-dimensional parameter a, and the classifier h a * that minimizes the prediction error over this class. The parameter a * is estimated by the empirical risk minimizer a n over the class, where the empirical risk is calculated on a training sample of size n. We apply the Kim Pollard Theorem to show that under certain differentiability assumptions,â n converges to a * with rate n −1/3 , and also present the asymptotic distribution of the renormalized estimator. For example, let V 0 denote the set of x on which, given X = x, the label Y = 1 is more likely (than the label Y = −1). If X is one-dimensional, the set V 0 is the union of disjoint intervals. The problem is then to estimate the thresholds of the intervals. We obtain the asymptotic distribution of the empirical risk minimizer when the classifiers have K thresholds, where K is fixed. We furthermore consider an extension to higher-dimensional X, assuming basically that V 0 has a smooth boundary in some given parametric class. We also discuss various rates of convergence when the differentiability conditions are possibly violated. Here, we again restrict ourselves to one-dimensional X. We show that the rate is n −1 in certain cases, and then also obtain the asymptotic distribution for the empirical prediction error."
            ],
            "keywords": [
                "asymptotic distribution",
                "classification theory",
                "estimation error",
                "nonparametric models",
                "threshold-based classifiers"
            ],
            "author": [
                "Leila Mohammadi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/mohammadi05a/mohammadi05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "pyts: A Python Package for Time Series Classification",
            "abstract": [
                "pyts is an open-source Python package for time series classification. This versatile toolbox provides implementations of many algorithms published in the literature, preprocessing functionalities, and data set loading utilities. pyts relies on the standard scientific Python packages numpy, scipy, scikit-learn, joblib, and numba, and is distributed under the BSD-3-Clause license. Documentation contains installation instructions, a detailed user guide, a full API description, and concrete self-contained examples. Source code and documentation can be downloaded from https://github.com/johannfaouzi/pyts."
            ],
            "keywords": [
                "time series",
                "classification",
                "machine learning",
                "python"
            ],
            "author": [
                "Johann Faouzi",
                "Hicham Janati",
                "Andreas Mueller"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-763/19-763.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "Michael W Mahoney",
                "Lorenzo Orecchia"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/mahoney12a/mahoney12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Gaussian Process Regression with a Student-t Likelihood",
            "abstract": [
                "This paper considers the robust and efficient implementation of Gaussian process regression with a Student-t observation model, which has a non-log-concave likelihood. The challenge with the Student-t model is the analytically intractable inference which is why several approximative methods have been proposed. Expectation propagation (EP) has been found to be a very accurate method in many empirical studies but the convergence of EP is known to be problematic with models containing non-log-concave site functions. In this paper we illustrate the situations where standard EP fails to converge and review different modifications and alternative algorithms for improving the convergence. We demonstrate that convergence problems may occur during the type-II maximum a posteriori (MAP) estimation of the hyperparameters and show that standard EP may not converge in the MAP values with some difficult data sets. We present a robust implementation which relies primarily on parallel EP updates and uses a moment-matching-based double-loop algorithm with adaptively selected step size in difficult cases. The predictive performance of EP is compared with Laplace, variational Bayes, and Markov chain Monte Carlo approximations."
            ],
            "keywords": [
                "Gaussian process",
                "robust regression",
                "Student-t distribution",
                "approximate inference",
                "expectation propagation"
            ],
            "author": [
                "Pasi Jylänki"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/jylanki11a/jylanki11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Using Trajectory Data to Improve Bayesian Optimization for Reinforcement Learning",
            "abstract": [
                "Recently, Bayesian Optimization (BO) has been used to successfully optimize parametric policies in several challenging Reinforcement Learning (RL) applications. BO is attractive for this problem because it exploits Bayesian prior information about the expected return and exploits this knowledge to select new policies to execute. Effectively, the BO framework for policy search addresses the exploration-exploitation tradeoff. In this work, we show how to more effectively apply BO to RL by exploiting the sequential trajectory information generated by RL agents. Our contributions can be broken into two distinct, but mutually beneficial, parts. The first is a new Gaussian process (GP) kernel for measuring the similarity between policies using trajectory data generated from policy executions. This kernel can be used in order to improve posterior estimates of the expected return thereby improving the quality of exploration. The second contribution, is a new GP mean function which uses learned transition and reward functions to approximate the surface of the objective. We show that the model-based approach we develop can recover from model inaccuracies when good transition and reward models cannot be learned. We give empirical results in a standard set of RL benchmarks showing that both our model-based and model-free approaches can speed up learning compared to competing methods. Further, we show that our contributions can be combined to yield synergistic improvement in some domains."
            ],
            "keywords": [
                "reinforcement learning",
                "Bayesian",
                "optimization",
                "policy search",
                "Markov decision process",
                "MDP"
            ],
            "author": [
                "Aaron Wilson",
                "Alan Fern",
                "Prasad Tadepalli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/wilson14a/wilson14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A distributed block coordinate descent method for training l 1 regularized linear classifiers",
            "abstract": [
                "Distributed training of l regularized classifiers has received great attention recently. Most existing methods approach this problem by taking steps obtained from approximating the objective by a quadratic approximation that is decoupled at the individual variable level. These methods are designed for multicore systems where communication costs are low. They are inefficient on systems such as Hadoop running on a cluster of commodity machines where communication costs are substantial. In this paper we design a distributed algorithm for l 1 regularization that is much better suited for such systems than existing algorithms. A careful cost analysis is used to support these points and motivate our method. The main idea of our algorithm is to do block optimization of many variables on the actual objective function within each computing node; this increases the computational cost per step that is matched with the communication cost, and decreases the number of outer iterations, thus yielding a faster overall method. Distributed Gauss-Seidel and Gauss-Southwell greedy schemes are used for choosing variables to update in each step. We establish global convergence theory for our algorithm, including Q-linear rate of convergence. Experiments on two benchmark problems show our method to be much faster than existing methods."
            ],
            "keywords": [],
            "author": [
                "Dhruv Mahajan",
                "S Sathiya Keerthi",
                "S Sundararajan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-484/14-484.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Filter: Learning to Preserve Privacy from Inference Attacks",
            "abstract": [
                "Preserving privacy of continuous and/or high-dimensional data such as images, videos and audios, can be challenging with syntactic anonymization methods which are designed for discrete attributes. Differentially privacy, which uses a more rigorous definition of privacy loss, has shown more success in sanitizing continuous data. However, both syntactic and differential privacy are susceptible to inference attacks, i.e., an adversary can accurately infer sensitive attributes from sanitized data. The paper proposes a novel filter-based mechanism which preserves privacy of continuous and high-dimensional attributes against inference attacks. Finding the optimal utility-privacy tradeoff is formulated as a min-diffmax optimization problem. The paper provides an ERM-like analysis of the generalization error and also a practical algorithm to perform minimax optimization. In addition, the paper proposes a noisy minimax filter which combines minimax filter and differentiallyprivate mechanism. Advantages of the method over purely noisy mechanisms is explained and demonstrated with examples. Experiments with several real-world tasks including facial expression classification, speech emotion classification, and activity classification from motion, show that the minimax filter can simultaneously achieve similar or higher target task accuracy and lower inference accuracy, often significantly lower than previous methods."
            ],
            "keywords": [
                "inference attack",
                "empirical risk minimization",
                "minimax optimization",
                "differential privacy",
                "k-anonymity"
            ],
            "author": [
                "Jihun Hamm"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-501/16-501.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Rules and Their Exceptions",
            "abstract": [
                "We present in this article a top-down inductive system, ALLiS, for learning linguistic structures. Two difficulties came up during the development of the system: the presence of a significant amount of noise in the data and the presence of exceptions linguistically motivated. It is then a challenge for an inductive system to learn rules from this kind of data. This leads us to add a specific mechanism, refinement, which enables learning rules and their exceptions. In the first part of this article we evaluate the usefulness of this device and show that it improves results when learning linguistic structures. In the second part, we explore how to improve the efficiency of the system by using prior knowledge. Since Natural Language is a strongly structured object, it may be important to investigate whether linguistic knowledge can help to make natural language learning more efficiently and accurately. This article presents some experiments demonstrating that linguistic knowledge improves learning. The system has been applied to the shared task of the CoNLL'00 workshop."
            ],
            "keywords": [
                "Symbolic Learning",
                "Rule Induction",
                "Learning Exceptions",
                "Natural Language Processing",
                "Chunking"
            ],
            "author": [
                "Hervé Déjean",
                "James Hammerton",
                "Miles Osborne",
                "Susan Armstrong",
                "Walter Daelemans"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/dejean02a/dejean02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality",
            "abstract": [
                "The varying-coefficient model is flexible and powerful for modeling the dynamic changes of regression coefficients. It is important to identify significant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L 0 penalty, and we investigate the global optimality properties of the varying-coefficient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefficient modeling could be infinite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefficient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efficient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches."
            ],
            "keywords": [
                "coordinate decent algorithm",
                "difference convex programming",
                "L 0 -regularization",
                "large-p small-n",
                "model selection",
                "nonparametric function",
                "oracle property",
                "truncated L 1 penalty"
            ],
            "author": [
                "Lan Xue",
                "Annie Qu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/xue12a/xue12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Randomization as Regularization: A Degrees of Freedom Explanation for Random Forest Success",
            "abstract": [
                "Random forests remain among the most popular off-the-shelf supervised machine learning tools with a well-established track record of predictive accuracy in both regression and classification settings. Despite their empirical success as well as a bevy of recent work investigating their statistical properties, a full and satisfying explanation for their success has yet to be put forth. Here we aim to take a step forward in this direction by demonstrating that the additional randomness injected into individual trees serves as a form of implicit regularization, making random forests an ideal model in low signal-to-noise ratio (SNR) settings. Specifically, from a model-complexity perspective, we show that the mtry parameter in random forests serves much the same purpose as the shrinkage penalty in explicitly regularized regression procedures like lasso and ridge regression. To highlight this point, we design a randomized linear-model-based forward selection procedure intended as an analogue to tree-based random forests and demonstrate its surprisingly strong empirical performance. Numerous demonstrations on both real and synthetic data are provided."
            ],
            "keywords": [
                "Regularization",
                "Bagging",
                "Degrees of Freedom",
                "Model Selection",
                "Interpolation"
            ],
            "author": [
                "Lucas Mentch",
                "Siyu Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-905/19-905.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Score Matching for Non-Negative Data",
            "abstract": [
                "A common challenge in estimating parameters of probability density functions is the intractability of the normalizing constant. While in such cases maximum likelihood estimation may be implemented using numerical integration, the approach becomes computationally intensive. The score matching method of Hyvärinen (2005) avoids direct calculation of the normalizing constant and yields closed-form estimates for exponential families of continuous distributions over R m. Hyvärinen (2007) extended the approach to distributions supported on the non-negative orthant, R m +. In this paper, we give a generalized form of score matching for non-negative data that improves estimation efficiency. As an example, we consider a general class of pairwise interaction models. Addressing an overlooked inexistence problem, we generalize the regularized score matching method of Lin et al. (2016) and improve its theoretical guarantees for non-negative Gaussian graphical models."
            ],
            "keywords": [
                "exponential family",
                "graphical model",
                "positive data",
                "score matching",
                "sparsity"
            ],
            "author": [
                "Shiqing Yu",
                "Mathias Drton",
                "Ali Shojaie"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-278/18-278.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Self-paced Multi-view Co-training",
            "abstract": [
                "Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a \"draw without replacement\" strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method."
            ],
            "keywords": [
                "Co-training",
                "self-paced learning",
                "multi-view learning",
                "semi-supervised learning",
                "-expansion theory",
                "probably approximately correct learnable"
            ],
            "author": [
                "Fan Ma",
                "Deyu Meng",
                "Xuanyi Dong",
                "Yi Yang",
                "Meng Dong Ma"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-794/18-794.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stability Properties of Empirical Risk Minimization over Donsker Classes",
            "abstract": [
                "We study some stability properties of algorithms which minimize (or almost-minimize) empirical error over Donsker classes of functions. We show that, as the number n of samples grows, the L 2diameter of the set of almost-minimizers of empirical error with tolerance ξ(n) = o(n − 1 2) converges to zero in probability. Hence, even in the case of multiple minimizers of expected error, as n increases it becomes less and less likely that adding a sample (or a number of samples) to the training set will result in a large jump to a new hypothesis. Moreover, under some assumptions on the entropy of the class, along with an assumption of Komlos-Major-Tusnady type, we derive a power rate of decay for the diameter of almost-minimizers. This rate, through an application of a uniform ratio limit inequality, is shown to govern the closeness of the expected errors of the almost-minimizers. In fact, under the above assumptions, the expected errors of almost-minimizers become closer with a rate strictly faster than n −1/2 ."
            ],
            "keywords": [
                "empirical risk minimization",
                "empirical processes",
                "stability",
                "Donsker classes"
            ],
            "author": [
                "Andrea Caponnetto",
                "Alexander Rakhlin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/caponnetto06a/caponnetto06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The DFS Fused Lasso: Linear-Time Denoising over General Graphs",
            "abstract": [
                "The fused lasso, also known as (anisotropic) total variation denoising, is widely used for piecewise constant signal estimation with respect to a given undirected graph. The fused lasso estimate is highly nontrivial to compute when the underlying graph is large and has an arbitrary structure. But for a special graph structure, namely, the chain graph, the fused lasso-or simply, 1d fused lasso-can be computed in linear time. In this paper, we revisit a result recently established in the online classification literature (Herbster et al., 2009; Cesa-Bianchi et al., 2013) and show that it has important implications for signal denoising on graphs. The result can be translated to our setting as follows. Given a general graph, if we run the standard depth-first search (DFS) traversal algorithm, then the total variation of any signal over the chain graph induced by DFS is no more than twice its total variation over the original graph. This result leads to several interesting theoretical and computational conclusions. Letting m and n denote the number of edges and nodes, respectively, of the graph in consideration, it implies that for an underlying signal with total variation t over the graph, the fused lasso (properly tuned) achieves a mean squared error rate of t 2/3 n −2/3. Moreover, precisely the same mean squared error rate is achieved by running the 1d fused lasso on the DFS-induced chain graph. Importantly, the latter estimator is simple and computationally cheap, requiring O(m) operations to construct the DFS-induced chain and O(n) operations to compute the 1d fused lasso solution over this chain. Further, for trees that have bounded maximum degree, the error rate of t 2/3 n −2/3 cannot be improved, in the sense that it is the"
            ],
            "keywords": [
                "fused lasso",
                "total variation denoising",
                "graph denoising",
                "depth-first search"
            ],
            "author": [
                "Oscar Hernan",
                "Sharpnack, Scott Madrid Padilla",
                "James Sharpnack",
                "James G Scott",
                "Ryan J Tibshirani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-532/16-532.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Matrix Completion with Noisy Entries and Outliers",
            "abstract": [
                "This paper considers the problem of matrix completion when the observed entries are noisy and contain outliers. It begins with introducing a new optimization criterion for which the recovered matrix is defined as its solution. This criterion uses the celebrated Huber function from the robust statistics literature to downweigh the effects of outliers. A practical algorithm is developed to solve the optimization involved. This algorithm is fast, straightforward to implement, and monotonic convergent. Furthermore, the proposed methodology is theoretically shown to be stable in a well defined sense. Its promising empirical performance is demonstrated via a sequence of simulation experiments, including image inpainting."
            ],
            "keywords": [
                "ES-Algorithm",
                "Huber function",
                "robust methods",
                "Soft-Impute",
                "stable recovery"
            ],
            "author": [
                "Raymond K W Wong",
                "Thomas C M Lee"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-076/15-076.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ERRATA On the Estimation of the Gradient Lines of a Density and the Consistency of the Mean-Shift Algorithm",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "Ery Arias-Castro",
                "David Mason",
                "Bruno Pelletier"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-527/16-527.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Sampling for Large Scale Boosting",
            "abstract": [
                "Classical boosting algorithms, such as AdaBoost, build a strong classifier without concern for the computational cost. Some applications, in particular in computer vision, may involve millions of training examples and very large feature spaces. In such contexts, the training time of off-the-shelf boosting algorithms may become prohibitive. Several methods exist to accelerate training, typically either by sampling the features or the examples used to train the weak learners. Even if some of these methods provide a guaranteed speed improvement, they offer no insurance of being more efficient than any other, given the same amount of time. The contributions of this paper are twofold: (1) a strategy to better deal with the increasingly common case where features come from multiple sources (for example, color, shape, texture, etc., in the case of images) and therefore can be partitioned into meaningful subsets; (2) new algorithms which balance at every boosting iteration the number of weak learners and the number of training examples to look at in order to maximize the expected loss reduction. Experiments in image classification and object recognition on four standard computer vision data sets show that the adaptive methods we propose outperform basic sampling and state-of-the-art bandit methods."
            ],
            "keywords": [
                "boosting",
                "large scale learning",
                "feature selection"
            ],
            "author": [
                "Charles Dubout"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/dubout14a/dubout14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimation of Graphical Models through Structured Norm Minimization",
            "abstract": [
                "Estimation of Markov Random Field and covariance models from high-dimensional data represents a canonical problem that has received a lot of attention in the literature. A key assumption, widely employed, is that of sparsity of the underlying model. In this paper, we study the problem of estimating such models exhibiting a more intricate structure comprising simultaneously of sparse, structured sparse and dense components. Such structures naturally arise in several scientific fields, including molecular biology, finance and political science. We introduce a general framework based on a novel structured norm that enables us to estimate such complex structures from high-dimensional data. The resulting optimization problem is convex and we introduce a linearized multi-block alternating direction method of multipliers (ADMM) algorithm to solve it efficiently. We illustrate the superior performance of the proposed framework on a number of synthetic data sets generated from both random and structured networks. Further, we apply the method to a number of real data sets and discuss the results."
            ],
            "keywords": [
                "Markov Random Fields",
                "Gaussian covariance graph model",
                "structured sparse norm",
                "regularization",
                "alternating direction method of multipliers (ADMM)",
                "convergence"
            ],
            "author": [
                "Davoud Ataee Tarzanagh",
                "George Michailidis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-486/16-486.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cluster Ensembles -A Knowledge Reuse Framework for Combining Multiple Partitions",
            "abstract": [
                "This paper introduces the problem of combining multiple partitionings of a set of objects into a single consolidated clustering without accessing the features or algorithms that determined these partitionings. We first identify several application scenarios for the resultant 'knowledge reuse' framework that we call cluster ensembles. The cluster ensemble problem is then formalized as a combinatorial optimization problem in terms of shared mutual information. In addition to a direct maximization approach, we propose three effective and efficient techniques for obtaining high-quality combiners (consensus functions). The first combiner induces a similarity measure from the partitionings and then reclusters the objects. The second combiner is based on hypergraph partitioning. The third one collapses groups of clusters into meta-clusters which then compete for each object to determine the combined clustering. Due to the low computational costs of our techniques, it is quite feasible to use a supra-consensus function that evaluates all three approaches against the objective function and picks the best solution for a given situation. We evaluate the effectiveness of cluster ensembles in three qualitatively different application scenarios: (i) where the original clusters were formed based on non-identical sets of features, (ii) where the original clustering algorithms worked on non-identical sets of objects, and (iii) where a common data-set is used and the main purpose of combining multiple clusterings is to improve the quality and robustness of the solution. Promising results are obtained in all three situations for synthetic as well as real data-sets."
            ],
            "keywords": [
                "cluster analysis",
                "clustering",
                "partitioning",
                "unsupervised learning",
                "multi-learner systems",
                "ensemble",
                "mutual information",
                "consensus functions",
                "knowledge reuse"
            ],
            "author": [
                "Alexander Strehl"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/strehl02a/strehl02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MULTIBOOST: A Multi-purpose Boosting Package",
            "abstract": [
                "The MULTIBOOST package provides a fast C++ implementation of multi-class/multi-label/multitask boosting algorithms. It is based on ADABOOST.MH but it also implements popular cascade classifiers and FILTERBOOST. The package contains common multi-class base learners (stumps, trees, products, Haar filters). Further base learners and strong learners following the boosting paradigm can be easily implemented in a flexible framework."
            ],
            "keywords": [
                "boosting",
                "ADABOOST.MH",
                "FILTERBOOST",
                "cascade classifier"
            ],
            "author": [
                "Djalel Benbouzid",
                "Norman Casagrande",
                "* R Busa-Fekete"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/benbouzid12a/benbouzid12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Online Convex Optimization Approach to Blackwell's Approachability",
            "abstract": [
                "The problem of approachability in repeated games with vector payoffs was introduced by Blackwell in the 1950s, along with geometric conditions and corresponding approachability strategies that rely on computing a sequence of direction vectors in the payoff space. For convex target sets, these vectors are obtained as projections from the current average payoff vector to the set. A recent paper by Abernethy, Batlett and Hazan (2011) proposed a class of approachability algorithms that rely on Online Linear Programming for obtaining alternative sequences of direction vectors. This is first implemented for target sets that are convex cones, and then generalized to any convex set by embedding it in a higher-dimensional convex cone. In this paper we present a more direct formulation that relies on general Online Convex Optimization (OCO) algorithms, along with basic properties of the support function of convex sets. This leads to a general class of approachability algorithms, depending on the choice of the OCO algorithm and the used norms. Blackwell's original algorithm and its convergence are recovered when Follow The Leader (or a regularized version thereof) is used for the OCO algorithm."
            ],
            "keywords": [
                "approachability",
                "online convex optimization",
                "repeated games with vector payoffs"
            ],
            "author": [
                "Nahum Shimkin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-339/15-339.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Beyond the Regret Minimization Barrier: Optimal Algorithms for Stochastic Strongly-Convex Optimization",
            "abstract": [
                "We give novel algorithms for stochastic strongly-convex optimization in the gradient oracle model which return a O(1 T)-approximate solution after T iterations. The first algorithm is deterministic, and achieves this rate via gradient updates and historical averaging. The second algorithm is randomized, and is based on pure gradient steps with a random step size. This rate of convergence is optimal in the gradient oracle model. This improves upon the previously known best rate of O(log(T) T), which was obtained by applying an online strongly-convex optimization algorithm with regret O(log(T)) to the batch setting. We complement this result by proving that any algorithm has expected regret of Ω(log(T)) in the online stochastic strongly-convex optimization setting. This shows that any online-to-batch conversion is inherently suboptimal for stochastic strongly-convex optimization. This is the first formal evidence that online convex optimization is strictly more difficult than batch stochastic convex optimization."
            ],
            "keywords": [
                "stochastic gradient descent",
                "convex optimization",
                "regret minimization",
                "online learning"
            ],
            "author": [
                "Elad Hazan",
                "Satyen Kale"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/hazan14a/hazan14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Mean Shrinkage Estimators",
            "abstract": [
                "A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel mean, is central to kernel methods in that it is used by many classical algorithms such as kernel principal component analysis, and it also forms the core inference step of modern kernel methods that rely on embedding probability distributions in RKHSs. Given a finite sample, an empirical average has been used commonly as a standard estimator of the true kernel mean. Despite a widespread use of this estimator, we show that it can be improved thanks to the wellknown Stein phenomenon. We propose a new family of estimators called kernel mean shrinkage estimators (KMSEs), which benefit from both theoretical justifications and good empirical performance. The results demonstrate that the proposed estimators outperform the standard one, especially in a \"large d, small n\" paradigm."
            ],
            "keywords": [
                "covariance operator",
                "James-Stein estimators",
                "kernel methods",
                "kernel mean",
                "shrinkage estimators",
                "Stein effect",
                "Tikhonov regularization"
            ],
            "author": [
                "Krikamol Muandet",
                "Arthur Gretton",
                "Bernhard Schölkopf"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-195/14-195.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Objective Markov Decision Processes for Data-Driven Decision Support",
            "abstract": [
                "We present new methodology based on Multi-Objective Markov Decision Processes for developing sequential decision support systems from data. Our approach uses sequential decision-making data to provide support that is useful to many different decision-makers, each with different, potentially time-varying preference. To accomplish this, we develop an extension of fitted-Q iteration for multiple objectives that computes policies for all scalarization functions, i.e. preference functions, simultaneously from continuous-state, finitehorizon data. We identify and address several conceptual and computational challenges along the way, and we introduce a new solution concept that is appropriate when different actions have similar expected outcomes. Finally, we demonstrate an application of our method using data from the Clinical Antipsychotic Trials of Intervention Effectiveness and show that our approach offers decision-makers increased choice by a larger class of optimal policies."
            ],
            "keywords": [
                "multi-objective optimization",
                "reinforcement learning",
                "Markov decision processes",
                "clinical decision support",
                "evidence-based medicine"
            ],
            "author": [
                "Daniel J Lizotte",
                "Eric B Laber"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-252/15-252.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Note on Quickly Sampling a Sparse Matrix with Low Rank Expectation",
            "abstract": [
                "Given matrices X,Y ∈ R n×K and S ∈ R K×K with positive elements, this paper proposes an algorithm fastRG to sample a sparse matrix A with low rank expectation E(A) = XSY T and independent Poisson elements. This allows for quickly sampling from a broad class of stochastic blockmodel graphs (degree-corrected, mixed membership, overlapping) all of which are specific parameterizations of the generalized random product graph model defined in Section 2.2. The basic idea of fastRG is to first sample the number of edges m and then sample each edge. The key insight is that because of the the low rank expectation, it is easy to sample individual edges. The naive \"element-wise\" algorithm requires O(n 2) operations to generate the n × n adjacency matrix A. In sparse graphs, where m = O(n), ignoring log terms, fastRG runs in time O(n). An implementation in R is available on github. A computational experiment in Section 2.4 simulates graphs up to n = 10, 000, 000 nodes with m = 100, 000, edges. For example, on a graph with n = 500, 000 and m = 5, 000, 000, fastRG runs in less than one second on a 3.5 GHz Intel i5."
            ],
            "keywords": [
                "Random Dot Product Graph",
                "Edge exchangeable",
                "Simulation"
            ],
            "author": [
                "Karl Rohe",
                "Jun Tao",
                "Xintian Han",
                "Norbert Binkiewicz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-128/17-128.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows",
            "abstract": [
                "We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called \"path coding\" penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efficiently solve by leveraging network flow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs."
            ],
            "keywords": [
                "convex and non-convex optimization",
                "network flow optimization",
                "graph sparsity"
            ],
            "author": [
                "Julien Mairal"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/mairal13a/mairal13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiple-Instance Learning of Real-Valued Data",
            "abstract": [
                "The multiple-instance learning model has received much attention recently with a primary application area being that of drug activity prediction. Most prior work on multiple-instance learning has been for concept learning, yet for drug activity prediction, the label is a real-valued affinity measurement giving the binding strength. We present extensions of k-nearest neighbors (k-NN), Citation-kNN, and the diverse density algorithm for the real-valued setting and study their performance on Boolean and real-valued data. We also provide a method for generating chemically realistic artificial data."
            ],
            "keywords": [],
            "author": [
                "Daniel R Dooly",
                "Qi Zhang",
                "Sally A Goldman",
                "Robert A Amar",
                "Carla E Brodley",
                "Andrea Danyluk"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/dooly02a/dooly02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Infinite-σ Limits For Tikhonov Regularization",
            "abstract": [
                "We consider the problem of Tikhonov regularization with a general convex loss function: this formalism includes support vector machines and regularized least squares. For a family of kernels that includes the Gaussian, parameterized by a \"bandwidth\" parameter σ, we characterize the limiting solution as σ → ∞. In particular, we show that if we set the regularization parameter λ =λσ −2p , the regularization term of the Tikhonov problem tends to an indicator function on polynomials of degree ⌊p⌋ (with residual regularization in the case where p ∈ Z). The proof rests on two key ideas: epi-convergence, a notion of functional convergence under which limits of minimizers converge to minimizers of limits, and a value-based formulation of learning, where we work with regularization on the function output values (y) as opposed to the function expansion coefficients in the RKHS. Our result generalizes and unifies previous results in this area."
            ],
            "keywords": [
                "Tikhonov regularization",
                "Gaussian kernel",
                "theory",
                "kernel machines"
            ],
            "author": [
                "Ross A Lippert",
                "Ryan M Rifkin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/lippert06a/lippert06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling",
            "abstract": [
                "It has become increasingly popular to obtain machine learning labels through commercial crowdsourcing services. The crowdsourcing workers or annotators are paid for each label they provide, but the task requester usually has only a limited amount of the budget. Since the data instances have different levels of labeling difficulty and the workers have different reliability for the labeling task, it is desirable to wisely allocate the budget among all the instances and workers such that the overall labeling quality is maximized. In this paper, we formulate the budget allocation problem as a Bayesian Markov decision process (MDP), which simultaneously conducts learning and decision making. The optimal allocation policy can be obtained by using the dynamic programming (DP) recurrence. However, DP quickly becomes computationally intractable when the size of the problem increases. To solve this challenge, we propose a computationally efficient approximate policy which is called optimistic knowledge gradient. Our method applies to both pull crowdsourcing marketplaces with homogeneous workers and push marketplaces with heterogeneous workers. It can also incorporate the contextual information of instances when they are available. The experiments on both simulated and real data show that our policy achieves a higher labeling quality than other existing policies at the same budget level."
            ],
            "keywords": [
                "crowdsourcing",
                "budget allocation",
                "Markov decision process",
                "dynamic programming",
                "optimistic knowledge gradient"
            ],
            "author": [
                "Xi Chen",
                "Qihang Lin",
                "Dengyong Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/chen15a/chen15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Transfer Learning for Reinforcement Learning Domains: A Survey",
            "abstract": [
                "The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work."
            ],
            "keywords": [
                "transfer learning",
                "reinforcement learning",
                "multi-task learning"
            ],
            "author": [
                "Matthew E Taylor",
                "Peter Stone"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/taylor09a/taylor09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Bounds and Observable Constraints for Non-deterministic Models",
            "abstract": [
                "Conditional independence relations involving latent variables do not necessarily imply observable independences. They may imply inequality constraints on observable parameters and causal bounds, which can be used for falsification and identification. The literature on computing such constraints often involve a deterministic underlying data generating process in a counterfactual framework. If an analyst is ignorant of the nature of the underlying mechanisms then they may wish to use a model which allows the underlying mechanisms to be probabilistic. A method of computation for a weaker model without any determinism is given here and demonstrated for the instrumental variable model, though applicable to other models. The approach is based on the analysis of mappings with convex polytopes in a decision theoretic framework and can be implemented in readily available polyhedral computation software. Well known constraints and bounds are replicated in a probabilistic model and novel ones are computed for instrumental variable models without non-deterministic versions of the randomization, exclusion restriction and monotonicity assumptions respectively."
            ],
            "keywords": [
                "instrumental variables",
                "instrumental inequality",
                "causal bounds",
                "convex polytope",
                "latent variables",
                "directed acyclic graph"
            ],
            "author": [
                "Roland R Ramsahai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/ramsahai12a/ramsahai12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization",
            "abstract": [
                "Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems."
            ],
            "keywords": [
                "hyperparameter optimization",
                "model selection",
                "infinite-armed bandits",
                "online optimization",
                "deep learning"
            ],
            "author": [
                "Lisha Li",
                "Kevin Jamieson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-558/16-558.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": null,
            "abstract": [
                "This paper comments on the published work dealing with robustness and regularization of support vector machines ("
            ],
            "keywords": [
                "kernel",
                "robustness",
                "support vector machine"
            ],
            "author": [
                "Yahya Forghani",
                "Hadi Sadoghi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/forghani13a/forghani13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Influence of the Kernel on the Consistency of Support Vector Machines",
            "abstract": [
                "In this article we study the generalization abilities of several classifiers of support vector machine (SVM) type using a certain class of kernels that we call universal. It is shown that the soft margin algorithms with universal kernels are consistent for a large class of classification problems including some kind of noisy tasks provided that the regularization parameter is chosen well. In particular we derive a simple sufficient condition for this parameter in the case of Gaussian RBF kernels. On the one hand our considerations are based on an investigation of an approximation property-the so-called universality-of the used kernels that ensures that all continuous functions can be approximated by certain kernel expressions. This approximation property also gives a new insight into the role of kernels in these and other algorithms. On the other hand the results are achieved by a precise study of the underlying optimization problems of the classifiers. Furthermore, we show consistency for the maximal margin classifier as well as for the soft margin SVM's in the presence of large margins. In this case it turns out that also constant regularization parameters ensure consistency for the soft margin SVM's. Finally we prove that even for simple, noise free classification problems SVM's with polynomial kernels can behave arbitrarily badly."
            ],
            "keywords": [
                "Computational learning theory",
                "pattern recognition",
                "PAC model",
                "support vector machines",
                "kernel methods 1. For mathematical notions see Section 2"
            ],
            "author": [
                "Ingo Steinwart"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/steinwart01a/steinwart01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Best Arm Identification for Contaminated Bandits",
            "abstract": [
                "This paper studies active learning in the context of robust statistics. Specifically, we propose a variant of the Best Arm Identification problem for contaminated bandits, where each arm pull has probability ε of generating a sample from an arbitrary contamination distribution instead of the true underlying distribution. The goal is to identify the best (or approximately best) true distribution with high probability, with a secondary goal of providing guarantees on the quality of this distribution. The primary challenge of the contaminated bandit setting is that the true distributions are only partially identifiable, even with infinite samples. To address this, we develop tight, non-asymptotic sample complexity bounds for high-probability estimation of the first two robust moments (median and median absolute deviation) from contaminated samples. These concentration inequalities are the main technical contributions of the paper and may be of independent interest. Using these results, we adapt several classical Best Arm Identification algorithms to the contaminated bandit setting and derive sample complexity upper bounds for our problem. Finally, we provide matching information-theoretic lower bounds on the sample complexity (up to a small logarithmic factor)."
            ],
            "keywords": [
                "multi-armed bandits",
                "best arm identification",
                "robust statistics",
                "contamination model",
                "partial identifiability"
            ],
            "author": [
                "Jason Altschuler",
                "Alan Malek"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-395/18-395.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Discrete Reproducing Kernel Hilbert Spaces: Sampling and Distribution of Dirac-masses",
            "abstract": [
                "We study reproducing kernels, and associated reproducing kernel Hilbert spaces (RKHSs) H over infinite, discrete and countable sets V. In this setting we analyze in detail the distributions of the corresponding Dirac point-masses of V. Illustrations include certain models from neural networks: An Extreme Learning Machine (ELM) is a neural networkconfiguration in which a hidden layer of weights are randomly sampled, and where the object is then to compute resulting output. For RKHSs H of functions defined on a prescribed countable infinite discrete set V , we characterize those which contain the Dirac masses δ x for all points x in V. Further examples and applications where this question plays an important role are: (i) discrete Brownian motion-Hilbert spaces, i.e., discrete versions of the Cameron-Martin Hilbert space; (ii) energy-Hilbert spaces corresponding to graph-Laplacians where the set V of vertices is then equipped with a resistance metric; and finally (iii) the study of Gaussian free fields."
            ],
            "keywords": [
                "Gaussian reproducing kernel Hilbert spaces",
                "sampling in discrete systems",
                "resistance metric",
                "graph Laplacians",
                "discrete Green's functions"
            ],
            "author": [
                "Palle Jorgensen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/jorgensen15a/jorgensen15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Significance-based community detection in weighted networks",
            "abstract": [
                "Community detection is the process of grouping strongly connected nodes in a network. Many community detection methods for un-weighted networks have a theoretical basis in a null model. Communities discovered by these methods therefore have interpretations in terms of statistical significance. In this paper, we introduce a null for weighted networks called the continuous configuration model. First, we propose a community extraction algorithm for weighted networks which incorporates iterative hypothesis testing under the null. We prove a central limit theorem for edge-weight sums and asymptotic consistency of the algorithm under a weighted stochastic block model. We then incorporate the algorithm in a community detection method called CCME. To benchmark the method, we provide a simulation framework involving the null to plant \"background\" nodes in weighted networks with communities. We show that the empirical performance of CCME on these simulations is competitive with existing methods, particularly when overlapping communities and background nodes are present. To further validate the method, we present two real-world networks with potential background nodes and analyze them with CCME, yielding results that reveal macro-features of the corresponding systems."
            ],
            "keywords": [
                "Community detection",
                "Multiple testing",
                "Network models",
                "Weighted networks",
                "Unsupervised learning"
            ],
            "author": [
                "John Palowitch",
                "Shankar Bhamidi",
                "Andrew B Nobel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-377/17-377.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Change-Point Computation for Large Graphical Models: A Scalable Algorithm for Gaussian Graphical Models with Change-Points",
            "abstract": [
                "Graphical models with change-points are computationally challenging to fit, particularly in cases where the number of observation points and the number of nodes in the graph are large. Focusing on Gaussian graphical models, we introduce an approximate majorizeminimize (MM) algorithm that can be useful for computing change-points in large graphical models. The proposed algorithm is an order of magnitude faster than a brute force search. Under some regularity conditions on the data generating process, we show that with high probability, the algorithm converges to a value that is within statistical error of the true change-point. A fast implementation of the algorithm using Markov Chain Monte Carlo is also introduced. The performances of the proposed algorithms are evaluated on synthetic data sets and the algorithm is also used to analyze structural changes in the S&P 500 over the period 2000-2016."
            ],
            "keywords": [
                "change-points",
                "Gaussian graphical models",
                "proximal gradient",
                "simulated annealing",
                "stochastic optimization"
            ],
            "author": [
                "Leland Bybee",
                "Yves Atchadé"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-218/17-218.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dual Control for Approximate Bayesian Reinforcement Learning",
            "abstract": [
                "Control of non-episodic, finite-horizon dynamical systems with uncertain dynamics poses a tough and elementary case of the exploration-exploitation trade-off. Bayesian reinforcement learning, reasoning about the effect of actions and future observations, offers a principled solution, but is intractable. We review, then extend an old approximate approach from control theory-where the problem is known as dual control-in the context of modern regression methods, specifically generalized linear regression. Experiments on simulated systems show that this framework offers a useful approximation to the intractable aspects of Bayesian RL, producing structured exploration strategies that differ from standard RL approaches. We provide simple examples for the use of this framework in (approximate) Gaussian process regression and feedforward neural networks for the control of exploration."
            ],
            "keywords": [
                "reinforcement learning",
                "control",
                "Gaussian processes",
                "filtering",
                "Bayesian inference"
            ],
            "author": [
                "Edgar D Klenske"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-162/15-162.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Evidence Contrary to the Statistical View of Boosting",
            "abstract": [
                "The statistical perspective on boosting algorithms focuses on optimization, drawing parallels with maximum likelihood estimation for logistic regression. In this paper we present empirical evidence that raises questions about this view. Although the statistical perspective provides a theoretical framework within which it is possible to derive theorems and create new algorithms in general contexts, we show that there remain many unanswered important questions. Furthermore, we provide examples that reveal crucial flaws in the many practical suggestions and new methods that are derived from the statistical view. We perform carefully designed experiments using simple simulation models to illustrate some of these flaws and their practical consequences."
            ],
            "keywords": [
                "boosting algorithms",
                "LogitBoost",
                "AdaBoost"
            ],
            "author": [
                "MEASE D@COB.SJSU David Mease",
                "Abraham Wyner"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/mease08a/mease08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Inverse Reinforcement Learning in Partially Observable Environments",
            "abstract": [
                "Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behavior of an expert. Most of the existing IRL algorithms assume that the environment is modeled as a Markov decision process (MDP), although it is desirable to handle partially observable settings in order to handle more realistic scenarios. In this paper, we present IRL algorithms for partially observable environments that can be modeled as a partially observable Markov decision process (POMDP). We deal with two cases according to the representation of the given expert's behavior, namely the case in which the expert's policy is explicitly given, and the case in which the expert's trajectories are available instead. The IRL in POMDPs poses a greater challenge than in MDPs since it is not only ill-posed due to the nature of IRL, but also computationally intractable due to the hardness in solving POMDPs. To overcome these obstacles, we present algorithms that exploit some of the classical results from the POMDP literature. Experimental results on several benchmark POMDP domains show that our work is useful for partially observable settings."
            ],
            "keywords": [
                "inverse reinforcement learning",
                "partially observable Markov decision process",
                "inverse optimization",
                "linear programming",
                "quadratically constrained programming"
            ],
            "author": [
                "Jaedeug Choi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/choi11a/choi11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Harder, Better, Faster, Stronger † Convergence Rates for Least-Squares Regression",
            "abstract": [
                "We consider the optimization of a quadratic objective function whose gradients are only accessible through a stochastic oracle that returns the gradient at any given point plus a zero-mean finite variance random error. We present the first algorithm that achieves jointly the optimal prediction error rates for least-squares regression, both in terms of forgetting the initial conditions in O(1/n 2), and in terms of dependence on the noise and dimension d of the problem, as O(d/n). Our new algorithm is based on averaged accelerated regularized gradient descent, and may also be analyzed through finer assumptions on initial conditions and the Hessian matrix, leading to dimension-free quantities that may still be small in some distances while the \"optimal\" terms above are large. In order to characterize the tightness of these new bounds, we consider an application to non-parametric regression and use the known lower bounds on the statistical performance (without computational limits), which happen to match our bounds obtained from a single pass on the data and thus show optimality of our algorithm in a wide variety of particular trade-offs between bias and variance."
            ],
            "keywords": [
                "convex optimization",
                "least-squares regression",
                "stochastic gradient",
                "accelerated gradient",
                "non-parametric estimation"
            ],
            "author": [
                "Aymeric Dieuleveut",
                "Nicolas Flammarion",
                "Francis Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-335/16-335.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximation algorithms for stochastic clustering",
            "abstract": [
                "We consider stochastic settings for clustering, and develop provably-good approximation algorithms for a number of these notions. These algorithms yield better approximation ratios compared to the usual deterministic clustering setting. Additionally, they offer a number of advantages including clustering which is fairer and has better long-term behavior for each user. In particular, they ensure that every user is guaranteed to get good service (on average). We also complement some of these with impossibility results."
            ],
            "keywords": [
                "clustering",
                "k-center",
                "k-median",
                "lottery",
                "approximation algorithms"
            ],
            "author": [
                "David G Harris",
                "Shi Li",
                "Thomas Pensyl",
                "Aravind Srinivasan",
                "Khoa Trinh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-716/18-716.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Learning in Case of Unbounded Losses Using Follow the Perturbed Leader Algorithm",
            "abstract": [
                "In this paper the sequential prediction problem with expert advice is considered for the case where losses of experts suffered at each step cannot be bounded in advance. We present some modification of Kalai and Vempala algorithm of following the perturbed leader where weights depend on past losses of the experts. New notions of a volume and a scaled fluctuation of a game are introduced. We present a probabilistic algorithm protected from unrestrictedly large one-step losses. This algorithm has the optimal performance in the case when the scaled fluctuations of one-step losses of experts of the pool tend to zero."
            ],
            "keywords": [
                "prediction with expert advice",
                "follow the perturbed leader",
                "unbounded losses",
                "adaptive learning rate",
                "expected bounds",
                "Hannan consistency",
                "online sequential prediction"
            ],
            "author": [
                "Vladimir V V'yugin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/vyugin11a/vyugin11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On b-bit Min-wise Hashing for Large-scale Regression and Classification with Sparse Data",
            "abstract": [
                "Large-scale regression problems where both the number of variables, p, and the number of observations, n, may be large and in the order of millions or more, are becoming increasingly more common. Typically the data are sparse: only a fraction of a percent of the entries in the design matrix are non-zero. Nevertheless, often the only computationally feasible approach is to perform dimension reduction to obtain a new design matrix with far fewer columns and then work with this compressed data. b-bit min-wise hashing (Li and König, 2011; Li et al., 2011) is a promising dimension reduction scheme for sparse matrices which produces a set of random features such that regression on the resulting design matrix approximates a kernel regression with the resemblance kernel. In this work, we derive bounds on the prediction error of such regressions. For both linear and logistic models, we show that the average prediction error vanishes asymptotically as long as q β * 2 2 /n → 0, where q is the average number of non-zero entries in each row of the design matrix and β * is the coefficient of the linear predictor. We also show that ordinary least squares or ridge regression applied to the reduced data can in fact allow us fit more flexible models. We obtain non-asymptotic prediction error bounds for interaction models and for models where an unknown row normalisation must be applied in order for the signal to be linear in the predictors."
            ],
            "keywords": [
                "large-scale data",
                "min-wise hashing",
                "resemblance kernel",
                "ridge regression",
                "sparse data"
            ],
            "author": [
                "Rajen D Shah",
                "Nicolai Meinshausen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-587/16-587.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Consistency of Multiclass Classification Methods",
            "abstract": [
                "Binary classification is a well studied special case of the classification problem. Statistical properties of binary classifiers, such as consistency, have been investigated in a variety of settings. Binary classification methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classification method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufficient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature."
            ],
            "keywords": [
                "multiclass classification",
                "consistency",
                "Bayes risk"
            ],
            "author": [
                "Ambuj Tewari",
                "Peter L Bartlett"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/tewari07a/tewari07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Forecasting Web Page Views: Methods and Observations",
            "abstract": [
                "Web sites must forecast Web page views in order to plan computer resource allocation and estimate upcoming revenue and advertising growth. In this paper, we focus on extracting trends and seasonal patterns from page view series, two dominant factors in the variation of such series. We investigate the Holt-Winters procedure and a state space model for making relatively short-term prediction. It is found that Web page views exhibit strong impulsive changes occasionally. The impulses cause large prediction errors long after their occurrences. A method is developed to identify impulses and to alleviate their damage on prediction. We also develop a long-range trend and season extraction method, namely the Elastic Smooth Season Fitting (ESSF) algorithm, to compute scalable and smooth yearly seasons. ESSF derives the yearly season by minimizing the residual sum of squares under smoothness regularization, a quadratic optimization problem. It is shown that for longterm prediction, ESSF improves accuracy significantly over other methods that ignore the yearly seasonality."
            ],
            "keywords": [
                "web page views",
                "forecast",
                "Holt-Winters",
                "Kalman filtering",
                "elastic smooth season fitting"
            ],
            "author": [
                "Jia Li",
                "Andrew W Moore"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/li08a/li08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Nonparametric Crowdsourcing",
            "abstract": [
                "Crowdsourcing has been proven to be an effective and efficient tool to annotate large data-sets. User annotations are often noisy, so methods to combine the annotations to produce reliable estimates of the ground truth are necessary. We claim that considering the existence of clusters of users in this combination step can improve the performance. This is especially important in early stages of crowdsourcing implementations, where the number of annotations is low. At this stage there is not enough information to accurately estimate the bias introduced by each annotator separately, so we have to resort to models that consider the statistical links among them. In addition, finding these clusters is interesting in itself as knowing the behavior of the pool of annotators allows implementing efficient active learning strategies. Based on this, we propose in this paper two new fully unsupervised models based on a Chinese restaurant process (CRP) prior and a hierarchical structure that allows inferring these groups jointly with the ground truth and the properties of the users. Efficient inference algorithms based on Gibbs sampling with auxiliary variables are proposed. Finally, we perform experiments, both on synthetic and real databases, to show the advantages of our models over state-of-the-art algorithms."
            ],
            "keywords": [
                "multiple annotators",
                "Bayesian nonparametrics",
                "Dirichlet process",
                "hierarchical clustering",
                "Gibbs sampling"
            ],
            "author": [
                "Pablo G Moreno",
                "Yee Whye Teh",
                "Fernando Perez-Cruz",
                "G Moreno",
                "Antonio Artés-Rodríguez",
                "Yee Whye Teh",
                "Artés-Rodríguez Moreno"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/moreno15a/moreno15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Introduction to the Special Issue on Machine Learning Methods for Text and Images",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "Jaz Kandola",
                "Royal Holloway College",
                "Thomas Hofmann",
                "Tomaso Poggio",
                "John Shawe-Taylor"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/kandola03a/kandola03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Mixed Membership Stochastic Blockmodels",
            "abstract": [
                "Consider data consisting of pairwise measurements, such as presence or absence of links between pairs of objects. These data arise, for instance, in the analysis of protein interactions and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing pairwise measurements with probabilistic models requires special assumptions, since the usual independence or exchangeability assumptions no longer hold. Here we introduce a class of variance allocation models for pairwise measurements: mixed membership stochastic blockmodels. These models combine global parameters that instantiate dense patches of connectivity (blockmodel) with local parameters that instantiate node-specific variability in the connections (mixed membership). We develop a general variational inference algorithm for fast approximate posterior inference. We demonstrate the advantages of mixed membership stochastic blockmodels with applications to social networks and protein interaction networks."
            ],
            "keywords": [
                "hierarchical Bayes",
                "latent variables",
                "mean-field approximation",
                "statistical network analysis",
                "social networks",
                "protein interaction networks"
            ],
            "author": [
                "Edoardo M Airoldi",
                "David M Blei",
                "Stephen E Fienberg",
                "Eric P Xing"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/airoldi08a/airoldi08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Picasso: A Sparse Learning Library for High Dimensional Data Analysis in R and Python *",
            "abstract": [
                "We describe a new library named picasso 1 , which implements a unified framework of pathwise coordinate optimization for a variety of sparse learning problems (e.g., sparse linear regression, sparse logistic regression, sparse Poisson regression and scaled sparse linear regression) combined with efficient active set selection strategies. Besides, the library allows users to choose different sparsity-inducing regularizers, including the convex 1 , nonvoncex MCP and SCAD regularizers. The library is coded in C++ and has user-friendly R and Python wrappers. Numerical experiments demonstrate that picasso can scale up to large problems efficiently."
            ],
            "keywords": [],
            "author": [
                "Jason Ge",
                "Haoming Jiang",
                "Han Liu",
                "Tong Zhang",
                "Mengdi Wang",
                "Tuo Zhao",
                "* Jason Ge",
                "Xingguo Tuo Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-722/17-722.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Relative Error Bound Analysis for Nuclear Norm Regularized Matrix Completion",
            "abstract": [
                "In this paper, we develop a relative error bound for nuclear norm regularized matrix completion, with the focus on the completion of full-rank matrices. Under the assumption that the top eigenspaces of the target matrix are incoherent, we derive a relative upper bound for recovering the best low-rank approximation of the unknown matrix. Although multiple works have been devoted to analyzing the recovery error of full-rank matrix completion, their error bounds are usually additive, making it impossible to obtain the perfect recovery case and more generally difficult to leverage the skewed distribution of eigenvalues. Our analysis is built upon the optimality condition of the regularized formulation and existing guarantees for low-rank matrix completion. To the best of our knowledge, this is the first relative bound that has been proved for the regularized formulation of matrix completion."
            ],
            "keywords": [
                "matrix completion",
                "nuclear norm regularization",
                "least squares",
                "low-rank",
                "full-rank",
                "relative error bound"
            ],
            "author": [
                "Lijun Zhang",
                "Tianbao Yang",
                "Rong Jin",
                "Zhi-Hua Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/15-504/15-504.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Synergy of Monotonic Rules",
            "abstract": [
                "This article describes a method for constructing a special rule (we call it synergy rule) that uses as its input information the outputs (scores) of several monotonic rules which solve the same pattern recognition problem. As an example of scores of such monotonic rules we consider here scores of SVM classifiers."
            ],
            "keywords": [
                "conditional probability",
                "synergy",
                "ensemble learning",
                "intelligent teacher",
                "privileged information",
                "knowledge transfer",
                "support vector machines",
                "SVM+",
                "classification",
                "learning theory",
                "kernel functions",
                "regression"
            ],
            "author": [
                "Vladimir Vapnik",
                "Rauf Izmailov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-137/16-137.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Averaged Collapsed Variational Bayes Inference",
            "abstract": [
                "This paper presents the Averaged CVB (ACVB) inference and offers convergence-guaranteed and practically useful fast Collapsed Variational Bayes (CVB) inferences. CVB inferences yield more precise inferences of Bayesian probabilistic models than Variational Bayes (VB) inferences. However, their convergence aspect is fairly unknown and has not been scrutinized. To make CVB more useful, we study their convergence behaviors in a empirical and practical approach. We develop a convergence-guaranteed algorithm for any CVB-based inference called ACVB, which enables automatic convergence detection and frees non-expert practitioners from the difficult and costly manual monitoring of inference processes. In experiments, ACVB inferences are comparable to or better than those of existing inference methods and deterministic, fast, and provide easier convergence detection. These features are especially convenient for practitioners who want precise Bayesian inference with assured convergence."
            ],
            "keywords": [
                "nonparametric Bayes",
                "collapsed variational Bayes inference",
                "averaged CVB"
            ],
            "author": [
                "Katsuhiko Ishiguro",
                "Issei Sato",
                "Naonori Ueda"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-249/14-249.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Learning of Label Ranking by Soft Projections onto Polyhedra",
            "abstract": [
                "We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by defining a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a significantly smaller size and prove that it attains the same minimum. We then describe an efficient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron defined by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show significant improvements in run time over a state of the art interior-point algorithm. *. Part of S. Shalev-Shwartz's work was done while visiting Google. †. Author for correspondences."
            ],
            "keywords": [],
            "author": [
                "Shai Shalev-Shwartz",
                "Inc Google",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/shalev-shwartz06a/shalev-shwartz06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Differentially Private Empirical Risk Minimization Kamalika Chaudhuri",
            "abstract": [
                "Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the ε-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacypreserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance."
            ],
            "keywords": [
                "privacy",
                "classification",
                "optimization",
                "empirical risk minimization",
                "support vector machines",
                "logistic regression"
            ],
            "author": [
                "@ Ucsd Kchaudhuri",
                "Claire Monteleoni",
                "Anand D Sarwate"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/chaudhuri11a/chaudhuri11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Train and Test Tightness of LP Relaxations in Structured Prediction",
            "abstract": [
                "Structured prediction is used in areas including computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation for the striking observation that approximations based on linear programming (LP) relaxations are often tight (exact) on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that this training tightness generalizes to test data."
            ],
            "keywords": [],
            "author": [
                "Ofer Meshi",
                "Ben London",
                "Adrian Weller",
                "David Sontag"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-535/17-535.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence of Unregularized Online Learning Algorithms",
            "abstract": [
                "In this paper we study the convergence of online gradient descent algorithms in reproducing kernel Hilbert spaces (RKHSs) without regularization. We establish a sufficient condition and a necessary condition for the convergence of excess generalization errors in expectation. A sufficient condition for the almost sure convergence is also given. With high probability, we provide explicit convergence rates of the excess generalization errors for both averaged iterates and the last iterate, which in turn also imply convergence rates with probability one. To our best knowledge, this is the first high-probability convergence rate for the last iterate of online gradient descent algorithms in the general convex setting. Without any boundedness assumptions on iterates, our results are derived by a novel use of two measures of the algorithm's one-step progress, respectively by generalization errors and by distances in RKHSs, where the variances of the involved martingales are cancelled out by the descent property of the algorithm."
            ],
            "keywords": [
                "Learning theory",
                "Online learning",
                "Convergence analysis",
                "Reproducing kernel Hilbert space"
            ],
            "author": [
                "Yunwen Lei",
                "Lei Shi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-457/17-457.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing",
            "abstract": [
                "Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of lowquality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural \"no-free-lunch\" requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible mechanisms (that may or may not satisfy no-freelunch), our mechanism makes the smallest possible payment to spammers. We further extend our results to a more general setting in which workers are required to provide a quantized confidence for each question. Interestingly, this unique mechanism takes a \"multiplicative\" form. The simplicity of the mechanism is an added benefit. In preliminary experiments involving over 900 worker-task pairs, we observe a significant drop in the error rates under this unique mechanism for the same or lower monetary expenditure."
            ],
            "keywords": [
                "high-quality labels",
                "supervised learning",
                "crowdsourcing",
                "mechanism design",
                "proper scoring rules"
            ],
            "author": [
                "Nihar B Shah",
                "Dengyong Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-642/15-642.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Convergence Rates for Convex Distributed Optimization in Networks",
            "abstract": [
                "This work proposes a theoretical analysis of distributed optimization of convex functions using a network of computing units. We investigate this problem under two communication schemes (centralized and decentralized) and four classical regularity assumptions: Lipschitz continuity, strong convexity, smoothness, and a combination of strong convexity and smoothness. Under the decentralized communication scheme, we provide matching upper and lower bounds of complexity along with algorithms achieving this rate up to logarithmic constants. For non-smooth objective functions, while the dominant term of the error is in O(1/ √ t), the structure of the communication network only impacts a second-order term in O(1/t), where t is time. In other words, the error due to limits in communication resources decreases at a fast rate even in the case of non-strongly convex objective functions. Such a convergence rate is achieved by the novel multi-step primal-dual (MSPD) algorithm. Under the centralized communication scheme, we show that the naive distribution of standard optimization algorithms is optimal for smooth objective functions, and provide a simple yet efficient algorithm called distributed randomized smoothing (DRS) based on a local smoothing of the objective function for non-smooth functions. We then show that DRS is within a d 1/4 multiplicative factor of the optimal convergence rate, where d is the underlying dimension."
            ],
            "keywords": [
                "distributed optimization",
                "convex optimization",
                "first-order methods"
            ],
            "author": [
                "Kevin Scaman",
                "Francis Bach",
                "Sébastien Bubeck",
                "Laurent Massoulié"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-543/19-543.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MedLDA: Maximum Margin Supervised Topic Models",
            "abstract": [
                "A supervised topic model can use side information such as ratings or labels associated with documents or images to discover more predictive low dimensional topical representations of the data. However, existing supervised topic models predominantly employ likelihood-driven objective functions for learning and inference, leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper, we propose the maximum entropy discrimination latent Dirichlet allocation (MedLDA) model, which integrates the mechanism behind the max-margin prediction models (e.g., SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g., LDA) under a unified constrained optimization framework, and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classification or regression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. Efficient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance; and 3) be more efficient than existing supervised topic models, especially for classification."
            ],
            "keywords": [
                "supervised topic models",
                "max-margin learning",
                "maximum entropy discrimination",
                "latent Dirichlet allocation",
                "support vector machines"
            ],
            "author": [
                "Jun Zhu",
                "Amr Ahmed",
                "Eric P Xing"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/zhu12a/zhu12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models",
            "abstract": [
                "We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random fields (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter fitting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random field via the graphical lasso."
            ],
            "keywords": [],
            "author": [
                "Neil D Lawrence"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/lawrence12a/lawrence12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Continuous Time Bayesian Network Reasoning and Learning Engine",
            "abstract": [
                "We present a continuous time Bayesian network reasoning and learning engine (CTBN-RLE). A continuous time Bayesian network (CTBN) provides a compact (factored) description of a continuoustime Markov process. This software provides libraries and programs for most of the algorithms developed for CTBNs. For learning, CTBN-RLE implements structure and parameter learning for both complete and partial data. For inference, it implements exact inference and Gibbs and importance sampling approximate inference for any type of evidence pattern. Additionally, the library supplies visualization methods for graphically displaying CTBNs or trajectories of evidence."
            ],
            "keywords": [
                "continuous time Bayesian networks",
                "C++",
                "open source software"
            ],
            "author": [
                "Christian R Shelton",
                "Yu Fan",
                "William Lam",
                "Jing Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/shelton10a/shelton10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Easy-to-hard Learning Paradigm for Multiple Classes and Multiple Labels",
            "abstract": [
                "Many applications, such as human action recognition and object detection, can be formulated as a multiclass classification problem. One-vs-rest (OVR) is one of the most widely used approaches for multiclass classification due to its simplicity and excellent performance. However, many confusing classes in such applications will degrade its results. For example, hand clap and boxing are two confusing actions. Hand clap is easily misclassified as boxing, and vice versa. Therefore, precisely classifying confusing classes remains a challenging task. To obtain better performance for multiclass classifications that have confusing classes, we first develop a classifier chain model for multiclass classification (CCMC) to transfer class information between classifiers. Then, based on an analysis of our proposed model, we propose an easy-to-hard learning paradigm for multiclass classification to automatically identify easy and hard classes and then use the predictions from simpler classes to help solve harder classes. Similar to CCMC, the classifier chain (CC) model is also proposed by Read et al. (2009) to capture the label dependency for multi-label classification. However, CC does not consider the order of difficulty of the labels and achieves degenerated performance when there are many confusing labels. Therefore, it is non-trivial to learn the"
            ],
            "keywords": [
                "Multiclass Classification",
                "Multi-label Classification",
                "Classifier Chain",
                "Easyto-hard Learning Paradigm"
            ],
            "author": [
                "Weiwei Liu",
                "Ivor W Tsang",
                "Klaus- Robert Müller"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-212/16-212.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Magic Moments for Structured Output Prediction",
            "abstract": [
                "Most approaches to structured output prediction rely on a hypothesis space of prediction functions that compute their output by maximizing a linear scoring function. In this paper we present two novel learning algorithms for this hypothesis class, and a statistical analysis of their performance. The methods rely on efficiently computing the first two moments of the scoring function over the output space, and using them to create convex objective functions for training. We report extensive experimental results for sequence alignment, named entity recognition, and RNA secondary structure prediction."
            ],
            "keywords": [
                "structured output prediction",
                "discriminative learning",
                "Z-score",
                "discriminant analysis",
                "PAC bound"
            ],
            "author": [
                "Elisa Ricci",
                "Nello Cristianini"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/ricci08a/ricci08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Evolutionary Model Type Selection for Global Surrogate Modeling",
            "abstract": [
                "Due to the scale and computational complexity of currently used simulation codes, global surrogate (metamodels) models have become indispensable tools for exploring and understanding the design space. Due to their compact formulation they are cheap to evaluate and thus readily facilitate visualization, design space exploration, rapid prototyping, and sensitivity analysis. They can also be used as accurate building blocks in design packages or larger simulation environments. Consequently, there is great interest in techniques that facilitate the construction of such approximation models while minimizing the computational cost and maximizing model accuracy. Many surrogate model types exist (Support Vector Machines, Kriging, Neural Networks, etc.) but no type is optimal in all circumstances. Nor is there any hard theory available that can help make this choice. In this paper we present an automatic approach to the model type selection problem. We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. Different model types are evolved cooperatively using a Genetic Algorithm (heterogeneous evolution) and compete to approximate the iteratively selected data. In this way the optimal model type and complexity for a given data set or simulation code can be dynamically determined. Its utility and performance is demonstrated on a number of problems where it outperforms traditional sequential execution of each model type."
            ],
            "keywords": [
                "model type selection",
                "genetic algorithms",
                "global surrogate modeling",
                "function approximation",
                "active learning",
                "adaptive sampling"
            ],
            "author": [
                "Dirk Gorissen",
                "Filip De Turck"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/gorissen09a/gorissen09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Better Algorithms for Benign Bandits",
            "abstract": [
                "The online multi-armed bandit problem and its generalizations are repeated decision making problems, where the goal is to select one of several possible decisions in every round, and incur a cost associated with the decision, in such a way that the total cost incurred over all iterations is close to the cost of the best fixed decision in hindsight. The difference in these costs is known as the regret of the algorithm. The term bandit refers to the setting where one only obtains the cost of the decision used in a given iteration and no other information. A very general form of this problem is the non-stochastic bandit linear optimization problem, where the set of decisions is a convex set in some Euclidean space, and the cost functions are linear. Only recently an efficient algorithm attainingÕ(√ T) regret was discovered in this setting. In this paper we propose a new algorithm for the bandit linear optimization problem which obtains a tighter regret bound ofÕ(√ Q), where Q is the total variation in the cost functions. This regret bound, previously conjectured to hold in the full information case, shows that it is possible to incur much less regret in a slowly changing environment even in the bandit setting. Our algorithm is efficient and applies several new ideas to bandit optimization such as reservoir sampling."
            ],
            "keywords": [
                "multi-armed bandit",
                "regret minimization",
                "online learning"
            ],
            "author": [
                "Elad Hazan",
                "Satyen Kale"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/hazan11a/hazan11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance",
            "abstract": [
                "Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and interrelationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0, 1] range better than other normalized variants."
            ],
            "keywords": [
                "clustering comparison",
                "information theory",
                "adjustment for chance",
                "normalized information distance"
            ],
            "author": [
                "Xuan Nguyen",
                "Julien Epps",
                "James Bailey"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/vinh10a/vinh10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Mining Topological Structure in Graphs through Forest Representations",
            "abstract": [
                "We consider the problem of inferring simplified topological substructures-which we term backbones-in metric and non-metric graphs. Intuitively, these are subgraphs with 'few' nodes, multifurcations, and cycles, that model the topology of the original graph well. We present a multistep procedure for inferring these backbones. First, we encode local (geometric) information of each vertex in the original graph by means of the boundary coefficient (BC) to identify 'core' nodes in the graph. Next, we construct a forest representation of the graph, termed an f-pine, that connects every node of the graph to a local 'core' node. The final backbone is then inferred from the f-pine through CLOF (Constrained Leaves Optimal subForest), a novel graph optimization problem we introduce in this paper. On a theoretical level, we show that CLOF is NP-hard for general graphs. However, we prove that CLOF can be efficiently solved for forest graphs, a surprising fact given that CLOF induces a nontrivial monotone submodular set function maximization problem on tree graphs. This result is the basis of our method for mining backbones in graphs through forest representation. We qualitatively and quantitatively confirm the applicability, effectiveness, and scalability of our method for discovering backbones in a variety of graph-structured data, such as social networks, earthquake locations scattered across the Earth, and high-dimensional cell trajectory data."
            ],
            "keywords": [
                "topological data analysis",
                "graph mining",
                "metric spaces",
                "visualization",
                "topological skeletonization",
                "cluster coefficient",
                "cell trajectory inference"
            ],
            "author": [
                "Robin Vandaele",
                "Tijl De Bie"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-1032/19-1032.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood",
            "abstract": [
                "We propose an efficient and parameter-free scoring criterion, the factorized conditional log-likelihood (fCLL), for learning Bayesian network classifiers. The proposed score is an approximation of the conditional log-likelihood criterion. The approximation is devised in order to guarantee decomposability over the network structure, as well as efficient estimation of the optimal parameters, achieving the same time and space complexity as the traditional log-likelihood scoring criterion. The resulting criterion has an information-theoretic interpretation based on interaction information, which exhibits its discriminative nature. To evaluate the performance of the proposed criterion, we present an empirical comparison with state-of-the-art classifiers. Results on a large suite of benchmark data sets from the UCI repository show thatfCLL-trained classifiers achieve at least as good accuracy as the best compared classifiers, using significantly less computational resources."
            ],
            "keywords": [
                "Bayesian networks",
                "discriminative learning",
                "conditional log-likelihood",
                "scoring criterion",
                "classification",
                "approximation"
            ],
            "author": [
                "Alexandra M Carvalho",
                "Arlindo L Oliveira",
                "Teemu Roos",
                "Petri Myllymäki"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/carvalho11a/carvalho11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Perishability of Data: Dynamic Pricing under Varying-Coefficient Models",
            "abstract": [
                "We consider a firm that sells a large number of products to its customers in an online fashion. Each product is described by a high dimensional feature vector, and the market value of a product is assumed to be linear in the values of its features. Parameters of the valuation model are unknown and can change over time. The firm sequentially observes a product's features and can use the historical sales data (binary sale/no sale feedbacks) to set the price of current product, with the objective of maximizing the collected revenue. We measure the performance of a dynamic pricing policy via regret, which is the expected revenue loss compared to a clairvoyant that knows the sequence of model parameters in advance. We propose a pricing policy based on projected stochastic gradient descent (PSGD) and characterize its regret in terms of time T , features dimension d, and the temporal variability in the model parameters, δ t. We consider two settings. In the first one, feature vectors are chosen antagonistically by nature and we prove that the regret of PSGD pricing policy is of order O(√ T + T t=1 √ tδ t). In the second setting (referred to as stochastic features model), the feature vectors are drawn independently from an unknown distribution. We show that in this case, the regret of PSGD pricing policy is of order O(d 2 log T + T t=1 tδ t /d)."
            ],
            "keywords": [
                "Dynamic Pricing",
                "Revenue Management",
                "Varying-Coefficient Models",
                "Regret",
                "Stochastic Gradient Descent",
                "Hypothesis Testing"
            ],
            "author": [
                "Adel Javanmard"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-061/17-061.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimally Fuzzy Temporal Memory",
            "abstract": [
                "Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register-a moving window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an independent general purpose learner cannot a priori know the characteristic prediction-relevant timescale of the signal. Moreover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by resource availability. Here we construct a fuzzy memory system that optimally sacrifices the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from exponentially long timescales. Using several illustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better off committing to such a fuzzy memory system."
            ],
            "keywords": [],
            "author": [
                "Karthik H Shankar",
                "Marc W Howard"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/shankar13a/shankar13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Sampling from Time-Varying Log-Concave Distributions",
            "abstract": [
                "We propose a computationally efficient random walk on a convex body which rapidly mixes with respect to a fixed log-concave distribution and closely tracks a time-varying log-concave distribution. We develop general theoretical guarantees on the required number of steps; this number can be calculated on the fly according to the distance from and the shape of the next distribution. We then illustrate the technique on several examples. Within the context of exponential families, the proposed method produces samples from a posterior distribution which is updated as data arrive in a streaming fashion. The sampling technique can be used to track time-varying truncated distributions, as well as to obtain samples from a changing mixture model, fitted in a streaming fashion to data. In the setting of linear optimization, the proposed method has oracle complexity with best known dependence on the dimension for certain geometries. In the context of online learning and repeated games, the algorithm is an efficient method for implementing no-regret mixture forecasting strategies. Remarkably, in some of these examples, only one step of the random walk is needed to track the next distribution. 1 1. An extended abstract containing partial results appeared in the proceedings of the NIPS 2010 conference (Narayanan and Rakhlin, 2010)."
            ],
            "keywords": [],
            "author": [
                "Hariharan Narayanan",
                "Alexander Rakhlin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-223/14-223.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Integrating Naïve Bayes and FOIL *",
            "abstract": [
                "A novel relational learning approach that tightly integrates the naïve Bayes learning scheme with the inductive logic programming rule-learner FOIL is presented. In contrast to previous combinations that have employed naïve Bayes only for post-processing the rule sets, the presented approach employs the naïve Bayes criterion to guide its search directly. The proposed technique is implemented in the NFOIL and TFOIL systems, which employ standard naïve Bayes and tree augmented naïve Bayes models respectively. We show that these integrated approaches to probabilistic model and rule learning outperform post-processing approaches. They also yield significantly more accurate models than simple rule learning and are competitive with more sophisticated ILP systems."
            ],
            "keywords": [
                "rule learning",
                "naïve Bayes",
                "statistical relational learning",
                "inductive logic programming"
            ],
            "author": [
                "Niels Landwehr",
                "Kristian Kersting",
                "Luc De Raedt"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/landwehr07a/landwehr07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Policy Iteration with Nonparametric Function Spaces",
            "abstract": [
                "We study two regularization-based approximate policy iteration algorithms, namely REG-LSPI and REG-BRM, to solve reinforcement learning and planning problems in discounted Markov Decision Processes with large state and finite action spaces. The core of these algorithms are the regularized extensions of the Least-Squares Temporal Difference (LSTD) learning and Bellman Residual Minimization (BRM), which are used in the algorithms' policy evaluation steps. Regularization provides a convenient way to control the complexity of the function space to which the estimated value function belongs and as a result enables us to work with rich nonparametric function spaces. We derive efficient implementations of our methods when the function space is a reproducing kernel Hilbert space. We analyze the statistical properties of REG-LSPI and provide an upper bound on the policy evaluation error and the performance loss of the policy returned by this method. Our bound shows the dependence of the loss on the number of samples, the capacity of the function space, and some intrinsic properties of the underlying Markov Decision Process. The dependence of the policy evaluation bound on the number of samples is minimax optimal. This is the first work that provides such a strong guarantee for a nonparametric approximate policy iteration algorithm. 1"
            ],
            "keywords": [
                "reinforcement learning",
                "approximate policy iteration",
                "regularization",
                "nonparametric method",
                "finite-sample analysis"
            ],
            "author": [
                "Amir-Massoud Farahmand",
                "Mohammad Ghavamzadeh",
                "Csaba Szepesvári",
                "Shie Mannor"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/13-016/13-016.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A New Approach to Laplacian Solvers and Flow Problems",
            "abstract": [
                "This paper investigates the behavior of the Min-Sum message passing scheme to solve systems of linear equations in the Laplacian matrices of graphs and to compute electric flows. Voltage and flow problems involve the minimization of quadratic functions and are fundamental primitives that arise in several domains. Algorithms that have been proposed are typically centralized and involve multiple graph-theoretic constructions or sampling mechanisms that make them difficult to implement and analyze. On the other hand, message passing routines are distributed, simple, and easy to implement. In this paper we establish a framework to analyze Min-Sum to solve voltage and flow problems. We characterize the error committed by the algorithm on general weighted graphs in terms of hitting times of random walks defined on the computation trees that support the operations of the algorithms with time. For d-regular graphs with equal weights, we show that the convergence of the algorithms is controlled by the total variation distance between the distributions of non-backtracking random walks defined on the original graph that start from neighboring nodes. The framework that we introduce extends the analysis of Min-Sum to settings where the contraction arguments previously considered in the literature (based on the assumption of walk summability or scaled diagonal dominance) can not be used, possibly in the presence of constraints."
            ],
            "keywords": [
                "Min-Sum",
                "message passing",
                "decentralized algorithms",
                "Laplacian solver",
                "network flows"
            ],
            "author": [
                "Patrick Rebeschini",
                "Sekhar Tatikonda"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-286/17-286.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Conditional Gradient for Sparse Estimation",
            "abstract": [
                "Sparsity is an important modeling tool that expands the applicability of convex formulations for data analysis, however it also creates significant challenges for efficient algorithm design. In this paper we investigate the generalized conditional gradient (GCG) algorithm for solving sparse optimization problems-demonstrating that, with some enhancements, it can provide a more efficient alternative to current state of the art approaches. After studying the convergence properties of GCG for general convex composite problems, we develop efficient methods for evaluating polar operators, a subroutine that is required in each GCG iteration. In particular, we show how the polar operator can be efficiently evaluated in learning low-rank matrices, instantiated with detailed examples on matrix completion and dictionary learning. A further improvement is achieved by interleaving GCG with fixedrank local subspace optimization. A series of experiments on matrix completion, multi-class classification, and multi-view dictionary learning shows that the proposed method can significantly reduce the training cost of current alternatives."
            ],
            "keywords": [
                "generalized conditional gradient",
                "frank-wolfe",
                "dictionary learning",
                "matrix completion",
                "multi-view learning",
                "sparse estimation"
            ],
            "author": [
                "Yaoliang Yu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-348/14-348.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Survey of Accuracy Evaluation Metrics of Recommendation Tasks",
            "abstract": [
                "Recommender systems are now popular both commercially and in the research community, where many algorithms have been suggested for providing recommendations. These algorithms typically perform differently in various domains and tasks. Therefore, it is important from the research perspective, as well as from a practical view, to be able to decide on an algorithm that matches the domain and the task of interest. The standard way to make such decisions is by comparing a number of algorithms offline using some evaluation metric. Indeed, many evaluation metrics have been suggested for comparing recommendation algorithms. The decision on the proper evaluation metric is often critical, as each metric may favor a different algorithm. In this paper we review the proper construction of offline experiments for deciding on the most appropriate algorithm. We discuss three important tasks of recommender systems, and classify a set of appropriate well known evaluation metrics for each task. We demonstrate how using an improper evaluation metric can lead to the selection of an improper algorithm for the task of interest. We also discuss other important considerations when designing offline experiments."
            ],
            "keywords": [
                "recommender systems",
                "collaborative filtering",
                "statistical analysis",
                "comparative studies"
            ],
            "author": [
                "Asela Gunawardana",
                "Guy Shani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/gunawardana09a/gunawardana09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hierarchical Knowledge Gradient for Sequential Sampling",
            "abstract": [
                "We propose a sequential sampling policy for noisy discrete global optimization and ranking and selection, in which we aim to efficiently explore a finite set of alternatives before selecting an alternative as best when exploration stops. Each alternative may be characterized by a multidimensional vector of categorical and numerical attributes and has independent normal rewards. We use a Bayesian probability model for the unknown reward of each alternative and follow a fully sequential sampling policy called the knowledge-gradient policy. This policy myopically optimizes the expected increment in the value of sampling information in each time period. We propose a hierarchical aggregation technique that uses the common features shared by alternatives to learn about many alternatives from even a single measurement. This approach greatly reduces the measurement effort required, but it requires some prior knowledge on the smoothness of the function in the form of an aggregation function and computational issues limit the number of alternatives that can be easily considered to the thousands. We prove that our policy is consistent, finding a globally optimal alternative when given enough measurements, and show through simulations that it performs competitively with or significantly better than other policies."
            ],
            "keywords": [
                "sequential experimental design",
                "ranking and selection",
                "adaptive learning",
                "hierarchical statistics",
                "Bayesian statistics"
            ],
            "author": [
                "Martijn R K Mes",
                "Utwente Nl",
                "Warren B Powell",
                "Peter I Frazier"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/mes11a/mes11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Erratum: SGDQN is Less Careful than Expected",
            "abstract": [
                "The SGD-QN algorithm described in Bordes et al. (2009) contains a subtle flaw that prevents it from reaching its design goals. Yet the flawed SGD-QN algorithm has worked well enough to be a winner of the first Pascal Large Scale Learning Challenge (Sonnenburg et al., 2008). This document clarifies the situation, proposes a corrected algorithm, and evaluates its performance."
            ],
            "keywords": [
                "Soeren Sonnenburg",
                "Vojtech Franc",
                "Elad Yom-Tov and Michele Sebag stochastic gradient descent",
                "support vector machine",
                "conditional random fields"
            ],
            "author": [
                "Antoine Bordes",
                "Léon Bottou",
                "Patrick Gallinari",
                "Jonathan Chang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/bordes10a/bordes10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Challenges in multimodal gesture recognition",
            "abstract": [
                "This paper surveys the state of the art on multimodal gesture recognition and introduces the JMLR special topic on gesture recognition 2011-2015. We began right at the start of the Kinect T M revolution when inexpensive infrared cameras providing image depth recordings became available. We published papers using this technology and other more conventional methods, including regular video cameras, to record data, thus providing a good overview of uses of machine learning and computer vision using multimodal data in this area of application. Notably, we organized a series of challenges and made available several datasets we recorded for that purpose, including tens of thousands of videos, which are available to conduct further research. We also overview recent state of the art works on gesture recognition based on a proposed taxonomy for gesture recognition, discussing challenges and future lines of research."
            ],
            "keywords": [
                "Gesture Recognition",
                "Time Series Analysis",
                "Multimodal Data Analysis",
                "Computer Vision",
                "Pattern Recognition",
                "Wearable sensors",
                "Infrared Cameras",
                "Kinect T M"
            ],
            "author": [
                "Sergio Escalera",
                "Uta Edu",
                "Isabelle Guyon",
                "Zhuowen Tu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-468/14-468.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Identification of Overcomplete Dictionaries",
            "abstract": [
                "This paper presents the first theoretical results showing that stable identification of overcomplete µ-coherent dictionaries Φ ∈ R d×K is locally possible from training signals with sparsity levels S up to the order O(µ −2) and signal to noise ratios up to O(√ d). In particular the dictionary is recoverable as the local maximum of a new maximization criterion that generalizes the K-means criterion. For this maximization criterion results for asymptotic exact recovery for sparsity levels up to O(µ −1) and stable recovery for sparsity levels up to O(µ −2) as well as signal to noise ratios up to O(√ d) are provided. These asymptotic results translate to finite sample size recovery results with high probability as long as the sample size N scales as O(K 3 dSε −2), where the recovery precisionε can go down to the asymptotically achievable precision. Further, to actually find the local maxima of the new criterion, a very simple Iterative Thresholding and K (signed) Means algorithm (ITKM), which has complexity O(dKN) in each iteration, is presented and its local efficiency is demonstrated in several experiments."
            ],
            "keywords": [
                "dictionary learning",
                "dictionary identification",
                "sparse coding",
                "sparse component analysis",
                "vector quantization",
                "K-means",
                "finite sample size",
                "sample complexity",
                "maximization criterion",
                "sparse representation"
            ],
            "author": [
                "Karin Schnass"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/schnass15a/schnass15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear Convergence of Randomized Feasible Descent Methods Under the Weak Strong Convexity Assumption",
            "abstract": [
                "In this paper we generalize the framework of the Feasible Descent Method (FDM) to a Randomized (R-FDM) and a Randomized Coordinate-wise Feasible Descent Method (RC-FDM) framework. We show that many machine learning algorithms, including the famous SDCA algorithm for optimizing the SVM dual problem, or the stochastic coordinate descent method for the LASSO problem, fits into the framework of RC-FDM. We prove linear convergence for both R-FDM and RC-FDM under the weak strong convexity assumption. Moreover, we show that the duality gap converges linearly for RC-FDM, which implies that the duality gap also converges linearly for SDCA applied to the SVM dual problem."
            ],
            "keywords": [
                "feasible descent method",
                "stochastic methods",
                "iteration complexity",
                "convergence theory",
                "weak strong convexity"
            ],
            "author": [
                "Chenxin Ma",
                "Rachael Tappenden",
                "Martin Takáč"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-541/15-541.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Recovery of a Mixture of Gaussians by Sum-of-norms Clustering",
            "abstract": [
                "Sum-of-norms clustering is a method for assigning n points in R d to K clusters, 1 ≤ K ≤ n, using convex optimization. Recently, Panahi et al. (2017) proved that sum-of-norms clustering is guaranteed to recover a mixture of Gaussians under the restriction that the number of samples is not too large. The purpose of this note is to lift this restriction, that is, show that sum-of-norms clustering can recover a mixture of Gaussians even as the number of samples tends to infinity. Our proof relies on an interesting characterization of clusters computed by sum-of-norms clustering that was developed inside a proof of the agglomeration conjecture by Chiquet et al. (2017). Because we believe this theorem has independent interest, we restate and reprove the Chiquet et al. (2017) result herein."
            ],
            "keywords": [
                "Sum-of-norms Clustering",
                "Mixture of Gaussians",
                "Recovery Guarantees",
                "Unsupervised Learning"
            ],
            "author": [
                "Tao Jiang",
                "Stephen Vavasis",
                "Chen Wen Zhai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-218/19-218.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Precise Timing with LSTM Recurrent Networks",
            "abstract": [
                "The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals."
            ],
            "keywords": [
                "Recurrent Neural Networks",
                "Long Short-Term Memory",
                "Timing"
            ],
            "author": [
                "Felix A Gers",
                "Nicol N Schraudolph",
                "Jürgen Schmidhuber"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Static Prediction Games for Adversarial Learning Problems",
            "abstract": [
                "The standard assumption of identically distributed training and test data is violated when the test data are generated in response to the presence of a predictive model. This becomes apparent, for example, in the context of email spam filtering. Here, email service providers employ spam filters, and spam senders engineer campaign templates to achieve a high rate of successful deliveries despite the filters. We model the interaction between the learner and the data generator as a static game in which the cost functions of the learner and the data generator are not necessarily antagonistic. We identify conditions under which this prediction game has a unique Nash equilibrium and derive algorithms that find the equilibrial prediction model. We derive two instances, the Nash logistic regression and the Nash support vector machine, and empirically explore their properties in a case study on email spam filtering."
            ],
            "keywords": [
                "static prediction games",
                "adversarial classification",
                "Nash equilibrium"
            ],
            "author": [
                "Michael Brückner",
                "Christian Kanzow",
                "Tobias Scheffer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/brueckner12a/brueckner12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Completing Any Low-rank Matrix, Provably *",
            "abstract": [
                "Matrix completion, i.e., the exact and provable recovery of a low-rank matrix from a small subset of its elements, is currently only known to be possible if the matrix satisfies a restrictive structural constraint-known as incoherence-on its row and column spaces. In these cases, the subset of elements is assumed to be sampled uniformly at random. In this paper, we show that any rank-r n-by-n matrix can be exactly recovered from as few as O(nr log 2 n) randomly chosen elements, provided this random choice is made according to a specific biased distribution suitably dependent on the coherence structure of the matrix: the probability of any element being sampled should be at least a constant times the sum of the leverage scores of the corresponding row and column. Moreover, we prove that this specific form of sampling is nearly necessary, in a natural precise sense; this implies that many other perhaps more intuitive sampling schemes fail. We further establish three ways to use the above result for the setting when leverage scores are not known a priori. (a) We describe a provably-correct sampling strategy for the case when only the column space is incoherent and no assumption or knowledge of the row space is required. (b) We propose a two-phase sampling procedure for general matrices that first samples to estimate leverage scores followed by sampling for exact recovery. These two approaches assume control over the sampling procedure. (c) By using our main theorem in a reverse direction, we provide an analysis showing the advantages of the (empirically successful) weighted nuclear/trace-norm minimization approach over the vanilla un-weighted formulation given non-uniformly distributed observed elements. This approach does not require controlled sampling or knowledge of the leverage scores."
            ],
            "keywords": [
                "matrix completion",
                "coherence",
                "leverage score",
                "nuclear norm",
                "weighted nuclear norm"
            ],
            "author": [
                "Yudong Chen",
                "Srinadh Bhojanapalli",
                "Rachel Ward"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/chen15b/chen15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Monotone Learning with Rectified Wire Networks",
            "abstract": [
                "We introduce a new neural network model, together with a tractable and monotone online learning algorithm. Our model describes feed-forward networks for classification, with one output node for each class. The only nonlinear operation is rectification using a ReLU function with a bias. However, there is a rectifier on every edge rather than at the nodes of the network. There are also weights, but these are positive, static, and associated with the nodes. Our rectified wire networks are able to represent arbitrary Boolean functions. Only the bias parameters, on the edges of the network, are learned. Another departure in our approach, from standard neural networks, is that the loss function is replaced by a constraint. This constraint is simply that the value of the output node associated with the correct class should be zero. Our model has the property that the exact norm-minimizing parameter update, required to correctly classify a training item, is the solution to a quadratic program that can be computed with a few passes through the network. We demonstrate a training algorithm using this update, called sequential deactivation (SDA), on MNIST and some synthetic datasets. Upon adopting a natural choice for the nodal weights, SDA has no hyperparameters other than those describing the network structure. Our experiments explore behavior with respect to network size and depth in a family of sparse expander networks."
            ],
            "keywords": [
                "Neural Networks",
                "Online Training Algorithms",
                "Rectified Linear Unit"
            ],
            "author": [
                "Veit Elser",
                "Dan Schmidt",
                "Jonathan Yedidia"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-281/18-281.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning",
            "abstract": [
                "Judea Pearl's Causal Model is a rich framework that provides deep insight into the nature of causal relations. As yet, however, the Pearl Causal Model (PCM) has had a lesser impact on economics or econometrics than on other disciplines. This may be due in part to the fact that the PCM is not as well suited to analyzing structures that exhibit features of central interest to economists and econometricians: optimization, equilibrium, and learning. We offer the settable systems framework as an extension of the PCM that permits causal discourse in systems embodying optimization, equilibrium, and learning. Because these are common features of physical, natural, or social systems, our framework may prove generally useful for machine learning. Important features distinguishing the settable system framework from the PCM are its countable dimensionality and the use of partitioning and partition-specific response functions to accommodate the behavior of optimizing and interacting agents and to eliminate the requirement of a unique fixed point for the system. Refinements of the PCM include the settable systems treatment of attributes, the causal role of exogenous variables, and the dual role of variables as causes and responses. A series of closely related machine learning examples and examples from game theory and machine learning with feedback demonstrates some limitations of the PCM and motivates the distinguishing features of settable systems."
            ],
            "keywords": [
                "causal models",
                "game theory",
                "machine learning",
                "recursive estimation",
                "simultaneous equations"
            ],
            "author": [
                "Halbert White",
                "Hwhite @ Ucsd",
                "Karim Chalak"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/white09a/white09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DARWIN: A Framework for Machine Learning and Computer Vision Research and Development",
            "abstract": [
                "We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data flows."
            ],
            "keywords": [
                "machine learning",
                "graphical models",
                "computer vision",
                "open-source software"
            ],
            "author": [
                "Stephen Gould"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/gould12a/gould12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection",
            "abstract": [
                "We present a unifying framework for information theoretic feature selection, bringing almost two decades of research on heuristic filter criteria under a single theoretical interpretation. This is in response to the question: \"what are the implicit statistical assumptions of feature selection criteria based on mutual information?\". To answer this, we adopt a different strategy than is usual in the feature selection literature-instead of trying to define a criterion, we derive one, directly from a clearly specified objective function: the conditional likelihood of the training labels. While many hand-designed heuristic criteria try to optimize a definition of feature 'relevancy' and 'redundancy', our approach leads to a probabilistic framework which naturally incorporates these concepts. As a result we can unify the numerous criteria published over the last two decades, and show them to be low-order approximations to the exact (but intractable) optimisation problem. The primary contribution is to show that common heuristics for information based feature selection (including Markov Blanket algorithms as a special case) are approximate iterative maximisers of the conditional likelihood. A large empirical study provides strong evidence to favour certain classes of criteria, in particular those that balance the relative size of the relevancy/redundancy terms. Overall we conclude that the JMI criterion (Yang and Moody, 1999; Meyer et al., 2008) provides the best tradeoff in terms of accuracy, stability, and flexibility with small data samples."
            ],
            "keywords": [
                "feature selection",
                "mutual information",
                "conditional likelihood"
            ],
            "author": [
                "Gavin Brown",
                "Adam Pocock",
                "Ming-Jie Zhao",
                "Mikel Luján"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/brown12a/brown12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Two-Stage Penalized Least Squares Method for Constructing Large Systems of Structural Equations",
            "abstract": [
                "We propose a two-stage penalized least squares method to build large systems of structural equations based on the instrumental variables view of the classical two-stage least squares method. We show that, with large numbers of endogenous and exogenous variables, the system can be constructed via consistent estimation of a set of conditional expectations at the first stage, and consistent selection of regulatory effects at the second stage. While the consistent estimation at the first stage can be obtained via the ridge regression, the adaptive lasso is employed at the second stage to achieve the consistent selection. This method is computationally fast and allows for parallel implementation. We demonstrate its effectiveness via simulation studies and real data analysis."
            ],
            "keywords": [
                "graphical model",
                "high-dimensional data",
                "reciprocal graphical model",
                "simultaneous equation model",
                "structural equation model"
            ],
            "author": [
                "Chen Chen",
                "Min Ren",
                "Min Zhang",
                "Dabao Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-225/16-225.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Near-optimal Regret Bounds for Reinforcement Learning *",
            "abstract": [
                "For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s, s ′ there is a policy which moves from s to s ′ in at most D steps (on average). We present a reinforcement learning algorithm with total regretÕ(DS √ AT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of Ω(√ DSAT) on the total regret of any learning algorithm is given as well. These results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T. Finally, we also consider a setting where the MDP is allowed to change a fixed number of ℓ times. We present a modification of our algorithm that is able to deal with this setting and show a regret bound ofÕ(ℓ 1/3 T 2/3 DS √ A)."
            ],
            "keywords": [
                "undiscounted reinforcement learning",
                "Markov decision process",
                "regret",
                "online learning",
                "sample complexity"
            ],
            "author": [
                "Thomas Jaksch",
                "Ronald Ortner",
                "Peter Auer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/jaksch10a/jaksch10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Knowledge Matters: Importance of Prior Information for Optimization",
            "abstract": [
                "We explored the effect of introducing prior knowledge into the intermediate level of deep supervised neural networks on two tasks. On a task we designed, all black-box state-of-theart machine learning algorithms which we tested, failed to generalize well. We motivate our work from the hypothesis that, there is a training barrier involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals by using a form of supervision or guidance using a curriculum. Our results provide a positive evidence in favor of this hypothesis. In our experiments, we trained a two-tiered MLP architecture on a dataset for which each input image contains three sprites, and the binary target class is 1 if all of three shapes belong to the same category and otherwise the class is 0. In terms of generalization, black-box machine learning algorithms could not perform better than chance on this task. Standard deep supervised neural networks also failed to generalize. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allowed us to solve the task efficiently. We obtained much better than chance, but imperfect results by exploring different architectures and optimization variants. This observation might be an indication of optimization difficulty when the neural network trained without hints on this task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with the hypotheses on cultural learning inspired by the observations of training of neural networks sometimes getting stuck, even though good solutions exist, both in terms of training and generalization error."
            ],
            "keywords": [
                "deep learning",
                "neural networks",
                "optimization",
                "evolution of culture",
                "curriculum learning",
                "training with hints"
            ],
            "author": [
                "Yoshua Bengio"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/gulchere16a/gulchere16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large-scale SVD and Manifold Learning",
            "abstract": [
                "This paper examines the efficacy of sampling-based low-rank approximation techniques when applied to large dense kernel matrices. We analyze two common approximate singular value decomposition techniques, namely the Nyström and Column sampling methods. We present a theoretical comparison between these two methods, provide novel insights regarding their suitability for various tasks and present experimental results that support our theory. Our results illustrate the relative strengths of each method. We next examine the performance of these two techniques on the largescale task of extracting low-dimensional manifold structure given millions of high-dimensional face images. We address the computational challenges of non-linear dimensionality reduction via Isomap and Laplacian Eigenmaps, using a graph containing about 18 million nodes and 65 million edges. We present extensive experiments on learning low-dimensional embeddings for two large face data sets: CMU-PIE (35 thousand faces) and a web data set (18 million faces). Our comparisons show that the Nyström approximation is superior to the Column sampling method for this task. Furthermore, approximate Isomap tends to perform better than Laplacian Eigenmaps on both clustering and classification with the labeled CMU-PIE data set."
            ],
            "keywords": [
                "low-rank approximation",
                "manifold learning",
                "large-scale matrix factorization"
            ],
            "author": [
                "Ameet Talwalkar",
                "Sanjiv Kumar",
                "Mehryar Mohri",
                "Henry Rowley"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/talwalkar13a/talwalkar13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Latent Tree Graphical Models",
            "abstract": [
                "We study the problem of learning a latent tree graphical model where samples are available only from a subset of variables. We propose two consistent and computationally efficient algorithms for learning minimal latent trees, that is, trees without any redundant hidden nodes. Unlike many existing methods, the observed nodes (or variables) are not constrained to be leaf nodes. Our algorithms can be applied to both discrete and Gaussian random variables and our learned models are such that all the observed and latent variables have the same domain (state space). Our first algorithm, recursive grouping, builds the latent tree recursively by identifying sibling groups using so-called information distances. One of the main contributions of this work is our second algorithm, which we refer to as CLGrouping. CLGrouping starts with a pre-processing procedure in which a tree over the observed variables is constructed. This global step groups the observed nodes that are likely to be close to each other in the true latent tree, thereby guiding subsequent recursive grouping (or equivalent procedures such as neighbor-joining) on much smaller subsets of variables. This results in more accurate and efficient learning of latent trees. We also present regularized versions of our algorithms that learn latent tree approximations of arbitrary distributions. We compare the proposed algorithms to other methods by performing extensive numerical experiments on various latent tree graphical models such as hidden Markov models and star graphs. In addition, we demonstrate the applicability of our methods on real-world data sets by modeling the dependency structure of monthly stock returns in the S&P index and of the words in the 20 newsgroups data set."
            ],
            "keywords": [
                "graphical models",
                "Markov random fields",
                "hidden variables",
                "latent tree models",
                "structure learning"
            ],
            "author": [
                "Myung Jin Choi",
                "Vincent Y F Tan",
                "Wisc Edu",
                "Alan S Willsky",
                "Jin Choi",
                "Animashree Anandkumar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/choi11b/choi11b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classifier Cascades and Trees for Minimizing Feature Evaluation Cost",
            "abstract": [
                "Machine learning algorithms have successfully entered industry through many real-world applications (e.g. , search engines and product recommendations). In these applications, the test-time CPU cost must be budgeted and accounted for. In this paper, we examine two main components of the test-time CPU cost, classifier evaluation cost and feature extraction cost, and show how to balance these costs with the classifier accuracy. Since the computation required for feature extraction dominates the test-time cost of a classifier in these settings, we develop two algorithms to efficiently balance the performance with the test-time cost. Our first contribution describes how to construct and optimize a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific sub-partition of the input space. Our second contribution is a natural reduction of the tree of classifiers into a cascade. The cascade is particularly useful for class-imbalanced data sets as the majority of instances can be early-exited out of the cascade when the algorithm is sufficiently confident in its prediction. Because both approaches only compute features for inputs that benefit from them the most, we find our trained classifiers lead to high accuracies at a small fraction of the computational cost."
            ],
            "keywords": [
                "budgeted learning",
                "resource efficient machine learning",
                "feature cost sensitive learning",
                "web-search ranking",
                "tree of classifiers"
            ],
            "author": [
                "Eddie Xu",
                "Matt J Kusner",
                "Minmin Chen",
                "Olivier Chapelle"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/xu14a/xu14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Patient Risk Stratification with Time-Varying Parameters: A Multitask Learning Approach",
            "abstract": [
                "The proliferation of electronic health records (EHRs) frames opportunities for using machine learning to build models that help healthcare providers improve patient outcomes. However, building useful risk stratification models presents many technical challenges including the large number of factors (both intrinsic and extrinsic) influencing a patient's risk of an adverse outcome and the inherent evolution of that risk over time. We address these challenges in the context of learning a risk stratification model for predicting which patients are at risk of acquiring a Clostridium difficile infection (CDI). We take a novel data-centric approach, leveraging the contents of EHRs from nearly 50,000 hospital admissions. We show how, by adapting techniques from multitask learning, we can learn models for patient risk stratification with unprecedented classification performance. Our model, based on thousands of variables, both time-varying and time-invariant, changes over the course of a patient admission. Applied to a held out set of approximately 25,000 patient admissions, we achieve an area under the receiver operating characteristic curve of 0.81 (95% CI 0.78-0.84). The model has been integrated into the health record system at a large hospital in the US, and can be used to produce daily risk estimates for each inpatient. While more complex than traditional risk stratification methods, the widespread development and use of such data-driven models could ultimately enable cost-effective, targeted prevention strategies that lead to better patient outcomes."
            ],
            "keywords": [
                "risk stratification",
                "time-varying coefficients",
                "multitask learning",
                "Clostridium difficile",
                "healthcare-associated infections"
            ],
            "author": [
                "Jenna Wiens",
                "John Guttag",
                "Eric Horvitz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-177/15-177.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing",
            "abstract": [
                "We describe a class of sparse latent factor models, called graphical factor models (GFMs), and relevant sparse learning algorithms for posterior mode estimation. Linear, Gaussian GFMs have sparse, orthogonal factor loadings matrices, that, in addition to sparsity of the implied covariance matrices, also induce conditional independence structures via zeros in the implied precision matrices. We describe the models and their use for robust estimation of sparse latent factor structure and data/signal reconstruction. We develop computational algorithms for model exploration and posterior mode search, addressing the hard combinatorial optimization involved in the search over a huge space of potential sparse configurations. A mean-field variational technique coupled with annealing is developed to successively generate \"artificial\" posterior distributions that, at the limiting temperature in the annealing schedule, define required posterior modes in the GFM parameter space. Several detailed empirical studies and comparisons to related approaches are discussed, including analyses of handwritten digit image and cancer gene expression data."
            ],
            "keywords": [
                "annealing",
                "graphical factor models",
                "variational mean-field method",
                "MAP estimation",
                "sparse factor analysis",
                "gene expression profiling"
            ],
            "author": [
                "Ryo Yoshida"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/yoshida10a/yoshida10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Policies for Observing Time Series and Related Restless Bandit Problems",
            "abstract": [
                "The trade-off between the cost of acquiring and processing data, and uncertainty due to a lack of data is fundamental in machine learning. A basic instance of this trade-off is the problem of deciding when to make noisy and costly observations of a discrete-time Gaussian random walk, so as to minimise the posterior variance plus observation costs. We present the first proof that a simple policy, which observes when the posterior variance exceeds a threshold, is optimal for this problem. The proof generalises to a wide range of cost functions other than the posterior variance. It is based on a new verification theorem by Niño-Mora that guarantees threshold structure for Markov decision processes, and on the relation between binary sequences known as Christoffel words and the dynamics of discontinuous nonlinear maps, which frequently arise in physics, control and biology. This result implies that optimal policies for linear-quadratic-Gaussian control with costly observations have a threshold structure. It also implies that the restless bandit problem of observing multiple such time series, has a well-defined Whittle index policy. We discuss computation of that index, give closed-form formulae for it, and compare the performance of the associated index policy with heuristic policies."
            ],
            "keywords": [
                "restless bandits",
                "Whittle index",
                "Christoffel words",
                "Sturmian words",
                "Kalman filter",
                "linear-quadratic-Gaussian control"
            ],
            "author": [
                "Tomi Silander"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-185/17-185.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Entropy Search for Information-Efficient Global Optimization",
            "abstract": [
                "Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation."
            ],
            "keywords": [
                "optimization",
                "probability",
                "information",
                "Gaussian processes",
                "expectation propagation"
            ],
            "author": [
                "Philipp Hennig",
                "Christian J Schuler"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/hennig12a/hennig12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exact Simplification of Support Vector Solutions",
            "abstract": [
                "This paper demonstrates that standard algorithms for training support vector machines generally produce solutions with a greater number of support vectors than are strictly necessary. An algorithm is presented that allows unnecessary support vectors to be recognized and eliminated while leaving the solution otherwise unchanged. The algorithm is applied to a variety of benchmark data sets (for both classification and regression) and in most cases the procedure leads to a reduction in the number of support vectors. In some cases the reduction is substantial."
            ],
            "keywords": [],
            "author": [
                "Tom Downs",
                "Robert C Williamson"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/downs01a/downs01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Doubly Distributed Supervised Learning and Inference with High-Dimensional Correlated Outcomes",
            "abstract": [
                "This paper presents a unified framework for supervised learning and inference procedures using the divide-and-conquer approach for high-dimensional correlated outcomes. We propose a general class of estimators that can be implemented in a fully distributed and parallelized computational scheme. Modeling, computational and theoretical challenges related to high-dimensional correlated outcomes are overcome by dividing data at both outcome and subject levels, estimating the parameter of interest from blocks of data using a broad class of supervised learning procedures, and combining block estimators in a closed-form meta-estimator asymptotically equivalent to estimates obtained by Hansen (1982)'s generalized method of moments (GMM) that does not require the entire data to be reloaded on a common server. We provide rigorous theoretical justifications for the use of distributed estimators with correlated outcomes by studying the asymptotic behaviour of the combined estimator with fixed and diverging number of data divisions. Simulations illustrate the finite sample performance of the proposed method, and we provide an R package for ease of implementation."
            ],
            "keywords": [
                "Divide-and-conquer",
                "Generalized method of moments",
                "Estimating functions",
                "Parallel computing",
                "Scalable computing"
            ],
            "author": [
                "Emily C Hector",
                "-K Song"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-996/19-996.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantile Regression Forests",
            "abstract": [
                "Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power."
            ],
            "keywords": [
                "quantile regression",
                "random forests",
                "adaptive neighborhood regression"
            ],
            "author": [
                "Nicolai Meinshausen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/meinshausen06a/meinshausen06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Towards Ultrahigh Dimensional Feature Selection for Big Data",
            "abstract": [
                "In this paper, we present a new adaptive feature scaling scheme for ultrahigh-dimensional feature selection on Big Data, and then reformulate it as a convex semi-infinite programming (SIP) problem. To address the SIP, we propose an efficient feature generating paradigm. Different from traditional gradient-based approaches that conduct optimization on all input features, the proposed paradigm iteratively activates a group of features, and solves a sequence of multiple kernel learning (MKL) subproblems. To further speed up the training, we propose to solve the MKL subproblems in their primal forms through a modified accelerated proximal gradient approach. Due to such optimization scheme, some efficient cache techniques are also developed. The feature generating paradigm is guaranteed to converge globally under mild conditions, and can achieve lower feature selection bias. Moreover, the proposed method can tackle two challenging tasks in feature selection: 1) group-based feature selection with complex structures, and 2) nonlinear feature selection with explicit feature mappings. Comprehensive experiments on a wide range of synthetic and real-world data sets of tens of million data points with O(10 14) features demonstrate the competitive performance of the proposed method over state-of-the-art feature selection methods in terms of generalization performance and training efficiency."
            ],
            "keywords": [
                "big data",
                "ultrahigh dimensionality",
                "feature selection",
                "nonlinear feature selection",
                "multiple kernel learning",
                "feature generation"
            ],
            "author": [
                "Mingkui Tan",
                "Ivor W Tsang",
                "Li Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/tan14a/tan14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Overcomplete, Low Coherence Dictionaries with Linear Inference",
            "abstract": [
                "Finding overcomplete latent representations of data has applications in data analysis, signal processing, machine learning, theoretical neuroscience and many other fields. In an overcomplete representation, the number of latent features exceeds the data dimensionality, which is useful when the data is undersampled by the measurements (compressed sensing or information bottlenecks in neural systems) or composed from multiple complete sets of linear features, each spanning the data space. Independent Components Analysis (ICA) is a linear technique for learning sparse latent representations, which typically has a lower computational cost than sparse coding, a linear generative model which requires an iterative, nonlinear inference step. While well suited for finding complete representations, we show that overcompleteness poses a challenge to existing ICA algorithms. Specifically, the coherence control used in existing ICA and other dictionary learning algorithms, necessary to prevent the formation of duplicate dictionary features, is ill-suited in the overcomplete case. We show that in the overcomplete case, several existing ICA algorithms have undesirable global minima that maximize coherence. We provide a theoretical explanation of these failures and, based on the theory, propose improved coherence control costs for overcomplete ICA algorithms. Further, by comparing ICA algorithms to the computationally more expensive sparse coding on synthetic data, we show that the limited applicability of overcomplete, linear inference can be extended with the proposed cost functions. Finally, when trained on natural images, we show that the coherence control biases the exploration of the data manifold, sometimes yielding suboptimal, coherent solutions. All told, this study contributes new insights into and methods for coherence control for linear ICA, some of which are applicable to many other nonlinear models."
            ],
            "keywords": [
                "independent components analysis",
                "dictionary learning",
                "coherence"
            ],
            "author": [
                "Jesse A Livezey",
                "Alejandro F Bujan",
                "Friedrich T Sommer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-703/18-703.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Latent Variable Models by Pairwise Cluster Comparison: Part II − Algorithm and Evaluation",
            "abstract": [
                "It is important for causal discovery to identify any latent variables that govern a problem and the relationships among them, given measurements in the observed world. In Part I of this paper, we were interested in learning a discrete latent variable model (LVM) and introduced the concept of pairwise cluster comparison (PCC) to identify causal relationships from clusters of data points and an overview of a two-stage algorithm for learning PCC (LPCC). First, LPCC learns exogenous latent variables and latent colliders, as well as their observed descendants, by using pairwise comparisons between data clusters in the measurement space that may explain latent causes. Second, LPCC identifies endogenous latent non-colliders with their observed children. In Part I, we showed that if the true graph has no serial connections, then LPCC returns the true graph, and if the true graph has a serial connection, then LPCC returns a pattern of the true graph. In this paper (Part II), we formally introduce the LPCC algorithm that implements the PCC concept. In addition, we thoroughly evaluate LPCC using simulated and real-world data sets in comparison to state-of-the-art algorithms. Besides using three real-world data sets, which have already been tested in learning an LVM, we also evaluate the algorithms using data sets that represent two original problems. The first problem is identifying young drivers' involvement in road accidents, and the second is identifying cellular subpopulations of the immune system from mass cytometry. The results of our evaluation show that LPCC improves in accuracy with the sample size, can learn large LVMs, and is accurate in learning compared to state-of-the-art algorithms. The code for the LPCC algorithm and data sets used in the experiments reported here are available online."
            ],
            "keywords": [
                "learning latent variable models",
                "graphical models",
                "clustering",
                "pure measurement model"
            ],
            "author": [
                "Nuaman Asbeh",
                "Boaz Lerner",
                "Isabelle Guyon",
                "Alexander Statnikov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-402/14-402.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach *",
            "abstract": [
                "The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classification tasks, for example, it is not uncommon to have 10 to 10 7 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a definition of \"relevancy\" based on spectral properties of the Laplacian of the features' measurement matrix. The feature selection process is then based on a continuous ranking of the features defined by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a \"biased non-negativity\" of a key matrix in the process. As a result, a simple leastsquares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant."
            ],
            "keywords": [],
            "author": [
                "Lior Wolf",
                "Liorwolf @ Mit",
                "Amnon Shashua"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/wolf05a/wolf05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Support Vector Machine Active Learning with Applications to Text Classification",
            "abstract": [
                "Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings."
            ],
            "keywords": [
                "Active Learning",
                "Selective Sampling",
                "Support Vector Machines",
                "Classification",
                "Relevance Feedback"
            ],
            "author": [
                "Simon Tong",
                "Daphne Koller"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/tong01a/tong01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Statistical Perspective on Randomized Sketching for Ordinary Least-Squares",
            "abstract": [
                "We consider statistical as well as algorithmic aspects of solving large-scale least-squares (LS) problems using randomized sketching algorithms. For a LS problem with input data (X, Y) ∈ R n×p × R n , sketching algorithms use a \"sketching matrix,\" S ∈ R r×n , where r n. Then, rather than solving the LS problem using the full data (X, Y), sketching algorithms solve the LS problem using only the \"sketched data\" (SX, SY). Prior work has typically adopted an algorithmic perspective, in that it has made no statistical assumptions on the input X and Y , and instead it has been assumed that the data (X, Y) are fixed and worst-case (WC). Prior results show that, when using sketching matrices such as random projections and leverage-score sampling algorithms, with p r n, the WC error is the same as solving the original problem, up to a small constant. From a statistical perspective, we typically consider the mean-squared error performance of randomized sketching algorithms, when data (X, Y) are generated according to a statistical linear model Y = Xβ + , where is a noise process. In this paper, we provide a rigorous comparison of both perspectives leading to insights on how they differ. To do this, we first develop a framework for assessing, in a unified manner, algorithmic and statistical aspects of randomized sketching methods. We then consider the statistical prediction efficiency (PE) and the statistical residual efficiency (RE) of the sketched LS estimator; and we use our framework to provide upper bounds for several types of random projection and random sampling sketching algorithms. Among other results, we show that the RE can be upper bounded when p r n while the PE typically requires the sample size r to be substantially larger. Lower bounds developed in subsequent results show that our upper bounds on PE can not be improved."
            ],
            "keywords": [
                "algorithmic leveraging",
                "randomized linear algebra",
                "sketching",
                "random projection",
                "statistical leverage",
                "statistical efficiency"
            ],
            "author": [
                "Garvesh Raskutti",
                "Michael W Mahoney"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-440/15-440.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Semi-Supervised Learning with Measure Propagation",
            "abstract": [
                "We describe a new objective for graph-based semi-supervised learning based on minimizing the Kullback-Leibler divergence between discrete probability measures that encode class membership probabilities. We show how the proposed objective can be efficiently optimized using alternating minimization. We prove that the alternating minimization procedure converges to the correct optimum and derive a simple test for convergence. In addition, we show how this approach can be scaled to solve the semi-supervised learning problem on very large data sets, for example, in one instance we use a data set with over 10 8 samples. In this context, we propose a graph node ordering algorithm that is also applicable to other graph-based semi-supervised learning approaches. We compare the proposed approach against other standard semi-supervised learning algorithms on the semi-supervised learning benchmark data sets (Chapelle et al., 2007), and other real-world tasks such as text classification on Reuters and WebKB, speech phone classification on TIMIT and Switchboard, and linguistic dialog-act tagging on Dihana and Switchboard. In each case, the proposed approach outperforms the state-of-the-art. Lastly, we show that our objective can be generalized into a form that includes the standard squared-error loss, and we prove a geometric rate of convergence in that case."
            ],
            "keywords": [
                "graph-based semi-supervised learning",
                "transductive inference",
                "large-scale semi-supervised learning",
                "non-parametric models"
            ],
            "author": [
                "Amarnag Subramanya",
                "Jeff Bilmes"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/subramanya11a/subramanya11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Statistical Performance of Collaborative Inference",
            "abstract": [
                "The statistical analysis of massive and complex data sets will require the development of algorithms that depend on distributed computing and collaborative inference. Inspired by this, we propose a collaborative framework that aims to estimate the unknown mean θ of a random variable X. In the model we present, a certain number of calculation units, distributed across a communication network represented by a graph, participate in the estimation of θ by sequentially receiving independent data from X while exchanging messages via a stochastic matrix A defined over the graph. We give precise conditions on the matrix A under which the statistical precision of the individual units is comparable to that of a (gold standard) virtual centralized estimate, even though each unit does not have access to all of the data. We show in particular the fundamental role played by both the non-trivial eigenvalues of A and the Ramanujan class of expander graphs, which provide remarkable performance for moderate algorithmic cost."
            ],
            "keywords": [
                "distributed computing",
                "collaborative estimation",
                "stochastic matrix",
                "graph theory",
                "complexity",
                "Ramanujan graph"
            ],
            "author": [
                "Gérard Biau",
                "Kevin Bleakley"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-346/15-346.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra",
            "abstract": [
                "Numerous researchers recently applied empirical spectral analysis to the study of modern deep learning classifiers. We identify and discuss an important formal class/cross-class structure and show how it lies at the origin of the many visually striking features observed in deep neural network spectra, some of which were reported in recent articles, others are unveiled here for the first time. These include spectral outliers, \"spikes\", and small but distinct continuous distributions, \"bumps\", often seen beyond the edge of a \"main bulk\"."
            ],
            "keywords": [
                "deep learning",
                "Hessian",
                "spectral analysis",
                "low-rank approximation",
                "multinomial logistic regression"
            ],
            "author": [
                "Vardan Papyan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-933/20-933.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistent Selection of Tuning Parameters via Variable Selection Stability",
            "abstract": [
                "Penalized regression models are popularly used in high-dimensional data analysis to conduct variable selection and model fitting simultaneously. Whereas success has been widely reported in literature, their performances largely depend on the tuning parameters that balance the trade-off between model fitting and model sparsity. Existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability, such as cross validation, AIC and BIC. This article introduces a general tuning parameter selection criterion based on variable selection stability. The key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection. The asymptotic selection consistency is established for both fixed and diverging dimensions. Its effectiveness is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data."
            ],
            "keywords": [
                "kappa coefficient",
                "penalized regression",
                "selection consistency",
                "stability",
                "tuning"
            ],
            "author": [
                "Wei Sun",
                "Junhui Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/sun13b/sun13b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Stationary Subspace Analysis Toolbox",
            "abstract": [
                "The Stationary Subspace Analysis (SSA) algorithm linearly factorizes a high-dimensional time series into stationary and non-stationary components. The SSA Toolbox is a platform-independent efficient stand-alone implementation of the SSA algorithm with a graphical user interface written in Java, that can also be invoked from the command line and from Matlab. The graphical interface guides the user through the whole process; data can be imported and exported from comma separated values (CSV) and Matlab's .mat files."
            ],
            "keywords": [
                "non-stationarities",
                "blind source separation",
                "dimensionality reduction",
                "unsupervised learning"
            ],
            "author": [
                "Jan Saputra Müller",
                "Frank C Meinecke",
                "Franz J Király",
                "Klaus- Robert Müller"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/mueller11a/mueller11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Photonic Delay Systems as Machine Learning Implementations",
            "abstract": [
                "Nonlinear photonic delay systems present interesting implementation platforms for machine learning models. They can be extremely fast, offer great degrees of parallelism and potentially consume far less power than digital processors. So far they have been successfully employed for signal processing using the Reservoir Computing paradigm. In this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encoding of such systems. We perform physical experiments that demonstrate that the obtained input encodings work well in reality, and we show that optimized systems perform significantly better than the common Reservoir Computing approach. The results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro-inspired analog computers."
            ],
            "keywords": [
                "recurrent neural networks",
                "optical computing",
                "machine learning models"
            ],
            "author": [
                "Michiel Hermans",
                "Miguel C Soriano",
                "Joni Dambre",
                "Peter Bienstman",
                "Ingo Fischer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/hermans15a/hermans15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Semi-parametric Learning of Structured Temporal Point Processes",
            "abstract": [
                "We propose a general framework of using multi-level log-Gaussian Cox process to model repeatedly observed point processes with complex structures; such type of data have become increasingly available in various areas including medical research, social sciences, economics and finance due to technological advances. A novel nonparametric approach is developed to efficiently and consistently estimate the covariance functions of the latent Gaussian processes at all levels. To predict the functional principal component scores, we propose a consistent estimation procedure by maximizing the conditional likelihood of super-positions of point processes. We further extend our procedure to the bivariate point process case in which potential correlations between the processes can be assessed. Asymptotic properties of the proposed estimators are investigated, and the effectiveness of our procedures is illustrated through a simulation study and an application to a stock trading dataset."
            ],
            "keywords": [
                "Conditional Likelihood",
                "Log-Gaussian Cox Process",
                "Multi-level Analysis",
                "Principal Component Analysis",
                "Structured Temporal Point Processes"
            ],
            "author": [
                "Ganggang Xu",
                "Ming Wang",
                "Jiangze Bian",
                "Hui Huang",
                "Timothy R Burch",
                "Sandro C Andrade",
                "Jingfei Zhang",
                "Ming Xu",
                "Jiangze Wang",
                "Hui Bian",
                "Timothy R Huang",
                "Sandro C Burch",
                "Jingfei Andrade",
                "Yongtao Guan Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-735/18-735.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "S n FFT: A Julia Toolkit for Fourier Analysis of Functions over Permutations",
            "abstract": [
                "S n FFT is an easy to use software library written in the Julia language to facilitate Fourier analysis on the symmetric group (set of permutations) of degree n, denoted S n and make it more easily deployable within statistical machine learning algorithms. Our implementation internally creates the irreducible matrix representations of S n , and efficiently computes fast Fourier transforms (FFTs) and inverse fast Fourier transforms (iFFTs). Advanced users can achieve scalability and promising practical performance by exploiting various other forms of sparsity. Further, the library also supports the partial inverse Fourier transforms which utilizes the smoothness properties of functions by maintaining only the first few Fourier coefficients. Out of the box, S n FFT currently offers two non-trivial operations for functions defined on S n , namely convolution and correlation. While the potential applicability of S n FFT is fairly broad, as an example, we show how it can be used for clustering ranked data, where each ranking is modeled as a distribution on S n ."
            ],
            "keywords": [
                "permutations",
                "Fourier analysis",
                "fast Fourier transform",
                "Julia"
            ],
            "author": [
                "Gregory Plumb",
                "Risi Kondor",
                "Med Info"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/plumb15a/plumb15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Determining the Number of Latent Factors in Statistical Multi-Relational Learning",
            "abstract": [
                "Statistical relational learning is primarily concerned with learning and inferring relationships between entities in large-scale knowledge graphs. Nickel et al. (2011) proposed a RESCAL tensor factorization model for statistical relational learning, which achieves better or at least comparable results on common benchmark data sets when compared to other state-of-the-art methods. Given a positive integer s, RESCAL computes an s-dimensional latent vector for each entity. The latent factors can be further used for solving relational learning tasks, such as collective classification, collective entity resolution and link-based clustering. The focus of this paper is to determine the number of latent factors in the RESCAL model. Due to the structure of the RESCAL model, its log-likelihood function is not concave. As a result, the corresponding maximum likelihood estimators (MLEs) may not be consistent. Nonetheless, we design a specific pseudometric, prove the consistency of the MLEs under this pseudometric and establish its rate of convergence. Based on these results, we propose a general class of information criteria and prove their model selection consistencies when the number of relations is either bounded or diverges at a proper rate of the number of entities. Simulations and real data examples show that our proposed information criteria have good finite sample properties."
            ],
            "keywords": [
                "Information criteria",
                "Knowledge graph",
                "Model selection consistency",
                "RESCAL model",
                "Statistical relational learning",
                "Tensor factorization"
            ],
            "author": [
                "Chengchun Shi",
                "Wenbin Lu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-037/18-037.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": null,
            "abstract": NaN,
            "keywords": [],
            "author": [],
            "ref": "http://www.jmlr.org/papers/volume3/long02a/long02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems * Eyal Even-Dar",
            "abstract": [
                "We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O (n/ε 2) log(1/δ) times to find an ε-optimal arm with probability of at least 1 − δ. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over ε-greedy Q-learning."
            ],
            "keywords": [],
            "author": [],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/evendar06a/evendar06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pattern Alternating Maximization Algorithm for Missing Data in High-Dimensional Problems",
            "abstract": [
                "We propose a novel and efficient algorithm for maximizing the observed log-likelihood of a multivariate normal data matrix with missing values. We show that our procedure, based on iteratively regressing the missing on the observed variables, generalizes the standard EM algorithm by alternating between different complete data spaces and performing the E-Step incrementally. In this non-standard setup we prove numerical convergence to a stationary point of the observed log-likelihood. For high-dimensional data, where the number of variables may greatly exceed sample size, we perform regularization using a Lasso-type penalty. This introduces sparsity in the regression coefficients used for imputation, permits fast computation and warrants competitive performance in terms of estimating the missing entries. We show on simulated and real data that the new method often improves upon other modern imputation techniques such as k-nearest neighbors imputation, nuclear norm minimization or a penalized likelihood approach with an 1-penalty on the concentration matrix."
            ],
            "keywords": [
                "missing data",
                "observed likelihood",
                "(partial) E-and M-Step",
                "Lasso",
                "penalized variational free energy"
            ],
            "author": [
                "Nicolas Städler",
                "Daniel J Stekhoven",
                "Peter Bühlmann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/staedler14a/staedler14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimating Diffusion Networks: Recovery Conditions, Sample Complexity & Soft-thresholding Algorithm",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "Manuel Gomez-Rodriguez",
                "Hadi Daneshmand",
                "Bernhard Schölkopf"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-430/14-430.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Control of Stochastic Evolution: A Deep Reinforcement Learning Approach to Adaptively Targeting Emergent Drug Resistance",
            "abstract": [
                "The challenge in controlling stochastic systems in which low-probability events can set the system on catastrophic trajectories is to develop a robust ability to respond to such events without significantly compromising the optimality of the baseline control policy. This paper presents CelluDose, a stochastic simulation-trained deep reinforcement learning adaptive feedback control prototype for automated precision drug dosing targeting stochastic and heterogeneous cell proliferation. Drug resistance can emerge from random and variable mutations in targeted cell populations; in the absence of an appropriate dosing policy, emergent resistant subpopulations can proliferate and lead to treatment failure. Dynamic feedback dosage control holds promise in combatting this phenomenon, but the application of traditional control approaches to such systems is fraught with challenges due to the complexity of cell dynamics, uncertainty in model parameters, and the need in medical applications for a robust controller that can be trusted to properly handle unexpected outcomes. Here, training on a sample biological scenario identified single-drug and combination therapy policies that exhibit a 100% success rate at suppressing cell proliferation and responding to diverse system perturbations while establishing low-dose no-event baselines. These policies were found to be highly robust to variations in a key model parameter subject to significant uncertainty and unpredictable dynamical changes."
            ],
            "keywords": [
                "reinforcement learning",
                "deep learning",
                "control",
                "adaptive dosing",
                "drug resistance"
            ],
            "author": [
                "Dalit Engelhardt"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-546/19-546.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Selection and Sorting with Adversarial Comparators",
            "abstract": [
                "We study maximum selection and sorting of n numbers using imperfect pairwise comparators. The imperfect comparator returns the larger of the two inputs if the inputs are more than a given threshold apart and an adversarially-chosen input otherwise. We consider two adversarial models: a non-adaptive adversary that decides on the outcomes in advance and an adaptive adversary that decides on the outcome of each comparison depending on the previous comparisons and outcomes. Against the non-adaptive adversary, we derive a maximum-selection algorithm that uses at most 2n comparisons in expectation and a sorting algorithm that uses at most 2n ln n comparisons in expectation. In the presence of the adaptive adversary, the proposed maximum-selection algorithm uses Θ(n log(1/)) comparisons to output a correct answer with probability at least 1 − , resolving an open problem in Ajtai et al. (2015). Our study is motivated by a density-estimation problem. Given samples from an unknown distribution, we would like to find a distribution among a known class of n candidate distributions that is close to the underlying distribution in 1 distance. Scheffe's algorithm, for example, in Devroye and Lugosi (2001) outputs a distribution at an 1 distance at most 9 times the minimum and runs in time Θ(n 2 log n). Using our algorithm, the runtime reduces to Θ(n log n)."
            ],
            "keywords": [
                "noisy sorting",
                "adversarial comparators",
                "density estimation",
                "Scheffe estimator"
            ],
            "author": [
                "Jayadev Acharya",
                "Moein Falahatgar",
                "Ashkan Jafarpour",
                "Alon Orlitsky",
                "Ananda Theertha Suresh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-165/17-165.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Are Random Forests Truly the Best Classifiers?",
            "abstract": [
                "The JMLR study Do we need hundreds of classifiers to solve real world classification problems? benchmarks 179 classifiers in 17 families on 121 data sets from the UCI repository and claims that \"the random forest is clearly the best family of classifier\". In this response, we show that the study's results are biased by the lack of a held-out test set and the exclusion of trials with errors. Further, the study's own statistical tests indicate that random forests do not have significantly higher percent accuracy than support vector machines and neural networks, calling into question the conclusion that random forests are the best classifiers."
            ],
            "keywords": [
                "classification",
                "benchmarking",
                "random forests",
                "support vector machines",
                "neural networks"
            ],
            "author": [
                "Michael Wainberg",
                "Brendan J Frey"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-374/15-374.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalization Bounds for Ranking Algorithms via Algorithmic Stability *",
            "abstract": [
                "The problem of ranking, in which the goal is to learn a real-valued ranking function that induces a ranking or ordering over an instance space, has recently gained much attention in machine learning. We study generalization properties of ranking algorithms using the notion of algorithmic stability; in particular, we derive generalization bounds for ranking algorithms that have good stability properties. We show that kernel-based ranking algorithms that perform regularization in a reproducing kernel Hilbert space have such stability properties, and therefore our bounds can be applied to these algorithms; this is in contrast with generalization bounds based on uniform convergence, which in many cases cannot be applied to these algorithms. Our results generalize earlier results that were derived in the special setting of bipartite ranking (Agarwal and Niyogi, 2005) to a more general setting of the ranking problem that arises frequently in applications."
            ],
            "keywords": [
                "ranking",
                "generalization bounds",
                "algorithmic stability"
            ],
            "author": [
                "Shivani Agarwal",
                "Shivani @ Mit",
                "Partha Niyogi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/agarwal09a/agarwal09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High Dimensional Forecasting via Interpretable Vector Autoregression",
            "abstract": [
                "Vector autoregression (VAR) is a fundamental tool for modeling multivariate time series. However, as the number of component series is increased, the VAR model becomes overparameterized. Several authors have addressed this issue by incorporating regularized approaches, such as the lasso in VAR estimation. Traditional approaches address overparameterization by selecting a low lag order, based on the assumption of short range dependence, assuming that a universal lag order applies to all components. Such an approach constrains the relationship between the components and impedes forecast performance. The lasso-based approaches perform much better in high-dimensional situations but do not incorporate the notion of lag order selection. We propose a new class of hierarchical lag structures (HLag) that embed the notion of lag selection into a convex regularizer. The key modeling tool is a group lasso with nested groups which guarantees that the sparsity pattern of lag coefficients honors the VAR's ordered structure. The proposed HLag framework offers three basic structures, which allow for varying levels of flexibility, with many possible generalizations. A simulation study demonstrates improved performance in forecasting and lag order selection over previous approaches, and macroeconomic, financial, and energy applications further highlight forecasting improvements as well as HLag's convenient, interpretable output."
            ],
            "keywords": [
                "forecasting",
                "group lasso",
                "multivariate time series",
                "variable selection",
                "vector autoregression"
            ],
            "author": [
                "William B Nicholson",
                "Ines Wilms",
                "Jacob Bien",
                "David S Matteson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-777/19-777.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Decentralized Dictionary Learning Over Time-Varying Digraphs",
            "abstract": [
                "This paper studies Dictionary Learning problems wherein the learning task is distributed over a multi-agent network, modeled as a time-varying directed graph. This formulation is relevant, for instance, in Big Data scenarios where massive amounts of data are collected/stored in different locations (e.g., sensors, clouds) and aggregating and/or processing all data in a fusion center might be inefficient or unfeasible, due to resource limitations, communication overheads or privacy issues. We develop a unified decentralized algorithmic framework for this class of nonconvex problems, which is proved to converge to stationary solutions at a sublinear rate. The new method hinges on Successive Convex Approximation techniques, coupled with a decentralized tracking mechanism aiming at locally estimating the gradient of the smooth part of the sum-utility. To the best of our knowledge, this is the first provably convergent decentralized algorithm for Dictionary Learning and, more generally, bi-convex problems over (time-varying) (di)graphs."
            ],
            "keywords": [
                "Decentralized algorithms",
                "dictionary learning",
                "directed graph",
                "non-convex optimization",
                "time-varying network"
            ],
            "author": [
                "Amir Daneshmand",
                "Ying Sun",
                "Gesualdo Scutari",
                "Francisco Facchinei",
                "Brian M Sadler"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-534/17-534.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-Dimensional Interactions Detection with Sparse Principal Hessian Matrix",
            "abstract": [
                "In statistical learning framework with regressions, interactions are the contributions to the response variable from the products of the explanatory variables. In high-dimensional problems, detecting interactions is challenging due to combinatorial complexity and limited data information. We consider detecting interactions by exploring their connections with the principal Hessian matrix. Specifically, we propose a one-step synthetic approach for estimating the principal Hessian matrix by a penalized M-estimator. An alternating direction method of multipliers (ADMM) is proposed to efficiently solve the encountered regularized optimization problem. Based on the sparse estimator, we detect the interactions by identifying its nonzero components. Our method directly targets at the interactions, and it requires no structural assumption on the hierarchy of the interactions effects. We show that our estimator is theoretically valid, computationally efficient, and practically useful for detecting the interactions in a broad spectrum of scenarios."
            ],
            "keywords": [
                "Interaction Detection",
                "Principal Hessian Matrix",
                "ADMM",
                "Sparse M-Estimator"
            ],
            "author": [
                "Yong Cheng",
                "Ethan X Fang",
                "Yuexiao Dong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-071/19-071.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-Dimensional Covariance Decomposition into Sparse Markov and Independence Models",
            "abstract": [
                "Fitting high-dimensional data involves a delicate tradeoff between faithful representation and the use of sparse models. Too often, sparsity assumptions on the fitted model are too restrictive to provide a faithful representation of the observed data. In this paper, we present a novel framework incorporating sparsity in different domains. We decompose the observed covariance matrix into a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse independence model (with a sparse covariance matrix). Our framework incorporates sparse covariance and sparse precision estimation as special cases and thus introduces a richer class of high-dimensional models. We characterize sufficient conditions for identifiability of the two models, viz., Markov and independence models. We propose an efficient decomposition method based on a modification of the popular 1-penalized maximum-likelihood estimator (1-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both Markov and independence models, when the number of samples n scales as n = Ω(d 2 log p), where p is the number of variables and d is the maximum node degree in the Markov model. Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation."
            ],
            "keywords": [
                "high-dimensional covariance estimation",
                "sparse graphical model selection",
                "sparse covariance models",
                "sparsistency",
                "convex optimization"
            ],
            "author": [
                "Majid Janzamin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/janzamin14a/janzamin14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Proximal Methods for Hierarchical Sparse Coding",
            "abstract": [
                "Sparse coding consists in representing signals as sparse linear combinations of atoms selected from a dictionary. We consider an extension of this framework where the atoms are further assumed to be embedded in a tree. This is achieved using a recently introduced tree-structured sparse regularization norm, which has proven useful in several applications. This norm leads to regularized problems that are difficult to optimize, and in this paper, we propose efficient algorithms for solving them. More precisely, we show that the proximal operator associated with this norm is computable exactly via a dual approach that can be viewed as the composition of elementary proximal operators. Our procedure has a complexity linear, or close to linear, in the number of atoms, and allows the use of accelerated gradient techniques to solve the tree-structured sparse approximation problem at the same computational cost as traditional ones using the ℓ 1-norm. Our method is efficient and scales gracefully to millions of variables, which we illustrate in two types of applications: first, we consider fixed hierarchical dictionaries of wavelets to denoise natural images. Then, we apply our optimization tools in the context of dictionary learning, where learned dictionary elements naturally self-organize in a prespecified arborescent structure, leading to better performance in reconstruction of natural image patches. When applied to text documents, our method learns hierarchies of topics, thus providing a competitive alternative to probabilistic topic models."
            ],
            "keywords": [
                "Proximal methods",
                "dictionary learning",
                "structured sparsity",
                "matrix factorization"
            ],
            "author": [
                "Rodolphe Jenatton",
                "Guillaume Obozinski",
                "Francis Bach",
                "† Rodolphe Jenatton",
                "Rodolphe ©2011",
                "Julien Jenatton",
                "Guillaume Mairal",
                "Francis Obozinski"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/jenatton11a/jenatton11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parallel Algorithm for Learning Optimal Bayesian Network Structure",
            "abstract": [
                "We present a parallel algorithm for the score-based optimal structure search of Bayesian networks. This algorithm is based on a dynamic programming (DP) algorithm having O(n • 2 n) time and space complexity, which is known to be the fastest algorithm for the optimal structure search of networks with n nodes. The bottleneck of the problem is the memory requirement, and therefore, the algorithm is currently applicable for up to a few tens of nodes. While the recently proposed algorithm overcomes this limitation by a space-time trade-off, our proposed algorithm realizes direct parallelization of the original DP algorithm with O(n σ) time and space overhead calculations, where σ > 0 controls the communication-space trade-off. The overall time and space complexity is O(n σ+1 2 n). This algorithm splits the search space so that the required communication between independent calculations is minimal. Because of this advantage, our algorithm can run on distributed memory supercomputers. Through computational experiments, we confirmed that our algorithm can run in parallel using up to 256 processors with a parallelization efficiency of 0.74, compared to the original DP algorithm with a single processor. We also demonstrate optimal structure search for a 32-node network without any constraints, which is the largest network search presented in literature."
            ],
            "keywords": [],
            "author": [
                "Yoshinori Tamada",
                "Satoru Miyano"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/tamada11a/tamada11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning from Multiple Sources *",
            "abstract": [
                "We consider the problem of learning accurate models from multiple sources of \"nearby\" data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields general results for classification and regression. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. We discuss the related problem of learning parameters of a distribution from multiple data sources. Finally, we illustrate our theory through a series of synthetic simulations."
            ],
            "keywords": [],
            "author": [
                "Koby Crammer",
                "Michael Kearns",
                "Jennifer Wortman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/crammer08a/crammer08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Soft Margin Support Vector Classification as Buffered Probability Minimization",
            "abstract": [
                "In this paper, we show that the popular C-SVM, soft-margin support vector classifier is equivalent to minimization of Buffered Probability of Exceedance (bPOE), a recently introduced characterization of uncertainty. To show this, we introduce a new SVM formulation, called the EC-SVM, which is derived from a simple bPOE minimization problem that is easy to interpret with a meaningful free parameter, optimal objective value, and probabilistic derivation. Over the range of its free parameter, the EC-SVM has both a convex and non-convex case which we connect to existing SVM formulations. We first show that the C-SVM, formulated with any regularization norm, is equivalent to the convex EC-SVM. Similarly, we show that the Eν-SVM is equivalent to the EC-SVM over its entire parameter range, which includes both the convex and non-convex case. These equivalences, coupled with the interpretability of the EC-SVM, allow us to gain surprising new insights into the C-SVM and fully connect soft margin support vector classification with superquantile and bPOE concepts. We also show that the EC-SVM can easily be cast as a robust optimization problem, where bPOE is minimized with data lying in a fixed uncertainty set. This reformulation allows us to clearly differentiate between the convex and non-convex case, with convexity associated with pessimistic views of uncertainty and non-convexity associated with optimistic views of uncertainty. Finally, we address some practical considerations. First, we show that these new insights can assist in making parameter selection more efficient. Second, we discuss optimization approaches for solving the EC-SVM. Third, we address the issue of generalization, providing generalization bounds for both bPOE and misclassification rate."
            ],
            "keywords": [
                "Support Vector Machines",
                "Buffered Probability of Exceedance",
                "Conditional Value-at-Risk",
                "Binary Classification",
                "Robust Optimization"
            ],
            "author": [
                "Matthew Norton",
                "Alexander Mafusalov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-566/15-566.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Second Order Cone Programming Approaches for Handling Missing and Uncertain Data",
            "abstract": [
                "We propose a novel second order cone programming formulation for designing robust classifiers which can handle uncertainty in observations. Similar formulations are also derived for designing regression functions which are robust to uncertainties in the regression setting. The proposed formulations are independent of the underlying distribution, requiring only the existence of second order moments. These formulations are then specialized to the case of missing values in observations for both classification and regression problems. Experiments show that the proposed formulations outperform imputation."
            ],
            "keywords": [],
            "author": [
                "Pannagadatta K Shivaswamy",
                "Alexander J Smola",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/shivaswamy06a/shivaswamy06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Linear Identifiable Multivariate Modeling",
            "abstract": [
                "In this paper we consider sparse and identifiable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efficient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component δ-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identifiable Multivariate modeling), is validated and bench-marked on artificial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identifiability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identifiable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/."
            ],
            "keywords": [
                "parsimony",
                "sparsity",
                "identifiability",
                "factor models",
                "linear Bayesian networks"
            ],
            "author": [
                "Ricardo Henao",
                "Ole Winther",
                "* Ricardo Henao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/henao11a/henao11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantum Set Intersection and its Application to Associative Memory",
            "abstract": [
                "We describe a quantum algorithm for computing the intersection of two sets and its application to associative memory. The algorithm is based on a modification of Grover's quantum search algorithm (Grover, 1996). We present algorithms for pattern retrieval, pattern completion, and pattern correction. We show that the quantum associative memory can store an exponential number of memories and retrieve them in sub-exponential time. We prove that this model has advantages over known classical associative memories as well as previously proposed quantum models."
            ],
            "keywords": [
                "associative memory",
                "pattern completion",
                "pattern correction",
                "quantum computation",
                "quantum search"
            ],
            "author": [
                "Salman Tamer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/salman12a/salman12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Point-Based Value Iteration for Continuous POMDPs",
            "abstract": [
                "We propose a novel approach to optimize Partially Observable Markov Decisions Processes (POMDPs) defined on continuous spaces. To date, most algorithms for model-based POMDPs are restricted to discrete states, actions, and observations, but many real-world problems such as, for instance, robot navigation, are naturally defined on continuous spaces. In this work, we demonstrate that the value function for continuous POMDPs is convex in the beliefs over continuous state spaces, and piecewise-linear convex for the particular case of discrete observations and actions but still continuous states. We also demonstrate that continuous Bellman backups are contracting and isotonic ensuring the monotonic convergence of value-iteration algorithms. Relying on those properties, we extend the PERSEUS algorithm, originally developed for discrete POMDPs, to work in continuous state spaces by representing the observation, transition, and reward models using Gaussian mixtures, and the beliefs using Gaussian mixtures or particle sets. With these representations, the integrals that appear in the Bellman backup can be computed in closed form and, therefore, the algorithm is computationally feasible. Finally, we further extend PERSEUS to deal with continuous action and observation sets by designing effective sampling approaches."
            ],
            "keywords": [
                "planning under uncertainty",
                "partially observable Markov decision processes",
                "continuous state space",
                "continuous action space",
                "continuous observation space",
                "point-based value iteration"
            ],
            "author": [
                "Josep M Porta",
                "Nikos Vlassis",
                "Matthijs T J Spaan",
                "Pascal Poupart",
                "David R Cheriton",
                "Sven Koenig",
                "Shie Mannor",
                "Georgios Theocharous"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/porta06a/porta06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": null,
            "abstract": [
                "We describe a graphical model for probabilistic relationships|an alternative to the Bayesian network|called a dependency network. The graph of a dependency network, unlike a B a yesian network, is potentially cyclic. The probability component of a dependency network, like a B a yesian network, is a set of conditional distributions, one for each n o d e given its parents. We identify several basic properties of this representation and describe a computationally e cient procedure for learning the graph and probability components from data. We describe the application of this representation to probabilistic inference, collaborative ltering the task of predicting preferences, and the visualization of acausal predictive relationships."
            ],
            "keywords": [
                "Dependency networks",
                "Bayesian networks",
                "graphical models",
                "probabilistic inference",
                "data visualization",
                "exploratory data analysis",
                "collaborative ltering",
                "Gibbs sampling"
            ],
            "author": [
                "David Maxwell",
                "Christopher Meek",
                "Robert Rounthwaite",
                "Carl Kadie"
            ],
            "ref": "http://www.jmlr.org/papers/volume1/heckerman00a/heckerman00a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies",
            "abstract": [
                "When monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and AD D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove that the problem of finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1 − 1/e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding significant speedups. We also extend our approach to find placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets."
            ],
            "keywords": [
                "Gaussian processes",
                "experimental design",
                "active learning",
                "spatial learning; sensor networks"
            ],
            "author": [
                "Andreas Krause",
                "Carlos Guestrin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/krause08a/krause08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Perturbed Message Passing for Constraint Satisfaction Problems",
            "abstract": [
                "We introduce an efficient message passing scheme for solving Constraint Satisfaction Problems (CSPs), which uses stochastic perturbation of Belief Propagation (BP) and Survey Propagation (SP) messages to bypass decimation and directly produce a single satisfying assignment. Our first CSP solver, called Perturbed Belief Propagation, smoothly interpolates two well-known inference procedures; it starts as BP and ends as a Gibbs sampler, which produces a single sample from the set of solutions. Moreover we apply a similar perturbation scheme to SP to produce another CSP solver, Perturbed Survey Propagation. Experimental results on random and real-world CSPs show that Perturbed BP is often more successful and at the same time tens to hundreds of times more efficient than standard BP guided decimation. Perturbed BP also compares favorably with state-ofthe-art SP-guided decimation, which has a computational complexity that generally scales exponentially worse than our method (w.r.t. the cardinality of variable domains and constraints). Furthermore, our experiments with random satisfiability and coloring problems demonstrate that Perturbed SP can outperform SP-guided decimation, making it the best incomplete random CSP-solver in difficult regimes."
            ],
            "keywords": [
                "constraint satisfaction problem",
                "message passing",
                "belief propagation",
                "survey propagation",
                "Gibbs sampling",
                "decimation"
            ],
            "author": [
                "Siamak Ravanbakhsh",
                "Russell Greiner"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/ravanbakhsh15a/ravanbakhsh15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learnability, Stability and Uniform Convergence",
            "abstract": [
                "The problem of characterizing learnability is the most basic question of statistical learning theory. A fundamental and long-standing answer, at least for the case of supervised classification and regression, is that learnability is equivalent to uniform convergence of the empirical risk to the population risk, and that if a problem is learnable, it is learnable via empirical risk minimization. In this paper, we consider the General Learning Setting (introduced by Vapnik), which includes most statistical learning problems as special cases. We show that in this setting, there are non-trivial learning problems where uniform convergence does not hold, empirical risk minimization fails, and yet they are learnable using alternative mechanisms. Instead of uniform convergence, we identify stability as the key necessary and sufficient condition for learnability. Moreover, we show that the conditions for learnability in the general setting are significantly more complex than in supervised classification and regression."
            ],
            "keywords": [
                "statistical learning theory",
                "learnability",
                "uniform convergence",
                "stability",
                "stochastic convex optimization"
            ],
            "author": [
                "Shai Shalev-Shwartz",
                "Nathan Srebro"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/shalev-shwartz10a/shalev-shwartz10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PAC-Bayes Bounds with Data Dependent Priors",
            "abstract": [
                "This paper presents the prior PAC-Bayes bound and explores its capabilities as a tool to provide tight predictions of SVMs' generalization. The computation of the bound involves estimating a prior of the distribution of classifiers from the available data, and then manipulating this prior in the usual PAC-Bayes generalization bound. We explore two alternatives: to learn the prior from a separate data set, or to consider an expectation prior that does not need this separate data set. The prior PAC-Bayes bound motivates two SVM-like classification algorithms, prior SVM and ηprior SVM, whose regularization term pushes towards the minimization of the prior PAC-Bayes bound. The experimental work illustrates that the new bounds can be significantly tighter than the original PAC-Bayes bound when applied to SVMs, and among them the combination of the prior PAC-Bayes bound and the prior SVM algorithm gives the tightest bound."
            ],
            "keywords": [
                "PAC-Bayes bound",
                "support vector machine",
                "generalization capability prediction",
                "classification"
            ],
            "author": [
                "Emilio Parrado-Hernández",
                "John Shawe-Taylor"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/parrado12a/parrado12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hit Miss Networks with Applications to Instance Selection",
            "abstract": [
                "In supervised learning, a training set consisting of labeled instances is used by a learning algorithm for generating a model (classifier) that is subsequently employed for deciding the class label of new instances (for generalization). Characteristics of the training set, such as presence of noisy instances and size, influence the learning algorithm and affect generalization performance. This paper introduces a new network-based representation of a training set, called hit miss network (HMN), which provides a compact description of the nearest neighbor relation over pairs of instances from each pair of classes. We show that structural properties of HMN's correspond to properties of training points related to the one nearest neighbor (1-NN) decision rule, such as being border or central point. This motivates us to use HMN's for improving the performance of a 1-NN classifier by removing instances from the training set (instance selection). We introduce three new HMN-based algorithms for instance selection. HMN-C, which removes instances without affecting accuracy of 1-NN on the original training set, HMN-E, based on a more aggressive storage reduction, and HMN-EI, which applies iteratively HMN-E. Their performance is assessed on 22 data sets with different characteristics, such as input dimension, cardinality, class balance, number of classes, noise content, and presence of redundant variables. Results of experiments on these data sets show that accuracy of 1-NN classifier increases significantly when HMN-EI is applied. Comparison with state-of-the-art editing algorithms for instance selection on these data sets indicates best generalization performance of HMN-EI and no significant difference in storage requirements. In general, these results indicate that HMN's provide a powerful graph-based representation of a training set, which can be successfully applied for performing noise and redundance reduction in instance-based learning."
            ],
            "keywords": [
                "graph-based training set representation",
                "nearest neighbor",
                "instance selection for instancebased learning"
            ],
            "author": [
                "Elena Marchiori"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/marchiori08a/marchiori08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Methods for ℓ 1 -regularized Loss Minimization",
            "abstract": [
                "We describe and analyze two stochastic methods for ℓ 1 regularized loss minimization problems, such as the Lasso. The first method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature or example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets. 1"
            ],
            "keywords": [
                "L1 regularization",
                "optimization",
                "coordinate descent",
                "mirror descent",
                "sparsity"
            ],
            "author": [
                "Shai Shalev-Shwartz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/shalev-shwartz11a/shalev-shwartz11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stability Bounds for Stationary ϕ-mixing and β-mixing Processes",
            "abstract": [
                "Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm. In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties. However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed. In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence. This paper studies the scenario where the observations are drawn from a stationary ϕ-mixing or β-mixing sequence, a widely adopted assumption in the study of non-i.i.d. processes that implies a dependence between observations weakening over time. We prove novel and distinct stability-based generalization bounds for stationary ϕ-mixing and β-mixing sequences. These bounds strictly generalize the bounds given in the i.i.d. case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-i.i.d. scenarios. We also illustrate the application of our ϕ-mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms. These novel bounds can thus be viewed as the first theoretical basis for the use of these algorithms in non-i.i.d. scenarios."
            ],
            "keywords": [
                "learning in non-i.i.d. scenarios",
                "weakly dependent observations",
                "mixing distributions",
                "algorithmic stability",
                "generalization bounds",
                "learning theory"
            ],
            "author": [
                "Mehryar Mohri"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/mohri10a/mohri10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distance Metric Learning with Eigenvalue Optimization",
            "abstract": [
                "The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efficient metric learning algorithms. Indeed, first-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their convergence characteristics are rigorously established. Various experiments on benchmark data sets show the competitive performance of our new approaches. In addition, we report an encouraging result on a difficult and challenging face verification data set called Labeled Faces in the Wild (LFW)."
            ],
            "keywords": [
                "metric learning",
                "convex optimization",
                "semi-definite programming",
                "first-order methods",
                "eigenvalue optimization",
                "matrix factorization",
                "face verification"
            ],
            "author": [
                "Yiming Ying",
                "Peng Li",
                "Sören Sonnenburg",
                "Francis Bach",
                "Cheng Soon Ong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/ying12a/ying12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "CARP: Software for Fishing Out Good Clustering Algorithms",
            "abstract": [
                "This paper presents the CLUSTERING ALGORITHMS' REFEREE PACKAGE or CARP, an open source GNU GPL-licensed C package for evaluating clustering algorithms. Calibrating performance of such algorithms is important and CARP addresses this need by generating datasets of different clustering complexity and by assessing the performance of the concerned algorithm in terms of its ability to classify each dataset relative to the true grouping. This paper briefly describes the software and its capabilities."
            ],
            "keywords": [
                "CARP",
                "MIXSIM",
                "clustering algorithm",
                "Gaussian mixture",
                "overlap"
            ],
            "author": [
                "Volodymyr Melnykov",
                "MAITRA@IASTATE Ranjan Maitra"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/melnykov11a/melnykov11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Policy Search using Paired Comparisons",
            "abstract": [
                "Direct policy search is a practical way to solve reinforcement learning (RL) problems involving continuous state and action spaces. The goal becomes finding policy parameters that maximize a noisy objective function. The Pegasus method converts this stochastic optimization problem into a deterministic one, by using fixed start states and fixed random number sequences for comparing policies (Ng and Jordan, 2000). We evaluate Pegasus, and new paired comparison methods, using the mountain car problem, and a difficult pursuer-evader problem. We conclude that: (i) paired tests can improve performance of optimization procedures; (ii) several methods are available to reduce the 'overfitting' effect found with Pegasus; (iii) adapting the number of trials used for each comparison yields faster learning; (iv) pairing also helps stochastic search methods such as differential evolution."
            ],
            "keywords": [
                "Reinforcement Learning",
                "Policy Search",
                "Experiment Design"
            ],
            "author": [
                "Malcolm J A Strens",
                "Andrew W Moore",
                "Carla E Brodley",
                "Andrea Danyluk"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/strens02a/strens02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions",
            "abstract": [
                "Clustering is often formulated as a discrete optimization problem. The objective is to find, among all partitions of the data set, the best one according to some quality measure. However, in the statistical setting where we assume that the finite data set has been sampled from some underlying space, the goal is not to find the best partition of the given sample, but to approximate the true partition of the underlying space. We argue that the discrete optimization approach usually does not achieve this goal, and instead can lead to inconsistency. We construct examples which provably have this behavior. As in the case of supervised learning, the cure is to restrict the size of the function classes under consideration. For appropriate \"small\" function classes we can prove very general consistency theorems for clustering optimization schemes. As one particular algorithm for clustering with a restricted function space we introduce \"nearest neighbor clustering\". Similar to the k-nearest neighbor classifier in supervised learning, this algorithm can be seen as a general baseline algorithm to minimize arbitrary clustering objective functions. We prove that it is statistically consistent for all commonly used clustering objective functions."
            ],
            "keywords": [
                "clustering",
                "minimizing objective functions",
                "consistency"
            ],
            "author": [
                "Sébastien Bubeck",
                "Ulrike Von Luxburg"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/bubeck09a/bubeck09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": null,
            "abstract": NaN,
            "keywords": [],
            "author": [],
            "ref": "http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-class Discriminant Kernel Learning via Convex Programming",
            "abstract": [
                "Regularized kernel discriminant analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. Its performance depends on the selection of kernels. In this paper, we consider the problem of multiple kernel learning (MKL) for RKDA, in which the optimal kernel matrix is obtained as a linear combination of pre-specified kernel matrices. We show that the kernel learning problem in RKDA can be formulated as convex programs. First, we show that this problem can be formulated as a semidefinite program (SDP). Based on the equivalence relationship between RKDA and least square problems in the binary-class case, we propose a convex quadratically constrained quadratic programming (QCQP) formulation for kernel learning in RKDA. A semi-infinite linear programming (SILP) formulation is derived to further improve the efficiency. We extend these formulations to the multi-class case based on a key result established in this paper. That is, the multi-class RKDA kernel learning problem can be decomposed into a set of binary-class kernel learning problems which are constrained to share a common kernel. Based on this decomposition property, SDP formulations are proposed for the multi-class case. Furthermore, it leads naturally to QCQP and SILP formulations. As the performance of RKDA depends on the regularization parameter, we show that this parameter can also be optimized in a joint framework with the kernel. Extensive experiments have been conducted and analyzed, and connections to other algorithms are discussed."
            ],
            "keywords": [
                "model selection",
                "kernel discriminant analysis",
                "semidefinite programming",
                "quadratically constrained quadratic programming",
                "semi-infinite linear programming"
            ],
            "author": [
                "Jieping Ye",
                "Shuiwang Ji",
                "Jianhui Chen",
                "Isabelle Guyon",
                "Amir Saffari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/ye08b/ye08b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Neyman-Pearson Classification under High-Dimensional Settings",
            "abstract": [
                "Most existing binary classification methods target on the optimization of the overall classification risk and may fail to serve some real-world applications such as cancer diagnosis, where users are more concerned with the risk of misclassifying one specific class than the other. Neyman-Pearson (NP) paradigm was introduced in this context as a novel statistical framework for handling asymmetric type I/II error priorities. It seeks classifiers with a minimal type II error and a constrained type I error under a user specified level. This article is the first attempt to construct classifiers with guaranteed theoretical performance under the NP paradigm in high-dimensional settings. Based on the fundamental Neyman-Pearson Lemma, we used a plug-in approach to construct NP-type classifiers for Naive Bayes models. The proposed classifiers satisfy the NP oracle inequalities, which are natural NP paradigm counterparts of the oracle inequalities in classical binary classification. Besides their desirable theoretical properties, we also demonstrated their numerical advantages in prioritized error control via both simulation and real data studies."
            ],
            "keywords": [
                "classification",
                "high-dimension",
                "Naive Bayes",
                "Neyman-Pearson (NP) paradigm",
                "NP oracle inequality",
                "plug-in approach",
                "screening"
            ],
            "author": [
                "Anqi Zhao",
                "Yang Feng",
                "Xin Tong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-418/15-418.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Representation Learning for Dynamic Graphs: A Survey",
            "abstract": [
                "Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research."
            ],
            "keywords": [
                "graph representation learning",
                "dynamic graphs",
                "knowledge graph embedding",
                "heterogeneous information networks"
            ],
            "author": [
                "Seyed Mehran Kazemi",
                "Kshitij Jain",
                "Ivan Kobyzev",
                "Peter Forsyth",
                "Pascal Poupart",
                "C Seyed",
                "Mehran Kazemi",
                "Rishab Goel",
                "Akshay Sethi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-447/19-447.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Alexey Chervonenkis's Bibliography",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/gammerman15c/gammerman15c.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "How to Center Deep Boltzmann Machines",
            "abstract": [
                "This work analyzes centered Restricted Boltzmann Machines (RBMs) and centered Deep Boltzmann Machines (DBMs), where centering is done by subtracting offset values from visible and hidden variables. We show analytically that (i) centered and normal Boltzmann Machines (BMs) and thus RBMs and DBMs are different parameterizations of the same model class, such that any normal BM/RBM/DBM can be transformed to an equivalent centered BM/RBM/DBM and vice versa, and that this equivalence generalizes to artificial neural networks in general, (ii) the expected performance of centered binary BMs/RBMs/DBMs is invariant under simultaneous flip of data and offsets, for any offset value in the range of zero to one, (iii) centering can be reformulated as a different update rule for normal BMs/RBMs/DBMs, and (iv) using the enhanced gradient is equivalent to setting the offset values to the average over model and data mean. Furthermore, we present numerical simulations suggesting that (i) optimal generative performance is achieved by subtracting mean values from visible as well as hidden variables, (ii) centered binary RBMs/DBMs reach significantly higher log-likelihood values than normal binary RBMs/DBMs, (iii) centering variants whose offsets depend on the model mean, like the enhanced gradient, suffer from severe divergence problems, (iv) learning is stabilized if an exponentially moving average over the batch means is used for the offset values instead of the current batch mean, which also prevents the enhanced gradient from severe divergence, (v) on a similar level of log-likelihood values centered binary RBMs/DBMs have smaller weights and bigger bias parameters than normal binary RBMs/DBMs, (vi) centering leads to an update direction that is closer to the natural gradient, which is extremely efficient for training as we show for small binary RBMs, (vii) centering eliminates the need for greedy layer-wise pre-training of DBMs, which often even deteriorates the results independently of whether centering is used or not, and (ix) centering is also beneficial for auto encoders."
            ],
            "keywords": [
                "centering",
                "restricted Boltzmann machine",
                "deep Boltzmann machine",
                "generative model",
                "artificial neural network",
                "auto encoder",
                "enhanced gradient",
                "natural gradient",
                "stochastic maximum likelihood",
                "contrastive divergence",
                "parallel tempering"
            ],
            "author": [
                "Jan Melchior",
                "Laurenz Wiskott"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-237/14-237.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classification with Gaussians and Convex Loss",
            "abstract": [
                "This paper considers binary classification algorithms generated from Tikhonov regularization schemes associated with general convex loss functions and varying Gaussian kernels. Our main goal is to provide fast convergence rates for the excess misclassification error. Allowing varying Gaussian kernels in the algorithms improves learning rates measured by regularization error and sample error. Special structures of Gaussian kernels enable us to construct, by a nice approximation scheme with a Fourier analysis technique, uniformly bounded regularizing functions achieving polynomial decays of the regularization error under a Sobolev smoothness condition. The sample error is estimated by using a projection operator and a tight bound for the covering numbers of reproducing kernel Hilbert spaces generated by Gaussian kernels. The convexity of the general loss function plays a very important role in our analysis."
            ],
            "keywords": [
                "reproducing kernel Hilbert space",
                "binary classification",
                "general convex loss",
                "varying Gaussian kernels",
                "covering number",
                "approximation"
            ],
            "author": [
                "Dao-Hong Xiang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/xiang09a/xiang09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Gaussian Belief Propagation with Nodes of Arbitrary Size",
            "abstract": [
                "Gaussian belief propagation (GaBP) is a message-passing algorithm that can be used to perform approximate inference on a pairwise Markov graph (MG) constructed from a multivariate Gaussian distribution in canonical parameterization. The output of GaBP is a set of approximate univariate marginals for each variable in the pairwise MG. An extension of GaBP (labeled GaBP-m), allowing for the approximation of higher-dimensional marginal distributions, was explored by Kamper et al. (2019). The idea is to create an MG in which each node is allowed to receive more than one variable. As in the univariate case, the multivariate extension does not necessarily converge in loopy graphs and, even if convergence occurs, is not guaranteed to provide exact inference. To address the problem of convergence, we consider a multivariate extension of the principle of node regularization proposed by Kamper et al. (2018). We label this algorithm slow GaBP-m (sGaBP-m), where the term \"slow\" relates to the damping effect of the regularization on the message passing. We prove that, given sufficient regularization, this algorithm will converge and provide the exact marginal means at convergence, regardless of the way variables are assigned to nodes. The selection of the degree of regularization is addressed through the use of a heuristic, which is based on a tree representation of sGaBP-m. As a further contribution, we extend other GaBP variants in the literature to allow for higher-dimensional marginalization. We show that our algorithm compares favorably with these variants, both in terms of convergence speed and inference quality."
            ],
            "keywords": [
                "belief propagation",
                "Gaussian distributions",
                "regularization",
                "inference quality",
                "higher-dimensional marginals"
            ],
            "author": [
                "Francois Kamper",
                "Johan A Du Preez"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-101/18-101.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Complexity and Generalization Error of a Restricted Boltzmann Machine in Bayesian Estimation",
            "abstract": [
                "In this paper, we consider the asymptotic form of the generalization error for the restricted Boltzmann machine in Bayesian estimation. It has been shown that obtaining the maximum pole of zeta functions is related to the asymptotic form of the generalization error for hierarchical learning models (Watanabe, 2001a,b). The zeta function is defined by using a Kullback function. We use two methods to obtain the maximum pole: a new eigenvalue analysis method and a recursive blowing up process. We show that these methods are effective for obtaining the asymptotic form of the generalization error of hierarchical learning models."
            ],
            "keywords": [
                "Boltzmann machine",
                "non-regular learning machine",
                "resolution of singularities",
                "zeta function"
            ],
            "author": [
                "Miki Aoyagi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/aoyagi10a/aoyagi10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotic Accuracy of Distribution-Based Estimation of Latent Variables",
            "abstract": [
                "Hierarchical statistical models are widely employed in information science and data engineering. The models consist of two types of variables: observable variables that represent the given data and latent variables for the unobservable labels. An asymptotic analysis of the models plays an important role in evaluating the learning process; the result of the analysis is applied not only to theoretical but also to practical situations, such as optimal model selection and active learning. There are many studies of generalization errors, which measure the prediction accuracy of the observable variables. However, the accuracy of estimating the latent variables has not yet been elucidated. For a quantitative evaluation of this, the present paper formulates distribution-based functions for the errors in the estimation of the latent variables. The asymptotic behavior is analyzed for both the maximum likelihood and the Bayes methods."
            ],
            "keywords": [
                "unsupervised learning",
                "hierarchical parametric models",
                "latent variable",
                "maximum likelihood method",
                "Bayes method"
            ],
            "author": [],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/yamazaki14a/yamazaki14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unifying View of Sparse Approximate Gaussian Process Regression",
            "abstract": [
                "We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full GPs. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints."
            ],
            "keywords": [
                "Gaussian process",
                "probabilistic regression",
                "sparse approximation",
                "Bayesian committee machine"
            ],
            "author": [
                "Joaquin Quiñonero-Candela",
                "Carl Edward Rasmussen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dense Distributions from Sparse Samples: Improved Gibbs Sampling Parameter Estimators for LDA",
            "abstract": [
                "We introduce a novel approach for estimating Latent Dirichlet Allocation (LDA) parameters from collapsed Gibbs samples (CGS), by leveraging the full conditional distributions over the latent variable assignments to efficiently average over multiple samples, for little more computational cost than drawing a single additional collapsed Gibbs sample. Our approach can be understood as adapting the soft clustering methodology of Collapsed Variational Bayes (CVB0) to CGS parameter estimation, in order to get the best of both techniques. Our estimators can straightforwardly be applied to the output of any existing implementation of CGS, including modern accelerated variants. We perform extensive empirical comparisons of our estimators with those of standard collapsed inference algorithms on real-world data for both unsupervised LDA and Prior-LDA, a supervised variant of LDA for multi-label classification. Our results show a consistent advantage of our approach over traditional CGS under all experimental conditions, and over CVB0 inference in the majority of conditions. More broadly, our results highlight the importance of averaging over multiple samples in LDA parameter estimation, and the use of efficient computational techniques to do so."
            ],
            "keywords": [
                "Latent Dirichlet Allocation",
                "topic models",
                "unsupervised learning",
                "multi-label classification",
                "text mining",
                "collapsed Gibbs sampling",
                "CVB0",
                "Bayesian inference"
            ],
            "author": [
                "Yannis Papanikolaou",
                "James R Foulds",
                "Timothy N Rubin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-526/16-526.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "X -Armed Bandits",
            "abstract": [
                "We consider a generalization of stochastic bandits where the set of arms, X , is allowed to be a generic measurable space and the mean-payoff function is \"locally Lipschitz\" with respect to a dissimilarity function that is known to the decision maker. Under this condition we construct an arm selection policy, called HOO (hierarchical optimistic optimization), with improved regret bounds compared to previous results for a large class of problems. In particular, our results imply that if X is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally continuous with a known smoothness degree, then the expected regret of HOO is bounded up to a logarithmic factor by √ n, that is, the rate of growth of the regret is independent of the dimension of the space. We also prove the minimax optimality of our algorithm when the dissimilarity is a metric. Our basic strategy has quadratic computational complexity as a function of the number of time steps and does not rely on the doubling trick. We also introduce a modified strategy, which relies on the doubling trick but runs in linearithmic time. Both results are improvements with respect to previous approaches."
            ],
            "keywords": [
                "bandits with infinitely many arms",
                "optimistic online optimization",
                "regret bounds",
                "minimax rates"
            ],
            "author": [
                "Sébastien Bubeck",
                "Gilles Stoltz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/bubeck11a/bubeck11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Process Discovery with Artificial Negative Events",
            "abstract": [
                "Process discovery is the automated construction of structured process models from information system event logs. Such event logs often contain positive examples only. Without negative examples, it is a challenge to strike the right balance between recall and specificity, and to deal with problems such as expressiveness, noise, incomplete event logs, or the inclusion of prior knowledge. In this paper, we present a configurable technique that deals with these challenges by representing process discovery as a multi-relational classification problem on event logs supplemented with Artificially Generated Negative Events (AGNEs). This problem formulation allows using learning algorithms and evaluation techniques that are well-know in the machine learning community. Moreover, it allows users to have a declarative control over the inductive bias and language bias."
            ],
            "keywords": [
                "graph pattern discovery",
                "inductive logic programming",
                "Petri net",
                "process discovery",
                "positive data only"
            ],
            "author": [
                "Stijn Goedertier",
                "David Martens",
                "Jan Vanthienen",
                "Paolo Frasconi",
                "Kristian Kersting",
                "Hannu Toivonen",
                "Koji Tsuda",
                "* David Martens",
                "† Bart Baesens"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/goedertier09a/goedertier09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Extensive Empirical Study of Feature Selection Metrics for Text Classification",
            "abstract": [
                "Machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and more accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal perspectives-accuracy, F-measure, precision, and recall-since each is appropriate in different situations. The results reveal that a new feature selection metric we call 'Bi-Normal Separation' (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspective, BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together. When choosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a member of the pair-e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin."
            ],
            "keywords": [
                "support vector machines",
                "document categorization",
                "ROC",
                "supervised learning"
            ],
            "author": [
                "George Forman",
                "Isabelle Guyon",
                "André Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/forman03a/forman03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "FastInf: An Efficient Approximate Inference Library",
            "abstract": [
                "The FastInf C++ library is designed to perform memory and time efficient approximate inference in large-scale discrete undirected graphical models. The focus of the library is propagation based approximate inference methods, ranging from the basic loopy belief propagation algorithm to propagation based on convex free energies. Various message scheduling schemes that improve on the standard synchronous or asynchronous approaches are included. Also implemented are a clique tree based exact inference, Gibbs sampling, and the mean field algorithm. In addition to inference, FastInf provides parameter estimation capabilities as well as representation and learning of shared parameters. It offers a rich interface that facilitates extension of the basic classes to other inference and learning methods."
            ],
            "keywords": [
                "graphical models",
                "Markov random field",
                "loopy belief propagation",
                "approximate inference"
            ],
            "author": [
                "Ariel Jaimovich",
                "Ofer Meshi",
                "Ian Mcgraw"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/jaimovich10a/jaimovich10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distance Patterns in Structural Similarity",
            "abstract": [
                "Similarity of edge labeled graphs is considered in the sense of minimum squared distance between corresponding values. Vertex correspondences are established by isomorphisms if both graphs are of equal size and by subisomorphisms if one graph has fewer vertices than the other. Best fit isomorphisms and subisomorphisms amount to solutions of quadratic assignment problems and are computed exactly as well as approximately by minimum cost flow, linear assignment relaxations and related graph algorithms."
            ],
            "keywords": [
                "assignment problem",
                "best approximation",
                "branch and bound",
                "inexact graph matching",
                "model data base"
            ],
            "author": [
                "Thomas Kämpke"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/kaempke06a/kaempke06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rate of Convergence of k-Nearest-Neighbor Classification Rule",
            "abstract": [
                "A binary classification problem is considered. The excess error probability of the k-nearestneighbor classification rule according to the error probability of the Bayes decision is revisited by a decomposition of the excess error probability into approximation and estimation errors. Under a weak margin condition and under a modified Lipschitz condition or a local Lipschitz condition, tight upper bounds are presented such that one avoids the condition that the feature vector is bounded. The concept of modified Lipschitz condition is applied for discrete distributions, too. As a consequence of both concepts, we present the rate of convergence of L 2 error for the corresponding nearest neighbor regression estimate."
            ],
            "keywords": [
                "rate of convergence",
                "classification",
                "error probability",
                "k-nearest-neighbor rule"
            ],
            "author": [
                "Maik Döring",
                "László Györfi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-755/17-755.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Clustering on the Unit Hypersphere using von Mises-Fisher Distributions",
            "abstract": [
                "Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L 2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difficult clustering tasks in high-dimensional spaces."
            ],
            "keywords": [
                "clustering",
                "directional distributions",
                "mixtures",
                "von Mises-Fisher",
                "expectation maximization",
                "maximum likelihood",
                "high dimensional data"
            ],
            "author": [
                "Arindam Banerjee",
                "Inderjit S Dhillon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/banerjee05a/banerjee05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives",
            "abstract": [
                "In cognitive science and neuroscience, there have been two leading models describing how humans perceive and classify facial expressions of emotion-the continuous and the categorical model. The continuous model defines each facial expression of emotion as a feature vector in a face space. This model explains, for example, how expressions of emotion can be seen at different intensities. In contrast, the categorical model consists of C classifiers, each tuned to a specific emotion category. This model explains, among other findings, why the images in a morphing sequence between a happy and a surprise face are perceived as either happy or surprise but not something in between. While the continuous model has a more difficult time justifying this latter finding, the categorical model is not as good when it comes to explaining how expressions are recognized at different intensities or modes. Most importantly, both models have problems explaining how one can recognize combinations of emotion categories such as happily surprised versus angrily surprised versus surprise. To resolve these issues, in the past several years, we have worked on a revised model that justifies the results reported in the cognitive science and neuroscience literature. This model consists of C distinct continuous spaces. Multiple (compound) emotion categories can be recognized by linearly combining these C face spaces. The dimensions of these spaces are shown to be mostly configural. According to this model, the major task for the classification of facial expressions of emotion is precise, detailed detection of facial landmarks rather than recognition. We provide an overview of the literature justifying the model, show how the resulting model can be employed to build algorithms for the recognition of facial expression of emotion, and propose research directions in machine learning and computer vision researchers to keep pushing the state of the art in these areas. We also discuss how the model can aid in studies of human perception, social interactions and disorders."
            ],
            "keywords": [
                "vision",
                "face perception",
                "emotions",
                "computational modeling",
                "categorical perception",
                "face detection"
            ],
            "author": [
                "Aleix Martinez",
                "Isabelle Guyon",
                "Vassilis Athitsos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/martinez12a/martinez12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Distributed Online Prediction Using Mini-Batches",
            "abstract": [
                "Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem."
            ],
            "keywords": [
                "distributed computing",
                "online learning",
                "stochastic optimization",
                "regret bounds",
                "convex optimization"
            ],
            "author": [
                "Ofer Dekel",
                "Lin Xiao",
                "Microsoft Com"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/dekel12a/dekel12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Learning via Perfect Selective Classification",
            "abstract": [
                "We discover a strong relation between two known learning models: stream-based active learning and perfect selective classification (an extreme case of 'classification with a reject option'). For these models, restricted to the realizable case, we show a reduction of active learning to selective classification that preserves fast rates. Applying this reduction to recent results for selective classification, we derive exponential target-independent label complexity speedup for actively learning general (non-homogeneous) linear classifiers when the data distribution is an arbitrary high dimensional mixture of Gaussians. Finally, we study the relation between the proposed technique and existing label complexity measures, including teaching dimension and disagreement coefficient."
            ],
            "keywords": [
                "classification with a reject option",
                "perfect classification",
                "selective classification",
                "active learning",
                "selective sampling",
                "disagreement coefficient",
                "teaching dimension",
                "exploration vs. exploitation"
            ],
            "author": [
                "Ran El-Yaniv",
                "Yair Wiener"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/el-yaniv12a/el-yaniv12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Penalized Maximum Likelihood Estimation of Multi-layered Gaussian Graphical Models",
            "abstract": [
                "Analyzing multi-layered graphical models provides insight into understanding the conditional relationships among nodes within layers after adjusting for and quantifying the effects of nodes from other layers. We obtain the penalized maximum likelihood estimator for Gaussian multi-layered graphical models, based on a computational approach involving screening of variables, iterative estimation of the directed edges between layers and undirected edges within layers and a final refitting and stability selection step that provides improved performance in finite sample settings. We establish the consistency of the estimator in a high-dimensional setting. To obtain this result, we develop a strategy that leverages the biconvexity of the likelihood function to ensure convergence of the developed iterative algorithm to a stationary point, as well as careful uniform error control of the estimates over iterations. The performance of the maximum likelihood estimator is illustrated on synthetic data."
            ],
            "keywords": [
                "graphical models",
                "penalized likelihood",
                "block coordinate descent",
                "convergence",
                "consistency"
            ],
            "author": [
                "Jiahe Lin",
                "Sumanta Basu",
                "Moulinath Banerjee",
                "George Michailidis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-004/16-004.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency of Random Forests and Other Averaging Classifiers",
            "abstract": [
                "In the last years of his life, Leo Breiman promoted random forests for use in classification. He suggested using averaging as a means of obtaining good discrimination rules. The base classifiers used for averaging are simple and randomized, often based on random samples from the data. He left a few questions unanswered regarding the consistency of such rules. In this paper, we give a number of theorems that establish the universal consistency of averaging rules. We also show that some popular classifiers, including one suggested by Breiman, are not universally consistent."
            ],
            "keywords": [
                "random forests",
                "classification trees",
                "consistency",
                "bagging"
            ],
            "author": [
                "Gérard Biau"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/biau08a/biau08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Density Estimation in Infinite Dimensional Exponential Families",
            "abstract": NaN,
            "keywords": [
                "density estimation",
                "exponential family",
                "Fisher divergence",
                "kernel density estimator",
                "maximum likelihood",
                "interpolation space",
                "inverse problem",
                "reproducing kernel Hilbert space",
                "Tikhonov regularization",
                "score matching"
            ],
            "author": [
                "Bharath Sriperumbudur",
                "Arthur Gretton",
                "Aapo Hyvärinen",
                "Revant Kumar",
                "Kenji Fukumizu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-011/16-011.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Communication-Efficient Algorithms for Statistical Optimization",
            "abstract": [
                "We analyze two communication-efficient algorithms for distributed optimization in statistical settings involving large-scale data sets. The first algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error (MSE) that decays as O(N −1 + (N/m) −2). Whenever m ≤ √ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O(N −1 + (N/m) −3), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O(N −1 + (N/m) −3/2), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efficiently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 10 8 samples and d ≈ 740,000 covariates."
            ],
            "keywords": [
                "distributed learning",
                "stochastic optimization",
                "averaging",
                "subsampling"
            ],
            "author": [
                "Yuchen Zhang",
                "John C Duchi",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/zhang13b/zhang13b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large Scale Transductive SVMs",
            "abstract": [
                "We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the first time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach."
            ],
            "keywords": [
                "transduction",
                "transductive SVMs",
                "semi-supervised learning",
                "CCCP"
            ],
            "author": [
                "Ronan Collobert",
                "Fabian Sinz",
                "Jason Weston"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/collobert06a/collobert06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "New Learning Methods for Supervised and Unsupervised Preference Aggregation",
            "abstract": [
                "In this paper we present a general treatment of the preference aggregation problem, in which multiple preferences over objects must be combined into a single consensus ranking. We consider two instances of this problem: unsupervised aggregation where no information about a target ranking is available, and supervised aggregation where ground truth preferences are provided. For each problem class we develop novel learning methods that are applicable to a wide range of preference types. Specifically, for unsupervised aggregation we introduce the Multinomial Preference model (MPM) which uses a multinomial generative process to model the observed preferences. For the supervised problem we develop a supervised extension for MPM and then propose two fully supervised models. The first model employs SVD factorization to derive effective item features, transforming the aggregation problems into a learning-to-rank one. The second model aims to eliminate the costly SVD factorization and instantiates a probabilistic CRF framework, deriving unary and pairwise potentials directly from the observed preferences. Using a probabilistic framework allows us to directly optimize the expectation of any target metric, such as NDCG or ERR. All the proposed models operate on pairwise preferences and can thus be applied to a wide range of preference types. We empirically validate the models on rank aggregation and collaborative filtering data sets and demonstrate superior empirical accuracy."
            ],
            "keywords": [
                "preference aggregation",
                "meta-search",
                "learning-to-rank",
                "collaborative filtering"
            ],
            "author": [
                "Maksims N Volkovs",
                "Richard S Zemel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/volkovs14a/volkovs14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Two-Stage Approach to Multivariate Linear Regression with Sparsely Mismatched Data",
            "abstract": [
                "A tacit assumption in linear regression is that (response, predictor)-pairs correspond to identical observational units. A series of recent works have studied scenarios in which this assumption is violated under terms such as \"Unlabeled Sensing and \"Regression with Unknown Permutation\". In this paper, we study the setup of multiple response variables and a notion of mismatches that generalizes permutations in order to allow for missing matches as well as for one-to-many matches. A two-stage method is proposed under the assumption that most pairs are correctly matched. In the first stage, the regression parameter is estimated by handling mismatches as contaminations, and subsequently the generalized permutation is estimated by a basic variant of matching. The approach is both computationally convenient and equipped with favorable statistical guarantees. Specifically, it is shown that the conditions for permutation recovery become considerably less stringent as the number of responses m per observation increase. Particularly, for m = Ω(log n), the required signal-to-noise ratio no longer depends on the sample size n. Numerical results on synthetic and real data are presented to support the main findings of our analysis."
            ],
            "keywords": [],
            "author": [
                "Martin Slawski",
                "Emanuel Ben-David",
                "Ping Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-645/19-645.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "JCLAL: A Java Framework for Active Learning",
            "abstract": [
                "Active Learning has become an important area of research owing to the increasing number of real-world problems which contain labelled and unlabelled examples at the same time. JCLAL is a Java Class Library for Active Learning which has an architecture that follows strong principles of object-oriented design. It is easy to use, and it allows the developers to adapt, modify and extend the framework according to their needs. The library offers a variety of active learning methods that have been proposed in the literature. The software is available under the GPL license."
            ],
            "keywords": [
                "active learning",
                "framework",
                "java language",
                "object-oriented design"
            ],
            "author": [
                "Oscar Reyes",
                "Eduardo Pérez",
                "Habib M Fardoun",
                "Sebastián Ventura"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-347/15-347.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Big Gaussian Bayesian Networks: Partition, Estimation and Fusion",
            "abstract": [
                "Structure learning of Bayesian networks has always been a challenging problem. Nowadays, massive-size networks with thousands or more of nodes but fewer samples frequently appear in many areas. We develop a divide-and-conquer framework, called partition-estimationfusion (PEF), for structure learning of such big networks. The proposed method first partitions nodes into clusters, then learns a subgraph on each cluster of nodes, and finally fuses all learned subgraphs into one Bayesian network. The PEF method is designed in a flexible way so that any structure learning method may be used in the second step to learn a subgraph structure as either a DAG or a CPDAG. In the clustering step, we adapt hierarchical clustering to automatically choose a proper number of clusters. In the fusion step, we propose a novel hybrid method that sequentially adds edges between subgraphs. Extensive numerical experiments demonstrate the competitive performance of our PEF method, in terms of both speed and accuracy compared to existing methods. Our method can improve the accuracy of structure learning by 20% or more, while reducing running time up to two orders-of-magnitude."
            ],
            "keywords": [
                "Bayesian network",
                "conditional independence",
                "directed acyclic graph",
                "divideand-conquer",
                "structure learning"
            ],
            "author": [
                "Jiaying Gu",
                "Qing Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-318/19-318.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Impact of Random Models on Clustering Similarity",
            "abstract": [
                "Clustering is a central approach for unsupervised learning. After clustering is applied, the most fundamental analysis is to quantitatively compare clusterings. Such comparisons are crucial for the evaluation of clustering methods as well as other tasks such as consensus clustering. It is often argued that, in order to establish a baseline, clustering similarity should be assessed in the context of a random ensemble of clusterings. The prevailing assumption for the random clustering ensemble is the permutation model in which the number and sizes of clusters are fixed. However, this assumption does not necessarily hold in practice; for example, multiple runs of K-means clustering returns clusterings with a fixed number of clusters, while the cluster size distribution varies greatly. Here, we derive corrected variants of two clustering similarity measures (the Rand index and Mutual Information) in the context of two random clustering ensembles in which the number and sizes of clusters vary. In addition, we study the impact of one-sided comparisons in the scenario with a reference clustering. The consequences of different random models are illustrated using synthetic examples, handwriting recognition, and gene expression data. We demonstrate that the choice of random model can have a drastic impact on the ranking of similar clustering pairs, and the evaluation of a clustering method with respect to a random baseline; thus, the choice of random clustering model should be carefully justified."
            ],
            "keywords": [
                "clustering comparison",
                "clustering evaluation",
                "adjustment for chance",
                "Rand index",
                "normalized mutual information"
            ],
            "author": [
                "Alexander J Gates"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-039/17-039.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Tutorial on Conformal Prediction",
            "abstract": [
                "Conformal prediction uses past experience to determine precise levels of confidence in new predictions. Given an error probability ε, together with a method that makes a predictionŷ of a label y, it produces a set of labels, typically containingŷ, that also contains y with probability 1 − ε. Conformal prediction can be applied to any method for producingŷ: a nearest-neighbor method, a support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 − ε of the time, even though they are based on an accumulating data set rather than on independent data sets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in"
            ],
            "keywords": [
                "confidence",
                "on-line compression modeling",
                "on-line learning",
                "prediction regions"
            ],
            "author": [
                "Glenn Shafer",
                "Rutgers Edu",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/shafer08a/shafer08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multivariate Bayesian Structural Time Series Model",
            "abstract": [
                "This paper deals with inference and prediction for multiple correlated time series, where one also has the choice of using a candidate pool of contemporaneous predictors for each target series. Starting with a structural model for time series, we use Bayesian tools for model fitting, prediction and feature selection, thus extending some recent works along these lines for the univariate case. The Bayesian paradigm in this multivariate setting helps the model avoid overfitting, as well as captures correlations among multiple target time series with various state components. The model provides needed flexibility in selecting a different set of components and available predictors for each target series. The cyclical component in the model can handle large variations in the short term, which may be caused by external shocks. Extensive simulations were run to investigate properties such as estimation accuracy and performance in forecasting. This was followed by an empirical study with one-step-ahead prediction on the max log return of a portfolio of stocks that involve four leading financial institutions. Both the simulation studies and the extensive empirical study confirm that this multivariate model outperforms three other benchmark models, viz. a model that treats each target series as independent, the autoregressive integrated moving average model with regression (ARIMAX), and the multivariate ARIMAX (MARIMAX) model."
            ],
            "keywords": [
                "Multivariate Time Series",
                "Feature Selection",
                "Bayesian Model Averaging",
                "Cyclical Component",
                "Estimation and Prediction"
            ],
            "author": [
                "Jinwen Qiu",
                "S Rao Jammalamadaka",
                "Ning Ning"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-009/18-009.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Probabilistic Models of Link Structure",
            "abstract": [
                "Most real-world data is heterogeneous and richly interconnected. Examples include the Web, hypertext, bibliometric data and social networks. In contrast, most statistical learning methods work with \"flat\" data representations, forcing us to convert our data into a form that loses much of the link structure. The recently introduced framework of probabilistic relational models (PRMs) embraces the object-relational nature of structured data by capturing probabilistic interactions between attributes of related entities. In this paper, we extend this framework by modeling interactions between the attributes and the link structure itself. An advantage of our approach is a unified generative model for both content and relational structure. We propose two mechanisms for representing a probabilistic distribution over link structures: reference uncertainty and existence uncertainty. We describe the appropriate conditions for using each model and present learning algorithms for each. We present experimental results showing that the learned models can be used to predict link structure and, moreover, the observed link structure can be used to provide better predictions for the attributes in the model."
            ],
            "keywords": [
                "Probabilistic Relational Models",
                "Bayesian Networks",
                "Relational Learning"
            ],
            "author": [
                "Lise Getoor",
                "Nir Friedman",
                "Daphne Koller",
                "Benjamin Taskar"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/getoor02a/getoor02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Skill Rating for Multiplayer Games Introducing Hypernode Graphs and their Spectral Theory",
            "abstract": [
                "We consider the skill rating problem for multiplayer games, that is how to infer player skills from game outcomes in multiplayer games. We formulate the problem as a minimization problem arg min s s T ∆s where ∆ is a positive semidefinite matrix and s a real-valued function, of which some entries are the skill values to be inferred and other entries are constrained by the game outcomes. We leverage graph-based semi-supervised learning (SSL) algorithms for this problem. We apply our algorithms on several data sets of multiplayer games and obtain very promising results compared to Elo Duelling (see Elo, 1978) and TrueSkill (see Herbrich et al., 2006). As we leverage graph-based SSL algorithms and because games can be seen as relations between sets of players, we then generalize the approach. For this aim, we introduce a new finite model, called hypernode graph, defined to be a set of weighted binary relations between sets of nodes. We define Laplacians of hypernode graphs. Then, we show that the skill rating problem for multiplayer games can be formulated as arg min s s T ∆s where ∆ is the Laplacian of a hypernode graph constructed from a set of games. From a fundamental perspective, we show that hypernode graph Laplacians are symmetric positive semidefinite matrices with constant functions in their null space. We show that problems on hypernode graphs can not be solved with graph constructions and graph kernels. We relate hypernode graphs to signed graphs showing that positive relations between groups can lead to negative relations between individuals."
            ],
            "keywords": [
                "Hypergraphs",
                "Graph Laplacians",
                "Graph Kernels",
                "Spectral Learning",
                "Semisupervised Learning",
                "Multiplayer Games",
                "Skill Rating Algorithms"
            ],
            "author": [
                "Thomas Ricatte",
                "Marc Tommasi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/13-561/13-561.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Transport Analysis of Infinitely Deep Neural Network",
            "abstract": [
                "We investigated the feature map inside deep neural networks (DNNs) by tracking the transport map. We are interested in the role of depth-why do DNNs perform better than shallow models?-and the interpretation of DNNs-what do intermediate layers do? Despite the rapid development in their application, DNNs remain analytically unexplained because the hidden layers are nested and the parameters are not faithful. Inspired by the integral representation of shallow NNs, which is the continuum limit of the width, or the hidden unit number, we developed the flow representation and transport analysis of DNNs. The flow representation is the continuum limit of the depth, or the hidden layer number, and it is specified by an ordinary differential equation (ODE) with a vector field. We interpret an ordinary DNN as a transport map or an Euler broken line approximation of the flow. Technically speaking, a dynamical system is a natural model for the nested feature maps. In addition, it opens a new way to the coordinate-free treatment of DNNs by avoiding the redundant parametrization of DNNs. Following Wasserstein geometry, we analyze a flow in three aspects: dynamical system, continuity equation, and Wasserstein gradient flow. A key finding is that we specified a series of transport maps of the denoising autoencoder (DAE), which is a cornerstone for the development of deep learning. Starting from the shallow DAE, this paper develops three topics: the transport map of the deep DAE, the equivalence between the stacked DAE and the composition of DAEs, and the development of the double continuum limit or the integral representation of the flow representation. As partial answers to the research questions, we found that deeper DAEs converge faster and the extracted features are better; in addition, a deep Gaussian DAE transports mass to decrease the Shannon entropy of the data distribution. We expect that further investigations on these questions lead to the development of an interpretable and principled alternatives to DNNs."
            ],
            "keywords": [
                "representation learning",
                "denoising autoencoder",
                "flow representation",
                "continuum limit",
                "backward heat equation",
                "Wasserstein geometry",
                "ridgelet analysis"
            ],
            "author": [
                "Sho Sonoda",
                "Noboru Murata"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/16-243/16-243.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation",
            "abstract": [
                "Despite tremendous progress in computer vision, there has not been an attempt to apply machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's Picture Archiving and Communication System. With natural language processing, we mine a collection of ∼216K representative two-dimensional images selected by clinicians for diagnostic reference and match the images with their descriptions in an automated manner. We then employ a weakly supervised approach using all of our available data to build models for generating approximate interpretations of patient images. Finally, we demonstrate a more strictly supervised approach to detect the presence and absence of a number of frequent disease types, providing more specific interpretations of patient scans. A relatively small amount of data is used for this part, due to the challenge in gathering quality labels from large raw text data. Our work shows the feasibility of large-scale learning and prediction in electronic patient records available in most modern clinical institutions. It also demonstrates the trade-offs to consider in designing machine learning systems for analyzing large medical data."
            ],
            "keywords": [
                "Deep learning",
                "Convolutional Neural Networks",
                "Topic Models",
                "Natural Language Processing",
                "Medical Imaging"
            ],
            "author": [
                "Hoo-Chang Shin",
                "Le Lu",
                "Lauren Kim",
                "Ari Seff",
                "Jianhua Yao",
                "Ronald M Summers"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-176/15-176.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Finite-Time Bounds for Fitted Value Iteration",
            "abstract": [
                "In this paper we develop a theoretical analysis of the performance of sampling-based fitted value iteration (FVI) to solve infinite state-space, discounted-reward Markovian decision processes (MDPs) under the assumption that a generative model of the environment is available. Our main results come in the form of finite-time bounds on the performance of two versions of sampling-based FVI. The convergence rate results obtained allow us to show that both versions of FVI are well behaving in the sense that by using a sufficiently large number of samples for a large class of MDPs, arbitrary good performance can be achieved with high probability. An important feature of our proof technique is that it permits the study of weighted L p-norm performance bounds. As a result, our technique applies to a large class of function-approximation methods (e.g., neural networks, adaptive regression trees, kernel machines, locally weighted learning), and our bounds scale well with the effective horizon of the MDP. The bounds show a dependence on the stochastic stability properties of the MDP: they scale with the discounted-average concentrability of the future-state distributions. They also depend on a new measure of the approximation power of the function space, the inherent Bellman residual, which reflects how well the function space is \"aligned\" with the dynamics and rewards of the MDP. The conditions of the main result, as well as the concepts introduced in the analysis, are extensively discussed and compared to previous theoretical results. Numerical experiments are used to substantiate the theoretical findings."
            ],
            "keywords": [
                "fitted value iteration",
                "discounted Markovian decision processes",
                "generative model",
                "reinforcement learning",
                "supervised learning",
                "regression",
                "Pollard's inequality",
                "statistical learning theory",
                "optimal control"
            ],
            "author": [
                "Rémi Munos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/munos08a/munos08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parallelizing Exploration-Exploitation Tradeoffs in Gaussian Process Bandit Optimization",
            "abstract": [
                "How can we take advantage of opportunities for experimental parallelization in explorationexploitation tradeoffs? In many experimental scenarios, it is often desirable to execute experiments simultaneously or in batches, rather than only performing one at a time. Additionally, observations may be both noisy and expensive. We introduce Gaussian Process Batch Upper Confidence Bound (GP-BUCB), an upper confidence bound-based algorithm, which models the reward function as a sample from a Gaussian process and which can select batches of experiments to run in parallel. We prove a general regret bound for GP-BUCB, as well as the surprising result that for some common kernels, the asymptotic average regret can be made independent of the batch size. The GP-BUCB algorithm is also applicable in the related case of a delay between initiation of an experiment and observation of its results, for which the same regret bounds hold. We also introduce Gaussian Process Adaptive Upper Confidence Bound (GP-AUCB), a variant of GP-BUCB which can exploit parallelism in an adaptive manner. We evaluate GP-BUCB and GP-AUCB on several simulated and real data sets. These experiments show that GP-BUCB and GP-AUCB are competitive with state-of-the-art heuristics."
            ],
            "keywords": [
                "Gaussian process",
                "upper confidence bound",
                "batch",
                "active learning",
                "regret bound"
            ],
            "author": [
                "Thomas Desautels",
                "Andreas Krause",
                "Joel W Burdick"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/desautels14a/desautels14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear-Time Computation of Similarity Measures for Sequential Data",
            "abstract": [
                "Efficient and expressive comparison of sequences is an essential procedure for learning with sequential data. In this article we propose a generic framework for computation of similarity measures for sequences, covering various kernel, distance and non-metric similarity functions. The basis for comparison is embedding of sequences using a formal language, such as a set of natural words, k-grams or all contiguous subsequences. As realizations of the framework we provide linear-time algorithms of different complexity and capabilities using sorted arrays, tries and suffix trees as underlying data structures. Experiments on data sets from bioinformatics, text processing and computer security illustrate the efficiency of the proposed algorithms-enabling peak performances of up to 10 pairwise comparisons per second. The utility of distances and non-metric similarity measures for sequences as alternatives to string kernels is demonstrated in applications of text categorization, network intrusion detection and transcription site recognition in DNA."
            ],
            "keywords": [
                "string kernels",
                "string distances",
                "learning with sequential data"
            ],
            "author": [
                "Konrad Rieck",
                "Pavel Laskov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/rieck08a/rieck08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Convergence of Optimistic Policy Iteration",
            "abstract": [
                "We consider a finite-state Markov decision problem and establish the convergence of a special case of optimistic policy iteration that involves Monte Carlo estimation of Q-values, in conjunction with greedy policy selection. We provide convergence results for a number of algorithmic variations, including one that involves temporal difference learning (bootstrapping) instead of Monte Carlo estimation. We also indicate some extensions that either fail or are unlikely to go through."
            ],
            "keywords": [
                "Markov Decision Problem",
                "Dynamic Programming",
                "Reinforcement Learning",
                "Monte Carlo",
                "Stochastic Approximation",
                "Temporal Differences"
            ],
            "author": [
                "John N Tsitsiklis"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/tsitsiklis02a/tsitsiklis02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Smoothing Might be Unable to Certify ∞ Robustness for High-Dimensional Images",
            "abstract": [
                "We show a hardness result for random smoothing to achieve certified adversarial robustness against attacks in the p ball of radius when p > 2. Although random smoothing has been well understood for the 2 case using the Gaussian distribution, much remains unknown concerning the existence of a noise distribution that works for the case of p > 2. This has been posed as an open problem by Cohen et al. (2019) and includes many significant paradigms such as the ∞ threat model. In this work, we show that any noise distribution D over R d that provides p robustness for all base classifiers with p > 2 must satisfy E η 2 i = Ω(d 1−2/p 2 (1 − δ)/δ 2) for 99% of the features (pixels) of vector η ∼ D, where is the robust radius and δ is the score gap between the highest-scored class and the runner-up. Therefore, for high-dimensional images with pixel values bounded in [0, 255], the required noise will eventually dominate the useful information in the images, leading to trivial smoothed classifiers."
            ],
            "keywords": [
                "random smoothing",
                "certified adversarial robustness",
                "hardness results",
                "high-dimensional data",
                "p adversarial examples"
            ],
            "author": [
                "Avrim Blum",
                "Travis Dick",
                "Naren Manoj",
                "Hongyang Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-209/20-209.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Learning of Causal Networks with Intervention Experiments and Optimal Designs",
            "abstract": [
                "The causal discovery from data is important for various scientific investigations. Because we cannot distinguish the different directed acyclic graphs (DAGs) in a Markov equivalence class learned from observational data, we have to collect further information on causal structures from experiments with external interventions. In this paper, we propose an active learning approach for discovering causal structures in which we first find a Markov equivalence class from observational data, and then we orient undirected edges in every chain component via intervention experiments separately. In the experiments, some variables are manipulated through external interventions. We discuss two kinds of intervention experiments, randomized experiment and quasi-experiment. Furthermore, we give two optimal designs of experiments, a batch-intervention design and a sequential-intervention design, to minimize the number of manipulated variables and the set of candidate structures based on the minimax and the maximum entropy criteria. We show theoretically that structural learning can be done locally in subgraphs of chain components without need of checking illegal v-structures and cycles in the whole network and that a Markov equivalence subclass obtained after each intervention can still be depicted as a chain graph."
            ],
            "keywords": [
                "active learning",
                "causal networks",
                "directed acyclic graphs",
                "intervention",
                "Markov equivalence class",
                "optimal design",
                "structural learning"
            ],
            "author": [
                "Yang-Bo He"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/he08a/he08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral Regularization Algorithms for Learning Large Incomplete Matrices",
            "abstract": [
                "We use convex relaxation techniques to provide a sequence of regularized low-rank solutions for large-scale matrix completion problems. Using the nuclear norm as a regularizer, we provide a simple and very efficient convex algorithm for minimizing the reconstruction error subject to a bound on the nuclear norm. Our algorithm SOFT-IMPUTE iteratively replaces the missing elements with those obtained from a soft-thresholded SVD. With warm starts this allows us to efficiently compute an entire regularization path of solutions on a grid of values of the regularization parameter. The computationally intensive part of our algorithm is in computing a low-rank SVD of a dense matrix. Exploiting the problem structure, we show that the task can be performed with a complexity of order linear in the matrix dimensions. Our semidefinite-programming algorithm is readily scalable to large matrices; for example SOFT-IMPUTE takes a few hours to compute low-rank approximations of a 10 6 × 10 6 incomplete matrix with 10 observed entries, and fits a rank-95 approximation to the full Netflix training set in 3.3 hours. Our methods achieve good training and test errors and exhibit superior timings when compared to other competitive state-of-the-art techniques."
            ],
            "keywords": [
                "collaborative filtering",
                "nuclear norm",
                "spectral regularization",
                "netflix prize",
                "large scale convex optimization"
            ],
            "author": [
                "Rahul Mazumder",
                "Trevor Hastie",
                "Stanford Edu",
                "Robert Tibshirani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/mazumder10a/mazumder10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Approximate kNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection",
            "abstract": [
                "Nearest neighbor graphs are widely used in data mining and machine learning. A brute-force method to compute the exact kNN graph takes Θ(dn 2) time for n data points in the d dimensional Euclidean space. We propose two divide and conquer methods for computing an approximate kNN graph in Θ(dn t) time for high dimensional data (large d). The exponent t ∈ (1, 2) is an increasing function of an internal parameter α which governs the size of the common region in the divide step. Experiments show that a high quality graph can usually be obtained with small overlaps, that is, for small values of t. A few of the practical details of the algorithms are as follows. First, the divide step uses an inexpensive Lanczos procedure to perform recursive spectral bisection. After each conquer step, an additional refinement step is performed to improve the accuracy of the graph. Finally, a hash table is used to avoid repeating distance calculations during the divide and conquer process. The combination of these techniques is shown to yield quite effective algorithms for building kNN graphs."
            ],
            "keywords": [
                "nearest neighbors graph",
                "high dimensional data",
                "divide and conquer",
                "Lanczos algorithm",
                "spectral method"
            ],
            "author": [
                "Jie Chen",
                "@ Hrfang",
                "Anl Mcs",
                "Yousef Saad"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/chen09b/chen09b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improved Asynchronous Parallel Optimization Analysis for Stochastic Incremental Methods",
            "abstract": [
                "As data sets continue to increase in size and multi-core computer architectures are developed, asynchronous parallel optimization algorithms become more and more essential to the field of Machine Learning. Unfortunately, conducting the theoretical analysis asynchronous methods is difficult, notably due to the introduction of delay and inconsistency in inherently sequential algorithms. Handling these issues often requires resorting to simplifying but unrealistic assumptions. Through a novel perspective, we revisit and clarify a subtle but important technical issue present in a large fraction of the recent convergence rate proofs for asynchronous parallel optimization algorithms, and propose a simplification of the recently introduced \"perturbed iterate\" framework that resolves it. We demonstrate the usefulness of our new framework by analyzing three distinct asynchronous parallel incremental optimization algorithms: Hogwild (asynchronous Sgd), Kromagnon (asynchronous Svrg) and Asaga, a novel asynchronous parallel version of the incremental gradient algorithm Saga that enjoys fast linear convergence rates. We are able to both remove problematic assumptions and obtain better theoretical results. Notably, we prove that Asaga and Kromagnon can obtain a theoretical linear speedup on multi-core systems even without sparsity assumptions. We present results of an implementation on a 40-core architecture illustrating the practical speedups as well as the hardware overhead. Finally, we investigate the overlap constant, an ill-understood but central quantity for the theoretical analysis of asynchronous parallel algorithms. We find that it encompasses much more complexity than suggested in previous work, and often is order-of-magnitude bigger than traditionally thought."
            ],
            "keywords": [
                "optimization",
                "machine learning",
                "large scale",
                "asynchronous parallel",
                "sparsity"
            ],
            "author": [
                "Rémi Leblond",
                "Fabian Pedregosa",
                "Simon Lacoste-Julien"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-650/17-650.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Distribution Embeddings: Universal Kernels, Characteristic Kernels and Kernel Metrics on Distributions",
            "abstract": [
                "Kernel mean embeddings have become a popular tool in machine learning. They map probability measures to functions in a reproducing kernel Hilbert space. The distance between two mapped measures defines a semi-distance over the probability measures known as the maximum mean discrepancy (MMD). Its properties depend on the underlying kernel and have been linked to three fundamental concepts of the kernel literature: universal, characteristic and strictly positive definite kernels. The contributions of this paper are threefold. First, by slightly extending the usual definitions of universal, characteristic and strictly positive definite kernels, we show that these three concepts are essentially equivalent. Second, we give the first complete characterization of those kernels whose associated MMD-distance metrizes the weak convergence of probability measures. Third, we show that kernel mean embeddings can be extended from probability measures to generalized measures called Schwartz-distributions and analyze a few properties of these distribution embeddings."
            ],
            "keywords": [
                "kernel mean embedding",
                "universal kernel",
                "characteristic kernel",
                "Schwartzdistributions",
                "kernel metrics on distributions",
                "metrisation of the weak topology"
            ],
            "author": [
                "Carl-Johann Simon-Gabriel",
                "Bernhard Schölkopf"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-291/16-291.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distances between Data Sets Based on Summary Statistics",
            "abstract": [
                "The concepts of similarity and distance are crucial in data mining. We consider the problem of defining the distance between two data sets by comparing summary statistics computed from the data sets. The initial definition of our distance is based on geometrical notions of certain sets of distributions. We show that this distance can be computed in cubic time and that it has several intuitive properties. We also show that this distance is the unique Mahalanobis distance satisfying certain assumptions. We also demonstrate that if we are dealing with binary data sets, then the distance can be represented naturally by certain parity functions, and that it can be evaluated in linear time. Our empirical tests with real world data show that the distance works well."
            ],
            "keywords": [
                "data mining theory",
                "complex data",
                "binary data",
                "itemsets"
            ],
            "author": [
                "Nikolaj Tatti"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/tatti07a/tatti07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Path-Based Spectral Clustering: Guarantees, Robustness to Outliers, and Fast Algorithms",
            "abstract": [
                "We consider the problem of clustering with the longest-leg path distance (LLPD) metric, which is informative for elongated and irregularly shaped clusters. We prove finite-sample guarantees on the performance of clustering with respect to this metric when random samples are drawn from multiple intrinsically low-dimensional clusters in high-dimensional space, in the presence of a large number of highdimensional outliers. By combining these results with spectral clustering with respect to LLPD, we provide conditions under which the Laplacian eigengap statistic correctly determines the number of clusters for a large class of data sets, and prove guarantees on the labeling accuracy of the proposed algorithm. Our methods are quite general and provide performance guarantees for spectral clustering with any ultrametric. We also introduce an efficient, easy to implement approximation algorithm for the LLPD based on a multiscale analysis of adjacency graphs, which allows for the runtime of LLPD spectral clustering to be quasilinear in the number of data points."
            ],
            "keywords": [
                "unsupervised learning",
                "spectral clustering",
                "manifold learning",
                "fast algorithms",
                "shortest path distance"
            ],
            "author": [
                "Anna Little",
                "Mauro Maggioni",
                "James M Murphy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-085/18-085.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Player Bandits: The Adversarial Case",
            "abstract": [
                "We consider a setting where multiple players sequentially choose among a common set of actions (arms). Motivated by an application to cognitive radio networks, we assume that players incur a loss upon colliding, and that communication between players is not possible. Existing approaches assume that the system is stationary. Yet this assumption is often violated in practice, e.g., due to signal strength fluctuations. In this work, we design the first multi-player Bandit algorithm that provably works in arbitrarily changing environments, where the losses of the arms may even be chosen by an adversary. This resolves an open problem posed by Rosenski et al. (2016)."
            ],
            "keywords": [
                "Multi-Armed Bandits",
                "Multi-Player Problems",
                "Online Learning",
                "Sequential Decision Making",
                "Cognitive Radio Networks"
            ],
            "author": [
                "Pragnya Alatur",
                "Andreas Krause"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-912/19-912.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Decoupling Sparsity and Smoothness in the Dirichlet Variational Autoencoder Topic Model",
            "abstract": [
                "Recent work on variational autoencoders (VAEs) has enabled the development of generative topic models using neural networks. Topic models based on latent Dirichlet allocation (LDA) successfully use the Dirichlet distribution as a prior for the topic and word distributions to enforce sparseness. However, there is a trade-off between sparsity and smoothness in Dirichlet distributions. Sparsity is important for a low reconstruction error during training of the autoencoder, whereas smoothness enables generalization and leads to a better loglikelihood of the test data. Both of these properties are encoded in the Dirichlet parameter vector. By rewriting this parameter vector into a product of a sparse binary vector and a smoothness vector, we decouple the two properties, leading to a model that features both a competitive topic coherence and a high log-likelihood. Efficient training is enabled using rejection sampling variational inference for the reparameterization of the Dirichlet distribution. Our experiments show that our method is competitive with other recent VAE topic models."
            ],
            "keywords": [
                "variational autoencoders",
                "topic models",
                "Dirichlet distribution",
                "reparameterization",
                "generative models"
            ],
            "author": [
                "Sophie Burkhardt",
                "Stefan Kramer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-569/18-569.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Monotone DNF from a Teacher that Almost Does Not Answer Membership Queries",
            "abstract": [
                "We present results concerning the learning of Monotone DNF (MDNF) from Incomplete Membership Queries and Equivalence Queries. Our main result is a new algorithm that allows efficient learning of MDNF using Equivalence Queries and Incomplete Membership Queries with probability of p = 1 − 1/poly(n, t) of failing. Our algorithm is expected to make O tn 1 − p 2 queries, when learning a MDNF formula with t terms over n variables. Note that this is polynomial for any failure probability p = 1 − 1/poly(n, t). The algorithm's running time is also polynomial in t, n, and 1/(1 − p). In a sense this is the best possible, as learning with p = 1 − 1/ω(poly(n, t)) would imply learning MDNF, and thus also DNF, from equivalence queries alone. 1"
            ],
            "keywords": [],
            "author": [
                "Nader H Bshouty"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/bshouty02a/bshouty02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parallel MCMC with Generalized Elliptical Slice Sampling",
            "abstract": [
                "Probabilistic models are conceptually powerful tools for finding structure in data, but their practical effectiveness is often limited by our ability to perform inference in them. Exact inference is frequently intractable, so approximate inference is often performed using Markov chain Monte Carlo (MCMC). To achieve the best possible results from MCMC, we want to efficiently simulate many steps of a rapidly mixing Markov chain which leaves the target distribution invariant. Of particular interest in this regard is how to take advantage of multi-core computing to speed up MCMC-based inference, both to improve mixing and to distribute the computational load. In this paper, we present a parallelizable Markov chain Monte Carlo algorithm for efficiently sampling from continuous probability distributions that can take advantage of hundreds of cores. This method shares information between parallel Markov chains to build a scale-location mixture of Gaussians approximation to the density function of the target distribution. We combine this approximation with a recently developed method known as elliptical slice sampling to create a Markov chain with no step-size parameters that can mix rapidly without requiring gradient or curvature computations."
            ],
            "keywords": [
                "Markov chain Monte Carlo",
                "parallelism",
                "slice sampling",
                "elliptical slice sampling",
                "approximate inference"
            ],
            "author": [
                "Robert Nishihara",
                "Iain Murray",
                "Ryan P Adams"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/nishihara14a/nishihara14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Gibbs Sampler for Learning DAGs",
            "abstract": [
                "We propose a Gibbs sampler for structure learning in directed acyclic graph (DAG) models. The standard Markov chain Monte Carlo algorithms used for learning DAGs are random-walk Metropolis-Hastings samplers. These samplers are guaranteed to converge asymptotically but often mix slowly when exploring the large graph spaces that arise in structure learning. In each step, the sampler we propose draws entire sets of parents for multiple nodes from the appropriate conditional distribution. This provides an efficient way to make large moves in graph space, permitting faster mixing whilst retaining asymptotic guarantees of convergence. The conditional distribution is related to variable selection with candidate parents playing the role of covariates or inputs. We empirically examine the performance of the sampler using several simulated and real data examples. The proposed method gives robust results in diverse settings, outperforming several existing Bayesian and frequentist methods. In addition, our empirical results shed some light on the relative merits of Bayesian and constraint-based methods for structure learning."
            ],
            "keywords": [
                "structure learning",
                "DAGs",
                "Bayesian networks",
                "Gibbs sampling",
                "Markov chain Monte Carlo",
                "variable selection"
            ],
            "author": [
                "Robert J B Goudie",
                "Sach Mukherjee"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-486/14-486.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "-PAL: An Active Learning Approach to the Multi-Objective Optimization Problem",
            "abstract": [
                "In many fields one encounters the challenge of identifying out of a pool of possible designs those that simultaneously optimize multiple objectives. In many applications an exhaustive search for the Pareto-optimal set is infeasible. To address this challenge, we propose the-Pareto Active Learning (-PAL) algorithm which adaptively samples the design space to predict a set of Pareto-optimal solutions that cover the true Pareto front of the design space with some granularity regulated by a parameter. Key features of-PAL include (1) modeling the objectives as draws from a Gaussian process distribution to capture structure and accommodate noisy evaluation; (2) a method to carefully choose the next design to evaluate to maximize progress; and (3) the ability to control prediction accuracy and sampling cost. We provide theoretical bounds on-PAL's sampling cost required to achieve a desired accuracy. Further, we perform an experimental evaluation on three real-world data sets that demonstrate-PAL's effectiveness; in comparison to the state-of-the-art active learning algorithm PAL,-PAL reduces the amount of computations and the number of samples from the design space required to meet the user's desired level of accuracy. In addition, we show that-PAL improves significantly over a state-of-the-art multi-objective optimization method, saving in most cases 30% to 70% evaluations to achieve the same accuracy."
            ],
            "keywords": [
                "multi-objective optimization",
                "active learning",
                "pareto optimality",
                "Bayesian optimization",
                "design space exploration"
            ],
            "author": [
                "Marcela Zuluaga",
                "Andreas Krause",
                "Markus Püschel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-047/15-047.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Counting and Exploring Sizes of Markov Equivalence Classes of Directed Acyclic Graphs",
            "abstract": [
                "When learning a directed acyclic graph (DAG) model via observational data, one generally cannot identify the underlying DAG, but can potentially obtain a Markov equivalence class. The size (the number of DAGs) of a Markov equivalence class is crucial to infer causal effects or to learn the exact causal DAG via further interventions. Given a set of Markov equivalence classes, the distribution of their sizes is a key consideration in developing learning methods. However, counting the size of an equivalence class with many vertices is usually computationally infeasible, and the existing literature reports the size distributions only for equivalence classes with ten or fewer vertices. In this paper, we develop a method to compute the size of a Markov equivalence class. We first show that there are five types of Markov equivalence classes whose sizes can be formulated as five functions of the number of vertices respectively. Then we introduce a new concept of a rooted sub-class. The graph representations of rooted subclasses of a Markov equivalence class are used to partition this class recursively until the sizes of all rooted subclasses can be computed via the five functions. The proposed size counting is efficient for Markov equivalence classes of sparse DAGs with hundreds of vertices. Finally, we explore the size and edge distributions of Markov equivalence classes and find experimentally that, in general, (1) most Markov equivalence classes are half completed and their average sizes are small, and (2) the sizes of sparse classes grow approximately exponentially with the numbers of vertices."
            ],
            "keywords": [
                "directed acyclic graphs",
                "Markov equivalence class",
                "size distribution",
                "causality"
            ],
            "author": [
                "Jinzhu Jia",
                "Isabelle Guyon",
                "Alexander Statnikov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/he15a/he15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ultra-High Dimensional Single-Index Quantile Regression",
            "abstract": [
                "We consider a flexible semiparametric single-index quantile regression model where the number of covariates may be ultra-high dimensional, and the number of the relevant covariates is potentially diverging. The approach is particularly appealing to uncover the complex heterogeneity in high-dimensional data, incorporate nonlinearity and potential interaction, avoid the curse of dimensionality, and allow different variables to be included at different quantile levels. We estimate the unknown function via polynomial splines nonparametrically and adopt a nonconvex penalty function to identify the sparse variable set. We further extend it to partially linear single-index quantile model where both the single-index components in the nonparametric term and the partially linear components can be in ultra-high dimension. However, a number of major challenges arise in developing both theory and computation: (a) The model is highly nonlinear in single-index coefficients because the high-dimensional single-index covariates are embedded inside the unknown flexible function. (b) The data are ultra-high dimensional where the dimension of the single-index covariates (p n) is diverging or even in the exponential order of sample size n. (c) The objective function is non-smooth for quantile regression. (d) Nonconvex variable selection such as SCAD is adopted for regularization. (e) The extended partially linear single-index quantile models may include both ultra-high dimensional (p n) singleindex covariates and ultra-high dimensional (q n) partially linear covariates. We develop a novel approach using empirical process techniques in establishing the theoretical properties of the nonconvex penalized estimators for partially linear single-index quantile models and show those estimators indeed possess the oracle property in ultra-high dimensional setting. We propose an efficient algorithm to circumvent the computational challenges. The results of Monte Carlo simulations and an application to gene expression data demonstrate the effectiveness of the proposed models and estimation method."
            ],
            "keywords": [
                "High-dimensional data",
                "nonparametric",
                "oracle property",
                "variable selection",
                "SCAD"
            ],
            "author": [
                "Yuankun Zhang",
                "Heng Lian",
                "Yan Yu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-173/19-173.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Analysis of Active Learning",
            "abstract": [
                "This work establishes distribution-free upper and lower bounds on the minimax label complexity of active learning with general hypothesis classes, under various noise models. The results reveal a number of surprising facts. In particular, under the noise model of Tsybakov (2004), the minimax label complexity of active learning with a VC class is always asymptotically smaller than that of passive learning, and is typically significantly smaller than the best previously-published upper bounds in the active learning literature. In highnoise regimes, it turns out that all active learning problems of a given VC dimension have roughly the same minimax label complexity, which contrasts with well-known results for bounded noise. In low-noise regimes, we find that the label complexity is well-characterized by a simple combinatorial complexity measure we call the star number. Interestingly, we find that almost all of the complexity measures previously explored in the active learning literature have worst-case values exactly equal to the star number. We also propose new active learning strategies that nearly achieve these minimax label complexities."
            ],
            "keywords": [
                "active learning",
                "selective sampling",
                "sequential design",
                "adaptive sampling",
                "statistical learning theory",
                "margin condition",
                "Tsybakov noise",
                "sample complexity",
                "minimax analysis"
            ],
            "author": [
                "Steve Hanneke",
                "Liu Yang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/hanneke15a/hanneke15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Robust Minimax Approach to Classification",
            "abstract": [
                "When constructing a classifier, the probability of correct classification of future data points should be maximized. We consider a binary classification problem where the mean and covariance matrix of each class are assumed to be known. No further assumptions are made with respect to the classconditional distributions. Misclassification probabilities are then controlled in a worst-case setting: that is, under all possible choices of class-conditional densities with given mean and covariance matrix, we minimize the worst-case (maximum) probability of misclassification of future data points. For a linear decision boundary, this desideratum is translated in a very direct way into a (convex) second order cone optimization problem, with complexity similar to a support vector machine problem. The minimax problem can be interpreted geometrically as minimizing the maximum of the Mahalanobis distances to the two classes. We address the issue of robustness with respect to estimation errors (in the means and covariances of the classes) via a simple modification of the input data. We also show how to exploit Mercer kernels in this setting to obtain nonlinear decision boundaries, yielding a classifier which proves to be competitive with current methods, including support vector machines. An important feature of this method is that a worst-case bound on the probability of misclassification of future data is always obtained explicitly."
            ],
            "keywords": [
                "classification",
                "kernel methods",
                "convex optimization",
                "second order cone programming"
            ],
            "author": [
                "Gert R G Lanckriet",
                "Laurent El Ghaoui",
                "Chiranjib Bhattacharyya",
                "Michael I Jordan"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/lanckriet02a/lanckriet02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms",
            "abstract": [
                "Motivated by the problem of effectively executing clustering algorithms on very large data sets, we address a model for large scale distributed clustering methods. To this end, we briefly recall some standards on the quantization problem and some results on the almost sure convergence of the competitive learning vector quantization (CLVQ) procedure. A general model for linear distributed asynchronous algorithms well adapted to several parallel computing architectures is also discussed. Our approach brings together this scalable model and the CLVQ algorithm, and we call the resulting technique the distributed asynchronous learning vector quantization algorithm (DALVQ). An indepth analysis of the almost sure convergence of the DALVQ algorithm is performed. A striking result is that we prove that the multiple versions of the quantizers distributed among the processors in the parallel architecture asymptotically reach a consensus almost surely. Furthermore, we also show that these versions converge almost surely towards the same nearly optimal value for the quantization criterion."
            ],
            "keywords": [
                "k-means",
                "vector quantization",
                "distributed",
                "asynchronous",
                "stochastic optimization",
                "scalability",
                "distributed consensus"
            ],
            "author": [
                "Benoît Patra"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/patra11a/patra11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Synergistic Face Detection and Pose Estimation with Energy-Based Models",
            "abstract": [
                "We describe a novel method for simultaneously detecting faces and estimating their pose in real time. The method employs a convolutional network to map images of faces to points on a lowdimensional manifold parametrized by pose, and images of non-faces to points far away from that manifold. Given an image, detecting a face and estimating its pose is viewed as minimizing an energy function with respect to the face/non-face binary variable and the continuous pose parameters. The system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones. The system is designed to handle very large range of poses without retraining. The performance of the system was tested on three standard data sets-for frontal views, rotated faces, and profilesis comparable to previous systems that are designed to handle a single one of these data sets. We show that a system trained simuiltaneously for detection and pose estimation is more accurate on both tasks than similar systems trained for each task separately. 1"
            ],
            "keywords": [
                "face detection",
                "pose estimation",
                "convolutional networks",
                "energy based models",
                "object recognition"
            ],
            "author": [
                "Margarita Osadchy",
                "Yann Le Cun",
                "Matthew L Miller"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/osadchy07a/osadchy07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Semiparametric Mean Field Variational Bayes: General Principles and Numerical Issues",
            "abstract": [
                "We introduce the term semiparametric mean field variational Bayes to describe the relaxation of mean field variational Bayes in which some density functions in the product density restriction are pre-specified to be members of convenient parametric families. This notion has appeared in various guises in the mean field variational Bayes literature during its history and we endeavor to unify this important topic. We lay down a general framework and explain how previous relevant methodologies fall within this framework. A major contribution is elucidation of numerical issues that impact semiparametric mean field variational Bayes in practice."
            ],
            "keywords": [
                "Bayesian Computing",
                "Factor Graph",
                "Fixed-form Variational Bayes",
                "Fixedpoint Iteration",
                "Non-conjugate Variational Message Passing",
                "Nonlinear Conjugate Gradient Method"
            ],
            "author": [
                "David Rohde"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-276/15-276.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Theorems for Generalized Alternating Minimization Procedures",
            "abstract": [
                "The EM algorithm is widely used to develop iterative parameter estimation procedures for statistical models. In cases where these procedures strictly follow the EM formulation, the convergence properties of the estimation procedures are well understood. In some instances there are practical reasons to develop procedures that do not strictly fall within the EM framework. We study EM variants in which the E-step is not performed exactly, either to obtain improved rates of convergence, or due to approximations needed to compute statistics under a model family over which E-steps cannot be realized. Since these variants are not EM procedures, the standard (G)EM convergence results do not apply to them. We present an information geometric framework for describing such algorithms and analyzing their convergence properties. We apply this framework to analyze the convergence properties of incremental EM and variational EM. For incremental EM, we discuss conditions under these algorithms converge in likelihood. For variational EM, we show how the E-step approximation prevents convergence to local maxima in likelihood."
            ],
            "keywords": [
                "EM",
                "variational EM",
                "incremental EM",
                "convergence",
                "information geometry"
            ],
            "author": [
                "Asela Gunawardana",
                "William Byrne"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/gunawardana05a/gunawardana05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Incremental Support Vector Learning: Analysis, Implementation and Applications",
            "abstract": [
                "Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efficient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network traffic, can be foreseen."
            ],
            "keywords": [
                "incremental SVM",
                "online learning",
                "drug discovery",
                "intrusion detection"
            ],
            "author": [
                "Pavel Laskov",
                "Stefan Krüger",
                "Klaus- Robert Müller",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/laskov06a/laskov06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimum Description Length Penalization for Group and Multi-Task Sparse Learning",
            "abstract": [
                "We propose a framework MIC (Multiple Inclusion Criterion) for learning sparse models based on the information theoretic Minimum Description Length (MDL) principle. MIC provides an elegant way of incorporating arbitrary sparsity patterns in the feature space by using two-part MDL coding schemes. We present MIC based models for the problems of grouped feature selection (MIC-GROUP) and multi-task feature selection (MIC-MULTI). MIC-GROUP assumes that the features are divided into groups and induces two level sparsity, selecting a subset of the feature groups, and also selecting features within each selected group. MIC-MULTI applies when there are multiple related tasks that share the same set of potentially predictive features. It also induces two level sparsity, selecting a subset of the features, and then selecting which of the tasks each feature should be added to. Lastly, we propose a model, TRANSFEAT, that can be used to transfer knowledge from a set of previously learned tasks to a new task that is expected to share similar features. All three methods are designed for selecting a small set of predictive features from a large pool of candidate features. We demonstrate the effectiveness of our approach with experimental results on data from genomics and from word sense disambiguation problems."
            ],
            "keywords": [
                "feature selection",
                "minimum description length principle",
                "multi-task learning"
            ],
            "author": [
                "Paramveer S Dhillon",
                "Lyle H Ungar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/dhillon11a/dhillon11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Iteration Complexity of Feasible Descent Methods for Convex Optimization",
            "abstract": [
                "In many machine learning problems such as the dual form of SVM, the objective function to be minimized is convex but not strongly convex. This fact causes difficulties in obtaining the complexity of some commonly used optimization algorithms. In this paper, we proved the global linear convergence on a wide range of algorithms when they are applied to some non-strongly convex problems. In particular, we are the first to prove O(log(1/)) time complexity of cyclic coordinate descent methods on dual problems of support vector classification and regression."
            ],
            "keywords": [
                "convergence rate",
                "convex optimization",
                "iteration complexity",
                "feasible descent methods"
            ],
            "author": [
                "Po-Wei Wang",
                "Chih-Jen Lin",
                "S Sathiya Keerthi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/wang14a/wang14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis",
            "abstract": [
                "Small sample high-dimensional principal component analysis (PCA) suffers from variance inflation and lack of generalizability. It has earlier been pointed out that a simple leave-one-out variance renormalization scheme can cure the problem. In this paper we generalize the cure in two directions: First, we propose a computationally less intensive approximate leave-one-out estimator, secondly, we show that variance inflation is also present in kernel principal component analysis (kPCA) and we provide a non-parametric renormalization scheme which can quite efficiently restore generalizability in kPCA. As for PCA our analysis also suggests a simplified approximate expression."
            ],
            "keywords": [
                "PCA",
                "kernel PCA",
                "generalizability",
                "variance renormalization"
            ],
            "author": [
                "Trine Julie Abrahamsen",
                "Lars Kai Hansen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/abrahamsen11a/abrahamsen11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Automatic Smoothing for Generalized Additive Models",
            "abstract": [
                "Generalized additive models (GAMs) are regression models wherein parameters of probability distributions depend on input variables through a sum of smooth functions, whose degrees of smoothness are selected by L 2 regularization. Such models have become the de-facto standard nonlinear regression models when interpretability and flexibility are required, but reliable and fast methods for automatic smoothing in large data sets are still lacking. We develop a general methodology for automatically learning the optimal degree of L 2 regularization for GAMs using an empirical Bayes approach. The smooth functions are penalized by hyper-parameters that are learned simultaneously by maximization of a marginal likelihood using an approximate expectation-maximization algorithm. The latter involves a double Laplace approximation at the E-step, and leads to an efficient M-step. Empirical analysis shows that the resulting algorithm is numerically stable, faster than the best existing methods and achieves state-of-the-art accuracy. For illustration, we apply it to an important and challenging problem in the analysis of extremal data."
            ],
            "keywords": [
                "Automatic L 2 Regularization",
                "Empirical Bayes",
                "Expectation-maximization Algorithm",
                "Generalized Additive Model",
                "Laplace Approximation",
                "Marginal Maximum Likelihood"
            ],
            "author": [
                "Yousra El-Bachir",
                "Anthony C Davison"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-659/18-659.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Perturbed Proximal Gradient Algorithms",
            "abstract": [
                "We study a version of the proximal gradient algorithm for which the gradient is intractable and is approximated by Monte Carlo methods (and in particular Markov Chain Monte Carlo). We derive conditions on the step size and the Monte Carlo batch size under which convergence is guaranteed: both increasing batch size and constant batch size are considered. We also derive non-asymptotic bounds for an averaged version. Our results cover both the cases of biased and unbiased Monte Carlo approximation. To support our findings, we discuss the inference of a sparse generalized linear model with random effect and the problem of learning the edge structure and parameters of sparse undirected graphical models."
            ],
            "keywords": [
                "Proximal Gradient Methods",
                "Stochastic Optimization",
                "Monte Carlo approximations",
                "Perturbed Majorization-Minimization algorithms"
            ],
            "author": [
                "Yves F Atchadé",
                "Eric Moulines"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-038/15-038.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ultrahigh Dimensional Feature Selection: Beyond The Linear Model Jianqing Fan",
            "abstract": [
                "Variable selection in high-dimensional space characterizes many contemporary problems in scientific discovery and decision making. Many frequently-used techniques are based on independence screening; examples include correlation ranking (Fan & Lv, 2008) or feature selection using a twosample t-test in high-dimensional classification (Tibshirani et al., 2003). Within the context of the linear model, Fan & Lv (2008) showed that this simple correlation ranking possesses a sure independence screening property under certain conditions and that its revision, called iteratively sure independent screening (ISIS), is needed when the features are marginally unrelated but jointly related to the response variable. In this paper, we extend ISIS, without explicit definition of residuals, to a general pseudo-likelihood framework, which includes generalized linear models as a special case. Even in the least-squares setting, the new method improves ISIS by allowing feature deletion in the iterative process. Our technique allows us to select important features in high-dimensional classification where the popularly used two-sample t-method fails. A new technique is introduced to reduce the false selection rate in the feature screening stage. Several simulated and two real data examples are presented to illustrate the methodology."
            ],
            "keywords": [
                "classification",
                "feature screening",
                "generalized linear models",
                "robust regression",
                "feature selection"
            ],
            "author": [
                "Jqfan @ Princeton",
                "Richard Samworth",
                "Yichao Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/fan09a/fan09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Algorithms for the Classification Restricted Boltzmann Machine",
            "abstract": [
                "Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely semi-supervised and multitask learning."
            ],
            "keywords": [
                "restricted Boltzmann machine",
                "classification",
                "discriminative learning",
                "generative learning"
            ],
            "author": [
                "Hugo Larochelle",
                "Michael Mandel",
                "Yoshua Bengio"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/larochelle12a/larochelle12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Inference for Spatio-temporal Spike-and-Slab Priors",
            "abstract": [
                "In this work, we address the problem of solving a series of underdetermined linear inverse problemblems subject to a sparsity constraint. We generalize the spike-and-slab prior distribution to encode a priori correlation of the support of the solution in both space and time by imposing a transformed Gaussian process on the spike-and-slab probabilities. An expectation propagation (EP) algorithm for posterior inference under the proposed model is derived. For large scale problems, the standard EP algorithm can be prohibitively slow. We therefore introduce three different approximation schemes to reduce the computational complexity. Finally, we demonstrate the proposed model using numerical experiments based on both synthetic and real data sets."
            ],
            "keywords": [
                "Linear inverse problems",
                "bayesian inference",
                "expectation propagation",
                "sparsitypromoting priors",
                "spike-and-slab priors"
            ],
            "author": [
                "Michael Riis Andersen",
                "Ole Winther",
                "Lars Kai Hansen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-464/15-464.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "LIBMF: A Library for Parallel Matrix Factorization in Shared-memory Systems",
            "abstract": [
                "Matrix factorization (MF) plays a key role in many applications such as recommender systems and computer vision, but MF may take long running time for handling large matrices commonly seen in the big data era. Many parallel techniques have been proposed to reduce the running time, but few parallel MF packages are available. Therefore, we present an open source library, LIBMF, based on recent advances of parallel MF for sharedmemory systems. LIBMF includes easy-to-use command-line tools, interfaces to C/C++ languages, and comprehensive documentation. Our experiments demonstrate that LIBMF outperforms state of the art packages. LIBMF is BSD-licensed, so users can freely use, modify, and redistribute the code."
            ],
            "keywords": [
                "Matrix factorization",
                "non-negative matrix factorization",
                "binary matrix factorization",
                "logistic matrix factorization",
                "one-class matrix factorization",
                "stochastic gradient method",
                "adaptive learning rate",
                "parallel computation"
            ],
            "author": [
                "Wei-Sheng Chin",
                "Bo-Wen Yuan",
                "Yuan Yang",
                "Yong Zhuang",
                "Chih-Jen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-471/15-471.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Operator-valued Kernels for Learning from Functional Response Data",
            "abstract": [
                "In this paper 1 we consider the problems of supervised classification and regression in the case where attributes and labels are functions: a data is represented by a set of functions, and the label is also a function. We focus on the use of reproducing kernel Hilbert space theory to learn from such functional data. Basic concepts and properties of kernel-based learning are extended to include the estimation of function-valued functions. In this setting, the representer theorem is restated, a set of rigorously defined infinite-dimensional operatorvalued kernels that can be valuably applied when the data are functions is described, and a learning algorithm for nonlinear functional data analysis is introduced. The methodology is illustrated through speech and audio signal processing experiments."
            ],
            "keywords": [
                "nonlinear functional data analysis",
                "operator-valued kernels",
                "function-valued reproducing kernel Hilbert spaces",
                "audio signal processing"
            ],
            "author": [
                "Hachem Kadri",
                "Emmanuel Duflos",
                "Philippe Preux",
                "Stéphane Canu",
                "Alain Rakotomamonjy",
                "Julien Audiffren"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/11-315/11-315.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Algorithms for Universal Portfolios",
            "abstract": [
                "A constant rebalanced portfolio is an investment strategy that keeps the same distribution of wealth among a set of stocks from day to day. There has been much work on Cover's Universal algorithm, which is competitive with the best constant rebalanced portfolio de"
            ],
            "keywords": [],
            "author": [
                "Adam Kalai"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/kalai02a/kalai02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning the Variance of the Reward-To-Go",
            "abstract": [
                "In Markov decision processes (MDPs), the variance of the reward-to-go is a natural measure of uncertainty about the long term performance of a policy, and is important in domains such as finance, resource allocation, and process control. Currently however, there is no tractable procedure for calculating it in large scale MDPs. This is in contrast to the case of the expected reward-to-go, also known as the value function, for which effective simulationbased algorithms are known, and have been used successfully in various domains. In this paper 1 we extend temporal difference (TD) learning algorithms to estimating the variance of the reward-to-go for a fixed policy. We propose variants of both TD(0) and LSTD(λ) with linear function approximation, prove their convergence, and demonstrate their utility in an option pricing problem. Our results show a dramatic improvement in terms of sample efficiency over standard Monte-Carlo methods, which are currently the state-of-the-art."
            ],
            "keywords": [
                "Reinforcement learning",
                "Markov decision processes",
                "variance estimation",
                "simulation",
                "temporal differences"
            ],
            "author": [
                "Aviv Tamar",
                "Dotan Di Castro",
                "Shie Mannor"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-335/14-335.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral Deconfounding via Perturbed Sparse Linear Models",
            "abstract": [
                "Standard high-dimensional regression methods assume that the underlying coefficient vector is sparse. This might not be true in some cases, in particular in presence of hidden, confounding variables. Such hidden confounding can be represented as a high-dimensional linear model where the sparse coefficient vector is perturbed. For this model, we develop and investigate a class of methods that are based on running the Lasso on preprocessed data. The preprocessing step consists of applying certain spectral transformations that change the singular values of the design matrix. We show that, under some assumptions, one can achieve the usual Lasso 1-error rate for estimating the underlying sparse coefficient vector, despite the presence of confounding. Our theory also covers the Lava estimator (Chernozhukov et al., 2017) for a special model class. The performance of the methodology is illustrated on simulated data and a genomic dataset."
            ],
            "keywords": [
                "confounding",
                "data transformation",
                "Lasso",
                "latent variables",
                "principal components"
            ],
            "author": [
                "Peter Bühlmann",
                "Nicolai Meinshausen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-545/19-545.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Group-Theoretic Framework for Data Augmentation",
            "abstract": [
                "Data augmentation is a widely used trick when training deep neural networks: in addition to the original data, properly transformed data are also added to the training set. However, to the best of our knowledge, a clear mathematical framework to explain the performance benefits of data augmentation is not available. In this paper, we develop such a theoretical framework. We show data augmentation is equivalent to an averaging operation over the orbits of a certain group that keeps the data distribution approximately invariant. We prove that it leads to variance reduction. We study empirical risk minimization, and the examples of exponential families, linear regression, and certain two-layer neural networks. We also discuss how data augmentation could be used in problems with symmetry where other approaches are prevalent, such as in cryo-electron microscopy (cryo-EM)."
            ],
            "keywords": [
                "Data Augmentation",
                "Deep Learning",
                "Empirical Risk Minimization",
                "Invariance",
                "Variance Reduction"
            ],
            "author": [
                "Shuxiao Chen",
                "Edgar Dobriban",
                "Jane H Lee"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-163/20-163.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Estimation of Derivatives in Nonparametric Regression",
            "abstract": [
                "We propose a simple framework for estimating derivatives without fitting the regression function in nonparametric regression. Unlike most existing methods that use the symmetric difference quotients, our method is constructed as a linear combination of observations. It is hence very flexible and applicable to both interior and boundary points, including most existing methods as special cases of ours. Within this framework, we define the variance-minimizing estimators for any order derivative of the regression function with a fixed bias-reduction level. For the equidistant design, we derive the asymptotic variance and bias of these estimators. We also show that our new method will, for the first time, achieve the asymptotically optimal convergence rate for difference-based estimators. Finally, we provide an effective criterion for selection of tuning parameters and demonstrate the usefulness of the proposed method through extensive simulation studies of the firstand second-order derivative estimators."
            ],
            "keywords": [
                "Linear combination",
                "Nonparametric derivative estimation",
                "Nonparametric regression",
                "Optimal sequence",
                "Taylor expansion"
            ],
            "author": [
                "Wenlin Dai",
                "Hong Kong",
                "Marc G Genton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-640/15-640.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Recursive Teaching Dimension, VC-Dimension and Sample Compression",
            "abstract": [
                "This paper is concerned with various combinatorial parameters of classes that can be learned from a small set of examples. We show that the recursive teaching dimension, recently introduced by Zilles et al. (2008), is strongly connected to known complexity notions in machine learning, e.g., the self-directed learning complexity and the VC-dimension. To the best of our knowledge these are the first results unveiling such relations between teaching and query learning as well as between teaching and the VC-dimension. It will turn out that for many natural classes the RTD is upper-bounded by the VCD, e.g., classes of VCdimension 1, intersection-closed classes and finite maximum classes. However, we will also show that there are certain (but rare) classes for which the recursive teaching dimension exceeds the VC-dimension. Moreover, for maximum classes, the combinatorial structure induced by the RTD, called teaching plan, is highly similar to the structure of sample compression schemes. Indeed one can transform any repetition-free teaching plan for a maximum class C into an unlabeled sample compression scheme for C and vice versa, while the latter is produced by (i) the corner-peeling algorithm of Rubinstein and Rubinstein (2012) and (ii) the tail matching algorithm of Kuzmin and Warmuth (2007)."
            ],
            "keywords": [
                "recursive teaching",
                "combinatorial parameters",
                "Vapnik-Chervonenkis dimension",
                "upper bounds",
                "compression schemes",
                "tail matching algorithm"
            ],
            "author": [
                "Thorsten Doliwa",
                "Hans Ulrich Simon",
                "Sandra Zilles"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/doliwa14a/doliwa14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "EP-GIG Priors and Applications in Bayesian Sparse Learning",
            "abstract": [
                "In this paper we propose a novel framework for the construction of sparsity-inducing priors. In particular, we define such priors as a mixture of exponential power distributions with a generalized inverse Gaussian density (EP-GIG). EP-GIG is a variant of generalized hyperbolic distributions, and the special cases include Gaussian scale mixtures and Laplace scale mixtures. Furthermore, Laplace scale mixtures can subserve a Bayesian framework for sparse learning with nonconvex penalization. The densities of EP-GIG can be explicitly expressed. Moreover, the corresponding posterior distribution also follows a generalized inverse Gaussian distribution. We exploit these properties to develop EM algorithms for sparse empirical Bayesian learning. We also show that these algorithms bear an interesting resemblance to iteratively reweighted ℓ 2 or ℓ 1 methods. Finally, we present two extensions for grouped variable selection and logistic regression."
            ],
            "keywords": [
                "sparsity priors",
                "scale mixtures of exponential power distributions",
                "generalized inverse Gaussian distributions",
                "expectation-maximization algorithms",
                "iteratively reweighted minimization methods"
            ],
            "author": [
                "Zhihua Zhang",
                "Shusen Wang",
                "Dehua Liu",
                "Michael I Jordan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/zhang12b/zhang12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Incremental Sigmoid Belief Networks for Grammar Learning",
            "abstract": [
                "We propose a class of Bayesian networks appropriate for structured prediction problems where the Bayesian network's model structure is a function of the predicted output structure. These incremental sigmoid belief networks (ISBNs) make decoding possible because inference with partial output structures does not require summing over the unboundedly many compatible model structures, due to their directed edges and incrementally specified model structure. ISBNs are specifically targeted at challenging structured prediction problems such as natural language parsing, where learning the domain's complex statistical dependencies benefits from large numbers of latent variables. While exact inference in ISBNs with large numbers of latent variables is not tractable, we propose two efficient approximations. First, we demonstrate that a previous neural network parsing model can be viewed as a coarse mean-field approximation to inference with ISBNs. We then derive a more accurate but still tractable variational approximation, which proves effective in artificial experiments. We compare the effectiveness of these models on a benchmark natural language parsing task, where they achieve accuracy competitive with the state-of-the-art. The model which is a closer approximation to an ISBN has better parsing accuracy, suggesting that ISBNs are an appropriate abstract model of natural language grammar learning."
            ],
            "keywords": [
                "Bayesian networks",
                "dynamic Bayesian networks",
                "grammar learning",
                "natural language parsing",
                "neural networks"
            ],
            "author": [
                "James Henderson",
                "Ivan Titov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/henderson10a/henderson10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularization-Free Principal Curve Estimation",
            "abstract": [
                "Principal curves and manifolds provide a framework to formulate manifold learning within a statistical context. Principal curves define the notion of a curve passing through the middle of a distribution. While the intuition is clear, the formal definition leads to some technical and practical difficulties. In particular, principal curves are saddle points of the mean-squared projection distance, which poses severe challenges for estimation and model selection. This paper demonstrates that the difficulties in model selection associated with the saddle point property of principal curves are intrinsically tied to the minimization of the mean-squared projection distance. We introduce a new objective function, facilitated through a modification of the principal curve estimation approach, for which all critical points are principal curves and minima. Thus, the new formulation removes the fundamental issue for model selection in principal curve estimation. A gradient-descent-based estimator demonstrates the effectiveness of the new formulation for controlling model complexity on numerical experiments with synthetic and real data."
            ],
            "keywords": [
                "principal curve",
                "manifold estimation",
                "unsupervised learning",
                "model complexity",
                "model selection"
            ],
            "author": [
                "Samuel Gerber",
                "Ross Whitaker"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/gerber13a/gerber13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stability of Controllers for Gaussian Process Dynamics",
            "abstract": [
                "Learning control has become an appealing alternative to the derivation of control laws based on classic control theory. However, a major shortcoming of learning control is the lack of performance guarantees which prevents its application in many real-world scenarios. As a step towards widespread deployment of learning control, we provide stability analysis tools for controllers acting on dynamics represented by Gaussian processes (GPs). We consider differentiable Markovian control policies and system dynamics given as (i) the mean of a GP, and (ii) the full GP distribution. For both cases, we analyze finite and infinite time horizons. Furthermore, we study the effect of disturbances on the stability results. Empirical evaluations on simulated benchmark problems support our theoretical results."
            ],
            "keywords": [
                "Stability",
                "Reinforcement Learning",
                "Control",
                "Gaussian Process"
            ],
            "author": [
                "Julia Vinogradska",
                "Jan Peters"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-590/16-590.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Complexities of Gaussian Mixtures in Variational Bayesian Approximation",
            "abstract": [
                "Bayesian learning has been widely used and proved to be effective in many data modeling problems. However, computations involved in it require huge costs and generally cannot be performed exactly. The variational Bayesian approach, proposed as an approximation of Bayesian learning, has provided computational tractability and good generalization performance in many applications. The properties and capabilities of variational Bayesian learning itself have not been clarified yet. It is still unknown how good approximation the variational Bayesian approach can achieve. In this paper, we discuss variational Bayesian learning of Gaussian mixture models and derive upper and lower bounds of variational stochastic complexities. The variational stochastic complexity, which corresponds to the minimum variational free energy and a lower bound of the Bayesian evidence, not only becomes important in addressing the model selection problem, but also enables us to discuss the accuracy of the variational Bayesian approach as an approximation of true Bayesian learning."
            ],
            "keywords": [
                "Gaussian mixture model",
                "variational Bayesian learning",
                "stochastic complexity"
            ],
            "author": [
                "Kazuho Watanabe",
                "Sumio Watanabe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/watanabe06a/watanabe06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Behavior of Intrinsically High-Dimensional Spaces: Distances, Direct and Reverse Nearest Neighbors, and Hubness",
            "abstract": [
                "Over the years, different characterizations of the curse of dimensionality have been provided, usually stating the conditions under which, in the limit of the infinite dimensionality, distances become indistinguishable. However, these characterizations almost never address the form of associated distributions in the finite, although high-dimensional, case. This work aims to contribute in this respect by investigating the distribution of distances, and of direct and reverse nearest neighbors, in intrinsically high-dimensional spaces. Indeed, we derive a closed form for the distribution of distances from a given point, for the expected distance from a given point to its kth nearest neighbor, and for the expected size of the approximate set of neighbors of a given point in finite high-dimensional spaces. Additionally, the hubness problem is considered, which is related to the form of the function N k representing the number of points that have a given point as one of their k nearest neighbors, which is also called the number of k-occurrences. Despite the extensive use of this function, the precise characterization of its form is a longstanding problem. We derive a closed form for the number of k-occurrences associated with a given point in finite high-dimensional spaces, together with the associated limiting probability distribution. By investigating the relationships with the hubness phenomenon emerging in network science, we find that the distribution of node (in-)degrees of some real-life, large-scale networks has connections with the distribution of k-occurrences described herein."
            ],
            "keywords": [
                "high-dimensional data",
                "distance concentration",
                "distribution of distances",
                "nearest neighbors",
                "reverse nearest neighbors",
                "hubness"
            ],
            "author": [
                "Fabrizio Angiulli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-151/17-151.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Characterization of a Class of Fisher-Consistent Loss Functions and its Application to Boosting",
            "abstract": [
                "Accurate classification of categorical outcomes is essential in a wide range of applications. Due to computational issues with minimizing the empirical 0/1 loss, Fisher consistent losses have been proposed as viable proxies. However, even with smooth losses, direct minimization remains a daunting task. To approximate such a minimizer, various boosting algorithms have been suggested. For example, with exponential loss, the AdaBoost algorithm (Freund and Schapire, 1995) is widely used for two-class problems and has been extended to the multi-class setting (Zhu et al., 2009). Alternative loss functions, such as the logistic and the hinge losses, and their corresponding boosting algorithms have also been proposed (Zou et al., 2008; Wang, 2012). In this paper we demonstrate that a broad class of losses, including non-convex functions, achieve Fisher consistency, and in addition can be used for explicit estimation of the conditional class probabilities. Furthermore, we provide a generic boosting algorithm that is not loss-specific. Extensive simulation results suggest that the proposed boosting algorithms could outperform existing methods with properly chosen losses and bags of weak learners."
            ],
            "keywords": [
                "Boosting",
                "Fisher-Consistency",
                "Multiclass Classification",
                "SAMME"
            ],
            "author": [
                "Matey Neykov",
                "Jun S Liu",
                "Tianxi Cai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-306/14-306.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gradient Hard Thresholding Pursuit",
            "abstract": [
                "Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems. This method has been shown to have strong theoretical guarantee and impressive numerical performance. In this article, we generalize HTP from compressed sensing to a generic problem setup of sparsity-constrained convex optimization. The proposed algorithm iterates between a standard gradient descent step and a hard-thresholding step with or without debiasing. We analyze the parameter estimation and sparsity recovery performance of the proposed method. Extensive numerical results confirm our theoretical predictions and demonstrate the superiority of our method to the state-of-the-art greedy selection methods in sparse linear regression, sparse logistic regression and sparse precision matrix estimation problems."
            ],
            "keywords": [
                "Hard Thresholding Pursuit",
                "Sparsity Recovery",
                "Greedy Selection"
            ],
            "author": [
                "Xiao-Tong Yuan",
                "Ping Li",
                "Tong Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-415/14-415.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantifying the Informativeness of Similarity Measurements",
            "abstract": [
                "In this paper, we describe an unsupervised measure for quantifying the 'informativeness' of correlation matrices formed from the pairwise similarities or relationships among data instances. The measure quantifies the heterogeneity of the correlations and is defined as the distance between a correlation matrix and the nearest correlation matrix with constant off-diagonal entries. This non-parametric notion generalizes existing test statistics for equality of correlation coefficients by allowing for alternative distance metrics, such as the Bures and other distances from quantum information theory. For several distance and dissimilarity metrics, we derive closed-form expressions of informativeness, which can be applied as objective functions for machine learning applications. Empirically, we demonstrate that informativeness is a useful criterion for selecting kernel parameters, choosing the dimension for kernel-based nonlinear dimensionality reduction, and identifying structured graphs. We also consider the problem of finding a maximally informative correlation matrix around a target matrix, and explore parameterizing the optimization in terms of the coordinates of the sample or through a lower-dimensional embedding. In the latter case, we find that maximizing the Bures-based informativeness measure, which is maximal for centered rank-1 correlation matrices, is equivalent to minimizing a specific matrix norm, and present an algorithm to solve the minimization problem using the norm's proximal operator. The proposed correlation denoising algorithm consistently improves spectral clustering. Overall, we find informativeness to be a novel and useful criterion for identifying non-trivial correlation structure."
            ],
            "keywords": [
                "correlation matrices",
                "similarity information",
                "kernel methods",
                "information theory",
                "quantum information theory",
                "clustering Brockmeier",
                "Mu",
                "Ananiadou",
                "and Goulermas"
            ],
            "author": [
                "Austin J Brockmeier",
                "Tingting Mu",
                "Sophia Ananiadou",
                "John Y Goulermas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-296/16-296.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GluonTS: Probabilistic and Neural Time Series Modeling in Python",
            "abstract": [
                "We introduce the Gluon Time Series Toolkit (GluonTS), a Python library for deep learning based time series modeling for ubiquitous tasks, such as forecasting and anomaly detection. GluonTS simplifies the time series modeling pipeline by providing the necessary components and tools for quick model development, efficient experimentation and evaluation. In addition, it contains reference implementations of state-of-the-art time series models that enable simple benchmarking of new algorithms."
            ],
            "keywords": [
                "time series",
                "deep learning",
                "Python",
                "scientific toolkit",
                "benchmarking"
            ],
            "author": [
                "Alexander Alexandrov",
                "Michael Bohlke-Schneider",
                "Valentin Flunkert",
                "Tim Januschowski",
                "Danielle C Maddix",
                "David Salinas",
                "Jasper Schulz",
                "Lorenzo Stella",
                "Ali Caner Türkmen",
                "Yuyang Wang",
                "Konstantinos Benidis",
                "Jan Gasthaus",
                "Syama Rangapuram"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-820/19-820.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Meka: A Multi-label/Multi-target Extension to Weka",
            "abstract": [
                "Multi-label classification has rapidly attracted interest in the machine learning literature, and there are now a large number and considerable variety of methods for this type of learning. We present Meka: an open-source Java framework based on the well-known Weka library. Meka provides interfaces to facilitate practical application, and a wealth of multi-label classifiers, evaluation metrics, and tools for multi-label experiments and development. It supports multi-label and multi-target data, including in incremental and semi-supervised contexts."
            ],
            "keywords": [
                "classification",
                "learning",
                "multi-label",
                "multi-target",
                "incremental"
            ],
            "author": [
                "Jesse Read",
                "Peter Reutemann",
                "Bernhard Pfahringer",
                "Geoff Holmes"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/12-164/12-164.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs",
            "abstract": [
                "The investigation of directed acyclic graphs (DAGs) encoding the same Markov property, that is the same conditional independence relations of multivariate observational distributions, has a long tradition; many algorithms exist for model selection and structure learning in Markov equivalence classes. In this paper, we extend the notion of Markov equivalence of DAGs to the case of interventional distributions arising from multiple intervention experiments. We show that under reasonable assumptions on the intervention experiments, interventional Markov equivalence defines a finer partitioning of DAGs than observational Markov equivalence and hence improves the identifiability of causal models. We give a graph theoretic criterion for two DAGs being Markov equivalent under interventions and show that each interventional Markov equivalence class can, analogously to the observational case, be uniquely represented by a chain graph called interventional essential graph (also known as CPDAG in the observational case). These are key insights for deriving a generalization of the Greedy Equivalence Search algorithm aimed at structure learning from interventional data. This new algorithm is evaluated in a simulation study."
            ],
            "keywords": [
                "causal inference",
                "interventions",
                "graphical model",
                "Markov equivalence",
                "greedy equivalence search"
            ],
            "author": [
                "Alain Hauser",
                "Peter Bühlmann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/hauser12a/hauser12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Interactive Algorithms: Pool, Stream and Precognitive Stream",
            "abstract": [
                "We consider interactive algorithms in the pool-based setting, and in the stream-based setting. Interactive algorithms observe suggested elements (representing actions or queries), and interactively select some of them and receive responses. Pool-based algorithms can select elements at any order, while stream-based algorithms observe elements in sequence, and can only select elements immediately after observing them. We further consider an intermediate setting, which we term precognitive stream, in which the algorithm knows in advance the identity of all the elements in the sequence, but can select them only in the order of their appearance. For all settings, we assume that the suggested elements are generated independently from some source distribution, and ask what is the stream size required for emulating a pool algorithm with a given pool size, in the stream-based setting and in the precognitive stream setting. We provide algorithms and matching lower bounds for general pool algorithms, and for utility-based pool algorithms. We further derive nearly matching upper and lower bounds on the gap between the two settings for the special case of active learning for binary classification."
            ],
            "keywords": [
                "interactive algorithms",
                "active learning",
                "pool-based",
                "stream-based"
            ],
            "author": [
                "Sivan Sabato",
                "Tom Hess"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-424/16-424.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Learning for Cost-Sensitive Classification",
            "abstract": [
                "We design an active learning algorithm for cost-sensitive multiclass classification: problems where different errors have different costs. Our algorithm, COAL, makes predictions by regressing to each label's cost and predicting the smallest. On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort. We empirically compare COAL to passive learning and several active learning baselines, showing significant improvements in labeling effort and test cost on real-world datasets."
            ],
            "keywords": [
                "Active Learning",
                "Cost-sensitive Learning",
                "Structured Prediction",
                "Statistical Learning Theory",
                "Oracle-based Algorithms"
            ],
            "author": [
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "Tzu-Kuo Huang",
                "John Langford",
                "Hal Daumé III"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-681/17-681.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Shared Subspace Models for Multi-Group Covariance Estimation",
            "abstract": [
                "We develop a model-based method for evaluating heterogeneity among several p × p covariance matrices in the large p, small n setting. This is done by assuming a spiked covariance model for each group and sharing information about the space spanned by the group-level eigenvectors. We use an empirical Bayes method to identify a low-dimensional subspace which explains variation across all groups and use an MCMC algorithm to estimate the posterior uncertainty of eigenvectors and eigenvalues on this subspace. The implementation and utility of our model is illustrated with analyses of high-dimensional multivariate gene expression."
            ],
            "keywords": [
                "covariance estimation",
                "spiked covariance model",
                "Stiefel manifold",
                "large p, small n",
                "high-dimensional data",
                "empirical Bayes",
                "gene expression data"
            ],
            "author": [
                "Alexander M Franks",
                "Peter Hoff"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-484/18-484.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Adjustment Sets for Population Average Causal Treatment Effect Estimation in Graphical Models",
            "abstract": [
                "The method of covariate adjustment is often used for estimation of total treatment effects from observational studies. Restricting attention to causal linear models, a recent article (Henckel et al., 2019) derived two novel graphical criteria: one to compare the asymptotic variance of linear regression treatment effect estimators that control for certain distinct adjustment sets and another to identify the optimal adjustment set that yields the least squares estimator with the smallest asymptotic variance. In this paper we show that the same graphical criteria can be used in non-parametric causal graphical models when treatment effects are estimated using non-parametrically adjusted estimators of the interventional means. We also provide a new graphical criterion for determining the optimal adjustment set among the minimal adjustment sets and another novel graphical criterion for comparing time dependent adjustment sets. We show that uniformly optimal time dependent adjustment sets do not always exist. For point interventions, we provide a sound and complete graphical criterion for determining when a non-parametric optimally adjusted estimator of an interventional mean, or of a contrast of interventional means, is semiparametric efficient under the non-parametric causal graphical model. In addition, when the criterion is not met, we provide a sound algorithm that checks for possible simplifications of the efficient influence function of the parameter. Finally, we find an interesting connection between identification and efficient covariate adjustment estimation. Specifically, we show that if there exists an identifying formula for an interventional mean that depends only on treatment, outcome and mediators, then the non-parametric optimally adjusted estimator can never be globally efficient under the causal graphical model."
            ],
            "keywords": [
                "adjustment sets",
                "back-door formula",
                "Bayesian networks",
                "causal inference",
                "semiparametric inference"
            ],
            "author": [
                "Andrea Rotnitzky",
                "Ezequiel Smucler"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-1026/19-1026.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Approach to One-Bit Compressed Sensing Based on Probably Approximately Correct Learning Theory",
            "abstract": [
                "In this paper, the problem of one-bit compressed sensing (OBCS) is formulated as a problem in probably approximately correct (PAC) learning. It is shown that the Vapnik-Chervonenkis (VC-) dimension of the set of half-spaces in R n generated by k-sparse vectors is bounded below by k(lg(n/k) + 1) and above by 2k lg(en). By coupling this estimate with well-established results in PAC learning theory, we show that a consistent algorithm can recover a k-sparse vector with O(k lg n) measurements, given only the signs of the measurement vector. This result holds for all probability measures on R n. The theory is also applicable to the case of noisy labels, where the signs of the measurements are flipped with some unknown probability. 1. This term is standard in statistical learning theory and is defined later."
            ],
            "keywords": [],
            "author": [
                "Eren Mehmet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-504/17-504.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Finite-Sample Analysis of Least-Squares Policy Iteration",
            "abstract": [
                "In this paper, we report a performance bound for the widely used least-squares policy iteration (LSPI) algorithm. We first consider the problem of policy evaluation in reinforcement learning, that is, learning the value function of a fixed policy, using the least-squares temporal-difference (LSTD) learning method, and report finite-sample analysis for this algorithm. To do so, we first derive a bound on the performance of the LSTD solution evaluated at the states generated by the Markov chain and used by the algorithm to learn an estimate of the value function. This result is general in the sense that no assumption is made on the existence of a stationary distribution for the Markov chain. We then derive generalization bounds in the case when the Markov chain possesses a stationary distribution and is β-mixing. Finally, we analyze how the error at each policy evaluation step is propagated through the iterations of a policy iteration method, and derive a performance bound for the LSPI algorithm."
            ],
            "keywords": [
                "Markov decision processes",
                "reinforcement learning",
                "least-squares temporal-difference",
                "least-squares policy iteration",
                "generalization bounds",
                "finite-sample analysis"
            ],
            "author": [
                "Alessandro Lazaric",
                "Mohammad Ghavamzadeh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/lazaric12a/lazaric12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning",
            "abstract": [
                "We study pool-based active learning in the presence of noise, that is, the agnostic setting. It is known that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space. Although there are many cases on which active learning is very useful, it is also easy to construct examples that no active learning algorithm can have an advantage. Previous works have shown that the label complexity of active learning relies on the disagreement coefficient which often characterizes the intrinsic difficulty of the learning problem. In this paper, we study the disagreement coefficient of classification problems for which the classification boundary is smooth and the data distribution has a density that can be bounded by a smooth function. We prove upper and lower bounds for the disagreement coefficients of both finitely and infinitely smooth problems. Combining with existing results, it shows that active learning is superior to passive supervised learning for smooth problems."
            ],
            "keywords": [
                "active learning",
                "disagreement coefficient",
                "label complexity",
                "smooth function"
            ],
            "author": [
                "Liwei Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/wang11b/wang11b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models",
            "abstract": [
                "Directed acyclic graphs (DAGs) have been widely used as a representation of conditional independence in machine learning and statistics. Moreover, hidden or latent variables are often an important component of graphical models. However, DAG models suffer from an important limitation: the family of DAGs is not closed under marginalization of hidden variables. This means that in general we cannot use a DAG to represent the independencies over a subset of variables in a larger DAG. Directed mixed graphs (DMGs) are a representation that includes DAGs as a special case, and overcomes this limitation. This paper introduces algorithms for performing Bayesian inference in Gaussian and probit DMG models. An important requirement for inference is the specification of the distribution over parameters of the models. We introduce a new distribution for covariance matrices of Gaussian DMGs. We discuss and illustrate how several Bayesian machine learning tasks can benefit from the principle presented here: the power to model dependencies that are generated from hidden variables, but without necessarily modeling such variables explicitly."
            ],
            "keywords": [
                "graphical models",
                "structural equation models",
                "Bayesian inference",
                "Markov chain Monte Carlo",
                "latent variable models"
            ],
            "author": [
                "Ricardo Silva"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/silva09a/silva09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Shallow Parsing using Specialized HMMs",
            "abstract": [
                "We present a unified technique to solve different shallow parsing tasks as a tagging problem using a Hidden Markov Model-based approach (HMM). This technique consists of the incorporation of the relevant information for each task into the models. To do this, the training corpus is transformed to take into account this information. In this way, no change is necessary for either the training or tagging process, so it allows for the use of a standard HMM approach. Taking into account this information, we construct a Specialized HMM which gives more complete contextual models. We have tested our system on chunking and clause identification tasks using different specialization criteria. The results obtained are in line with the results reported for most of the relevant state-of-the-art approaches."
            ],
            "keywords": [
                "Shallow Parsing",
                "Text Chunking",
                "Clause Identification",
                "Statistical Language Modeling",
                "Specialized HMMs"
            ],
            "author": [
                "Antonio Molina",
                "Ferran Pla",
                "James Hammerton",
                "Miles Osborne",
                "Susan Armstrong",
                "Walter Daelemans"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/molina02a/molina02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Global Convergence of Online Limited Memory BFGS",
            "abstract": [
                "Global convergence of an online (stochastic) limited memory version of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method for solving optimization problems with stochastic objectives that arise in large scale machine learning is established. Lower and upper bounds on the Hessian eigenvalues of the sample functions are shown to suffice to guarantee that the curvature approximation matrices have bounded determinants and traces, which, in turn, permits establishing convergence to optimal arguments with probability 1. Experimental evaluation on a search engine advertising problem showcase reductions in convergence time relative to stochastic gradient descent algorithms."
            ],
            "keywords": [
                "quasi-Newton methods",
                "large-scale optimization",
                "stochastic optimization"
            ],
            "author": [
                "Aryan Mokhtari",
                "Alejandro Ribeiro"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/mokhtari15a/mokhtari15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Facilitating Score and Causal Inference Trees for Large Observational Studies",
            "abstract": [
                "Assessing treatment effects in observational studies is a multifaceted problem that not only involves heterogeneous mechanisms of how the treatment or cause is exposed to subjects, known as propensity, but also differential causal effects across sub-populations. We introduce a concept termed the facilitating score to account for both the confounding and interacting impacts of covariates on the treatment effect. Several approaches for estimating the facilitating score are discussed. In particular, we put forward a machine learning method, called causal inference tree (CIT), to provide a piecewise constant approximation of the facilitating score. With interpretable rules, CIT splits data in such a way that both the propensity and the treatment effect become more homogeneous within each resultant partition. Causal inference at different levels can be made on the basis of CIT. Together with an aggregated grouping procedure, CIT stratifies data into strata where causal effects can be conveniently assessed within each. Besides, a feasible way of predicting individual causal effects (ICE) is made available by aggregating ensemble CIT models. Both the stratified results and the estimated ICE provide an assessment of heterogeneity of causal effects and can be integrated for estimating the average causal effect (ACE). Mean square consistency of CIT is also established. We evaluate the performance of proposed methods with simulations and illustrate their use with the NSW data in Dehejia and Wahba (1999) where the objective is to assess the impact of"
            ],
            "keywords": [
                "CART",
                "causal inference",
                "confounding",
                "interaction",
                "observational study",
                "personalized medicine",
                "recursive partitioning"
            ],
            "author": [
                "Xiaogang Su",
                "Joseph Kang",
                "Richard A Levine",
                "Xin Yan",
                "Joseph Su",
                "Juanjuan Kang",
                "Richard A Fan",
                "Xin Levine",
                "FAN, LEVINE Yan Kang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/su12a/su12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analysis of Classification-based Policy Iteration Algorithms",
            "abstract": [
                "We introduce a variant of the classification-based approach to policy iteration which uses a cost-sensitive loss function weighting each classification mistake by its actual regret, that is, the difference between the action-value of the greedy action and of the action chosen by the classifier. For this algorithm, we provide a full finite-sample analysis. Our results state a performance bound in terms of the number of policy improvement steps, the number of rollouts used in each iteration, the capacity of the considered policy space (classifier), and a capacity measure which indicates how well the policy space can approximate policies that are greedy with respect to any of its members. The analysis reveals a tradeoff between the estimation and approximation errors in this classification-based policy iteration setting. Furthermore it confirms the intuition that classification-based policy iteration algorithms could be favorably compared to value-based approaches when the policies can be approximated more easily than their corresponding value functions. We also study the consistency of the algorithm when there exists a sequence of policy spaces with increasing capacity."
            ],
            "keywords": [
                "reinforcement learning",
                "policy iteration",
                "classification-based approach to policy iteration",
                "finite-sample analysis"
            ],
            "author": [
                "Alessandro Lazaric"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/10-364/10-364.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximate Marginals in Latent Gaussian Models",
            "abstract": [
                "We consider the problem of improving the Gaussian approximate posterior marginals computed by expectation propagation and the Laplace method in latent Gaussian models and propose methods that are similar in spirit to the Laplace approximation of Tierney and Kadane (1986). We show that in the case of sparse Gaussian models, the computational complexity of expectation propagation can be made comparable to that of the Laplace method by using a parallel updating scheme. In some cases, expectation propagation gives excellent estimates where the Laplace approximation fails. Inspired by bounds on the correct marginals, we arrive at factorized approximations, which can be applied on top of both expectation propagation and the Laplace method. The factorized approximations can give nearly indistinguishable results from the non-factorized approximations and their computational complexity scales linearly with the number of variables. We experienced that the expectation propagation based marginal approximations we introduce are typically more accurate than the methods of similar complexity proposed by Rue et al. (2009)."
            ],
            "keywords": [
                "approximate marginals",
                "Gaussian Markov random fields",
                "Laplace approximation",
                "variational inference",
                "expectation propagation"
            ],
            "author": [
                "Botond Cseke",
                "Tom Heskes"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/cseke11a/cseke11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convex Regression with Interpretable Sharp Partitions",
            "abstract": [
                "We consider the problem of predicting an outcome variable on the basis of a small number of covariates, using an interpretable yet non-additive model. We propose convex regression with interpretable sharp partitions (CRISP) for this task. CRISP partitions the covariate space into blocks in a data-adaptive way, and fits a mean model within each block. Unlike other partitioning methods, CRISP is fit using a non-greedy approach by solving a convex optimization problem, resulting in low-variance fits. We explore the properties of CRISP, and evaluate its performance in a simulation study and on a housing price data set."
            ],
            "keywords": [
                "convex optimization",
                "interpretability",
                "non-additivity",
                "non-parametric regression",
                "prediction"
            ],
            "author": [
                "Ashley Petersen",
                "Noah Simon",
                "Daniela Witten"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-344/15-344.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unifying Framework in Vector-valued Reproducing Kernel Hilbert Spaces for Manifold Regularization and Co-Regularized Multi-view Learning",
            "abstract": [
                "This paper presents a general vector-valued reproducing kernel Hilbert spaces (RKHS) framework for the problem of learning an unknown functional dependency between a structured input space and a structured output space. Our formulation encompasses both Vector-valued Manifold Regularization and Co-regularized Multi-view Learning, providing in particular a unifying framework linking these two important learning approaches. In the case of the least square loss function, we provide a closed form solution, which is obtained by solving a system of linear equations. In the case of Support Vector Machine (SVM) classification, our formulation generalizes in particular both the binary Laplacian SVM to the multi-class, multi-view settings and the multi-class Simplex Cone SVM to the semisupervised, multi-view settings. The solution is obtained by solving a single quadratic optimization problem, as in standard SVM, via the Sequential Minimal Optimization (SMO) approach. Empirical results obtained on the task of object recognition, using several challenging data sets, demonstrate the competitiveness of our algorithms compared with other state-of-the-art methods."
            ],
            "keywords": [
                "kernel methods",
                "vector-valued RKHS",
                "multi-view learning",
                "multi-modality learning",
                "multi-kernel learning",
                "manifold regularization",
                "multi-class classification"
            ],
            "author": [
                "Hà Quang",
                "Loris Bazzani",
                "Vittorio Murino"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-036/14-036.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral Algorithms for Community Detection in Directed Networks",
            "abstract": [
                "Community detection in large social networks is affected by degree heterogeneity of nodes. The D-SCORE algorithm for directed networks was introduced to reduce this effect by taking the element-wise ratios of the singular vectors of the adjacency matrix before clustering. Meaningful results were obtained for the statistician citation network, but rigorous analysis on its performance was missing. First, this paper establishes theoretical guarantee for this algorithm and its variants for the directed degree-corrected block model (Directed-DCBM). Second, this paper provides significant improvements for the original D-SCORE algorithms by attaching the nodes outside of the community cores using the information of the original network instead of the singular vectors."
            ],
            "keywords": [
                "directed networks",
                "community detection",
                "clustering",
                "degree-corrected block model",
                "k-means",
                "principle component analysis"
            ],
            "author": [
                "Zhe Wang",
                "Yingbin Liang",
                "Pengsheng Ji"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-581/18-581.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Response-Based Approachability with Applications to Generalized No-Regret Problems",
            "abstract": [
                "Blackwell's theory of approachability provides fundamental results for repeated games with vector-valued payoffs, which have been usefully applied in the theory of learning in games, and in devising online learning algorithms in the adversarial setup. A target set S is approachable by a player (the agent) in such a game if he can ensure that the average payoff vector converges to S, no matter what the opponent does. Blackwell provided two equivalent conditions for a convex set to be approachable. Standard approachability algorithms rely on the primal condition, which is a geometric separation condition, and essentially require to compute at each stage a projection direction from a certain point to S. Here we introduce an approachability algorithm that relies on Blackwell's dual condition, which requires the agent to have a feasible response to each mixed action of the opponent, namely a mixed action such that the expected payoff vector belongs to S. Thus, rather than projections, the proposed algorithm relies on computing the response to a certain action of the opponent at each stage. We demonstrate the utility of the proposed approach by applying it to certain generalizations of the classical regret minimization problem, which incorporate side constraints, reward-to-cost criteria, and so-called global cost functions. In these extensions, computation of the projection is generally complex while the response is readily obtainable."
            ],
            "keywords": [],
            "author": [
                "Andrey Bernstein",
                "Nahum Shimkin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/bernstein15a/bernstein15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scikit-Multiflow: A Multi-output Streaming Framework",
            "abstract": [
                "scikit-multiflow is a framework for learning from data streams and multi-output learning in Python. Conceived to serve as a platform to encourage the democratization of stream learning research, it provides multiple state-of-the-art learning methods, data generators and evaluators for different stream learning problems, including single-output, multi-output and multi-label. scikit-multiflow builds upon popular open source frameworks including scikitlearn, MOA and MEKA. Development follows the FOSS principles. Quality is enforced by complying with PEP8 guidelines, using continuous integration and functional testing. The source code is available at https://github.com/scikit-multiflow/scikit-multiflow."
            ],
            "keywords": [
                "Machine Learning",
                "Stream Data",
                "Multi-output",
                "Drift Detection",
                "Python"
            ],
            "author": [
                "Jacob Montiel",
                "Jesse Read",
                "Albert Bifet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-251/18-251.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel-estimated Nonparametric Overlap-Based Syncytial Clustering",
            "abstract": [
                "Commonly-used clustering algorithms usually find ellipsoidal, spherical or other regularstructured clusters, but are more challenged when the underlying groups lack formal structure or definition. Syncytial clustering is the name that we introduce for methods that merge groups obtained from standard clustering algorithms in order to reveal complex group structure in the data. Here, we develop a distribution-free fully-automated syncytial clustering algorithm that can be used with k-means and other algorithms. Our approach estimates the cumulative distribution function of the normed residuals from an appropriately fit k-groups model and calculates the estimated nonparametric overlap between each pair of clusters. Groups with high pairwise overlap are merged as long as the estimated generalized overlap decreases. Our methodology is always a top performer in identifying groups with regular and irregular structures in several datasets and can be applied to datasets with scatter or incomplete records. The approach is also used to identify the distinct kinds of gamma ray bursts in the Burst and Transient Source Experiment 4Br catalog and the distinct kinds of activation in a functional Magnetic Resonance Imaging study."
            ],
            "keywords": [
                "BATSE",
                "DEMP",
                "DEMP+",
                "DBSCAN*",
                "density peaks algorithm",
                "GRB",
                "GSL-NN",
                "k-clips",
                "k-means",
                "k m -means",
                "kernel density estimation",
                "KNOB-SynC",
                "MixModCombi",
                "MGHD",
                "MSAL",
                "overlap",
                "PGMM",
                "SDSS",
                "spectral clustering",
                "TiK-means"
            ],
            "author": [
                "Israel A Almodóvar-Rivera",
                "Ranjan Maitra"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-435/18-435.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Support Vector Hazards Machine: A Counting Process Framework for Learning Risk Scores for Censored Outcomes",
            "abstract": [
                "Learning risk scores to predict dichotomous or continuous outcomes using machine learning approaches has been studied extensively. However, how to learn risk scores for time-to-event outcomes subject to right censoring has received little attention until recently. Existing approaches rely on inverse probability weighting or rank-based regression, which may be inefficient. In this paper, we develop a new support vector hazards machine (SVHM) approach to predict censored outcomes. Our method is based on predicting the counting process associated with the time-to-event outcomes among subjects at risk via a series of support vector machines. Introducing counting processes to represent time-to-event data leads to a connection between support vector machines in supervised learning and hazards regression in standard survival analysis. To account for different at risk populations at observed event times, a time-varying offset is used in estimating risk scores. The resulting optimization is a convex quadratic programming problem that can easily incorporate nonlinearity using kernel trick. We demonstrate an interesting link from the profiled empirical risk function of SVHM to the Cox partial likelihood. We then formally show that SVHM is optimal in discriminating covariate-specific hazard function from population average hazard function, and establish the consistency and learning rate of the predicted risk using the estimated risk scores. Simulation studies show improved prediction accuracy of the event times using SVHM compared to existing machine learning methods and standard conventional approaches. Finally, we analyze two real world biomedical study data where we use clinical markers and neuroimaging biomarkers to predict age-at-onset of a disease, and demonstrate superiority of SVHM in distinguishing high risk versus low risk subjects."
            ],
            "keywords": [
                "support vector machine",
                "survival analysis",
                "risk bound",
                "risk prediction",
                "neuroimaging biomarkers",
                "early disease detection"
            ],
            "author": [
                "Yuanjia Wang",
                "Tianle Chen",
                "Donglin Zeng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-007/16-007.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stopping Criterion for Boosting-Based Data Reduction Techniques: from Binary to Multiclass Problems",
            "abstract": [
                "So far, boosting has been used to improve the quality of moderately accurate learning algorithms, by weighting and combining many of their weak hypotheses into a final classifier with theoretically high accuracy. In a recent work (Sebban, Nock and Lallich, 2001), we have attempted to adapt boosting properties to data reduction techniques. In this particular context, the objective was not only to improve the success rate, but also to reduce the time and space complexities due to the storage requirements of some costly learning algorithms, such as nearest-neighbor classifiers. In that framework, each weak hypothesis, which is usually built and weighted from the learning set, is replaced by a single learning instance. The weight given by boosting defines in that case the relevance of the instance, and a statistical test allows one to decide whether it can be discarded without damaging further classification tasks. In Sebban, Nock and Lallich (2001), we addressed problems with two classes. It is the aim of the present paper to relax the class constraint, and extend our contribution to multiclass problems. Beyond data reduction, experimental results are also provided on twenty-three datasets, showing the benefits that our boosting-derived weighting rule brings to weighted nearest neighbor classifiers."
            ],
            "keywords": [],
            "author": [
                "Marc Sebban",
                "Richard Nock",
                "Carla E Brodley",
                "Andrea Danyluk"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/sebban02a/sebban02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Universality, Characteristic Kernels and RKHS Embedding of Measures",
            "abstract": [
                "Over the last few years, two different notions of positive definite (pd) kernels-universal and characteristic-have been developing in parallel in machine learning: universal kernels are proposed in the context of achieving the Bayes risk by kernel-based classification/regression algorithms while characteristic kernels are introduced in the context of distinguishing probability measures by embedding them into a reproducing kernel Hilbert space (RKHS). However, the relation between these two notions is not well understood. The main contribution of this paper is to clarify the relation between universal and characteristic kernels by presenting a unifying study relating them to RKHS embedding of measures, in addition to clarifying their relation to other common notions of strictly pd, conditionally strictly pd and integrally strictly pd kernels. For radial kernels on R d , all these notions are shown to be equivalent."
            ],
            "keywords": [
                "kernel methods",
                "characteristic kernels",
                "Hilbert space embeddings",
                "universal kernels",
                "strictly positive definite kernels",
                "integrally strictly positive definite kernels",
                "conditionally strictly positive definite kernels",
                "translation invariant kernels",
                "radial kernels",
                "binary classification",
                "homogeneity testing"
            ],
            "author": [
                "Bharath K Sriperumbudur",
                "Gert R G Lanckriet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/sriperumbudur11a/sriperumbudur11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning",
            "abstract": [
                "Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution p. These methods rely on a singular value decomposition of a matrix H S , called the empirical Hankel matrix, that records the frequencies of (some of) the observed strings S. The accuracy of the learned distribution depends both on the quantity of information embedded in H S and on the distance between H S and its mean H p. Existing concentration bounds seem to indicate that the concentration over H p gets looser with its dimensions, suggesting that it might be necessary to bound the dimensions of H S for learning. We prove new dimensionfree concentration bounds for classical Hankel matrices and several variants, based on prefixes or factors of strings, that are useful for learning. Experiments demonstrate that these bounds are tight and that they significantly improve existing (dimension-dependent) bounds. One consequence of these results is that the spectral learning approach remains consistent even if all the observations are recorded within the empirical matrix."
            ],
            "keywords": [
                "Hankel matrices",
                "Matrix Bernstein bounds",
                "Probabilistic Grammatical Inference",
                "Rational series",
                "Spectral learning"
            ],
            "author": [
                "François Denis",
                "Mattias Gybels"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-501/14-501.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Second-Order Stochastic Optimization for Machine Learning in Linear Time",
            "abstract": [
                "First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per-iteration cost of gradient based methods, and in certain settings improve upon the overall running time over popular first-order methods. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data."
            ],
            "keywords": [
                "second-order optimization",
                "convex optimization",
                "regression"
            ],
            "author": [
                "Naman Agarwal",
                "Brian Bullins",
                "Elad Hazan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-491/16-491.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Decrypting \"Cryptogenic\" Epilepsy: Semi-supervised Hierarchical Conditional Random Fields For Detecting Cortical Lesions In MRI-Negative Patients",
            "abstract": [
                "Focal cortical dysplasia (FCD) is the most common cause of pediatric epilepsy and the third most common cause in adults with treatment-resistant epilepsy. Surgical resection of the lesion is the most effective treatment to stop seizures. Technical advances in MRI have revolutionized the diagnosis of FCD, leading to high success rates for resective surgery. However, 45% of histologically confirmed FCD patients have normal MRIs (MRI-negative). Without a visible lesion, the success rate of surgery drops from 66% to 29%. In this work, we cast the problem of detecting potential FCD lesions using MRI scans of MRI-negative patients in an image segmentation framework based on hierarchical conditional random fields (HCRF). We use surface based morphometry to model the cortical surface as a twodimensional surface which is then segmented at multiple scales to extract superpixels of different sizes. Each superpixel is assigned an outlier score by comparing it to a control c 2016 Bilal Ahmed et al. Ahmed et al. population. The lesion is detected by fusing the outlier probabilities across multiple scales using a tree-structured HCRF. The proposed method achieves a higher detection rate, with superior recall and precision on a sample of twenty MRI-negative FCD patients as compared to a baseline across four morphological features and their combinations."
            ],
            "keywords": [
                "Conditional Random Fields",
                "LOF",
                "Focal Cortical Dysplasia",
                "Epilepsy"
            ],
            "author": [
                "Bilal Ahmed",
                "Thomas Thesen",
                "Karen E Blackmon",
                "Ruben Kuzniecky",
                "Orrin Devinsky",
                "Carla E Brodley"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-428/15-428.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A General System of Differential Equations to Model First-Order Adaptive Algorithms",
            "abstract": [
                "First-order optimization algorithms play a major role in large scale machine learning. A new class of methods, called adaptive algorithms, were recently introduced to adjust iteratively the learning rate for each coordinate. Despite great practical success in deep learning, their behavior and performance on more general loss functions are not well understood. In this paper, we derive a non-autonomous system of differential equations, which is the continuous time limit of adaptive optimization methods. We study the convergence of its trajectories and give conditions under which the differential system, underlying all adaptive algorithms, is suitable for optimization. We discuss convergence to a critical point in the non-convex case and give conditions for the dynamics to avoid saddle points and local maxima. For convex loss function, we introduce a suitable Lyapunov functional which allows us to study its rate of convergence. Several other properties of both the continuous and discrete systems are briefly discussed. The differential system studied in the paper is general enough to encompass many other classical algorithms (such as Heavy Ball and Nesterov's accelerated method) and allow us to recover several known results for these algorithms."
            ],
            "keywords": [
                "Adaptive algorithms",
                "convex and non-convex optimization",
                "first-order methods",
                "differential equation",
                "forward Euler discretization"
            ],
            "author": [
                "André Belotto Da Silva",
                "Maxime Gazeau"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-808/18-808.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Hierarchical Kernel Learning",
            "abstract": [
                "This paper generalizes the framework of Hierarchical Kernel Learning (HKL) and illustrates its utility in the domain of rule learning. HKL involves Multiple Kernel Learning over a set of given base kernels assumed to be embedded on a directed acyclic graph. This paper proposes a twofold generalization of HKL: the first is employing a generic 1 / ρ block-norm regularizer (ρ ∈ (1, 2]) that alleviates a key limitation of the HKL formulation. The second is a generalization to the case of multi-class, multi-label and more generally, multi-task applications. The main technical contribution of this work is the derivation of a highly specialized partial dual of the proposed generalized HKL formulation and an efficient mirror descent based active set algorithm for solving it. Importantly, the generic regularizer enables the proposed formulation to be employed in the Rule Ensemble Learning (REL) where the goal is to construct an ensemble of conjunctive propositional rules. Experiments on benchmark REL data sets illustrate the efficacy of the proposed generalizations."
            ],
            "keywords": [
                "multiple kernel learning",
                "mixed-norm regularization",
                "multi-task learning",
                "rule ensemble learning",
                "active set method"
            ],
            "author": [
                "Pratik Jawanpuria",
                "Jagarlapudi Saketha Nath"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/jawanpuria15a/jawanpuria15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Taxonomy Adaptation in Large-scale Classification",
            "abstract": [
                "In this paper, we study flat and hierarchical classification strategies in the context of largescale taxonomies. Addressing the problem from a learning-theoretic point of view, we first propose a multi-class, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. Based on this bound, we also propose a technique for modifying a given taxonomy through pruning, that leads to a lower value of the upper bound as compared to the original taxonomy. We then present another method for hierarchy pruning by studying approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies."
            ],
            "keywords": [
                "Large-scale classification",
                "Hierarchical classification",
                "Taxonomy adaptation",
                "Rademacher complexity",
                "Meta-learning"
            ],
            "author": [
                "Rohit Babbar",
                "Eric Gaussier",
                "Massih-Reza Amini",
                "Cécile Amblard"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-207/14-207.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Introduction to Special Issue on Machine Learning Approaches to Shallow Parsing",
            "abstract": [
                "This article introduces the problem of partial or shallow parsing (assigning partial syntactic structure to sentences) and explains why it is an important natural language processing (NLP) task. The complexity of the task makes Machine Learning an attractive option in comparison to the handcrafting of rules. On the other hand, because of the same task complexity, shallow parsing makes an excellent benchmark problem for evaluating machine learning algorithms. We sketch the origins of shallow parsing as a specific task for machine learning of language, and introduce the articles accepted for this special issue, a representative sample of current research in this area. Finally, future directions for machine learning of shallow parsing are suggested."
            ],
            "keywords": [],
            "author": [
                "James Hammerton",
                "Susan Armstrong",
                "Walter Daelemans"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/hammerton02a/hammerton02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Occlusive Components Analysis",
            "abstract": [
                "We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, specified in the model parameters, combine to form the image. We show that the object parameters can be learned from an unlabeled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. Tractable approximations can be derived, however, by applying a truncated variational approach to Expectation Maximization (EM). In numerical experiments it is shown that these approximations recover the underlying set of object parameters including data noise and sparsity. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating components. The studied approach demonstrates that the multiple-causes generative approach can be generalized to extract occluding components, which links research on occlusion to the field of sparse coding approaches."
            ],
            "keywords": [
                "generative models",
                "occlusion",
                "unsupervised learning",
                "sparse coding",
                "expectation truncation Henniges",
                "Turner",
                "Sahani",
                "Eggert",
                "and Lücke"
            ],
            "author": [
                "Marc Henniges",
                "Richard E Turner",
                "Julian Eggert",
                "Maneesh Sahani",
                "Jörg Lücke"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/henniges14a/henniges14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bridging Supervised Learning and Test-Based Co-optimization",
            "abstract": [
                "This paper takes a close look at the important commonalities and subtle differences between the well-established field of supervised learning and the much younger one of cooptimization. It explains the relationships between the problems, algorithms and views on cost and performance of the two fields, all throughout providing a two-way dictionary for the respective terminologies used to describe these concepts. The intent is to facilitate advancement of both fields through transfer and cross-pollination of ideas, techniques and results. As a proof of concept, a theoretical study is presented on the connection between existence / lack of free lunch in the two fields, showcasing a few ideas for improving computational complexity of certain supervised learning approaches."
            ],
            "keywords": [
                "supervised learning",
                "active learning",
                "co-optimization",
                "free lunch",
                "optimal algorithms"
            ],
            "author": [
                "Elena Popovici"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-223/16-223.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Bayes-Optimality of F-Measure Maximizers",
            "abstract": [
                "The F-measure, which has originally been introduced in information retrieval, is nowadays routinely used as a performance metric for problems such as binary classification, multi-label classification, and structured output prediction. Optimizing this measure is a statistically and computationally challenging problem, since no closed-form solution exists. Adopting a decision-theoretic perspective, this article provides a formal and experimental analysis of different approaches for maximizing the F-measure. We start with a Bayes-risk analysis of related loss functions, such as Hamming loss and subset zero-one loss, showing that optimizing such losses as a surrogate of the F-measure leads to a high worst-case regret. Subsequently, we perform a similar type of analysis for F-measure maximizing algorithms, showing that such algorithms are approximate, while relying on additional assumptions regarding the statistical distribution of the binary response variables. Furthermore, we present a new algorithm which is not only computationally efficient but also Bayesoptimal, regardless of the underlying distribution. To this end, the algorithm requires only a quadratic (with respect to the number of binary responses) number of parameters of the joint distribution. We illustrate the practical performance of all analyzed methods by means of experiments with multi-label classification problems."
            ],
            "keywords": [
                "F-measure",
                "Bayes-optimal predictions",
                "regret",
                "statistical decision theory",
                "multi-label classification",
                "structured output prediction"
            ],
            "author": [
                "Willem Waegeman",
                "Arkadiusz Jachnik",
                "Weiwei Cheng",
                "Eyke Hüllermeier"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/waegeman14a/waegeman14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "New Convergence Aspects of Stochastic Gradient Algorithms",
            "abstract": [
                "The classical convergence analysis of SGD is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is violated for cases where the objective function is strongly convex. In Bottou et al. (2018), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. We show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime. We then move on to the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime in the case of diminished learning rate. It is well-known that SGD converges if a sequence of learning rates {η t } satisfies"
            ],
            "keywords": [
                "Stochastic Gradient Algorithms",
                "Asynchronous Stochastic Optimization",
                "SGD",
                "Hogwild!",
                "bounded gradient"
            ],
            "author": [
                "Phuong Lam M Nguyen",
                "Phuong Ha Nguyen",
                "Peter Richtárik",
                "Martin Takáč",
                "Ha Nguyen",
                "Katya Scheinberg",
                "Marten Van"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-759/18-759.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning",
            "abstract": [
                "The growth of information available to learning systems and the increasing complexity of learning tasks determine the need for devising algorithms that scale well with respect to all learning parameters. In the context of supervised sequential learning, the Viterbi algorithm plays a fundamental role, by allowing the evaluation of the best (most probable) sequence of labels with a time complexity linear in the number of time events, and quadratic in the number of labels. In this paper we propose CarpeDiem, a novel algorithm allowing the evaluation of the best possible sequence of labels with a sub-quadratic time complexity. 1 We provide theoretical grounding together with solid empirical results supporting two chief facts. CarpeDiem always finds the optimal solution requiring, in most cases, only a small fraction of the time taken by the Viterbi algorithm; meantime, CarpeDiem is never asymptotically worse than the Viterbi algorithm, thus confirming it as a sound replacement."
            ],
            "keywords": [
                "Viterbi algorithm",
                "sequence labeling",
                "conditional models",
                "classifiers optimization",
                "exact inference"
            ],
            "author": [
                "Roberto Esposito",
                "Daniele P Radicioni"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/esposito09a/esposito09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Graphical Representation of Equivalence Classes of AMP Chain Graphs",
            "abstract": [
                "This paper deals with chain graph models under alternative AMP interpretation. A new representative of an AMP Markov equivalence class, called the largest deflagged graph, is proposed. The representative is based on revealed internal structure of the AMP Markov equivalence class. More specifically, the AMP Markov equivalence class decomposes into finer strong equivalence classes and there exists a distinguished strong equivalence class among those forming the AMP Markov equivalence class. The largest deflagged graph is the largest chain graph in that distinguished strong equivalence class. A composed graphical procedure to get the largest deflagged graph on the basis of any AMP Markov equivalent chain graph is presented. In general, the largest deflagged graph differs from the AMP essential graph, which is another representative of the AMP Markov equivalence class."
            ],
            "keywords": [
                "chain graph",
                "AMP Markov equivalence",
                "strong equivalence",
                "largest deflagged graph",
                "component merging procedure",
                "deflagging procedure",
                "essential graph"
            ],
            "author": [
                "Alberto Roverato",
                "Milan Studený"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/roverato06a/roverato06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pyro: Deep Universal Probabilistic Programming",
            "abstract": [
                "Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large data sets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs."
            ],
            "keywords": [
                "Probabilistic programming",
                "graphical models",
                "approximate Bayesian inference",
                "generative models",
                "deep learning"
            ],
            "author": [
                "Eli Bingham",
                "Jonathan P Chen",
                "Martin Jankowiak",
                "Fritz Obermeyer",
                "Rohit Singh",
                "Paul Szerlip",
                "Paul Horsfall",
                "Noah D Goodman",
                "Neeraj Pradhan",
                "Theofanis Karaletsos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-403/18-403.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral Ranking using Seriation",
            "abstract": [
                "We describe a seriation algorithm for ranking a set of items given pairwise comparisons between these items. Intuitively, the algorithm assigns similar rankings to items that compare similarly with all others. It does so by constructing a similarity matrix from pairwise comparisons, using seriation methods to reorder this matrix and construct a ranking. We first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order. We then show that ranking reconstruction is still exact when some pairwise comparisons are corrupted or missing, and that seriation based spectral ranking is more robust to noise than classical scoring methods. Finally, we bound the ranking error when only a random subset of the comparions are observed. An additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems. Experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods."
            ],
            "keywords": [
                "Ranking",
                "seriation",
                "spectral methods"
            ],
            "author": [
                "Fajwel Fogel",
                "C M A P Ecole",
                "Polytechnique Palaiseau",
                "Milan Vojnovic"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-035/16-035.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model Selection: Beyond the Bayesian/Frequentist Divide",
            "abstract": [
                "The principle of parsimony also known as \"Ockham's razor\" has inspired many theories of model selection. Yet such theories, all making arguments in favor of parsimony, are based on very different premises and have developed distinct methodologies to derive algorithms. We have organized challenges and edited a special issue of JMLR and several conference proceedings around the theme of model selection. In this editorial, we revisit the problem of avoiding overfitting in light of the latest results. We note the remarkable convergence of theories as different as Bayesian theory, Minimum Description Length, bias/variance tradeoff, Structural Risk Minimization, and regularization, in some approaches. We also present new and interesting examples of the complementarity of theories leading to hybrid algorithms, neither frequentist, nor Bayesian, or perhaps both frequentist and Bayesian!"
            ],
            "keywords": [
                "model selection",
                "ensemble methods",
                "multilevel inference",
                "multilevel optimization",
                "performance prediction",
                "bias-variance tradeoff",
                "Bayesian priors",
                "structural risk minimization",
                "guaranteed risk minimization",
                "over-fitting",
                "regularization",
                "minimum description length"
            ],
            "author": [
                "Isabelle Guyon",
                "Amir Saffari",
                "Gideon Dror",
                "Gavin Cawley"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/guyon10a/guyon10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "scikit-multilearn: A scikit-based Python environment for performing multi-label classification",
            "abstract": [
                "The scikit-multilearn is a Python library for performing multi-label classification. It is compatible with the scikit-learn and scipy ecosystems and uses sparse matrices for all internal operations; provides native Python implementations of popular multi-label classification methods alongside a novel framework for label space partitioning and division and includes modern algorithm adaptation methods, network-based label space division approaches, which extracts label dependency information and multi-label embedding classifiers. The library provides Python wrapped access to the extensive multi-label method stack from Java libraries and makes it possible to extend deep learning single-label methods for multi-label tasks. The library allows multi-label stratification and data set management. The implementation is more efficient in problem transformation than other established libraries, has good test coverage and follows PEP8. Source code and documentation can be downloaded from http://scikit.ml and also via pip. The project is BSD-licensed."
            ],
            "keywords": [
                "Python",
                "multi-label classification",
                "label-space clustering",
                "multi-label embedding",
                "multi-label stratification"
            ],
            "author": [
                "Piotr Szymański",
                "Tomasz Kajdanowicz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-100/17-100.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Co-Training",
            "abstract": [
                "Co-training (or more generally, co-regularization) has been a popular algorithm for semi-supervised learning in data with two feature representations (or views), but the fundamental assumptions underlying this type of models are still unclear. In this paper we propose a Bayesian undirected graphical model for co-training, or more generally for semi-supervised multi-view learning. This makes explicit the previously unstated assumptions of a large class of co-training type algorithms, and also clarifies the circumstances under which these assumptions fail. Building upon new insights from this model, we propose an improved method for co-training, which is a novel co-training kernel for Gaussian process classifiers. The resulting approach is convex and avoids local-maxima problems, and it can also automatically estimate how much each view should be trusted to accommodate noisy or unreliable views. The Bayesian co-training approach can also elegantly handle data samples with missing views, that is, some of the views are not available for some data points at learning time. This is further extended to an active sensing framework, in which the missing (sample, view) pairs are actively acquired to improve learning performance. The strength of active sensing model is that one actively sensed (sample, view) pair would improve the joint multi-view classification on all the samples. Experiments on toy data and several real world data sets illustrate the benefits of this approach."
            ],
            "keywords": [
                "co-training",
                "multi-view learning",
                "semi-supervised learning",
                "Gaussian processes",
                "undirected graphical models",
                "active sensing"
            ],
            "author": [
                "Shipeng Yu",
                "Rómer Rosales",
                "R Bharat Rao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/yu11a/yu11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Joint PLDA for Simultaneous Modeling of Two Factors",
            "abstract": [
                "Probabilistic linear discriminant analysis (PLDA) is a method used for biometric problems like speaker or face recognition that models the variability of the samples using two latent variables, one that depends on the class of the sample and another one that is assumed independent across samples and models the within-class variability. In this work, we propose a generalization of PLDA that enables joint modeling of two sample-dependent factors: the class of interest and a nuisance condition. The approach does not change the basic form of PLDA but rather modifies the training procedure to consider the dependency across samples of the latent variable that models within-class variability. While the identity of the nuisance condition is needed during training, it is not needed during testing since we propose a scoring procedure that marginalizes over the corresponding latent variable. We show results on a multilingual speaker-verification task, where the language spoken is considered a nuisance condition. The proposed joint PLDA approach leads to significant performance gains in this task for two different data sets, in particular when the training data contains mostly or only monolingual speakers."
            ],
            "keywords": [
                "Probabilistic linear discriminant analysis",
                "speaker recognition",
                "factor analysis",
                "language variability",
                "robustness to acoustic conditions"
            ],
            "author": [
                "Luciana Ferrer",
                "Mitchell Mclaren"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-134/18-134.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimality of Poisson Processes Intensity Learning with Gaussian Processes",
            "abstract": [
                "In this paper we provide theoretical support for the so-called \"Sigmoidal Gaussian Cox Process\" approach to learning the intensity of an inhomogeneous Poisson process on a ddimensional domain. This method was proposed by Adams, Murray and MacKay (ICML, 2009), who developed a tractable computational approach and showed in simulation and real data experiments that it can work quite satisfactorily. The results presented in the present paper provide theoretical underpinning of the method. In particular, we show how to tune the priors on the hyper parameters of the model in order for the procedure to automatically adapt to the degree of smoothness of the unknown intensity, and to achieve optimal convergence rates."
            ],
            "keywords": [
                "inhomogeneous Poisson process",
                "Bayesian intensity learning",
                "Gaussian process prior",
                "optimal rates",
                "adaptation to smoothness"
            ],
            "author": [
                "Alisa Kirichenko"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/kirichenko15a/kirichenko15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples",
            "abstract": [
                "We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework."
            ],
            "keywords": [
                "semi-supervised learning",
                "graph transduction",
                "regularization",
                "kernel methods",
                "manifold learning",
                "spectral graph theory",
                "unlabeled data",
                "support vector machines"
            ],
            "author": [
                "Mikhail Belkin",
                "Partha Niyogi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/belkin06a/belkin06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exact Clustering of Weighted Graphs via Semidefinite Programming",
            "abstract": [
                "As a model problem for clustering, we consider the densest k-disjoint-clique problem of partitioning a weighted complete graph into k disjoint subgraphs such that the sum of the densities of these subgraphs is maximized. We establish that such subgraphs can be recovered from the solution of a particular semidefinite relaxation with high probability if the input graph is sampled from a distribution of clusterable graphs. Specifically, the semidefinite relaxation is exact if the graph consists of k large disjoint subgraphs, corresponding to clusters, with weight concentrated within these subgraphs, plus a moderate number of nodes not belonging to any cluster. Further, we establish that if noise is weakly obscuring these clusters, i.e, the between-cluster edges are assigned very small weights, then we can recover significantly smaller clusters. For example, we show that in approximately sparse graphs, where the between-cluster weights tend to zero as the size n of the graph tends to infinity, we can recover clusters of size polylogarithmic in n under certain conditions on the distribution of edge weights. Empirical evidence from numerical simulations is also provided to support these theoretical phase transitions to perfect recovery of the cluster structure."
            ],
            "keywords": [
                "clustering",
                "densest subgraph",
                "stochastic block models",
                "semidefinite programming",
                "sparse graphs"
            ],
            "author": [
                "Aleksis Pirinen",
                "Brendan Ames"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/16-128/16-128.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems",
            "abstract": [
                "We present a method for training support vector machine (SVM)-based classification systems for combination with other classification systems designed for the same task. Ideally, a new system should be designed such that, when combined with existing systems, the resulting performance is optimized. We present a simple model for this problem and use the understanding gained from this analysis to propose a method to achieve better combination performance when training SVM systems. We include a regularization term in the SVM objective function that aims to reduce the average class-conditional covariance between the resulting scores and the scores produced by the existing systems, introducing a trade-off between such covariance and the system's individual performance. That is, the new system \"takes one for the team\", falling somewhat short of its best possible performance in order to increase the diversity of the ensemble. We report results on the NIST 2005 and 2006 speaker recognition evaluations (SREs) for a variety of subsystems. We show a gain of 19% on the equal error rate (EER) of a combination of four systems when applying the proposed method with respect to the performance obtained when the four systems are trained independently of each other."
            ],
            "keywords": [
                "system combination",
                "ensemble diversity",
                "multiple classifier systems",
                "support vector machines",
                "speaker recognition",
                "kernel methods"
            ],
            "author": [
                "Luciana Ferrer",
                "Elizabeth Shriberg"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/ferrer09a/ferrer09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Complete Search for Feature Selection in Decision Trees",
            "abstract": [
                "The search space for the feature selection problem in decision tree learning is the lattice of subsets of the available features. We design an exact enumeration procedure of the subsets of features that lead to all and only the distinct decision trees built by a greedy top-down decision tree induction algorithm. The procedure stores, in the worst case, a number of trees linear in the number of features. By exploiting a further pruning of the search space, we design a complete procedure for finding δ-acceptable feature subsets, which depart by at most δ from the best estimated error over any feature subset. Feature subsets with the best estimated error are called best feature subsets. Our results apply to any error estimator function, but experiments are mainly conducted under the wrapper model, in which the misclassification error over a search set is used as an estimator. The approach is also adapted to the design of a computational optimization of the sequential backward elimination heuristic, extending its applicability to large dimensional datasets. The procedures of this paper are implemented in a multi-core data parallel C++ system. We investigate experimentally the properties and limitations of the procedures on a collection of 20 benchmark datasets, showing that oversearching increases both overfitting and instability."
            ],
            "keywords": [
                "Feature Selection",
                "Decision Trees",
                "Wrapper models",
                "Complete Search"
            ],
            "author": [
                "Salvatore Ruggieri"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-035/18-035.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Investigation of Missing Data Methods for Classification Trees Applied to Binary Response Data",
            "abstract": [
                "There are many different methods used by classification tree algorithms when missing data occur in the predictors, but few studies have been done comparing their appropriateness and performance. This paper provides both analytic and Monte Carlo evidence regarding the effectiveness of six popular missing data methods for classification trees applied to binary response data. We show that in the context of classification trees, the relationship between the missingness and the dependent variable, as well as the existence or non-existence of missing values in the testing data, are the most helpful criteria to distinguish different missing data methods. In particular, separate class is clearly the best method to use when the testing set has missing values and the missingness is related to the response variable. A real data set related to modeling bankruptcy of a firm is then analyzed. The paper concludes with discussion of adaptation of these results to logistic regression, and other potential generalizations."
            ],
            "keywords": [
                "classification tree",
                "missing data",
                "separate class",
                "RPART",
                "C4.5",
                "CART"
            ],
            "author": [
                "Yufeng Ding",
                "Jeffrey S Simonoff"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/ding10a/ding10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "glm-ie: Generalised Linear Models Inference & Estimation Toolbox",
            "abstract": [
                "The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean field. Scalable and efficient inference in fully-connected undirected graphical models or Markov random fields with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling. We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX files to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classification as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit."
            ],
            "keywords": [
                "sparse linear models",
                "generalised linear models",
                "Bayesian inference",
                "approximate inference",
                "probabilistic regression and classification",
                "penalised least squares estimation",
                "lazy evaluation matrix class"
            ],
            "author": [
                "Hannes Nickisch"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/nickisch12a/nickisch12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension *",
            "abstract": [
                "We design an online algorithm for Principal Component Analysis. In each trial the current instance is centered and projected into a probabilistically chosen low dimensional subspace. The regret of our online algorithm, that is, the total expected quadratic compression loss of the online algorithm minus the total quadratic compression loss of the batch algorithm, is bounded by a term whose dependence on the dimension of the instances is only logarithmic. We first develop our methodology in the expert setting of online learning by giving an algorithm for learning as well as the best subset of experts of a certain size. This algorithm is then lifted to the matrix setting where the subsets of experts correspond to subspaces. The algorithm represents the uncertainty over the best subspace as a density matrix whose eigenvalues are bounded. The running time is O(n 2) per trial, where n is the dimension of the instances."
            ],
            "keywords": [
                "principal component analysis",
                "online learning",
                "density matrix",
                "expert setting",
                "quantum Bayes rule"
            ],
            "author": [
                "Manfred K Warmuth",
                "Dima Kuzmin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/warmuth08a/warmuth08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Trend Filtering on Graphs",
            "abstract": [
                "We introduce a family of adaptive estimators on graphs, based on penalizing the 1 norm of discrete graph differences. This generalizes the idea of trend filtering (Kim et al., 2009; Tibshirani, 2014), used for univariate nonparametric regression, to graphs. Analogous to the univariate case, graph trend filtering exhibits a level of local adaptivity unmatched by the usual-based graph smoothers. It is also defined by a convex minimization problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We demonstrate the merits of graph trend filtering through both examples and theory."
            ],
            "keywords": [
                "trend filtering",
                "graph smoothing",
                "total variation denoising",
                "fused lasso",
                "local adaptivity"
            ],
            "author": [
                "Yu-Xiang Wang",
                "James Sharpnack",
                "Alexander J Smola",
                "Ryan J Tibshirani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-147/15-147.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Learnability of Shuffle Ideals",
            "abstract": [
                "PAC learning of unrestricted regular languages is long known to be a difficult problem. The class of shuffle ideals is a very restricted subclass of regular languages, where the shuffle ideal generated by a string u is the collection of all strings containing u as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shuffle ideals appears quite difficult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP = NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efficient algorithm for properly learning shuffle ideals in the statistical query (and therefore also PAC) model under the uniform distribution."
            ],
            "keywords": [
                "PAC learning",
                "statistical queries",
                "regular languages",
                "deterministic finite automata",
                "shuffle ideals",
                "subsequences"
            ],
            "author": [
                "Dana Angluin",
                "James Aspnes",
                "Sarah Eisenstat",
                "Aryeh Kontorovich"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/angluin13a/angluin13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems",
            "abstract": [
                "A generalized discriminant analysis based on a new optimization criterion is presented. The criterion extends the optimization criteria of the classical Linear Discriminant Analysis (LDA) when the scatter matrices are singular. An efficient algorithm for the new optimization problem is presented. The solutions to the proposed criterion form a family of algorithms for generalized LDA, which can be characterized in a closed form. We study two specific algorithms, namely Uncorrelated LDA (ULDA) and Orthogonal LDA (OLDA). ULDA was previously proposed for feature extraction and dimension reduction, whereas OLDA is a novel algorithm proposed in this paper. The features in the reduced space of ULDA are uncorrelated, while the discriminant vectors of OLDA are orthogonal to each other. We have conducted a comparative study on a variety of real-world data sets to evaluate ULDA and OLDA in terms of classification accuracy."
            ],
            "keywords": [
                "dimension reduction",
                "linear discriminant analysis",
                "uncorrelated LDA",
                "orthogonal LDA",
                "singular value decomposition"
            ],
            "author": [
                "Jieping Ye"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/ye05a/ye05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Data-driven Calibration of Penalties for Least-Squares Regression",
            "abstract": [
                "Penalization procedures often suffer from their dependence on multiplying factors, whose optimal values are either unknown or hard to estimate from data. We propose a completely data-driven calibration algorithm for these parameters in the least-squares regression framework, without assuming a particular shape for the penalty. Our algorithm relies on the concept of minimal penalty, recently introduced by Birgé and Massart (2007) in the context of penalized least squares for Gaussian homoscedastic regression. On the positive side, the minimal penalty can be evaluated from the data themselves, leading to a data-driven estimation of an optimal penalty which can be used in practice; on the negative side, their approach heavily relies on the homoscedastic Gaussian nature of their stochastic framework. The purpose of this paper is twofold: stating a more general heuristics for designing a datadriven penalty (the slope heuristics) and proving that it works for penalized least-squares regression with a random design, even for heteroscedastic non-Gaussian data. For technical reasons, some exact mathematical results will be proved only for regressogram bin-width selection. This is at least a first step towards further results, since the approach and the method that we use are indeed general."
            ],
            "keywords": [
                "data-driven calibration",
                "non-parametric regression",
                "model selection by penalization",
                "heteroscedastic data",
                "regressogram"
            ],
            "author": [
                "Sylvain Arlot",
                "Pascal Massart"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/arlot09a/arlot09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Constraint-based Causal Discovery from Multiple Interventions over Overlapping Variable Sets",
            "abstract": [
                "Scientific practice typically involves repeatedly studying a system, each time trying to unravel a different perspective. In each study, the scientist may take measurements under different experimental conditions (interventions, manipulations, perturbations) and measure different sets of quantities (variables). The result is a collection of heterogeneous data sets coming from different data distributions. In this work, we present algorithm COmbINE , which accepts a collection of data sets over overlapping variable sets under different experimental conditions; COmbINE then outputs a summary of all causal models indicating the invariant and variant structural characteristics of all models that simultaneously fit all of the input data sets. COmbINE converts estimated dependencies and independencies in the data into path constraints on the data-generating causal model and encodes them as a SAT instance. The algorithm is sound and complete in the sample limit. To account for conflicting constraints arising from statistical errors, we introduce a general method for sorting constraints in order of confidence, computed as a function of their corresponding p-values. In our empirical evaluation, COmbINE outperforms in terms of efficiency the only pre-existing similar algorithm; the latter additionally admits feedback cycles, but does not admit conflicting constraints which hinders the applicability on real data. As a proof-of-concept, COmbINE is employed to co-analyze 4 real, mass-cytometry data sets measuring phosphorylated protein concentrations of overlapping protein sets under 3 different interventions."
            ],
            "keywords": [
                "causality",
                "causal discovery",
                "graphical models",
                "maximal ancestral graphs",
                "semi-Markov causal models",
                "randomized experiments",
                "latent variables"
            ],
            "author": [
                "Sofia Triantafillou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/triantafillou15a/triantafillou15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Rank-Breaking: Computational and Statistical Tradeoffs",
            "abstract": [
                "For massive and heterogeneous modern datasets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of rank aggregation, for the Plackett-Luce model, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data. Further, the proposed generalized rank-breaking algorithm involves set-wise comparisons as opposed to traditional pairwise comparisons. The maximum likelihood estimate of pairwise comparisons is computed efficiently using the celebrated minorization maximization algorithm (Hunter, 2004). To compute the pseudo-maximum likelihood estimate of the set-wise comparisons, we provide a generalization of the minorization maximization algorithm and give guarantees on its convergence."
            ],
            "keywords": [
                "Rank aggregation",
                "Plackett-Luce model",
                "Sample and Computational tradeoff"
            ],
            "author": [
                "Ashish Khetan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-412/16-412.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Smoothing Multivariate Performance Measures",
            "abstract": [
                "Optimizing multivariate performance measure is an important task in Machine Learning. Joachims (2005) introduced a Support Vector Method whose underlying optimization problem is commonly solved by cutting plane methods (CPMs) such as SVM-Perf and BMRM. It can be shown that CPMs converge to an ε accurate solution in O 1 λε iterations, where λ is the trade-off parameter between the regularizer and the loss function. Motivated by the impressive convergence rate of CPM on a number of practical problems, it was conjectured that these rates can be further improved. We disprove this conjecture in this paper by constructing counter examples. However, surprisingly, we further discover that these problems are not inherently hard, and we develop a novel smoothing strategy, which in conjunction with Nesterov's accelerated gradient method, can find an ε accurate solution in O * min 1 ε , 1 √ λε iterations. Computationally, our smoothing technique is also particularly advantageous for optimizing multivariate performance scores such as precision/recall break-even point and ROCArea; the cost per iteration remains the same as that of CPMs. Empirical evaluation on some of the largest publicly available data sets shows that our method converges significantly faster than CPMs without sacrificing generalization ability."
            ],
            "keywords": [
                "non-smooth optimization",
                "max-margin methods",
                "multivariate performance measures",
                "Support Vector Machines",
                "smoothing"
            ],
            "author": [
                "Xinhua Zhang",
                "Ankan Saha",
                "S V N Vishwanathan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/zhang12d/zhang12d.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiple Kernel Learning Algorithms",
            "abstract": [
                "In recent years, several methods have been proposed to combine multiple kernels instead of using a single one. These different kernels may correspond to using different notions of similarity or may be using information coming from multiple sources (different representations or different feature subsets). In trying to organize and highlight the similarities and differences between them, we give a taxonomy of and review several multiple kernel learning algorithms. We perform experiments on real data sets for better illustration and comparison of existing algorithms. We see that though there may not be large differences in terms of accuracy, there is difference between them in complexity as given by the number of stored support vectors, the sparsity of the solution as given by the number of used kernels, and training time complexity. We see that overall, using multiple kernels instead of a single one is useful and believe that combining kernels in a nonlinear or data-dependent way seems more promising than linear combination in fusing information provided by simple linear kernels, whereas linear methods are more reasonable when combining complex Gaussian kernels."
            ],
            "keywords": [
                "support vector machines",
                "kernel machines",
                "multiple kernel learning"
            ],
            "author": [
                "Mehmet Gönen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/gonen11a/gonen11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimate Sequences for Stochastic Composite Optimization: Variance Reduction, Acceleration, and Robustness to Noise",
            "abstract": [
                "In this paper, we propose a unified view of gradient-based algorithms for stochastic convex composite optimization by extending the concept of estimate sequence introduced by Nesterov. More precisely, we interpret a large class of stochastic optimization methods as procedures that iteratively minimize a surrogate of the objective, which covers the stochastic gradient descent method and variants of the incremental approaches SAGA, SVRG, and MISO/Finito/SDCA. This point of view has several advantages: (i) we provide a simple generic proof of convergence for all of the aforementioned methods; (ii) we naturally obtain new algorithms with the same guarantees; (iii) we derive generic strategies to make these algorithms robust to stochastic noise, which is useful when data is corrupted by small random perturbations. Finally, we propose a new accelerated stochastic gradient descent algorithm and a new accelerated SVRG algorithm that is robust to stochastic noise."
            ],
            "keywords": [
                "convex optimization",
                "variance reduction",
                "stochastic optimization"
            ],
            "author": [
                "Andrei Kulunchakov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-073/19-073.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification",
            "abstract": [
                "Large-scale linear classification is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difficulties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we first broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efficient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data."
            ],
            "keywords": [
                "L1 regularization",
                "linear classification",
                "optimization methods",
                "logistic regression",
                "support vector machines",
                "document classification"
            ],
            "author": [
                "Guo-Xun Yuan",
                "Kai-Wei Chang",
                "Cho-Jui Hsieh",
                "Chih-Jen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/yuan10c/yuan10c.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Connecting Spectral Clustering to Maximum Margins and Level Sets",
            "abstract": [
                "We study the connections between spectral clustering and the problems of maximum margin clustering, and estimation of the components of level sets of a density function. Specifically, we obtain bounds on the eigenvectors of graph Laplacian matrices in terms of the between cluster separation, and within cluster connectivity. These bounds ensure that the spectral clustering solution converges to the maximum margin clustering solution as the scaling parameter is reduced towards zero. The sensitivity of maximum margin clustering solutions to outlying points is well known, but can be mitigated by first removing such outliers, and applying maximum margin clustering to the remaining points. If outliers are identified using an estimate of the underlying probability density, then the remaining points may be seen as an estimate of a level set of this density function. We show that such an approach can be used to consistently estimate the components of the level sets of a density function under very mild assumptions."
            ],
            "keywords": [
                "spectral clustering",
                "maximum margin clustering",
                "density clustering",
                "level sets",
                "convergence",
                "asymptotics",
                "consistency"
            ],
            "author": [
                "David P Hofmeyr"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-850/18-850.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Learning via Manifold Regularization",
            "abstract": [
                "This paper frames causal structure estimation as a machine learning task. The idea is to treat indicators of causal relationships between variables as 'labels' and to exploit available data on the variables of interest to provide features for the labelling task. Background scientific knowledge or any available interventional data provide labels on some causal relationships and the remainder are treated as unlabelled. To illustrate the key ideas, we develop a distance-based approach (based on bivariate histograms) within a manifold regularization framework. We present empirical results on three different biological data sets (including examples where causal effects can be verified by experimental intervention), that together demonstrate the efficacy and general nature of the approach as well as its simplicity from a user's point of view."
            ],
            "keywords": [
                "causal learning",
                "manifold regularization",
                "semi-supervised learning",
                "interventional data",
                "causal graphs"
            ],
            "author": [
                "Steven M Hill",
                "Chris J Oates",
                "Duncan A Blythe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-383/18-383.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Union Support Recovery in Multi-task Learning",
            "abstract": [
                "We sharply characterize the performance of different penalization schemes for the problem of selecting the relevant variables in the multi-task setting. Previous work focuses on the regression problem where conditions on the design matrix complicate the analysis. A clearer and simpler picture emerges by studying the Normal means model. This model, often used in the field of statistics, is a simplified model that provides a laboratory for studying complex procedures."
            ],
            "keywords": [
                "high-dimensional inference",
                "multi-task learning",
                "sparsity",
                "normal means",
                "minimax estimation"
            ],
            "author": [
                "Mladen Kolar",
                "John Lafferty",
                "Larry Wasserman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/kolar11a/kolar11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Confidence Intervals and Hypothesis Testing for High-Dimensional Regression",
            "abstract": [
                "Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or pvalues for these models. We consider here high-dimensional linear regression problem, and propose an efficient algorithm for constructing confidence intervals and p-values. The resulting confidence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. Our approach is based on constructing a 'de-biased' version of regularized M-estimators. The new construction improves over recent work in the field in that it does not assume a special structure on the design matrix. We test our method on synthetic data and a highthroughput genomic data set about riboflavin production rate, made publicly available by Bühlmann et al. (2014)."
            ],
            "keywords": [
                "hypothesis testing",
                "confidence intervals",
                "LASSO",
                "high-dimensional models",
                "bias of an estimator"
            ],
            "author": [
                "Adel Javanmard",
                "Andrea Montanari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/javanmard14a/javanmard14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Matrix Inversion with Scaled Lasso",
            "abstract": [
                "We propose a new method of learning a sparse nonnegative-definite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm first estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other ℓ 1 regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the ℓ 1 and spectrum norms of the target inverse matrix diverges to infinity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso."
            ],
            "keywords": [
                "precision matrix",
                "concentration matrix",
                "inverse matrix",
                "graphical model",
                "scaled Lasso",
                "linear regression",
                "spectrum norm"
            ],
            "author": [
                "Tingni Sun",
                "Cun-Hui Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/sun13a/sun13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Well-Conditioned and Sparse Estimation of Covariance and Inverse Covariance Matrices Using a Joint Penalty",
            "abstract": [
                "We develop a method for estimating well-conditioned and sparse covariance and inverse covariance matrices from a sample of vectors drawn from a sub-Gaussian distribution in high dimensional setting. The proposed estimators are obtained by minimizing the quadratic loss function and joint penalty of 1 norm and variance of its eigenvalues. In contrast to some of the existing methods of covariance and inverse covariance matrix estimation, where often the interest is to estimate a sparse matrix, the proposed method is flexible in estimating both a sparse and well-conditioned covariance matrix simultaneously. The proposed estimators are optimal in the sense that they achieve the mini-max rate of estimation in operator norm for the underlying class of covariance and inverse covariance matrices. We give a very fast algorithm for computation of these covariance and inverse covariance matrices which is easily scalable to large scale data analysis problems. The simulation study for varying sample sizes and variables shows that the proposed estimators performs better than several other estimators for various choices of structured covariance and inverse covariance matrices. We also use our proposed estimator for tumor tissues classification using gene expression data and compare its performance with some other classification methods."
            ],
            "keywords": [
                "Sparsity",
                "Eigenvalue Penalty",
                "Penalized Estimation"
            ],
            "author": [
                "Ashwini Maurya"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-345/15-345.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Introduction to Causal Inference",
            "abstract": [
                "The goal of many sciences is to understand the mechanisms by which variables came to take on the values they have (that is, to find a generative model), and to predict what the values of those variables would be if the naturally occurring mechanisms were subject to outside manipulations. The past 30 years has seen a number of conceptual developments that are partial solutions to the problem of causal inference from observational sample data or a mixture of observational sample and experimental data, particularly in the area of graphical causal modeling. However, in many domains, problems such as the large numbers of variables, small samples sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of these developments. The articles in the Special Topic on Causality address these and other problems in applying graphical causal modeling algorithms. This introduction to the Special Topic on Causality provides a brief introduction to graphical causal modeling, places the articles in a broader context, and describes the differences between causal inference and ordinary machine learning classification and prediction problems."
            ],
            "keywords": [
                "Bayesian networks",
                "causation",
                "causal inference"
            ],
            "author": [
                "Peter Spirtes"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/spirtes10a/spirtes10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "New Horn Revision Algorithms",
            "abstract": [
                "A revision algorithm is a learning algorithm that identifies the target concept, starting from an initial concept. Such an algorithm is considered efficient if its complexity (in terms of the measured resource) is polynomial in the syntactic distance between the initial and the target concept, but only polylogarithmic in the number of variables in the universe. We give efficient revision algorithms in the model of learning with equivalence and membership queries. The algorithms work in a general revision model where both deletion and addition revision operators are allowed. In this model one of the main open problems is the efficient revision of Horn formulas. Two revision algorithms are presented for special cases of this problem: for depth-1 acyclic Horn formulas, and for definite Horn formulas with unique heads."
            ],
            "keywords": [
                "theory revision",
                "Horn formulas",
                "query learning",
                "exact learning",
                "computational learning theory"
            ],
            "author": [
                "Judy Goldsmith",
                "Robert H Sloan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/goldsmith05a/goldsmith05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learnability of Solutions to Conjunctive Queries",
            "abstract": [
                "The problem of learning the solution space of an unknown formula has been studied in multiple embodiments in computational learning theory. In this article, we study a family of such learning problems; this family contains, for each relational structure, the problem of learning the solution space of an unknown conjunctive query evaluated on the structure. A progression of results aimed to classify the learnability of each of the problems in this family, and thus far a culmination thereof was a positive learnability result generalizing all previous ones. This article completes the classification program towards which this progression of results strived, by presenting a negative learnability result that complements the mentioned positive learnability result. In addition, a further negative learnability result is exhibited, which indicates a dichotomy within the problems to which the first negative result applies. In order to obtain our negative results, we make use of universal-algebraic concepts."
            ],
            "keywords": [
                "concept learning",
                "computational learning theory",
                "dichotomy theorems",
                "reductions",
                "universal algebra"
            ],
            "author": [
                "Hubie Chen",
                "Matthew Valeriote"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-734/17-734.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Refinement of Reproducing Kernels",
            "abstract": [
                "We continue our recent study on constructing a refinement kernel for a given kernel so that the reproducing kernel Hilbert space associated with the refinement kernel contains that with the original kernel as a subspace. To motivate this study, we first develop a refinement kernel method for learning, which gives an efficient algorithm for updating a learning predictor. Several characterizations of refinement kernels are then presented. It is shown that a nontrivial refinement kernel for a given kernel always exists if the input space has an infinite cardinal number. Refinement kernels for translation invariant kernels and Hilbert-Schmidt kernels are investigated. Various concrete examples are provided."
            ],
            "keywords": [
                "reproducing kernels",
                "reproducing kernel Hilbert spaces",
                "learning with kernels",
                "refinement kernels",
                "translation invariant kernels",
                "Hilbert-Schmidt kernels"
            ],
            "author": [
                "Yuesheng Xu",
                "Haizhang Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/xu09a/xu09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Robust-Equitable Measure for Feature Ranking and Selection",
            "abstract": [
                "In many applications, not all the features used to represent data samples are important. Often only a few features are relevant for the prediction task. The choice of dependence measures often affect the final result of many feature selection methods. To select features that have complex nonlinear relationships with the response variable, the dependence measure should be equitable, a concept proposed by Reshef et al. (2011); that is, the dependence measure treats linear and nonlinear relationships equally. Recently, Kinney and Atwal (2014) gave a mathematical definition of self-equitability. In this paper, we introduce a new concept of robust-equitability and identify a robust-equitable copula dependence measure, the robust copula dependence (RCD) measure. RCD is based on the L 1-distance of the copula density from uniform and we show that it is equitable under both equitability definitions. We also prove theoretically that RCD is much easier to estimate than mutual information. Because of these theoretical properties, the RCD measure has the following advantages compared to existing dependence measures: it is robust to different relationship forms and robust to unequal sample sizes of different features. Experiments on both synthetic and real-world data sets confirm the theoretical analysis, and illustrate the advantage of using the dependence measure RCD for feature selection."
            ],
            "keywords": [
                "dependence measure",
                "feature selection",
                "copula",
                "equitability",
                "mutual information"
            ],
            "author": [
                "A Adam Ding",
                "Jennifer G Dy",
                "Yi Li",
                "Yale Chang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-079/16-079.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Latent Variable Models by Pairwise Cluster Comparison: Part I − Theory and Overview",
            "abstract": [
                "Identification of latent variables that govern a problem and the relationships among them, given measurements in the observed world, are important for causal discovery. This identification can be accomplished by analyzing the constraints imposed by the latents in the measurements. We introduce the concept of pairwise cluster comparison (PCC) to identify causal relationships from clusters of data points and provide a two-stage algorithm called learning PCC (LPCC) that learns a latent variable model (LVM) using PCC. First, LPCC learns exogenous latents and latent colliders, as well as their observed descendants, by using pairwise comparisons between data clusters in the measurement space that may explain latent causes. Since in this first stage LPCC cannot distinguish endogenous latent non-colliders from their exogenous ancestors, a second stage is needed to extract the former, with their observed children, from the latter. If the true graph has no serial connections, LPCC returns the true graph, and if the true graph has a serial connection, LPCC returns a pattern of the true graph. LPCC's most important advantage is that it is not limited to linear or latent-tree models and makes only mild assumptions about the distribution. The paper is divided in two parts: Part I (this paper) provides the necessary preliminaries, theoretical foundation to PCC, and an overview of LPCC; Part II formally introduces the LPCC algorithm and experimentally evaluates its merit in different synthetic and real domains. The code for the LPCC algorithm and data sets used in the experiments reported in Part II are available online."
            ],
            "keywords": [
                "causal discovery",
                "clustering",
                "learning latent variable model",
                "multiple indicator model",
                "pure measurement model"
            ],
            "author": [
                "Nuaman Asbeh",
                "Boaz Lerner",
                "Isabelle Guyon",
                "Alexander Statnikov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-401/14-401.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling",
            "abstract": [
                "We present a unified and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including 1 , 2 , 2 2 , and 1 + 2 2 style regularization. We propose an algorithm solving a large and general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and unifies techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models."
            ],
            "keywords": [
                "maximum entropy",
                "density estimation",
                "regularization",
                "iterative scaling",
                "species distribution modeling"
            ],
            "author": [
                "Miroslav Dudík",
                "Steven J Phillips",
                "Robert E Schapire"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/dudik07a/dudik07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistent Feature Selection for Pattern Recognition in Polynomial Time",
            "abstract": [
                "We analyze two different feature selection problems: finding a minimal feature set optimal for classification (MINIMAL-OPTIMAL) vs. finding all features relevant to the target variable (ALL-RELEVANT). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL-RELEVANT is much harder than MINIMAL-OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks."
            ],
            "keywords": [
                "learning theory",
                "relevance",
                "classification",
                "Markov blanket",
                "bioinformatics"
            ],
            "author": [
                "Roland Nilsson",
                "José M Peña",
                "Johan Björkegren"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/nilsson07a/nilsson07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning to Match via Inverse Optimal Transport",
            "abstract": [
                "We propose a unified data-driven framework based on inverse optimal transport that can learn adaptive, nonlinear interaction cost function from noisy and incomplete empirical matching matrix and predict new matching in various matching contexts. We emphasize that the discrete optimal transport plays the role of a variational principle which gives rise to an optimization based framework for modeling the observed empirical matching data. Our formulation leads to a non-convex optimization problem which can be solved efficiently by an alternating optimization method. A key novel aspect of our formulation is the incorporation of marginal relaxation via regularized Wasserstein distance, significantly improving the robustness of the method in the face of noisy or missing empirical matching data. Our model falls into the category of prescriptive models, which not only predict potential future matching, but is also able to explain what leads to empirical matching and quantifies the impact of changes in matching factors. The proposed approach has wide applicability including predicting matching in online dating, labor market, college application and crowdsourcing. We back up our claims with numerical experiments on both synthetic data and real world data sets."
            ],
            "keywords": [
                "Matching",
                "Inverse Problem",
                "Optimal Transport",
                "Robustification",
                "Variational Inference"
            ],
            "author": [
                "Ruilin Li",
                "Xiaojing Ye",
                "Haomin Zhou",
                "Hongyuan Zha"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-700/18-700.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Harmless Overfitting: Using Denoising Autoencoders in Estimation of Distribution Algorithms",
            "abstract": [
                "Estimation of Distribution Algorithms (EDAs) are metaheuristics where learning a model and sampling new solutions replaces the variation operators recombination and mutation used in standard Genetic Algorithms. The choice of these models as well as the corresponding training processes are subject to the bias/variance tradeoff, also known as under-and overfitting: simple models cannot capture complex interactions between problem variables, whereas complex models are susceptible to modeling random noise. This paper suggests using Denoising Autoencoders (DAEs) as generative models within EDAs (DAE-EDA). The resulting DAE-EDA is able to model complex probability distributions. Furthermore, overfitting is less harmful, since DAEs overfit by learning the identity function. This overfitting behavior introduces unbiased random noise into the samples, which is no major problem for the EDA but just leads to higher population diversity. As a result, DAE-EDA runs for more generations before convergence and searches promising parts of the solution space more thoroughly. We study the performance of DAE-EDA on several combinatorial single-objective optimization problems. In comparison to the Bayesian Optimization Algorithm, DAE-EDA requires a similar number of evaluations of the objective function but is much faster and can be parallelized efficiently, making it the preferred choice especially for large and difficult optimization problems."
            ],
            "keywords": [
                "denoising autoencoder",
                "estimation of distribution algorithm",
                "overfitting",
                "combinatorial optimization",
                "neural networks"
            ],
            "author": [
                "Malte Probst",
                "Franz Rothlauf"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/16-543/16-543.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Graphical Models With Hubs",
            "abstract": [
                "We consider the problem of learning a high-dimensional graphical model in which there are a few hub nodes that are densely-connected to many other nodes. Many authors have studied the use of an 1 penalty in order to learn a sparse graph in the high-dimensional setting. However, the 1 penalty implicitly assumes that each edge is equally likely and independent of all other edges. We propose a general framework to accommodate more realistic networks with hub nodes, using a convex formulation that involves a row-column overlap norm penalty. We apply this general framework to three widely-used probabilistic graphical models: the Gaussian graphical model, the covariance graph model, and the binary Ising model. An alternating direction method of multipliers algorithm is used to solve the corresponding convex optimization problems. On synthetic data, we demonstrate that our proposed framework outperforms competitors that do not explicitly model hub nodes. We illustrate our proposal on a webpage data set and a gene expression data set."
            ],
            "keywords": [
                "Gaussian graphical model",
                "covariance graph",
                "binary network",
                "lasso",
                "hub",
                "alternating direction method of multipliers"
            ],
            "author": [
                "Ming Kean",
                "Palma London",
                "Karthik Mohan",
                "Maryam Fazel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/tan14b/tan14b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Seglearn: A Python Package for Learning Sequences and Time Series",
            "abstract": [
                "seglearn is an open-source Python package for performing machine learning on time series or sequences. The implementation provides a flexible pipeline for tackling classification, regression, and forecasting problems with multivariate sequence and contextual data. Sequences and series may be learned directly with deep learning models or via feature representation with classical machine learning estimators. This package is compatible with scikit-learn and is listed under scikit-learn \"Related Projects\". The package depends on numpy, scipy, and scikit-learn. seglearn is distributed under the BSD 3-Clause License. Documentation includes a detailed API description, user guide, and examples. Unit tests provide a high degree of code coverage. Source code and documentation can be downloaded from https://github.com/dmbee/seglearn."
            ],
            "keywords": [
                "Machine-Learning",
                "Time-Series",
                "Sequences",
                "Python"
            ],
            "author": [
                "David M Burns",
                "Cari M Whyne"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-160/18-160.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Inference for Computational Imaging Inverse Problems",
            "abstract": [
                "Machine learning methods for computational imaging require uncertainty estimation to be reliable in real settings. While Bayesian models offer a computationally tractable way of recovering uncertainty, they need large data volumes to be trained, which in imaging applications implicates prohibitively expensive collections with specific imaging instruments. This paper introduces a novel framework to train variational inference for inverse problems exploiting in combination few experimentally collected data, domain expertise and existing image data sets. In such a way, Bayesian machine learning models can solve imaging inverse problems with minimal data collection efforts. Extensive simulated experiments show the advantages of the proposed framework. The approach is then applied to two real experimental optics settings: holographic image reconstruction and imaging through highly scattering media. In both settings, state of the art reconstructions are achieved with little collection of training data."
            ],
            "keywords": [
                "Inverse Problems",
                "Approximate Inference",
                "Bayesian Inference",
                "Computational Imaging"
            ],
            "author": [
                "Francesco Tonolini",
                "Jack Radford",
                "Alex Turpin",
                "Daniele Faccio",
                "Roderick Murray-Smith"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-151/20-151.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Segregating Event Streams and Noise with a Markov Renewal Process Model",
            "abstract": [
                "We describe an inference task in which a set of timestamped event observations must be clustered into an unknown number of temporal sequences with independent and varying rates of observations. Various existing approaches to multi-object tracking assume a fixed number of sources and/or a fixed observation rate; we develop an approach to inferring structure in timestamped data produced by a mixture of an unknown and varying number of similar Markov renewal processes, plus independent clutter noise. The inference simultaneously distinguishes signal from noise as well as clustering signal observations into separate source streams. We illustrate the technique via synthetic experiments as well as an experiment to track a mixture of singing birds. Source code is available."
            ],
            "keywords": [
                "multi-target tracking",
                "clustering",
                "point processes",
                "flow network",
                "sound"
            ],
            "author": [
                "Dan Stowell",
                "Mark D Plumbley"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/stowell13a/stowell13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Electronic Health Record Analysis via Deep Poisson Factor Models",
            "abstract": [
                "Electronic Health Record (EHR) phenotyping utilizes patient data captured through normal medical practice, to identify features that may represent computational medical phenotypes. These features may be used to identify at-risk patients and improve prediction of patient morbidity and mortality. We present a novel deep multi-modality architecture for EHR analysis (applicable to joint analysis of multiple forms of EHR data), based on Poisson Factor Analysis (PFA) modules. Each modality, composed of observed counts, is represented as a Poisson distribution, parameterized in terms of hidden binary units. Information from different modalities is shared via a deep hierarchy of common hidden units. Activation of these binary units occurs with probability characterized as Bernoulli-Poisson link functions, instead of more traditional logistic link functions. In addition, we demonstrate that PFA modules can be adapted to discriminative modalities. To compute model parameters, we derive efficient Markov Chain Monte Carlo (MCMC) inference that scales efficiently, with significant computational gains when compared to related models based on logistic link functions. To explore the utility of these models, we apply them to a subset of patients from the Duke-Durham patient cohort. We identified a cohort of over 16,000 patients with Type 2 Diabetes Mellitus (T2DM) based on diagnosis codes and laboratory tests out of our patient population of over 240,000. Examining the common hidden units uniting the PFA modules, we identify patient features that represent medical concepts. Experiments indicate that our learned features are better able to predict mortality and morbidity than clinical features identified previously in a large-scale clinical trial."
            ],
            "keywords": [
                "Deep learning",
                "multi-modality learning",
                "Poisson factor model",
                "electronic health records",
                "phenotyping"
            ],
            "author": [
                "Ricardo Henao",
                "James T Lu",
                "Joseph E Lucas",
                "Jeffrey Ferranti",
                "Lawrence Carin",
                "Lu, Lucas, Ferranti Lawrence Carin Henao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-429/15-429.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Agnostic Estimation for Misspecified Phase Retrieval Models",
            "abstract": [
                "The goal of noisy high-dimensional phase retrieval is to estimate an s-sparse parameter β * ∈ R d from n realizations of the model Y = (X β *) 2 + ε. Based on this model, we propose a significant semi-parametric generalization called misspecified phase retrieval (MPR), in which Y = f (X β * , ε) with unknown f and Cov(Y, (X β *) 2) > 0. For example, MPR encompasses Y = h(|X β * |) + ε with increasing h as a special case. Despite the generality of the MPR model, it eludes the reach of most existing semi-parametric estimators. In this paper, we propose an estimation procedure, which consists of solving a cascade of two convex programs and provably recovers the direction of β *. Furthermore, we prove that our procedure is minimax optimal over the class of MPR models. Interestingly, our minimax analysis characterizes the statistical price of misspecifying the link function in phase retrieval models. Our theory is backed up by thorough numerical results."
            ],
            "keywords": [],
            "author": [
                "Zhaoran Wang",
                "Han Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-259/18-259.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Evolving Static Representations for Task Transfer",
            "abstract": [
                "An important goal for machine learning is to transfer knowledge between tasks. For example, learning to play RoboCup Keepaway should contribute to learning the full game of RoboCup soccer. Previous approaches to transfer in Keepaway have focused on transforming the original representation to fit the new task. In contrast, this paper explores the idea that transfer is most effective if the representation is designed to be the same even across different tasks. To demonstrate this point, a bird's eye view (BEV) representation is introduced that can represent different tasks on the same two-dimensional map. For example, both the 3 vs. 2 and 4 vs. 3 Keepaway tasks can be represented on the same BEV. Yet the problem is that a raw two-dimensional map is high-dimensional and unstructured. This paper shows how this problem is addressed naturally by an idea from evolutionary computation called indirect encoding, which compresses the representation by exploiting its geometry. The result is that the BEV learns a Keepaway policy that transfers without further learning or manipulation. It also facilitates transferring knowledge learned in a different domain, Knight Joust, into Keepaway. Finally, the indirect encoding of the BEV means that its geometry can be changed without altering the solution. Thus static representations facilitate several kinds of transfer."
            ],
            "keywords": [
                "transfer learning",
                "task transfer",
                "evolutionary computation",
                "neuroevolution",
                "indirect encoding"
            ],
            "author": [
                "Phillip Verbancsics",
                "Kenneth O Stanley"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/verbancsics10a/verbancsics10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "CrossCat: A Fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data",
            "abstract": [
                "There is a widespread need for statistical methods that can analyze high-dimensional datasets without imposing restrictive or opaque modeling assumptions. This paper describes a domain-general data analysis method called CrossCat. CrossCat infers multiple non-overlapping views of the data, each consisting of a subset of the variables, and uses a separate nonparametric mixture to model each view. CrossCat is based on approximately Bayesian inference in a hierarchical, nonparametric model for data tables. This model consists of a Dirichlet process mixture over the columns of a data table in which each mixture component is itself an independent Dirichlet process mixture over the rows; the inner mixture components are simple parametric models whose form depends on the types of data in the table. CrossCat combines strengths of mixture modeling and Bayesian network structure learning. Like mixture modeling, CrossCat can model a broad class of distributions by positing latent variables, and produces representations that can be efficiently conditioned and sampled from for prediction. Like Bayesian networks, CrossCat represents the dependencies and independencies between variables, and thus remains accurate when there are multiple statistical signals. Inference is done via a scalable Gibbs sampling scheme; this paper shows that it works well in practice. This paper also includes empirical results on heterogeneous tabular data of up to 10 million cells, such as hospital cost and quality measures, voting records, unemployment rates, gene expression measurements, and images of handwritten digits. CrossCat infers structure that is consistent with accepted findings and common-sense knowledge in multiple domains and yields predictive accuracy competitive with generative, discriminative, and model-free alternatives."
            ],
            "keywords": [
                "Bayesian nonparametrics",
                "Dirichlet processes",
                "Markov chain Monte Carlo",
                "multivariate analysis",
                "structure learning",
                "unsupervised learning",
                "semi-supervised learning"
            ],
            "author": [
                "Vikash Mansinghka",
                "Patrick Shafto",
                "Eric Jonas",
                "Max Gasner",
                "Joshua B Tenenbaum"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/11-392/11-392.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ensemble Pruning Via Semi-definite Programming",
            "abstract": [
                "An ensemble is a group of learning models that jointly solve a problem. However, the ensembles generated by existing techniques are sometimes unnecessarily large, which can lead to extra memory usage, computational costs, and occasional decreases in effectiveness. The purpose of ensemble pruning is to search for a good subset of ensemble members that performs as well as, or better than, the original ensemble. This subset selection problem is a combinatorial optimization problem and thus finding the exact optimal solution is computationally prohibitive. Various heuristic methods have been developed to obtain an approximate solution. However, most of the existing heuristics use simple greedy search as the optimization method, which lacks either theoretical or empirical quality guarantees. In this paper, the ensemble subset selection problem is formulated as a quadratic integer programming problem. By applying semi-definite programming (SDP) as a solution technique, we are able to get better approximate solutions. Computational experiments show that this SDP-based pruning algorithm outperforms other heuristics in the literature. Its application in a classifier-sharing study also demonstrates the effectiveness of the method."
            ],
            "keywords": [
                "ensemble pruning",
                "semi-definite programming",
                "heuristics",
                "knowledge sharing"
            ],
            "author": [
                "Yi Zhang",
                "Samuel Burer",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/zhang06a/zhang06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "One-shot Learning Gesture Recognition from RGB-D Data Using Bag of Features",
            "abstract": [
                "For one-shot learning gesture recognition, two important challenges are: how to extract distinctive features and how to learn a discriminative model from only one training sample per gesture class. For feature extraction, a new spatio-temporal feature representation called 3D enhanced motion scale-invariant feature transform (3D EMoSIFT) is proposed, which fuses RGB-D data. Compared with other features, the new feature set is invariant to scale and rotation, and has more compact and richer visual representations. For learning a discriminative model, all features extracted from training samples are clustered with the k-means algorithm to learn a visual codebook. Then, unlike the traditional bag of feature (BoF) models using vector quantization (VQ) to map each feature into a certain visual codeword, a sparse coding method named simulation orthogonal matching pursuit (SOMP) is applied and thus each feature can be represented by some linear combination of a small number of codewords. Compared with VQ, SOMP leads to a much lower reconstruction error and achieves better performance. The proposed approach has been evaluated on ChaLearn gesture database and the result has been ranked amongst the top best performing techniques on ChaLearn gesture challenge (round 2)."
            ],
            "keywords": [
                "gesture recognition",
                "bag of features (BoF) model",
                "one-shot learning",
                "3D enhanced motion scale invariant feature transform (3D EMoSIFT)",
                "Simulation Orthogonal Matching Pursuit (SOMP)"
            ],
            "author": [
                "Jun Wan",
                "Wei Li",
                "Shuang Deng",
                "Isabelle Guyon",
                "Vassilis Athitsos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/wan13a/wan13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The LRP Toolbox for Artificial Neural Networks",
            "abstract": [
                "The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. With the LRP Toolbox we provide platform-agnostic implementations for explaining the predictions of pre-trained state of the art Caffe networks and stand-alone implementations for fully connected Neural Network models. The implementations for Matlab and python shall serve as a playing field to familiarize oneself with the LRP algorithm and are implemented with readability and transparency in mind. Models and data can be imported and exported using raw text formats, Matlab's .mat files and the .npy format for numpy or plain text."
            ],
            "keywords": [
                "layer-wise relevance propagation",
                "explaining classifiers",
                "deep learning",
                "artificial neural networks",
                "computer vision"
            ],
            "author": [
                "Sebastian Lapuschkin",
                "Alexander Binder",
                "Grégoire Montavon",
                "Klaus- Robert Müller",
                "Wojciech Samek"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-618/15-618.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral bandits",
            "abstract": [
                "Smooth functions on graphs have wide applications in manifold and semi-supervised learning. In this work, we study a bandit problem where the payoffs of arms are smooth on a graph. This framework is suitable for solving online learning problems that involve graphs, such as content-based recommendation. In this problem, each item we can recommend is a node of an undirected graph and its expected rating is similar to the one of its neighbors. The goal is to recommend items that have high expected ratings. We aim for the algorithms where the cumulative regret with respect to the optimal policy would not scale poorly with the number of nodes. In particular, we introduce the notion of an effective dimension, which is small in real-world graphs, and propose three algorithms for solving our problem that scale linearly and sublinearly in this dimension. Our experiments on content recommendation problem show that a good estimator of user preferences for thousands of items can be learned from just tens of node evaluations. * also affiliated with Inria Lille-Nord Europe, SequeL team"
            ],
            "keywords": [],
            "author": [
                "Tomáš Kocák",
                "Rémi Munos",
                "Branislav Kveton",
                "Michal Valko"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/16-529/16-529.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Addressing Environment Non-Stationarity by Repeating Q-learning Updates *",
            "abstract": [
                "Q-learning (QL) is a popular reinforcement learning algorithm that is guaranteed to converge to optimal policies in Markov decision processes. However, QL exhibits an artifact: in expectation, the effective rate of updating the value of an action depends on the probability of choosing that action. In other words, there is a tight coupling between the learning dynamics and underlying execution policy. This coupling can cause performance degradation in noisy non-stationary environments. Here, we introduce Repeated Update Q-learning (RUQL), a learning algorithm that resolves the undesirable artifact of Q-learning while maintaining simplicity. We analyze the similarities and differences between RUQL, QL, and the closest state-of-the-art algorithms theoretically. Our analysis shows that RUQL maintains the convergence guarantee of QL in stationary environments, while relaxing the coupling between the execution policy and the learning dynamics. Experimental results confirm the theoretical insights and show how RUQL outperforms both QL and the closest state-of-the-art algorithms in noisy non-stationary environments."
            ],
            "keywords": [
                "reinforcement learning",
                "Q-learning",
                "multi-agent learning",
                "non-stationary environments"
            ],
            "author": [
                "Sherief Abdallah",
                "Michael Kaisers"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-037/14-037.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gaussian Kullback-Leibler Approximate Inference",
            "abstract": [
                "We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufficient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design."
            ],
            "keywords": [
                "generalised linear models",
                "latent linear models",
                "variational approximate inference",
                "large scale inference",
                "sparse learning",
                "experimental design",
                "active learning",
                "Gaussian processes"
            ],
            "author": [
                "Edward Challis",
                "David Barber"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/challis13a/challis13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent",
            "abstract": [
                "The SGD-QN algorithm is a stochastic gradient descent algorithm that makes careful use of secondorder information and splits the parameter update into independently scheduled components. Thanks to this design, SGD-QN iterates nearly as fast as a first-order stochastic gradient descent but requires less iterations to achieve the same accuracy. This algorithm won the \"Wild Track\" of the first PAS"
            ],
            "keywords": [
                "Soeren Sonnenburg",
                "Vojtech Franc",
                "Elad Yom-Tov and Michele Sebag support vector machine",
                "stochastic gradient descent"
            ],
            "author": [
                "Antoine Bordes",
                "Léon Bottou",
                "Patrick Gallinari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/bordes09a/bordes09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "L 1 -Regularized Least Squares for Support Recovery of High Dimensional Single Index Models with Gaussian Designs",
            "abstract": [
                "It is known that for a certain class of single index models (SIMs) Y = f (X p×1 β 0 , ε), support recovery is impossible when X ∼ N (0, I p×p) and a model complexity adjusted sample size is below a critical threshold. Recently, optimal algorithms based on Sliced Inverse Regression (SIR) were suggested. These algorithms work provably under the assumption that the design X comes from an i.i.d. Gaussian distribution. In the present paper we analyze algorithms based on covariance screening and least squares with L 1 penalization (i.e. LASSO) and demonstrate that they can also enjoy optimal (up to a scalar) rescaled sample size in terms of support recovery, albeit under slightly different assumptions on f and ε compared to the SIR based algorithms. Furthermore, we show more generally, that LASSO succeeds in recovering the signed support of β 0 if X ∼ N (0, Σ), and the covariance Σ satisfies the irrepresentable condition. Our work extends existing results on the support recovery of LASSO for the linear model, to a more general class of SIMs."
            ],
            "keywords": [
                "Single index models",
                "Sparsity",
                "Support recovery",
                "High-dimensional statistics",
                "LASSO"
            ],
            "author": [
                "Jun S Liu",
                "Tianxi Cai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-006/16-006.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Neural Probabilistic Language Model",
            "abstract": [
                "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts."
            ],
            "keywords": [
                "Statistical language modeling",
                "artificial neural networks",
                "distributed representation",
                "curse of dimensionality"
            ],
            "author": [
                "Yoshua Bengio",
                "Réjean Ducharme",
                "Pascal Vincent",
                "Christian Jauvin",
                "Jaz Kandola",
                "Thomas Hofmann",
                "Tomaso Poggio",
                "John Shawe-Taylor"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Rates in Permutation Estimation for Feature Matching",
            "abstract": [
                "The problem of matching two sets of features appears in various tasks of computer vision and can be often formalized as a problem of permutation estimation. We address this problem from a statistical point of view and provide a theoretical analysis of the accuracy of several natural estimators. To this end, the minimax rate of separation is investigated and its expression is obtained as a function of the sample size, noise level and dimension of the features. We consider the cases of homoscedastic and heteroscedastic noise and establish, in each case, tight upper bounds on the separation distance of several estimators. These upper bounds are shown to be unimprovable both in the homoscedastic and heteroscedastic settings. Interestingly, these bounds demonstrate that a phase transition occurs when the dimension d of the features is of the order of the logarithm of the number of features n. For d = O(log n), the rate is dimension free and equals σ(log n) 1/2 , where σ is the noise level. In contrast, when d is larger than c log n for some constant c > 0, the minimax rate increases with d and is of the order of σ(d log n) 1/4. We also discuss the computational aspects of the estimators and provide empirical evidence of their consistency on synthetic data. Finally, we show that our results extend to more general matching criteria."
            ],
            "keywords": [
                "permutation estimation",
                "minimax rate of separation",
                "feature matching"
            ],
            "author": [
                "Olivier Collier",
                "Arnak S Dalalyan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/collier16a/collier16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unified q-Memorization Framework for Asynchronous Stochastic Optimization",
            "abstract": [
                "Asynchronous stochastic algorithms with various variance reduction techniques (such as SVRG, S2GD, SAGA and q-SAGA) are popular in solving large scale learning problems. Recently, Reddi et al. (2015) proposed an unified variance reduction framework (i.e., HSAG) to analyze the asynchronous stochastic gradient optimization. However, the HSAG framework cannot incorporate the S2GD technique, the analysis of the HSAG framework is limited to the SVRG and SAGA techniques on the smooth convex optimization. They did not analyze other important various variance techniques (e.g., S2GD and q-SAGA) and other important optimization problems (e.g., convex optimization with non-smooth regularization and non-convex optimization with cardinality constraint). In this paper, we bridge this gap by using an unified q-memorization framework for various variance reduction techniques (including SVRG, S2GD, SAGA, q-SAGA) to analyze asynchronous stochastic algorithms for three important optimization problems. Specifically, based on the q-memorization framework, 1) we propose an asynchronous stochastic gradient hard thresholding algorithm with q-memorization (AsySGHT-qM) for the non-convex optimization with cardinality constraint, and prove that the convergence rate of AsySGHT-qM before reaching the inherent error induced by gradient hard thresholding methods is geometric. 2) We propose an asynchronous stochastic proximal gradient algorithm (AsySPG-qM) for the convex optimization with non-smooth regularization, and prove that AsySPG-qM can achieve a linear convergence rate. 3) We propose an asynchronous stochastic gradient descent algorithm (AsySGD-qM) for the general non-convex optimization problem, and prove that AsySGD-qM can achieve a sublinear convergence rate to stationary points. The experimental results on various large-scale datasets confirm the fast convergence of our AsySGHT-qM, AsySPG-qM and AsySGD-qM through concrete realizations of SVRG and SAGA."
            ],
            "keywords": [
                "Stochastic optimization",
                "q-memorization",
                "asynchronous parallel computing",
                "variance reduction",
                "proximal operator",
                "hard thresholding"
            ],
            "author": [
                "Bin Gu",
                "Wenhan Xian",
                "Zhouyuan Huo",
                "Cheng Deng",
                "Heng Huang",
                "Bin ©2020",
                "Wenhan Gu",
                "Zhouyuan Xian",
                "Cheng Huo",
                "Heng Deng",
                "Xian, Huo, Deng Huang Gu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-434/18-434.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bounding the Probability of Error for High Precision Optical Character Recognition",
            "abstract": [
                "We consider a model for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low recall. If some variables can be identified with near certainty, they can be conditioned upon, allowing further inference to be done efficiently. Specifically, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. This \"clean set\" is subsequently used as document-specific training data. While OCR systems produce confidence measures for the identity of each letter or word, thresholding these values still produces a significant number of errors. We introduce a novel technique for identifying a set of correct words with very high precision. Rather than estimating posterior probabilities, we bound the probability that any given word is incorrect using an approximate worst case analysis. We give empirical results on a data set of difficult historical newspaper scans, demonstrating that our method for identifying correct words makes only two errors in 56 documents. Using document-specific character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1% from an initial OCR system's translation. 1"
            ],
            "keywords": [
                "optical character recognition",
                "probability bounding",
                "document-specific modeling",
                "computer vision"
            ],
            "author": [
                "Gary B Huang",
                "Andrew Kae",
                "Carl Doersch"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/huang12a/huang12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Partial Policies to Speedup MDP Tree Search via Reduction to I.I.D. Learning",
            "abstract": [
                "A popular approach for online decision-making in large MDPs is time-bounded tree search. The effectiveness of tree search, however, is largely influenced by the action branching factor, which limits the search depth given a time bound. An obvious way to reduce action branching is to consider only a subset of potentially good actions at each state as specified by a provided partial policy. In this work, we consider offline learning of such partial policies with the goal of speeding up search without significantly reducing decision-making quality. Our first contribution consists of reducing the learning problem to set learning. We give a reduction-style analysis of three such algorithms, each making different assumptions, which relates the set learning objectives to the sub-optimality of search using the learned partial policies. Our second contribution is to describe concrete implementations of the algorithms within the popular framework of Monte-Carlo tree search. Finally, the third contribution is to evaluate the learning algorithms on two challenging MDPs with large action branching factors. The results show that the learned partial policies can significantly improve the anytime performance of Monte-Carlo tree search."
            ],
            "keywords": [
                "online sequential decision-making",
                "partial policy",
                "partial policy learning",
                "imitation learning",
                "Monte-Carlo tree search",
                "reductions"
            ],
            "author": [
                "Jervis Pinto",
                "Alan Fern"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-251/15-251.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques",
            "abstract": [
                "We introduce novel online Bayesian methods for the identification of a family of noisy recurrent neural networks (RNNs). We present Bayesian active learning techniques for stimulus selection given past experiences. In particular, we consider the unknown parameters as stochastic variables and use A-optimality and D-optimality principles to choose optimal stimuli. We derive myopic cost functions in order to maximize the information gain concerning network parameters at each time step. We also derive the A-optimal and D-optimal estimations of the additive noise that perturbs the dynamical system of the RNN. Here we investigate myopic as well as non-myopic estimations, and study the problem of simultaneous estimation of both the system parameters and the noise. Employing conjugate priors our derivations remain approximation-free and give rise to simple update rules for the online learning of the parameters. The efficiency of our method is demonstrated for a number of selected cases, including the task of controlled independent component analysis."
            ],
            "keywords": [
                "active learning",
                "system identification",
                "online Bayesian learning",
                "A-optimality",
                "Doptimality",
                "infomax control",
                "optimal design"
            ],
            "author": [
                "Barnabás Póczos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/poczos09a/poczos09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Training Energy-Based Models for Time-Series Imputation",
            "abstract": [
                "Imputing missing values in high dimensional time-series is a difficult problem. This paper presents a strategy for training energy-based graphical models for imputation directly, bypassing difficulties probabilistic approaches would face. The training strategy is inspired by recent work on optimization-based learning (Domke, 2012) and allows complex neural models with convolutional and recurrent structures to be trained for imputation tasks. In this work, we use this training strategy to derive learning rules for three substantially different neural architectures. Inference in these models is done by either truncated gradient descent or variational mean-field iterations. In our experiments, we found that the training methods outperform the Contrastive Divergence learning algorithm. Moreover, the training methods can easily handle missing values in the training data itself during learning. We demonstrate the performance of this learning scheme and the three models we introduce on one artificial and two real-world data sets."
            ],
            "keywords": [
                "neural networks",
                "energy-based models",
                "time-series",
                "missing values",
                "optimization"
            ],
            "author": [
                "Philémon Brakel",
                "Dirk Stroobandt",
                "Benjamin Schrauwen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/brakel13a/brakel13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Why do deep convolutional networks generalize so poorly to small image transformations?",
            "abstract": [
                "Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network's prediction. In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are sufficient to achieve the desired invariance. Specifically, we show that the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem, and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set. We discuss two possible solutions to this problem: (1) antialiasing the intermediate representations and (2) increasing data augmentation and show that they provide only a partial solution at best. Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved."
            ],
            "keywords": [
                "Machine Learning",
                "Deep Convolutional Neural Networks",
                "Generalization"
            ],
            "author": [
                "Aharon Azulay",
                "Yair Weiss"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-519/19-519.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Targeted Fused Ridge Estimation of Inverse Covariance Matrices from Multiple High-Dimensional Data Classes",
            "abstract": [
                "We consider the problem of jointly estimating multiple inverse covariance matrices from high-dimensional data consisting of distinct classes. An 2-penalized maximum likelihood approach is employed. The suggested approach is flexible and generic, incorporating several other 2-penalized estimators as special cases. In addition, the approach allows specification of target matrices through which prior knowledge may be incorporated and which can stabilize the estimation procedure in high-dimensional settings. The result is a targeted. Shared first authorship."
            ],
            "keywords": [
                "differential network estimation",
                "Gaussian graphical modeling",
                "generalized fused ridge",
                "high-dimensional data",
                "2 -penalized maximum likelihood",
                "structural metaanalysis"
            ],
            "author": [
                "Anders Ellern Bilgrau",
                "Aalborg Ø Denmark",
                "Carel F W Peeters",
                "Poul Svante Eriksen",
                "Martin Bøgsted",
                "Wessel N Van Wieringen",
                "F W Peeters",
                "Poul Svante Eriksen",
                "Wessel N Van"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/15-509/15-509.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An asymptotic analysis of distributed nonparametric methods",
            "abstract": [
                "We investigate and compare the fundamental performance of several distributed learning methods that have been proposed recently. We do this in the context of a distributed version of the classical signal-in-Gaussian-white-noise model, which serves as a benchmark model for studying performance in this setting. The results show how the design and tuning of a distributed method can have great impact on convergence rates and validity of uncertainty quantification. Moreover, we highlight the difficulty of designing nonparametric distributed procedures that automatically adapt to smoothness."
            ],
            "keywords": [
                "distributed learning",
                "nonparametric models",
                "high-dimensional models",
                "Gaussian processes",
                "convergence rates"
            ],
            "author": [
                "Botond Szabó",
                "Harry Van Zanten"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-666/17-666.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Breaking the Curse of Dimensionality with Convex Neural Networks",
            "abstract": [
                "We consider neural networks with a single hidden layer and non-decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the non-convex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non-convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomialtime algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities."
            ],
            "keywords": [
                "Neural networks",
                "non-parametric estimation",
                "convex optimization",
                "convex relaxation"
            ],
            "author": [
                "Francis Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-546/14-546.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Characterizing the Function Space for Bayesian Kernel Models",
            "abstract": [
                "Kernel methods have been very popular in the machine learning literature in the last ten years, mainly in the context of Tikhonov regularization algorithms. In this paper we study a coherent Bayesian kernel model based on an integral operator defined as the convolution of a kernel with a signed measure. Priors on the random signed measures correspond to prior distributions on the functions mapped by the integral operator. We study several classes of signed measures and their image mapped by the integral operator. In particular, we identify a general class of measures whose image is dense in the reproducing kernel Hilbert space (RKHS) induced by the kernel. A consequence of this result is a function theoretic foundation for using non-parametric prior specifications in Bayesian modeling, such as Gaussian process and Dirichlet process prior distributions. We discuss the construction of priors on spaces of signed measures using Gaussian and Lévy processes, with the Dirichlet processes being a special case the latter. Computational issues involved with sampling from the posterior distribution are outlined for a univariate regression and a high dimensional classification problem."
            ],
            "keywords": [
                "reproducing kernel Hilbert space",
                "non-parametric Bayesian methods",
                "Lévy processes",
                "Dirichlet processes",
                "integral operator",
                "Gaussian processes"
            ],
            "author": [
                "Natesh S Pillai",
                "Qiang Wu",
                "Feng Liang",
                "Sayan Mukherjee",
                "Robert L Wolpert",
                "S Pillai",
                "WU, LIANG, MUKHERJEE AND WOLPERT Pillai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/pillai07a/pillai07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Can We Trust the Bootstrap in High-dimensions? The Case of Linear Models",
            "abstract": [
                "We consider the performance of the bootstrap in high-dimensions for the setting of linear regression, where p < n but p/n is not close to zero. We consider ordinary least-squares as well as robust regression methods and adopt a minimalist performance requirement: can the bootstrap give us good confidence intervals for a single coordinate of β (where β is the true regression vector)? We show through a mix of numerical and theoretical work that the bootstrap is fraught with problems. Both of the most commonly used methods of bootstrapping for regressionresidual bootstrap and pairs bootstrap-give very poor inference on β as the ratio p/n grows. We find that the residual bootstrap tend to give anti-conservative estimates (inflated Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of power) as the ratio p/n grows. We also show that the jackknife resampling technique for estimating the variance ofβ severely overestimates the variance in high dimensions. We contribute alternative procedures based on our theoretical results that result in dimensionality adaptive and robust bootstrap methods."
            ],
            "keywords": [
                "Bootstrap",
                "high-dimensional inference",
                "random matrices",
                "resampling"
            ],
            "author": [
                "Noureddine El Karoui",
                "Elizabeth Purdom"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-006/17-006.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "In Search of Non-Gaussian Components of a High-Dimensional Distribution",
            "abstract": [
                "Finding non-Gaussian components of high-dimensional data is an important preprocessing step for efficient information processing. This article proposes a new linear method to identify the \"non-Gaussian subspace\" within a very general semi-parametric framework. Our proposed method, called NGCA (non-Gaussian component analysis), is based on a linear operator which, to any arbitrary nonlinear (smooth) function, associates a vector belonging to the low dimensional non-Gaussian target subspace, up to an estimation error. By applying this operator to a family of different nonlinear functions, one obtains a family of different vectors lying in a vicinity of the target space. As a final step, the target space itself is estimated by applying PCA to this family of vectors. We show that this procedure is consistent in the sense that the estimaton error tends to zero at a parametric rate, uniformly over the family, Numerical examples demonstrate the usefulness of our method."
            ],
            "keywords": [],
            "author": [
                "Gilles Blanchard",
                "Masashi Sugiyama",
                "Vladimir Spokoiny",
                "Klaus-Robert Robert Müller",
                "Motoaki Kawanabe",
                "KAWANABE, SUGIYAMA, SPOKOINY AND MÜLLER Blanchard"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/blanchard06a/blanchard06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gap Safe Screening Rules for Sparsity Enforcing Penalties",
            "abstract": [
                "In high dimensional regression settings, sparsity enforcing penalties have proved useful to regularize the data-fitting term. A recently introduced technique called screening rules propose to ignore some variables in the optimization leveraging the expected sparsity of the solutions and consequently leading to faster solvers. When the procedure is guaranteed not to discard variables wrongly the rules are said to be safe. In this work, we propose a unifying framework for generalized linear models regularized with standard sparsity enforcing penalties such as 1 or 1 { 2 norms. Our technique allows to discard safely more variables than previously considered safe rules, particularly for low regularization parameters. Our proposed Gap Safe rules (so called because they rely on duality gap computation) can cope with any iterative solver but are particularly well suited to (block) coordinate descent methods. Applied to many standard learning tasks, Lasso, Sparse-Group Lasso, multitask Lasso, binary and multinomial logistic regression, etc., we report significant speed-ups compared to previously proposed safe rules on all tested data sets."
            ],
            "keywords": [
                "Convex optimization",
                "screening rules",
                "Lasso",
                "multi-task Lasso",
                "sparse logistic regression",
                "Sparse-Group Lasso"
            ],
            "author": [
                "Eugene Ndiaye",
                "Olivier Fercoq"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-577/16-577.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Jointly Informative Feature Selection Made Tractable by Gaussian Modeling",
            "abstract": [
                "We address the problem of selecting groups of jointly informative, continuous, features in the context of classification and propose several novel criteria for performing this selection. The proposed class of methods is based on combining a Gaussian modeling of the feature responses with derived bounds on and approximations to their mutual information with the class label. Furthermore, specific algorithmic implementations of these criteria are presented which reduce the computational complexity of the proposed feature selection algorithms by up to two-orders of magnitude. Consequently we show that feature selection based on the joint mutual information of features and class label is in fact tractable; this runs contrary to prior works that largely depend on marginal quantities. An empirical evaluation using several types of classifiers on multiple data sets show that this class of methods outperforms state-of-the-art baselines, both in terms of speed and classification accuracy."
            ],
            "keywords": [
                "feature selection",
                "mutual information",
                "entropy",
                "mixture of Gaussians"
            ],
            "author": [
                "Leonidas Lefakis",
                "François Fleuret"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-026/15-026.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Module Networks",
            "abstract": [
                "Methods for learning Bayesian networks can discover dependency structure between observed variables. Although these methods are useful in many applications, they run into computational and statistical problems in domains that involve a large number of variables. In this paper, 1 we consider a solution that is applicable when many variables have similar behavior. We introduce a new class of models, module networks, that explicitly partition the variables into modules, so that the variables in each module share the same parents in the network and the same conditional probability distribution. We define the semantics of module networks, and describe an algorithm that learns the modules' composition and their dependency structure from data. Evaluation on real data in the domains of gene expression and the stock market shows that module networks generalize better than Bayesian networks, and that the learned module network structure reveals regularities that are obscured in learned Bayesian networks."
            ],
            "keywords": [],
            "author": [
                "Eran Segal",
                "Dana Pe'er",
                "Daphne Koller",
                "Koller @ Stanford Cs",
                "Nir Friedman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/segal05a/segal05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pattern for Python",
            "abstract": [
                "Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classifiers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern."
            ],
            "keywords": [
                "Python",
                "data mining",
                "natural language processing",
                "machine learning",
                "graph networks"
            ],
            "author": [
                "Tom De Smedt",
                "Walter Daelemans"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/desmedt12a/desmedt12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters",
            "abstract": [
                "Statistical spam filters are known to be vulnerable to adversarial attacks. One of the more common adversarial attacks, known as the good word attack, thwarts spam filters by appending to spam messages sets of \"good\" words, which are words that are common in legitimate email but rare in spam. We present a counterattack strategy that attempts to differentiate spam from legitimate email in the input space by transforming each email into a bag of multiple segments, and subsequently applying multiple instance logistic regression on the bags. We treat each segment in the bag as an instance. An email is classified as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. We show that a classifier using our multiple instance counterattack strategy is more robust to good word attacks than its single instance counterpart and other single instance learners commonly used in the spam filtering domain."
            ],
            "keywords": [
                "spam filtering",
                "multiple instance learning",
                "good word attack",
                "adversarial learning"
            ],
            "author": [
                "Zach Jorgensen",
                "Yan Zhou",
                "Meador Inge"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/jorgensen08a/jorgensen08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Metric and Kernel Learning Using a Linear Transformation",
            "abstract": [
                "Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework-that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction."
            ],
            "keywords": [
                "metric learning",
                "kernel learning",
                "linear transformation",
                "matrix divergences",
                "logdet divergence"
            ],
            "author": [
                "Prateek Jain",
                "Brian Kulis",
                "Jason V Davis",
                "Sören Sonnenburg",
                "Francis Bach",
                "Cheng Soon Ong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/jain12a/jain12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results",
            "abstract": [
                "One of the nice properties of kernel classifiers such as SVMs is that they often produce sparse solutions. However, the decision functions of these classifiers cannot always be used to estimate the conditional probability of the class label. We investigate the relationship between these two properties and show that these are intimately related: sparseness does not occur when the conditional probabilities can be unambiguously estimated. We consider a family of convex loss functions and derive sharp asymptotic results for the fraction of data that becomes support vectors. This enables us to characterize the exact trade-off between sparseness and the ability to estimate conditional probabilities for these loss functions."
            ],
            "keywords": [
                "kernel methods",
                "support vector machines",
                "sparseness",
                "estimating conditional probabilities"
            ],
            "author": [
                "Peter L Bartlett"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/bartlett07a/bartlett07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Imitation Learning: Formal and Practical Reductions to I.I.D. Learning",
            "abstract": [
                "In standard passive imitation learning, the goal is to learn a policy that performs as well as a target policy by passively observing full execution trajectories of it. Unfortunately, generating such trajectories can require substantial expert effort and be impractical in some cases. In this paper, we consider active imitation learning with the goal of reducing this effort by querying the expert about the desired action at individual states, which are selected based on answers to past queries and the learner's interactions with an environment simulator. We introduce a new approach based on reducing active imitation learning to active i.i.d. learning, which can leverage progress in the i.i.d. setting. Our first contribution is to analyze reductions for both non-stationary and stationary policies, showing for the first time that the label complexity (number of queries) of active imitation learning can be less than that of passive learning. Our second contribution is to introduce a practical algorithm inspired by the reductions, which is shown to be highly effective in five test domains compared to a number of alternatives."
            ],
            "keywords": [
                "imitation learning",
                "active learning",
                "active imitation learning",
                "reductions"
            ],
            "author": [
                "Kshitij Judah",
                "Thomas G Dietterich",
                "Prasad Tadepalli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/judah14a/judah14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ProxSARAH: An Efficient Algorithmic Framework for Stochastic Composite Nonconvex Optimization",
            "abstract": [
                "We propose a new stochastic first-order algorithmic framework to solve stochastic composite nonconvex optimization problems that covers both finite-sum and expectation settings. Our algorithms rely on the SARAH estimator introduced in Nguyen et al. (2017a) and consist of two steps: a proximal gradient and an averaging step making them different from existing nonconvex proximal-type algorithms. The algorithms only require an average smoothness assumption of the nonconvex objective term and additional bounded variance assumption if applied to expectation problems. They work with both constant and dynamic stepsizes, while allowing single sample and mini-batches. In all these cases, we prove that our algorithms can achieve the best-known complexity bounds in terms of stochastic first-order oracle. One key step of our methods is the new constant and dynamic step-sizes resulting in the desired complexity bounds while improving practical performance. Our constant step-size is much larger than existing methods including proximal SVRG scheme in the single sample case. We also specify our framework to the non-composite case that covers existing state-of-the-arts in terms of oracle complexity bounds. Our update also allows one to trade-off between step-sizes and mini-batch sizes to improve performance. We test the proposed algorithms on two composite nonconvex problems and neural networks using several well-known data sets."
            ],
            "keywords": [
                "Stochastic proximal gradient descent",
                "variance reduction",
                "composite nonconvex optimization",
                "finite-sum minimization",
                "expectation minimization"
            ],
            "author": [
                "Nhan H Pham",
                "Lam M Nguyen",
                "Quoc Tran-Dinh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-248/19-248.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simple Classification Using Binary Data",
            "abstract": [
                "Binary, or one-bit, representations of data arise naturally in many applications, and are appealing in both hardware implementations and algorithm design. In this work, we study the problem of data classification from binary data obtained from the sign pattern of low-dimensional projections and propose a framework with low computation and resource costs. We illustrate the utility of the proposed approach through stylized and realistic numerical experiments, and provide a theoretical analysis for a simple case. We hope that our framework and analysis will serve as a foundation for studying similar types of approaches."
            ],
            "keywords": [
                "binary measurements",
                "one-bit representations",
                "classification"
            ],
            "author": [
                "Deanna Needell",
                "Rayan Saab",
                "Tina Woolf",
                "David Wipf"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-383/17-383.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SFO: A Toolbox for Submodular Function Optimization",
            "abstract": [
                "In recent years, a fundamental problem structure has emerged as very useful in a variety of machine learning applications: Submodularity is an intuitive diminishing returns property, stating that adding an element to a smaller set helps more than adding it to a larger set. Similarly to convexity, submodularity allows one to efficiently find provably (near-) optimal solutions for large problems. We present SFO, a toolbox for use in MATLAB or Octave that implements algorithms for minimization and maximization of submodular functions. A tutorial script illustrates the application of submodularity to machine learning and AI problems such as feature selection, clustering, inference and optimized information gathering."
            ],
            "keywords": [],
            "author": [
                "Andreas Krause"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/krause10a/krause10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Early Stopping and Non-parametric Regression: An Optimal Data-dependent Stopping Rule",
            "abstract": [
                "Early stopping is a form of regularization based on choosing when to stop running an iterative algorithm. Focusing on non-parametric regression in a reproducing kernel Hilbert space, we analyze the early stopping strategy for a form of gradient-descent applied to the least-squares loss function. We propose a data-dependent stopping rule that does not involve hold-out or cross-validation data, and we prove upper bounds on the squared error of the resulting function estimate, measured in either the L 2 (P) and L 2 (P n) norm. These upper bounds lead to minimax-optimal rates for various kernel classes, including Sobolev smoothness classes and other forms of reproducing kernel Hilbert spaces. We show through simulation that our stopping rule compares favorably to two other stopping rules, one based on hold-out data and the other based on Stein's unbiased risk estimate. We also establish a tight connection between our early stopping strategy and the solution path of a kernel ridge regression estimator."
            ],
            "keywords": [
                "early stopping",
                "non-parametric regression",
                "kernel ridge regression",
                "stopping rule",
                "reproducing kernel hilbert space",
                "rademacher complexity",
                "empirical processes"
            ],
            "author": [
                "Garvesh Raskutti",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/raskutti14a/raskutti14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Herded Gibbs Sampling",
            "abstract": [
                "The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an O(1/T) convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem."
            ],
            "keywords": [
                "Gibbs sampling",
                "herding",
                "deterministic sampling"
            ],
            "author": [
                "Yutian Chen",
                "Luke Bornn",
                "Mareija Eskelin",
                "Max Welling"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/chen16a/chen16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model-based Boosting 2.0",
            "abstract": [
                "We describe version 2.0 of the R add-on package mboost. The package implements boosting for optimizing general risk functions using component-wise (penalized) least squares estimates or regression trees as base-learners for fitting generalized linear, additive and interaction models to potentially high-dimensional data."
            ],
            "keywords": [
                "component-wise functional gradient descent",
                "splines",
                "decision trees"
            ],
            "author": [
                "Torsten Hothorn",
                "Peter Bühlmann",
                "Thomas Kneib",
                "Matthias Schmid",
                "Benjamin Hofner"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/hothorn10a/hothorn10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "HyperTools: a Python Toolbox for Gaining Geometric Insights into High-Dimensional Data",
            "abstract": [
                "Dimensionality reduction algorithms have played a foundational role in facilitating the deep understanding of complex high-dimensional data. One particularly useful application of dimensionality reduction techniques is in data visualization. Low-dimensional visualizations can help practitioners understand where machine learning algorithms might leverage the geometric properties of a dataset to improve performance. Another challenge is to generalize insights across datasets [e.g. data from multiple modalities describing the same system (Haxby et al., 2011), artwork or photographs of similar content in different styles (Zhu et al., 2017), etc.]. Several recently developed techniques (e.g. Haxby et al., 2011; Chen et al., 2015) use the procrustean transformation (Schönemann, 1966) to align the geometries of two or more spaces so that data with different axes may be plotted in a common space. We propose that each of these techniques (dimensionality reduction, alignment, and visualization) applied in sequence should be cast as a single conceptual hyperplot operation for gaining geometric insights into high-dimensional data. Our Python toolbox enables this operation in a single (highly flexible) function call."
            ],
            "keywords": [
                "visualization",
                "high-dimensional",
                "dimensionality reduction",
                "procrustes",
                "timeseries data"
            ],
            "author": [
                "Andrew C Heusser",
                "Kirsten Ziman",
                "Lucy L W Owen",
                "Jeremy R Manning"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-434/17-434.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Consistency of Feature Selection using Greedy Least Squares Regression",
            "abstract": [
                "This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches infinity. The condition is identical to a corresponding condition for Lasso. Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefficient is larger than a constant times the noise level. In comparison, Lasso may require the coefficients to be larger than O(√ s) times the noise level in the worst case, where s is the number of nonzero coefficients."
            ],
            "keywords": [
                "greedy algorithm",
                "feature selection",
                "sparsity"
            ],
            "author": [
                "Tong Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/zhang09a/zhang09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Similarity with Operator-valued Large-margin Classifiers",
            "abstract": [
                "A method is introduced to learn and represent similarity with linear operators in kernel induced Hilbert spaces. Transferring error bounds for vector valued large-margin classifiers to the setting of Hilbert-Schmidt operators leads to dimension free bounds on a risk functional for linear representations and motivates a regularized objective functional. Minimization of this objective is effected by a simple technique of stochastic gradient descent. The resulting representations are tested on transfer problems in image processing, involving plane and spatial geometric invariants, handwritten characters and face recognition."
            ],
            "keywords": [
                "learning similarity",
                "similarity",
                "transfer learning"
            ],
            "author": [
                "Andreas Maurer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/maurer08a/maurer08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers",
            "abstract": [
                "Support vector machine (SVM) is one of the most popular and promising classification algorithms. After a classification rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with finite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples."
            ],
            "keywords": [
                "k-fold cross-validation",
                "model evaluation",
                "perturbation-resampling",
                "prediction errors",
                "support vector machine"
            ],
            "author": [
                "Bo Jiang",
                "Xuegong Zhang",
                "Tianxi Cai",
                "Tcai @ Harvard Hsph"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/jiang08a/jiang08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Finding Predictors for Arbitrary Families of Processes",
            "abstract": [
                "The problem is sequence prediction in the following setting. A sequence x 1 ,. .. , x n ,. .. of discretevalued observations is generated according to some unknown probabilistic law (measure) µ. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure µ belongs to an arbitrary but known class C of stochastic process measures. We are interested in predictors ρ whose conditional probabilities converge (in some sense) to the \"true\" µ-conditional probabilities, if any µ ∈ C is chosen to generate the sequence. The contribution of this work is in characterizing the families C for which such predictors exist, and in providing a specific and simple form in which to look for a solution. We show that if any predictor works, then there exists a Bayesian predictor, whose prior is discrete, and which works too. We also find several sufficient and necessary conditions for the existence of a predictor, in terms of topological characterizations of the family C , as well as in terms of local behaviour of the measures in C , which in some cases lead to procedures for constructing such predictors. It should be emphasized that the framework is completely general: the stochastic processes considered are not required to be i.i.d., stationary, or to belong to any parametric or countable family."
            ],
            "keywords": [
                "sequence prediction",
                "time series",
                "online prediction",
                "Bayesian prediction"
            ],
            "author": [
                "Daniil Ryabko"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/ryabko10a/ryabko10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks",
            "abstract": [
                "With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a data set labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Since we do not have control over the quality of the annotators, very often the annotations can be dominated by spammers, defined as annotators who assign labels randomly without actually looking at the instance. Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the final consensus labels. In this paper we propose an empirical Bayesian algorithm called SpEM that iteratively eliminates the spammers and estimates the consensus labels based only on the good annotators. The algorithm is motivated by defining a spammer score that can be used to rank the annotators. Experiments on simulated and real data show that the proposed approach is better than (or as good as) the earlier approaches in terms of the accuracy and uses a significantly smaller number of annotators."
            ],
            "keywords": [
                "crowdsourcing",
                "multiple annotators",
                "ranking annotators",
                "spammers"
            ],
            "author": [
                "Vikas C Raykar",
                "Shipeng Yu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/raykar12a/raykar12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Streamlined Computing for Variational Inference with Higher Level Random Effects",
            "abstract": [
                "We derive and present explicit algorithms to facilitate streamlined computing for variational inference for models containing higher level random effects. Existing literature, such as Lee and Wand (2016), is such that streamlined variational inference is restricted to mean field variational Bayes algorithms for two-level random effects models. Here we provide the following extensions: (1) explicit Gaussian response mean field variational Bayes algorithms for three-level models, (2) explicit algorithms for the alternative variational message passing approach in the case of two-level and three-level models, and (3) an explanation of how arbitrarily high levels of nesting can be handled based on the recently published matrix algebraic results of the authors. A pay-off from (2) is simple extension to non-Gaussian response models. In summary, we remove barriers for streamlining variational inference algorithms based on either the mean field variational Bayes approach or the variational message passing approach when higher level random effects are present."
            ],
            "keywords": [
                "Factor Graph Fragment",
                "Longitudinal Data Analysis",
                "Mixed Models",
                "Multilevel Models",
                "Variational Message Passing"
            ],
            "author": [
                "Tui H Nolan",
                "Marianne Menictas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-222/19-222.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": null,
            "abstract": [
                "SHARK is an object-oriented library for the design of adaptive systems. It comprises methods for single-and multi-objective optimization (e.g., evolutionary and gradient-based algorithms) as well as kernel-based methods, neural networks, and other machine learning techniques."
            ],
            "keywords": [
                "machine learning software",
                "neural networks",
                "kernel-methods",
                "evolutionary algorithms",
                "optimization",
                "multi-objective-optimization"
            ],
            "author": [
                "Verena Heidrich-Meisner"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/igel08a/igel08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Trust-Region Variational Inference with Gaussian Mixture Models",
            "abstract": [
                "Many methods for machine learning rely on approximate inference from intractable probability distributions. Variational inference approximates such distributions by tractable models that can be subsequently used for approximate inference. Learning sufficiently accurate approximations requires a rich model family and careful exploration of the relevant modes of the target distribution. We propose a method for learning accurate GMM approximations of intractable probability distributions based on insights from policy search by using information-geometric trust regions for principled exploration. For efficient improvement of the GMM approximation, we derive a lower bound on the corresponding optimization objective enabling us to update the components independently. Our use of the lower bound ensures convergence to a stationary point of the original objective. The number of components is adapted online by adding new components in promising regions and by deleting components with negligible weight. We demonstrate on several domains that we can learn approximations of complex, multimodal distributions with a quality that is unmet by previous variational inference methods, and that the GMM approximation can be used for drawing samples that are on par with samples created by state-of-theart MCMC samplers while requiring up to three orders of magnitude less computational resources."
            ],
            "keywords": [
                "approximate inference",
                "variational inference",
                "sampling",
                "policy search",
                "mcmc",
                "markov chain monte carlo"
            ],
            "author": [
                "Oleg Arenz",
                "Mingjun Zhong",
                "Gerhard Neumann",
                "Zhong Arenz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-524/19-524.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rationally Inattentive Inverse Reinforcement Learning Explains YouTube Commenting Behavior",
            "abstract": [
                "We consider a novel application of inverse reinforcement learning with behavioral economics constraints to model, learn and predict the commenting behavior of YouTube viewers. Each group of users is modeled as a rationally inattentive Bayesian agent which solves a contextual bandit problem. Our methodology integrates three key components. First, to identify distinct commenting patterns, we use deep embedded clustering to estimate framing information (essential extrinsic features) that clusters users into distinct groups. Second, we present an inverse reinforcement learning algorithm that uses Bayesian revealed preferences to test for rationality: does there exist a utility function that rationalizes the given data, and if yes, can it be used to predict commenting behavior? Finally, we impose behavioral economics constraints stemming from rational inattention to characterize the attention span of groups of users. The test imposes a Rényi mutual information cost constraint which impacts how the agent can select attention strategies to maximize their expected utility. After a careful analysis of a massive YouTube dataset, our surprising result is that in most YouTube user groups, the commenting behavior is consistent with optimizing a Bayesian utility with rationally inattentive constraints. The paper also highlights how the rational inattention model can accurately predict commenting behavior. The massive YouTube dataset and analysis used in this paper are available on GitHub and completely reproducible."
            ],
            "keywords": [
                "Inverse Reinforcement Learning",
                "Bayesian Revealed Preference",
                "YouTube",
                "Rational Inattention",
                "Rényi Mutual Information",
                "Framing",
                "Behavioral Economics",
                "Deep Embedded Clustering",
                "Contextual Bandits"
            ],
            "author": [
                "William Hoiles",
                "Vikram Krishnamurthy",
                "Kunal Pattanayak"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-872/19-872.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tensor Decompositions for Learning Latent Variable Models",
            "abstract": [
                "This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models-including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation-which exploits a certain tensor structure in their low-order observable moments (typically, of second-and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models."
            ],
            "keywords": [
                "latent variable models",
                "tensor decompositions",
                "mixture models",
                "topic models",
                "method of moments",
                "power method"
            ],
            "author": [
                "Animashree Anandkumar",
                "Daniel Hsu",
                "Matus Telgarsky",
                "Rong Ge",
                "Sham M Kakade",
                "Ge, Hsu, Kakade Telgarsky Anandkumar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/anandkumar14b/anandkumar14b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DataWig: Missing Value Imputation for Tables",
            "abstract": [
                "With the growing importance of machine learning (ML) algorithms for practical applications, reducing data quality problems in ML pipelines has become a major focus of research. In many cases missing values can break data pipelines which makes completeness one of the most impactful data quality challenges. Current missing value imputation methods are focusing on numerical or categorical data and can be difficult to scale to datasets with millions of rows. We release DataWig, a robust and scalable approach for missing value imputation that can be applied to tables with heterogeneous data types, including unstructured text. DataWig combines deep learning feature extractors with automatic hyperparameter tuning. This enables users without a machine learning background, such as data engineers, to impute missing values with minimal effort in tables with more heterogeneous data types than supported in existing libraries, while requiring less glue code for feature engineering and offering more flexible modelling options. We demonstrate that DataWig compares favourably to existing imputation packages. Source code, documentation, and unit tests for this package are available at: github.com/awslabs/datawig"
            ],
            "keywords": [
                "missing value imputation",
                "deep learning",
                "heterogeneous data"
            ],
            "author": [
                "Felix Bießmann",
                "Phillipp Schmidt",
                "Prathik Naidu",
                "Sebastian Schelter",
                "Andrey Taptunov",
                "Dustin Lange",
                "David Salinas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-753/18-753.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structured Prediction, Dual Extragradient and Bregman Projections",
            "abstract": [
                "We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex flow, depending on the structure of the problem. We show that this approach provides a memoryefficient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm."
            ],
            "keywords": [
                "Markov networks",
                "large-margin methods",
                "structured prediction",
                "extragradient",
                "Bregman projections"
            ],
            "author": [
                "Ben Taskar",
                "Simon Lacoste-Julien",
                "Michael I Jordan",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/taskar06a/taskar06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "pomegranate: Fast and Flexible Probabilistic Modeling in Python",
            "abstract": [
                "We present pomegranate, an open source machine learning package for probabilistic modeling in Python. Probabilistic modeling encompasses a wide range of methods that explicitly describe uncertainty using probability distributions. Three widely used probabilistic models implemented in pomegranate are general mixture models, hidden Markov models, and Bayesian networks. A primary focus of pomegranate is to abstract away the complexities of training models from their definition. This allows users to focus on specifying the correct model for their application instead of being limited by their understanding of the underlying algorithms. An aspect of this focus involves the collection of additive sufficient statistics from data sets as a strategy for training models. This approach trivially enables many useful learning strategies, such as out-of-core learning, minibatch learning, and semi-supervised learning, without requiring the user to consider how to partition data or modify the algorithms to handle these tasks themselves. pomegranate is written in Cython to speed up calculations and releases the global interpreter lock to allow for built-in multithreaded parallelism, making it competitive with-or outperform-other implementations of similar algorithms. This paper presents an overview of the design choices in pomegranate, and how they have enabled complex features to be supported by simple code."
            ],
            "keywords": [
                "probabilistic modeling",
                "Python",
                "Cython",
                "machine learning",
                "big data"
            ],
            "author": [
                "Jacob Schreiber",
                "Paul G Allen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-636/17-636.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity",
            "abstract": [
                "This paper presents a new method of model selection for regression problems using the modulus of continuity. For this purpose, we suggest the prediction risk bounds of regression models using the modulus of continuity which can be interpreted as the complexity of functions. We also present the model selection criterion referred to as the modulus of continuity information criterion (MCIC) which is derived from the suggested prediction risk bounds. The suggested MCIC provides a risk estimate using the modulus of continuity for a trained regression model (or an estimation function) while other model selection criteria such as the AIC and BIC use structural information such as the number of training parameters. As a result, the suggested MCIC is able to discriminate the performances of trained regression models, even with the same structure of training models. To show the effectiveness of the proposed method, the simulation for function approximation using the multilayer perceptrons (MLPs) was conducted. Through the simulation for function approximation, it was demonstrated that the suggested MCIC provides a good selection tool for nonlinear regression models, even with the limited size of data."
            ],
            "keywords": [
                "regression models",
                "multilayer perceptrons",
                "model selection",
                "information criteria",
                "modulus of continuity"
            ],
            "author": [
                "Imhoi Koo",
                "Isabelle Guyon",
                "Amir Saffari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/koo08b/koo08b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "OpenEnsembles: A Python Resource for Ensemble Clustering",
            "abstract": [
                "In this paper we introduce OpenEnsembles, a Python toolkit for performing and analyzing ensemble clustering. Ensemble clustering is the process of creating many clustering solutions for a given dataset and utilizing the relationships observed across the ensemble to identify final solutions, which are more robust, stable or better than the individual solutions within the ensemble. The OpenEnsembles library provides a unified interface for applying transformations to data, clustering data, visualizing individual clustering solutions, visualizing and finishing the ensemble, and calculating validation metrics for a clustering solution for any given partitioning of the data. We have documented examples of using OpenEnsembles to create, analyze, and visualize a number of different types of ensemble approaches on toy and example datasets."
            ],
            "keywords": [
                "Unsupervised Learning",
                "Ensembles",
                "Clustering",
                "Ensemble Clustering",
                "Finishing Techniques"
            ],
            "author": [
                "Tom Ronan",
                "Shawn Anastasio",
                "Roman Sloutsky",
                "Kristen M Naegle",
                "Pedro Henrique",
                "S Vieira Tavares",
                "Zhijie Qi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-100/18-100.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously",
            "abstract": [
                "Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model f (x) = x T β with a fixed coefficient vector β) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models."
            ],
            "keywords": [
                "Rashomon",
                "permutation importance",
                "conditional variable importance",
                "Ustatistics",
                "transparency",
                "interpretable models"
            ],
            "author": [
                "Aaron Fisher",
                "Cynthia Rudin",
                "Francesca Dominici"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-760/18-760.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization",
            "abstract": [
                "This paper considers stochastic optimization problems for a large class of objective functions, including convex and continuous submodular. Stochastic proximal gradient methods have been widely used to solve such problems; however, their applicability remains limited when the problem dimension is large and the projection onto a convex set is computationally costly. Instead, stochastic conditional gradient algorithms are proposed as an alternative solution which rely on (i) Approximating gradients via a simple averaging technique requiring a single stochastic gradient evaluation per iteration; (ii) Solving a linear program to compute the descent/ascent direction. The gradient averaging technique reduces the noise of gradient approximations as time progresses, and replacing projection step in proximal methods by a linear program lowers the computational complexity of each iteration. We show that under convexity and smoothness assumptions, our proposed stochastic conditional gradient method converges to the optimal objective function value at a sublinear rate of O(1/t 1/3). Further, for a monotone and continuous DR-submodular function and subject to a general convex body constraint, we prove that our proposed method achieves a ((1 − 1/e)OPT −) guarantee (in expectation) with O(1/ 3) stochastic gradient computations. This guarantee matches the known hardness results and closes the gap between deterministic and stochastic continuous submodular maximization. Additionally, we achieve ((1/e)OPT −) guarantee after operating on O(1/ 3) stochastic gradients for the case that the objective function is continuous DR-submodular but non-monotone and the constraint set is a down-closed convex body. By using stochastic continuous optimization as an interface, we also provide the first (1 − 1/e) tight approximation guarantee for maximizing a monotone but stochastic submodular set function subject to a general matroid constraint and (1/e) approximation guarantee for the non-monotone case."
            ],
            "keywords": [
                "Stochastic optimization",
                "conditional gradient methods",
                "convex minimization",
                "submodular maximization",
                "gradient averaging",
                "Frank-Wolfe algorithm",
                "greedy algorithm"
            ],
            "author": [
                "Aryan Mokhtari",
                "Hamed Hassani",
                "Amin Karbasi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-764/18-764.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Analysis of Convex Relaxations for MAP Estimation of Discrete MRFs",
            "abstract": [
                "The problem of obtaining the maximum a posteriori estimate of a general discrete Markov random field (i.e., a Markov random field defined using a discrete set of labels) is known to be NP-hard. However, due to its central importance in many applications, several approximation algorithms have been proposed in the literature. In this paper, we present an analysis of three such algorithms based on convex relaxations: (i) LP-S: the linear programming (LP) relaxation proposed by Schlesinger (1976) for a special case and independently in Chekuri et al. (2001), Koster et al. (1998), and Wainwright et al. (2005) for the general case; (ii) QP-RL: the quadratic programming (QP) relaxation of Ravikumar and Lafferty (2006); and (iii) SOCP-MS: the second order cone programming (SOCP) relaxation first proposed by Muramatsu and Suzuki (2003) for two label problems and later extended by Kumar et al. (2006) for a general label set. We show that the SOCP-MS and the QP-RL relaxations are equivalent. Furthermore, we prove that despite the flexibility in the form of the constraints/objective function offered by QP and SOCP, the LP-S relaxation strictly dominates (i.e., provides a better approximation than) QP-RL and SOCP-MS. We generalize these results by defining a large class of SOCP (and equivalent QP) relaxations which is dominated by the LP-S relaxation. Based on these results we propose some novel SOCP relaxations which define constraints using random variables that form cycles or cliques in the graphical model representation of the random field. Using some examples we show that the new SOCP relaxations strictly dominate the previous approaches."
            ],
            "keywords": [
                "probabilistic models",
                "MAP estimation",
                "discrete MRF",
                "convex relaxations",
                "linear programming",
                "second-order cone programming",
                "quadratic programming",
                "dominating relaxations"
            ],
            "author": [
                "M Pawan Kumar",
                "Vladimir Kolmogorov",
                "Philip H S Torr"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/kumar09a/kumar09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Keep It Simple And Sparse: Real-Time Action Recognition",
            "abstract": [
                "Sparsity has been showed to be one of the most important properties for visual recognition purposes. In this paper we show that sparse representation plays a fundamental role in achieving one-shot learning and real-time recognition of actions. We start off from RGBD images, combine motion and appearance cues and extract state-of-the-art features in a computationally efficient way. The proposed method relies on descriptors based on 3D Histograms of Scene Flow (3DHOFs) and Global Histograms of Oriented Gradient (GHOGs); adaptive sparse coding is applied to capture high-level patterns from data. We then propose a simultaneous on-line video segmentation and recognition of actions using linear SVMs. The main contribution of the paper is an effective realtime system for one-shot action modeling and recognition; the paper highlights the effectiveness of sparse coding techniques to represent 3D actions. We obtain very good results on three different data sets: a benchmark data set for one-shot action learning (the ChaLearn Gesture Data Set), an in-house data set acquired by a Kinect sensor including complex actions and gestures differing by small details, and a data set created for human-robot interaction purposes. Finally we demonstrate that our system is effective also in a human-robot interaction setting and propose a memory game, \"All Gestures You Can\", to be played against a humanoid robot."
            ],
            "keywords": [
                "real-time action recognition",
                "sparse representation",
                "one-shot action learning",
                "human robot interaction"
            ],
            "author": [
                "Sean Ryan Fanello",
                "Giorgio Metta",
                "Francesca Odone",
                "Isabelle Guyon",
                "Vassilis Athitsos",
                "Ryan Fanello",
                "Ilaria Gori"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/fanello13a/fanello13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Network Structure Learning by Recursive Autonomy Identification",
            "abstract": [
                "We propose the recursive autonomy identification (RAI) algorithm for constraint-based (CB) Bayesian network structure learning. The RAI algorithm learns the structure by sequential application of conditional independence (CI) tests, edge direction and structure decomposition into autonomous sub-structures. The sequence of operations is performed recursively for each autonomous substructure while simultaneously increasing the order of the CI test. While other CB algorithms d-separate structures and then direct the resulted undirected graph, the RAI algorithm combines the two processes from the outset and along the procedure. By this means and due to structure decomposition, learning a structure using RAI requires a smaller number of CI tests of high orders. This reduces the complexity and run-time of the algorithm and increases the accuracy by diminishing the curse-of-dimensionality. When the RAI algorithm learned structures from databases representing synthetic problems, known networks and natural problems, it demonstrated superiority with respect to computational complexity, run-time, structural correctness and classification accuracy over the PC, Three Phase Dependency Analysis, Optimal Reinsertion, greedy search, Greedy Equivalence Search, Sparse Candidate, and Max-Min Hill-Climbing algorithms."
            ],
            "keywords": [],
            "author": [
                "Raanan Yehezkel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/yehezkel09a/yehezkel09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Policy Gradient in Continuous Time",
            "abstract": [
                "Policy search is a method for approximately solving an optimal control problem by performing a parametric optimization search in a given class of parameterized policies. In order to process a local optimization technique, such as a gradient method, we wish to evaluate the sensitivity of the performance measure with respect to the policy parameters, the so-called policy gradient. This paper is concerned with the estimation of the policy gradient for continuous-time, deterministic state dynamics, in a reinforcement learning framework, that is, when the decision maker does not have a model of the state dynamics. We show that usual likelihood ratio methods used in discrete-time, fail to proceed the gradient because they are subject to variance explosion when the discretization time-step decreases to 0. We describe an alternative approach based on the approximation of the pathwise derivative, which leads to a policy gradient estimate that converges almost surely to the true gradient when the timestep tends to 0. The underlying idea starts with the derivation of an explicit representation of the policy gradient using pathwise derivation. This derivation makes use of the knowledge of the state dynamics. Then, in order to estimate the gradient from the observable data only, we use a stochastic policy to discretize the continuous deterministic system into a stochastic discrete process, which enables to replace the unknown coefficients by quantities that solely depend on known data. We prove the almost sure convergence of this estimate to the true policy gradient when the discretization time-step goes to zero. The method is illustrated on two target problems, in discrete and continuous control spaces."
            ],
            "keywords": [
                "optimal control",
                "reinforcement learning",
                "policy search",
                "sensitivity analysis",
                "parametric optimization",
                "gradient estimate",
                "likelihood ratio method",
                "pathwise derivation"
            ],
            "author": [
                "Rémi Munos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/munos06b/munos06b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners",
            "abstract": [
                "In the process of concept learning, target concepts may have portions with short-term changes, other portions may support long-term changes, and yet others may not change at all. For this reason several local windows need to be handled. We suggest facing this problem, which naturally exists in the field of concept learning, by allocating windows which can adapt their size to portions of the target concept. We propose an incremental decision tree that is updated with incoming examples. Each leaf of the decision tree holds a time window and a local performance measure as the main parameter to be controlled. When the performance of a leaf decreases, the size of its local window is reduced. This learning algorithm, called OnlineTree2, automatically adjusts its internal parameters in order to face the current dynamics of the data stream. Results show that it is comparable to other batch algorithms when facing problems with no concept change, and it is better than evaluated methods in its ability to deal with concept drift when dealing with problems in which: concept change occurs at different speeds, noise may be present and, examples may arrive from different areas of the problem domain (virtual drift)."
            ],
            "keywords": [
                "incremental algorithms",
                "online learning",
                "concept drift",
                "decision trees",
                "robust learners"
            ],
            "author": [
                "Marlon Núñez",
                "Rafael Morales"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/nunez07a/nunez07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Low-Rank Kernel Learning with Bregman Matrix Divergences",
            "abstract": [
                "In this paper, we study low-rank matrix nearness problems, with a focus on learning low-rank positive semidefinite (kernel) matrices for machine learning applications. We propose efficient algorithms that scale linearly in the number of data points and quadratically in the rank of the input matrix. Existing algorithms for learning kernel matrices often scale poorly, with running times that are cubic in the number of data points. We employ Bregman matrix divergences as the measures of nearness-these divergences are natural for learning low-rank kernels since they preserve rank as well as positive semidefiniteness. Special cases of our framework yield faster algorithms for various existing learning problems, and experimental results demonstrate that our algorithms can effectively learn both low-rank and full-rank kernel matrices."
            ],
            "keywords": [
                "kernel methods",
                "Bregman divergences",
                "convex optimization",
                "kernel learning",
                "matrix nearness"
            ],
            "author": [
                "Brian Kulis",
                "Mátyás A Sustik",
                "Inderjit S Dhillon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/kulis09a/kulis09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Complexity of Learning Lexicographic Strategies",
            "abstract": [
                "Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-the-best searches for a sufficiently good ordering of cues (or features) in a task where objects are to be compared lexicographically. We investigate the computational complexity of finding optimal cue permutations for lexicographic strategies and prove that the problem is NP-complete. It follows that no efficient (that is, polynomial-time) algorithm computes optimal solutions, unless P = NP. We further analyze the complexity of approximating optimal cue permutations for lexicographic strategies. We show that there is no efficient algorithm that approximates the optimum to within any constant factor, unless P = NP. The results have implications for the complexity of learning lexicographic strategies from examples. They show that learning them in polynomial time within the model of agnostic probably approximately correct (PAC) learning is impossible, unless RP = NP. We further consider greedy approaches for building lexicographic strategies and determine upper and lower bounds for the performance ratio of simple algorithms. Moreover, we present a greedy algorithm that performs provably better than take-the-best. Tight bounds on the sample complexity for learning lexicographic strategies are also given in this article."
            ],
            "keywords": [
                "bounded rationality",
                "fast and frugal heuristic",
                "PAC learning",
                "NP-completeness",
                "hardness of approximation",
                "greedy method"
            ],
            "author": [
                "Michael Schmitt",
                "Laura Martignon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/schmitt06a/schmitt06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Bayesian Inference of Sigmoidal Gaussian Cox Processes",
            "abstract": [
                "We present an approximate Bayesian inference approach for estimating the intensity of a inhomogeneous Poisson process, where the intensity function is modelled using a Gaussian process (GP) prior via a sigmoid link function. Augmenting the model using a latent marked Poisson process and Pólya-Gamma random variables we obtain a representation of the likelihood which is conjugate to the GP prior. We estimate the posterior using a variational free-form mean field optimisation together with the framework of sparse GPs. Furthermore, as alternative approximation we suggest a sparse Laplace's method for the posterior, for which an efficient expectation-maximisation algorithm is derived to find the posterior's mode. Both algorithms compare well against exact inference obtained by a Markov Chain Monte Carlo sampler and standard variational Gauss approach solving the same model, while being one order of magnitude faster. Furthermore, the performance and speed of our method is competitive with that of another recently proposed Poisson process model based on a quadratic link function, while not being limited to GPs with squared exponential kernels and rectangular domains."
            ],
            "keywords": [
                "Poisson process",
                "Cox process",
                "Gaussian process",
                "data augmentation",
                "variational inference"
            ],
            "author": [
                "Christian Donner",
                "Manfred Opper"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-759/17-759.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Identifying a Minimal Class of Models for High-dimensional Data",
            "abstract": [
                "Model selection consistency in the high-dimensional regression setting can be achieved only if strong assumptions are fulfilled. We therefore suggest to pursue a different goal, which we call a minimal class of models. The minimal class of models includes models that are similar in their prediction accuracy but not necessarily in their elements. We suggest a random search algorithm to reveal candidate models. The algorithm implements simulated annealing while using a score for each predictor that we suggest to derive using a combination of the lasso and the elastic net. The utility of using a minimal class of models is demonstrated in the analysis of two data sets."
            ],
            "keywords": [
                "Model Selection",
                "High-dimensional Data",
                "Lasso",
                "Elastic Net",
                "Simulated Annealing"
            ],
            "author": [
                "Daniel Nevo"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-172/16-172.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Analysis of Multi-stage Convex Relaxation for Sparse Regularization",
            "abstract": [
                "We consider learning formulations with non-convex objective functions that often occur in practical applications. There are two approaches to this problem: • Heuristic methods such as gradient descent that only find a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. • Convex relaxation such as L 1-regularization that solves the problem under some conditions. However it often leads to a sub-optimal solution in reality. This paper tries to remedy the above gap between theory and practice. In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. For learning formulations with sparse regularization, we analyze the behavior of a specific multistage relaxation scheme. Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L 1 convex relaxation for learning sparse targets."
            ],
            "keywords": [
                "sparsity",
                "non-convex optimization",
                "convex relaxation",
                "multi-stage convex relaxation"
            ],
            "author": [
                "Tong Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/zhang10a/zhang10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning the Kernel with Hyperkernels",
            "abstract": [
                "This paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine, hence further automating machine learning. This goal is achieved by defining a reproducing kernel Hilbert space on the space of kernels itself. Such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional. We state the equivalent representer theorem for the choice of kernels and present a semidefinite programming formulation of the resulting optimization problem. Several recipes for constructing hyperkernels are provided, as well as the details of common machine learning problems. Experimental results for classification, regression and novelty detection on UCI data show the feasibility of our approach."
            ],
            "keywords": [
                "learning the kernel",
                "capacity control",
                "kernel methods",
                "support vector machines",
                "representer theorem",
                "semidefinite programming"
            ],
            "author": [
                "Soon Cheng",
                "Alexander J Smola",
                "Robert C Williamson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/ong05a/ong05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model-free Variable Selection in Reproducing Kernel Hilbert Space",
            "abstract": [
                "Variable selection is popular in high-dimensional data analysis to identify the truly informative variables. Many variable selection methods have been developed under various model assumptions. Whereas success has been widely reported in literature, their performances largely depend on validity of the assumed models, such as the linear or additive models. This article introduces a model-free variable selection method via learning the gradient functions. The idea is based on the equivalence between whether a variable is informative and whether its corresponding gradient function is substantially non-zero. The proposed variable selection method is then formulated in a framework of learning gradients in a flexible reproducing kernel Hilbert space. The key advantage of the proposed method is that it requires no explicit model assumption and allows for general variable effects. Its asymptotic estimation and selection consistencies are studied, which establish the convergence rate of the estimated sparse gradients and assure that the truly informative variables are correctly identified in probability. The effectiveness of the proposed method is also supported by a variety of simulated examples and two real-life examples."
            ],
            "keywords": [
                "group Lasso",
                "high-dimensional data",
                "kernel regression",
                "learning gradients",
                "reproducing kernel Hilbert space (RKHS)",
                "variable selection"
            ],
            "author": [
                "Lei Yang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-390/15-390.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tensor Train Decomposition on TensorFlow (T3F)",
            "abstract": [
                "Tensor Train decomposition is used across many branches of machine learning. We present T3F-a library for Tensor Train decomposition based on TensorFlow. T3F supports GPU execution, batch processing, automatic differentiation, and versatile functionality for the Riemannian optimization framework, which takes into account the underlying manifold structure to construct efficient optimization methods. The library makes it easier to implement machine learning papers that rely on the Tensor Train decomposition. T3F includes documentation, examples and 94% test coverage."
            ],
            "keywords": [
                "tensor decomposition",
                "tensor train",
                "software",
                "gpu",
                "tensorflow"
            ],
            "author": [
                "Alexander Novikov",
                "Valentin Khrulkov",
                "Michael Figurnov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-008/18-008.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Learning Using Smooth Relative Regret Approximations with Applications",
            "abstract": [
                "The disagreement coefficient of Hanneke has become a central data independent invariant in proving active learning rates. It has been shown in various ways that a concept class with low complexity together with a bound on the disagreement coefficient at an optimal solution allows active learning rates that are superior to passive learning ones. We present a different tool for pool based active learning which follows from the existence of a certain uniform version of low disagreement coefficient, but is not equivalent to it. In fact, we present two fundamental active learning problems of significant interest for which our approach allows nontrivial active learning bounds. However, any general purpose method relying on the disagreement coefficient bounds only, fails to guarantee any useful bounds for these problems. The applications of interest are: Learning to rank from pairwise preferences, and clustering with side information (a.k.a. semi-supervised clustering). The tool we use is based on the learner's ability to compute an estimator of the difference between the loss of any hypothesis and some fixed \"pivotal\" hypothesis to within an absolute error of at most ε times the disagreement measure (1 distance) between the two hypotheses. We prove that such an estimator implies the existence of a learning algorithm which, at each iteration, reduces its in-class excess risk to within a constant factor. Each iteration replaces the current pivotal hypothesis with the minimizer of the estimated loss difference function with respect to the previous pivotal hypothesis. The label complexity essentially becomes that of computing this estimator."
            ],
            "keywords": [
                "active learning",
                "learning to rank from pairwise preferences",
                "semi-supervised clustering",
                "clustering with side information",
                "disagreement coefficient",
                "smooth relative regret approximation"
            ],
            "author": [
                "Nir Ailon",
                "Ron Begleiter",
                "Esther Ezra"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/ailon14a/ailon14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Decision Process for Cost-Efficient Dynamic Ranking via Crowdsourcing",
            "abstract": [
                "Rank aggregation based on pairwise comparisons over a set of items has a wide range of applications. Although considerable research has been devoted to the development of rank aggregation algorithms, one basic question is how to efficiently collect a large amount of high-quality pairwise comparisons for the ranking purpose. Because of the advent of many crowdsourcing services, a crowd of workers are often hired to conduct pairwise comparisons with a small monetary reward for each pair they compare. Since different workers have different levels of reliability and different pairs have different levels of ambiguity, it is desirable to wisely allocate the limited budget for comparisons among the pairs of items and workers so that the global ranking can be accurately inferred from the comparison results. To this end, we model the active sampling problem in crowdsourced ranking as a Bayesian Markov decision process, which dynamically selects item pairs and workers to improve the ranking accuracy under a budget constraint. We further develop a computationally efficient sampling policy based on knowledge gradient as well as a moment matching technique for posterior approximation. Experimental evaluations on both synthetic and real data show that the proposed policy achieves high ranking accuracy with a lower labeling cost."
            ],
            "keywords": [
                "crowdsourced ranking",
                "Bayesian",
                "Markov decision process",
                "dynamic programming",
                "knowledge gradient",
                "moment matching"
            ],
            "author": [
                "Xi Chen",
                "Kevin Jiao",
                "Qihang Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-066/16-066.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ThunderGBM: Fast GBDTs and Random Forests on GPUs",
            "abstract": [
                "Gradient Boosting Decision Trees (GBDTs) and Random Forests (RFs) have been used in many real-world applications. They are often a standard recipe for building state-of-the-art solutions to machine learning and data mining problems. However, training and prediction are very expensive computationally for large and high dimensional problems. This article presents an efficient and open source software toolkit called ThunderGBM which exploits the high-performance Graphics Processing Units (GPUs) for GBDTs and RFs. Thun-derGBM supports classification, regression and ranking, and can run on single or multiple GPUs of a machine. Our experimental results show that ThunderGBM outperforms the existing libraries while producing similar models, and can handle high dimensional problems where existing GPU-based libraries fail. Documentation, examples, and more details about ThunderGBM are available at https://github.com/xtra-computing/thundergbm."
            ],
            "keywords": [
                "Gradient Boosting Decision Trees",
                "Random Forests",
                "GPUs",
                "Efficiency"
            ],
            "author": [
                "Zeyi Wen",
                "Hanfeng Liu",
                "Jiashuai Shi",
                "Qinbin Li",
                "Bingsheng He",
                "Jian Chen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-095/19-095.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Methods for Relation Extraction",
            "abstract": [
                "We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results."
            ],
            "keywords": [
                "Kernel Methods",
                "Natural Language Processing",
                "Information Extraction"
            ],
            "author": [
                "Dmitry Zelenko",
                "Chinatsu Aone",
                "Anthony Richardella",
                "Jaz Kandola",
                "Thomas Hofmann",
                "Tomaso Poggio",
                "John Shawe-Taylor"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/zelenko03a/zelenko03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination",
            "abstract": [
                "Predictive models benefit from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for filters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach."
            ],
            "keywords": [
                "trees",
                "resampling",
                "importance",
                "masking",
                "residuals"
            ],
            "author": [
                "Eugene Tuv",
                "Alexander Borisov",
                "George Runger",
                "Kari Torkkola",
                "Isabelle Guyon",
                "Reza Amir"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/tuv09a/tuv09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "AffectiveTweets: a Weka Package for Analyzing Affect in Tweets",
            "abstract": [
                "AffectiveTweets is a set of programs for analyzing emotion and sentiment of social media messages such as tweets. It is implemented as a package for the Weka machine learning workbench and provides methods for calculating state-of-the-art affect analysis features from tweets that can be fed into machine learning algorithms implemented in Weka. It also implements methods for building affective lexicons and distant supervision methods for training affective models from unlabeled tweets. The package was used by several teams in the shared tasks: EmoInt 2017 and Affect in Tweets SemEval 2018 Task 1."
            ],
            "keywords": [
                "Twitter",
                "Sentiment Analysis",
                "Emotion Analysis",
                "Affective Computing",
                "Lexicon Induction",
                "Distant Supervison"
            ],
            "author": [
                "Felipe Bravo-Marquez",
                "Bernhard Pfahringer",
                "Saif M Mohammad",
                "Eibe Frank"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-450/18-450.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "End-to-End Training of Deep Visuomotor Policies",
            "abstract": [
                "Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-toend provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods."
            ],
            "keywords": [
                "Reinforcement Learning",
                "Optimal Control",
                "Vision",
                "Neural Networks"
            ],
            "author": [
                "Sergey Levine",
                "Chelsea Finn",
                "Trevor Darrell",
                "Pieter Abbeel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-522/15-522.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral Clustering Based on Local PCA",
            "abstract": [
                "We propose a spectral clustering method based on local principal components analysis (PCA). After performing local PCA in selected neighborhoods, the algorithm builds a nearest neighbor graph weighted according to a discrepancy between the principal subspaces in the neighborhoods, and then applies spectral clustering. As opposed to standard spectral methods based solely on pairwise distances between points, our algorithm is able to resolve intersections. We establish theoretical guarantees for simpler variants within a prototypical mathematical framework for multi-manifold clustering, and evaluate our algorithm on various simulated data sets."
            ],
            "keywords": [
                "multi-manifold clustering",
                "spectral clustering",
                "local principal component analysis",
                "intersecting clusters"
            ],
            "author": [
                "Ery Arias-Castro",
                "Gilad Lerman",
                "Teng Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-318/14-318.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "AdaGrad stepsizes: Sharp convergence over nonconvex landscapes",
            "abstract": [
                "Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to fine-tune the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization. We bridge this gap by providing theoretical guarantees for the convergence of AdaGrad for smooth, nonconvex functions. We show that the norm version of AdaGrad (AdaGrad-Norm) converges to a stationary point at the O(log(N)/ √ N) rate in the stochastic setting, and at the optimal O(1/N) rate in the batch (non-stochastic) setting-in this sense, our convergence guarantees are \"sharp\". In particular, the convergence of AdaGrad-Norm is robust to the choice of all hyperparameters of the algorithm, in contrast to stochastic gradient descent whose convergence depends crucially on tuning the step-size to the (generally unknown) Lipschitz smoothness constant and level of stochastic noise on the gradient. Extensive numerical experiments are provided to corroborate our theoretical findings; moreover, the experiments suggest that the robustness of AdaGrad-Norm extends to the models in deep learning."
            ],
            "keywords": [
                "nonconvex optimization",
                "stochastic offline learning",
                "large-scale optimization",
                "adaptive gradient descent",
                "convergence"
            ],
            "author": [
                "Rachel Ward",
                "Xiaoxia Wu",
                "Léon Bottou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-352/18-352.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits",
            "abstract": [
                "BayesOpt is a library with state-of-the-art Bayesian optimization methods to solve nonlinear optimization, stochastic bandits or sequential experimental design problems. Bayesian optimization characterized for being sample efficient as it builds a posterior distribution to capture the evidence and prior knowledge of the target function. Built in standard C++, the library is extremely efficient while being portable and flexible. It includes a common interface for C, C++, Python, Matlab and Octave."
            ],
            "keywords": [
                "Bayesian optimization",
                "efficient global optimization",
                "sequential model-based optimization",
                "sequential experimental design",
                "Gaussian processes"
            ],
            "author": [
                "Ruben Martinez-Cantin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/martinezcantin14a/martinezcantin14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Matched Bipartite Block Model with Covariates",
            "abstract": [
                "Community detection or clustering is a fundamental task in the analysis of network data. Many real networks have a bipartite structure which makes community detection challenging. In this paper, we consider a model which allows for matched communities in the bipartite setting, in addition to node covariates with information about the matching. We derive a simple fast algorithm for fitting the model based on variational inference ideas and show its effectiveness on both simulated and real data. A variation of the model to allow for degree-correction is also considered, in addition to a novel approach to fitting such degree-corrected models."
            ],
            "keywords": [
                "bipartite networks",
                "community detection",
                "stochastic block model",
                "bipartite matching",
                "node attributes"
            ],
            "author": [
                "Zahra S Razaee",
                "Jessica Jingyi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-153/17-153.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Efficient Large Margin Semisupervised Learning: Method and Theory",
            "abstract": [
                "In classification, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. This imposes a great challenge in that it is difficult to achieve good classification performance through labeled data alone. To leverage unlabeled data for enhancing classification, this article introduces a large margin semisupervised learning method within the framework of regularization, based on an efficient margin loss for unlabeled data, which seeks efficient extraction of the information from unlabeled data for estimating the Bayes decision boundary for classification. For implementation, an iterative scheme is derived through conditional expectations. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method enables to recover the performance of its supervised counterpart based on complete data in rates of convergence, when possible."
            ],
            "keywords": [
                "difference convex programming",
                "classification",
                "nonconvex minimization",
                "regularization",
                "support vectors"
            ],
            "author": [
                "Junhui Wang",
                "Xiaotong Shen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/wang09a/wang09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Case Study on Meta-Generalising: A Gaussian Processes Approach",
            "abstract": [
                "We propose a novel model for meta-generalisation, that is, performing prediction on novel tasks based on information from multiple different but related tasks. The model is based on two coupled Gaussian processes with structured covariance function; one model performs predictions by learning a constrained covariance function encapsulating the relations between the various training tasks, while the second model determines the similarity of new tasks to previously seen tasks. We demonstrate empirically on several real and synthetic data sets both the strengths of the approach and its limitations due to the distributional assumptions underpinning it."
            ],
            "keywords": [
                "transfer learning",
                "meta-generalising",
                "multi-task learning",
                "Gaussian processes",
                "mixture of experts"
            ],
            "author": [
                "Grigorios Skolidis",
                "Guido Sanguinetti",
                "Sören Sonnenburg",
                "Francis Bach",
                "Cheng Soon Ong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/skolidis12a/skolidis12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classification with a Reject Option using a Hinge Loss",
            "abstract": [
                "We consider the problem of binary classification where the classifier can, for a particular cost, choose not to classify an observation. Just as in the conventional classification problem, minimization of the sample average of the cost is a difficult optimization problem. As an alternative, we propose the optimization of a certain convex loss function φ, analogous to the hinge loss used in support vector machines (SVMs). Its convexity ensures that the sample average of this surrogate loss can be efficiently minimized. We study its statistical properties. We show that minimizing the expected surrogate loss-the φ-risk-also minimizes the risk. We also study the rate at which the φ-risk approaches its minimum value. We show that fast rates are possible when the conditional probability P(Y = 1|X) is unlikely to be close to certain critical values."
            ],
            "keywords": [
                "Bayes classifiers",
                "classification",
                "convex surrogate loss",
                "empirical risk minimization",
                "hinge loss",
                "large margin classifiers",
                "margin condition",
                "reject option",
                "support vector machines"
            ],
            "author": [
                "Peter L Bartlett",
                "Marten H Wegkamp"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/bartlett08a/bartlett08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Determinantal Point Processes for Coresets",
            "abstract": [
                "When faced with a data set too large to be processed all at once, an obvious solution is to retain only part of it. In practice this takes a wide variety of different forms, and among them \"coresets\" are especially appealing. A coreset is a (small) weighted sample of the original data that comes with the following guarantee: a cost function can be evaluated on the smaller set instead of the larger one, with low relative error. For some classes of problems, and via a careful choice of sampling distribution (based on the so-called \"sensitivity\" metric), iid random sampling has turned to be one of the most successful methods for building coresets efficiently. However, independent samples are sometimes overly redundant, and one could hope that enforcing diversity would lead to better performance. The difficulty lies in proving coreset properties in non-iid samples. We show that the coreset property holds for samples formed with determinantal point processes (DPP). DPPs are interesting because they are a rare example of repulsive point processes with tractable theoretical properties, enabling us to prove general coreset theorems. We apply our results to both the k-means and the linear regression problems, and give extensive empirical evidence that the small additional computational cost of DPP sampling comes with superior performance over its iid counterpart. Of independent interest, we also provide analytical formulas for the sensitivity in the linear regression and 1-means cases."
            ],
            "keywords": [
                "Coresets",
                "Determinantal Point Processes",
                "Sensitivity"
            ],
            "author": [
                "Nicolas Tremblay",
                "Simon Barthelmé",
                "Pierre-Olivier Amblard"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-167/18-167.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tapkee: An Efficient Dimension Reduction Library",
            "abstract": [
                "We present Tapkee, a C++ template library that provides efficient implementations of more than 20 widely used dimensionality reduction techniques ranging from Locally Linear Embedding (Roweis and Saul, 2000) and Isomap (de Silva and Tenenbaum, 2002) to the recently introduced Barnes-Hut-SNE (van der Maaten, 2013). Our library was designed with a focus on performance and flexibility. For performance, we combine efficient multi-core algorithms, modern data structures and state-of-the-art low-level libraries. To achieve flexibility, we designed a clean interface for applying methods to user data and provide a callback API that facilitates integration with the library. The library is freely available as open-source software and is distributed under the permissive BSD 3-clause license. We encourage the integration of Tapkee into other open-source toolboxes and libraries. For example, Tapkee has been integrated into the codebase of the Shogun toolbox (Sonnenburg et al., 2010), giving us access to a rich set of kernels, distance measures and bindings to common programming languages including Python, Octave, Matlab, R, Java, C#, Ruby, Perl and Lua. Source code, examples and documentation are available at http://tapkee.lisitsyn.me."
            ],
            "keywords": [
                "dimensionality reduction",
                "machine learning",
                "C++",
                "open source software"
            ],
            "author": [
                "Sergey Lisitsyn",
                "Christian Widmer",
                "Fernando J Iglesias Garcia"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/lisitsyn13a/lisitsyn13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast MCMC Sampling Algorithms on Polytopes",
            "abstract": [
                "We propose and analyze two new MCMC sampling algorithms, the Vaidya walk and the John walk, for generating samples from the uniform distribution over a polytope. Both random walks are sampling algorithms derived from interior point methods. The former is based on volumetric-logarithmic barrier introduced by Vaidya whereas the latter uses John's ellipsoids. We show that the Vaidya walk mixes in significantly fewer steps than the logarithmic-barrier based Dikin walk studied in past work. For a polytope in R d defined by n > d linear constraints, we show that the mixing time from a warm start is bounded as O n 0.5 d 1.5 , compared to the O (nd) mixing time bound for the Dikin walk. The cost of each step of the Vaidya walk is of the same order as the Dikin walk, and at most twice as large in terms of constant pre-factors. For the John walk, we prove an O d 2.5 • log 4 (n/d) bound on its mixing time and conjecture that an improved variant of it could achieve a mixing time of O d 2 • poly-log(n/d). Additionally, we propose variants of the Vaidya and John walks that mix in polynomial time from a deterministic starting point. The speed-up of the Vaidya walk over the Dikin walk are illustrated in numerical examples."
            ],
            "keywords": [
                "MCMC methods",
                "interior point methods",
                "polytopes",
                "sampling from convex sets"
            ],
            "author": [
                "Yuansi Chen",
                "Martin J Wainwright",
                "Voleon Group"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-158/18-158.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exact Guarantees on the Absence of Spurious Local Minima for Non-negative Rank-1 Robust Principal Component Analysis",
            "abstract": [
                "This work is concerned with the non-negative rank-1 robust principal component analysis (RPCA), where the goal is to recover the dominant non-negative principal components of a data matrix precisely, where a number of measurements could be grossly corrupted with sparse and arbitrary large noise. Most of the known techniques for solving the RPCA rely on convex relaxation methods by lifting the problem to a higher dimension, which significantly increase the number of variables. As an alternative, the well-known Burer-Monteiro approach can be used to cast the RPCA as a non-convex and non-smooth 1 optimization problem with a significantly smaller number of variables. In this work, we show that the low-dimensional formulation of the symmetric and asymmetric positive rank-1 RPCA based on the Burer-Monteiro approach has benign landscape, i.e., 1) it does not have any spurious local solution, 2) has a unique global solution, and 3) its unique global solution coincides with the true components. An implication of this result is that simple local search algorithms are guaranteed to achieve a zero global optimality gap when directly applied to the low-dimensional formulation. Furthermore, we provide strong deterministic and probabilistic guarantees for the exact recovery of the true principal components. In particular, it is shown that a constant fraction of the measurements could be grossly corrupted and yet they would not create any spurious local solution."
            ],
            "keywords": [],
            "author": [
                "Salar Fattahi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-884/18-884.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Entropy Discrimination Markov Networks",
            "abstract": [
                "The standard maximum margin approach for structured prediction lacks a straightforward probabilistic interpretation of the learning scheme and the prediction rule. Therefore its unique advantages such as dual sparseness and kernel tricks cannot be easily conjoined with the merits of a probabilistic model such as Bayesian regularization, model averaging, and ability to model hidden variables. In this paper, we present a new general framework called maximum entropy discrimination Markov networks (MaxEnDNet, or simply, MEDN), which integrates these two approaches and combines and extends their merits. Major innovations of this approach include: 1) It extends the conventional max-entropy discrimination learning of classification rules to a new structural maxentropy discrimination paradigm of learning a distribution of Markov networks. 2) It generalizes the extant Markov network structured-prediction rule based on a point estimator of model coefficients to an averaging model akin to a Bayesian predictor that integrates over a learned posterior distribution of model coefficients. 3) It admits flexible entropic regularization of the model during learning. By plugging in different prior distributions of the model coefficients, it subsumes the wellknown maximum margin Markov networks (M 3 N) as a special case, and leads to a model similar to an L 1-regularized M 3 N that is simultaneously primal and dual sparse, or other new types of Markov networks. 4) It applies a modular learning algorithm that combines existing variational inference techniques and convex-optimization based M 3 N solvers as subroutines. Essentially, MEDN can be understood as a jointly maximum likelihood and maximum margin estimate of Markov network. It represents the first successful attempt to combine maximum entropy learning (a dual form of maximum likelihood learning) with maximum margin learning of Markov network for structured input/output problems; and the basic principle can be generalized to learning arbitrary graphical models, such as the generative Bayesian networks or models with structured hidden variables. We discuss a number of theoretical properties of this approach, and show that empirically it outperforms a wide array of competing methods for structured input/output learning on both synthetic and real OCR and web data extraction data sets."
            ],
            "keywords": [
                "maximum entropy discrimination",
                "structured input/output model",
                "maximum margin Markov network",
                "graphical models",
                "entropic regularization",
                "L 1 regularization"
            ],
            "author": [
                "Jun Zhu",
                "Eric P Xing"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/zhu09a/zhu09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Machine Learning with Data Dependent Hypothesis Classes",
            "abstract": [
                "We extend the VC theory of statistical learning to data dependent spaces of classifiers. This theory can be viewed as a decomposition of classifier design into two components; the first component is a restriction to a data dependent hypothesis class and the second is empirical risk minimization within that class. We define a measure of complexity for data dependent hypothesis classes and provide data dependent versions of bounds on error deviance and estimation error. We also provide a structural risk minimization procedure over data dependent hierarchies and prove consistency. We use this theory to provide a framework for studying the trade-offs between performance and computational complexity in classifier design. As a consequence we obtain a new family of classifiers with dimension independent performance bounds and efficient learning procedures."
            ],
            "keywords": [
                "Computational Learning Theory",
                "Empirical Process Theory",
                "Classification",
                "Shatter Coefficient",
                "Structural Risk Minimization"
            ],
            "author": [
                "Adam Cannon",
                "J Mark Ettinger",
                "Clint Scovel"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/cannon02a/cannon02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bounded Kernel-Based Online Learning *",
            "abstract": [
                "A common problem of kernel-based online algorithms, such as the kernel-based Perceptron algorithm, is the amount of memory required to store the online hypothesis, which may increase without bound as the algorithm progresses. Furthermore, the computational load of such algorithms grows linearly with the amount of memory used to store the hypothesis. To attack these problems, most previous work has focused on discarding some of the instances, in order to keep the memory bounded. In this paper we present a new algorithm, in which the instances are not discarded, but are instead projected onto the space spanned by the previous online hypothesis. We call this algorithm Projectron. While the memory size of the Projectron solution cannot be predicted before training, we prove that its solution is guaranteed to be bounded. We derive a relative mistake bound for the proposed algorithm, and deduce from it a slightly different algorithm which outperforms the Perceptron. We call this second algorithm Projectron++. We show that this algorithm can be extended to handle the multiclass and the structured output settings, resulting, as far as we know, in the first online bounded algorithm that can learn complex classification tasks. The method of bounding the hypothesis representation can be applied to any conservative online algorithm and to other online algorithms, as it is demonstrated for ALMA 2. Experimental results on various data sets show the empirical advantage of our technique compared to various bounded online algorithms, both in terms of memory and accuracy."
            ],
            "keywords": [
                "online learning",
                "kernel methods",
                "support vector machines",
                "bounded support set"
            ],
            "author": [
                "Francesco Orabona",
                "Joseph Keshet",
                "Barbara Caputo"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/orabona09a/orabona09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sally: A Tool for Embedding Strings in Vector Spaces",
            "abstract": [
                "Strings and sequences are ubiquitous in many areas of data analysis. However, only few learning methods can be directly applied to this form of data. We present Sally, a tool for embedding strings in vector spaces that allows for applying a wide range of learning methods to string data. Sally implements a generalized form of the bag-of-words model, where strings are mapped to a vector space that is spanned by a set of string features, such as words or n-grams of words. The implementation of Sally builds on efficient string algorithms and enables processing millions of strings and features. The tool supports several data formats and is capable of interfacing with common learning environments, such as Weka, Shogun, Matlab, or Pylab. Sally has been successfully applied for learning with natural language text, DNA sequences and monitored program behavior."
            ],
            "keywords": [
                "string embedding",
                "bag-of-words models",
                "learning with sequential data"
            ],
            "author": [
                "Konrad Rieck",
                "Alexander Bikadorov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/rieck12a/rieck12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Extrapolating Expected Accuracies for Large Multi-Class Problems",
            "abstract": [
                "The difficulty of multi-class classification generally increases with the number of classes. Using data for a small set of the classes, can we predict how well the classifier scales as the number of classes increases? We propose a framework for studying this question, assuming that classes in both sets are sampled from the same population and that the classifier is based on independently learned scoring functions. Under this framework, we can express the classification accuracy on a set of k classes as the (k − 1)st moment of a discriminability function; the discriminability function itself does not depend on k. We leverage this result to develop a non-parametric regression estimator for the discriminability function, which can extrapolate accuracy results to larger unobserved sets. We also formalize an alternative approach that extrapolates accuracy separately for each class, and identify tradeoffs between the two methods. We show that both methods can accurately predict classifier performance on label sets up to ten times the size of the original set, both in simulations as well as in realistic face recognition or character recognition tasks."
            ],
            "keywords": [
                "Multi-class problems",
                "face recognition",
                "object recognition",
                "transfer learning",
                "nonparametric models"
            ],
            "author": [
                "Charles Zheng",
                "Rakesh Achanta",
                "Yuval Benjamini"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-701/17-701.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Core Vector Machines: Fast SVM Training on Very Large Data Sets",
            "abstract": [
                "Standard SVM training has O(m 3) time and O(m) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such \"approximateness\" in this paper. We first show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efficient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and realworld data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about five million training patterns, in only 1.4 seconds on a 3.2GHz Pentium-4 PC."
            ],
            "keywords": [
                "kernel methods",
                "approximation algorithm",
                "minimum enclosing ball",
                "core set",
                "scalability"
            ],
            "author": [
                "Ivor W Tsang",
                "James T Kwok",
                "Pak-Ming Cheung"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/tsang05a/tsang05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-class Heterogeneous Domain Adaptation",
            "abstract": [
                "A crucial issue in heterogeneous domain adaptation (HDA) is the ability to learn a feature mapping between different types of features across domains. Inspired by language translation, a word translated from one language corresponds to only a few words in another language, we present an efficient method named Sparse Heterogeneous Feature Representation (SHFR) in this paper for multi-class HDA to learn a sparse feature transformation between domains with multiple classes. Specifically, we formulate the problem of learning the feature transformation as a compressed sensing problem by building multiple binary classifiers in the target domain as various measurement sensors, which are decomposed from the target multi-class classification problem. We show that the estimation error of the learned transformation decreases with the increasing number of binary classifiers. In other words, for adaptation across heterogeneous domains to be successful, it is necessary to construct a sufficient number of incoherent binary classifiers from the original multi-class classification problem. To achieve this, we propose to apply the error correcting output correcting (ECOC) scheme to generate incoherent classifiers. To speed up the learning of the feature transformation across domains, we apply an efficient batch-mode algorithm to solve the resultant nonnegative sparse recovery problem. Theoretically, we present a generalization error bound of our proposed HDA method under a multi-class setting. Lastly, we conduct extensive experiments on both synthetic and real-world datasets to demonstrate the superiority of our proposed method over existing state-of-the-art HDA methods in terms of prediction accuracy and training efficiency."
            ],
            "keywords": [
                "Heterogeneous domain adaptation",
                "multi-class classification",
                "compressed sensing"
            ],
            "author": [
                "Joey Tianyi Zhou",
                "Ivor W Tsang",
                "Sydney Australia"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/13-580/13-580.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variable Sparsity Kernel Learning",
            "abstract": [
                "This paper 1 presents novel algorithms and applications for a particular class of mixed-norm regularization based Multiple Kernel Learning (MKL) formulations. The formulations assume that the given kernels are grouped and employ l 1 norm regularization for promoting sparsity within RKHS norms of each group and l s , s ≥ 2 norm regularization for promoting non-sparse combinations across groups. Various sparsity levels in combining the kernels can be achieved by varying the grouping of kernels-hence we name the formulations as Variable Sparsity Kernel Learning (VSKL) formulations. While previous attempts have a non-convex formulation, here we present a convex formulation which admits efficient Mirror-Descent (MD) based solving techniques. The proposed MD based algorithm optimizes over product of simplices and has a computational complexity of O m 2 n tot log n max /ε 2 where m is no. training data points, n max , n tot are the maximum no. kernels in any group, total no. kernels respectively and ε is the error in approximating the objective. A detailed proof of convergence of the algorithm is also presented. Experimental results show that the VSKL formulations are well-suited for multi-modal learning tasks like object categorization. Results also show that the MD based algorithm outperforms state-of-the-art MKL solvers in terms of computational efficiency."
            ],
            "keywords": [
                "multiple kernel learning",
                "mirror descent",
                "mixed-norm",
                "object categorization",
                "scalability"
            ],
            "author": [
                "Jonathan Aflalo",
                "Aharon Ben-Tal",
                "Jagarlapudi Saketha Nath",
                "Sankaran Raman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/aflalo11a/aflalo11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Angle-based Multicategory Distance-weighted SVM",
            "abstract": [
                "Classification is an important supervised learning technique with numerous applications. We develop an angle-based multicategory distance-weighted support vector machine (MD-WSVM) classification method that is motivated from the binary distance-weighted support vector machine (DWSVM) classification method. The new method has the merits of both support vector machine (SVM) and distance-weighted discrimination (DWD) but also alleviates both the data piling issue of SVM and the imbalanced data issue of DWD. Theoretical and numerical studies demonstrate the advantages of MDWSVM method over existing angle-based methods."
            ],
            "keywords": [
                "Discriminant analysis",
                "Imbalanced data",
                "High dimension",
                "Support vector machine",
                "Distance-weighted discrimination"
            ],
            "author": [
                "Hui Sun",
                "Bruce A Craig",
                "Lingsong Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-003/17-003.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Probabilistic Symmetries and Invariant Neural Networks",
            "abstract": [
                "Treating neural network inputs and outputs as random variables, we characterize the structure of neural networks that can be used to model data that are invariant or equivariant under the action of a compact group. Much recent research has been devoted to encoding invariance under symmetry transformations into neural network architectures, in an effort to improve the performance of deep neural networks in data-scarce, non-i.i.d., or unsupervised settings. By considering group invariance from the perspective of probabilistic symmetry, we establish a link between functional and probabilistic symmetry, and obtain generative functional representations of probability distributions that are invariant or equivariant under the action of a compact group. Our representations completely characterize the structure of neural networks that can be used to model such distributions and yield a general program for constructing invariant stochastic or deterministic neural networks. We demonstrate that examples from the recent literature are special cases, and develop the details of the general program for exchangeable sequences and arrays."
            ],
            "keywords": [
                "probabilistic symmetry",
                "convolutional neural networks",
                "exchangeability",
                "neural architectures",
                "invariance",
                "equivariance",
                "sufficiency",
                "adequacy",
                "graph neural networks"
            ],
            "author": [
                "Benjamin Bloem-Reddy",
                "Yee Whye Teh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-322/19-322.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Accelerated Neural Evolution through Cooperatively Coevolved Synapses",
            "abstract": [
                "Many complex control problems require sophisticated solutions that are not amenable to traditional controller design. Not only is it difficult to model real world systems, but often it is unclear what kind of behavior is required to solve the task. Reinforcement learning (RL) approaches have made progress by using direct interaction with the task environment, but have so far not scaled well to large state spaces and environments that are not fully observable. In recent years, neuroevolution, the artificial evolution of neural networks, has had remarkable success in tasks that exhibit these two properties. In this paper, we compare a neuroevolution method called Cooperative Synapse Neuroevolution (CoSyNE), that uses cooperative coevolution at the level of individual synaptic weights, to a broad range of reinforcement learning algorithms on very difficult versions of the pole balancing problem that involve large (continuous) state spaces and hidden state. CoSyNE is shown to be significantly more efficient and powerful than the other methods on these tasks."
            ],
            "keywords": [
                "coevolution",
                "recurrent neural networks",
                "non-linear control",
                "genetic algorithms",
                "experimental comparison"
            ],
            "author": [
                "Faustino Gomez",
                "Risto Miikkulainen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/gomez08a/gomez08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Extensions to Metric-Based Model Selection",
            "abstract": [
                "Metric-based methods have recently been introduced for model selection and regularization, often yielding very significant improvements over the alternatives tried (including cross-validation). All these methods require unlabeled data over which to compare functions and detect gross differences in behavior away from the training points. We introduce three new extensions of the metric model selection methods and apply them to feature selection. The first extension takes advantage of the particular case of time-series data in which the task involves prediction with a horizon h. The idea is to use at t the h unlabeled examples that precede t for model selection. The second extension takes advantage of the different error distributions of cross-validation and the metric methods: crossvalidation tends to have a larger variance and is unbiased. A hybrid combining the two model selection methods is rarely beaten by any of the two methods. The third extension deals with the case when unlabeled data is not available at all, using an estimated input density. Experiments are described to study these extensions in the context of capacity control and feature subset selection."
            ],
            "keywords": [],
            "author": [
                "Yoshua Bengio",
                "Nicolas Chapados",
                "Isabelle Guyon",
                "André Elisseeff"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/bengio03b/bengio03b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks",
            "abstract": [
                "Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low-dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs."
            ],
            "keywords": [
                "short-term memory",
                "recurrent neural networks",
                "sparse signal recovery",
                "low-rank recovery",
                "restricted isometry property"
            ],
            "author": [
                "Adam S Charles",
                "Dong Yin",
                "Christopher J Rozell"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-270/16-270.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Approximation and Generalization of Deep Neural Network with Intrinsic Dimensionality",
            "abstract": [
                "In this study, we prove that an intrinsic low dimensionality of covariates is the main factor that determines the performance of deep neural networks (DNNs). DNNs generally provide outstanding empirical performance. Hence, numerous studies have actively investigated the theoretical properties of DNNs to understand their underlying mechanisms. In particular, the behavior of DNNs in terms of high-dimensional data is one of the most critical questions. However, this issue has not been sufficiently investigated from the aspect of covariates, although high-dimensional data have practically low intrinsic dimensionality. In this study, we derive bounds for an approximation error and a generalization error regarding DNNs with intrinsically low dimensional covariates. We apply the notion of the Minkowski dimension and develop a novel proof technique. Consequently, we show that convergence rates of the errors by DNNs do not depend on the nominal high dimensionality of data, but on its lower intrinsic dimension. We further prove that the rate is optimal in the minimax sense. We identify an advantage of DNNs by showing that DNNs can handle a broader class of intrinsic low dimensional data than other adaptive estimators. Finally, we conduct a numerical simulation to validate the theoretical results."
            ],
            "keywords": [
                "Deep Learning",
                "Deep Neural Network",
                "Generalization Analysis",
                "Intrinsic Dimension",
                "Minimax Optimal Rate"
            ],
            "author": [
                "Ryumei Nakada"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-002/20-002.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quasi-Geodesic Neural Learning Algorithms Over the Orthogonal Group: A Tutorial",
            "abstract": [
                "The aim of this contribution is to present a tutorial on learning algorithms for a single neural layer whose connection matrix belongs to the orthogonal group. The algorithms exploit geodesics appropriately connected as piece-wise approximate integrals of the exact differential learning equation. The considered learning equations essentially arise from the Riemannian-gradient-based optimization theory with deterministic and diffusion-type gradient. The paper aims specifically at reviewing the relevant mathematics (and at presenting it in as much transparent way as possible in order to make it accessible to readers that do not possess a background in differential geometry), at bringing together modern optimization methods on manifolds and at comparing the different algorithms on a common machine learning problem. As a numerical case-study, we consider an application to non-negative independent component analysis, although it should be recognized that Riemannian gradient methods give rise to general-purpose algorithms, by no means limited to ICA-related applications."
            ],
            "keywords": [
                "differential geometry",
                "diffusion-type gradient",
                "Lie groups",
                "non-negative independent component analysis",
                "Riemannian gradient"
            ],
            "author": [
                "Simone Fiori"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/fiori05a/fiori05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Family of Simple Non-Parametric Kernel Learning Algorithms",
            "abstract": [
                "Previous studies of Non-Parametric Kernel Learning (NPKL) usually formulate the learning task as a Semi-Definite Programming (SDP) problem that is often solved by some general purpose SDP solvers. However, for N data examples, the time complexity of NPKL using a standard interiorpoint SDP solver could be as high as O(N 6.5), which prohibits NPKL methods applicable to real applications, even for data sets of moderate size. In this paper, we present a family of efficient NPKL algorithms, termed \"SimpleNPKL\", which can learn non-parametric kernels from a large set of pairwise constraints efficiently. In particular, we propose two efficient SimpleNPKL algorithms. One is SimpleNPKL algorithm with linear loss, which enjoys a closed-form solution that can be efficiently computed by the Lanczos sparse eigen decomposition technique. Another one is SimpleNPKL algorithm with other loss functions (including square hinge loss, hinge loss, square loss) that can be re-formulated as a saddle-point optimization problem, which can be further resolved by a fast iterative algorithm. In contrast to the previous NPKL approaches, our empirical results show that the proposed new technique, maintaining the same accuracy, is significantly more efficient and scalable. Finally, we also demonstrate that the proposed new technique is also applicable to speed up many kernel learning tasks, including colored maximum variance unfolding, minimum volume embedding, and structure preserving embedding."
            ],
            "keywords": [
                "non-parametric kernel learning",
                "semi-definite programming",
                "semi-supervised learning",
                "side information",
                "pairwise constraints"
            ],
            "author": [
                "Jinfeng Zhuang",
                "Ivor W Tsang",
                "Steven C H Hoi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/zhuang11a/zhuang11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning the Structure of Linear Latent Variable Models",
            "abstract": [
                "We describe anytime search procedures that (1) find disjoint subsets of recorded variables for which the members of each subset are d-separated by a single common unrecorded cause, if such exists; (2) return information about the causal relations among the latent factors so identified. We prove the procedure is point-wise consistent assuming (a) the causal relations can be represented by a directed acyclic graph (DAG) satisfying the Markov Assumption and the Faithfulness Assumption; (b) unrecorded variables are not caused by recorded variables; and (c) dependencies are linear. We compare the procedure with standard approaches over a variety of simulated structures and sample sizes, and illustrate its practical value with brief studies of social science data sets. Finally, we consider generalizations for non-linear systems."
            ],
            "keywords": [
                "latent variable models",
                "causality",
                "graphical models"
            ],
            "author": [
                "Ricardo Silva",
                "Clark Glymour",
                "Peter Spirtes"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/silva06a/silva06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The CAM Software for Nonnegative Blind Source Separation in R-Java",
            "abstract": [
                "We describe a R-Java CAM (convex analysis of mixtures) package that provides comprehensive analytic functions and a graphic user interface (GUI) for blindly separating mixed nonnegative sources. This open-source multiplatform software implements recent and classic algorithms in the literature including Chan et al. (2008), Wang et al. (2010), Chen et al. (2011a) and Chen et al. (2011b). The CAM package offers several attractive features: (1) instead of using proprietary MATLAB, its analytic functions are written in R, which makes the codes more portable and easier to modify; (2) besides producing and plotting results in R, it also provides a Java GUI for automatic progress update and convenient visual monitoring; (3) multi-thread interactions between the R and Java modules are driven and integrated by a Java GUI, assuring that the whole CAM software runs responsively; (4) the package offers a simple mechanism to allow others to plug-in additional R-functions."
            ],
            "keywords": [
                "convex analysis of mixtures",
                "blind source separation",
                "affinity propagation clustering",
                "compartment modeling",
                "information-based model selection"
            ],
            "author": [
                "Niya Wang",
                "Fan Meng",
                "Li Chen",
                "Subha Madhavan",
                "Robert Clarke",
                "Eric P Hoffman",
                "Jianhua Xuan",
                "Yue Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/wang13d/wang13d.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonextensive Information Theoretic Kernels on Measures *",
            "abstract": [
                "Positive definite kernels on probability measures have been recently applied to classification problems involving text, images, and other types of structured data. Some of these kernels are related to classic information theoretic quantities, such as (Shannon's) mutual information and the Jensen-Shannon (JS) divergence. Meanwhile, there have been recent advances in nonextensive generalizations of Shannon's information theory. This paper bridges these two trends by introducing nonextensive information theoretic kernels on probability measures, based on new JS-type divergences. These new divergences result from extending the the two building blocks of the classical JS divergence: convexity and Shannon's entropy. The notion of convexity is extended to the wider concept of q-convexity, for which we prove a Jensen q-inequality. Based on this inequality, we introduce Jensen-Tsallis (JT) q-differences, a nonextensive generalization of the JS divergence, and define a k-th order JT q-difference between stochastic processes. We then define a new family of nonextensive mutual information kernels, which allow weights to be assigned to their arguments, and which includes the Boolean, JS, and linear kernels as particular cases. Nonextensive string kernels are also defined that generalize the p-spectrum kernel. We illustrate the performance of these kernels on text categorization tasks, in which documents are modeled both as bags of words and as sequences of characters."
            ],
            "keywords": [
                "positive definite kernels",
                "nonextensive information theory",
                "Tsallis entropy",
                "Jensen-Shannon divergence",
                "string kernels"
            ],
            "author": [
                "André F T Martins",
                "Noah A Smith",
                "Eric P Xing",
                "Pedro M Q Aguiar",
                "F T Martins",
                "Mário A T Figueiredo",
                "SMITH, XING, AGUIAR AND FIGUEIREDO Martins"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/martins09a/martins09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning to Identify Concise Regular Expressions that Describe Email Campaigns",
            "abstract": [
                "This paper addresses the problem of inferring a regular expression from a given set of strings that resembles, as closely as possible, the regular expression that a human expert would have written to identify the language. This is motivated by our goal of automating the task of postmasters who use regular expressions to describe and blacklist email spam campaigns. Training data contains batches of messages and corresponding regular expressions that an expert postmaster feels confident to blacklist. We model this task as a two-stage learning problem with structured output spaces and appropriate loss functions. We derive decoders and the resulting optimization problems which can be solved using standard cutting plane methods. We report on a case study conducted with an email service provider."
            ],
            "keywords": [
                "applications of machine learning",
                "learning with structured output spaces",
                "supervised learning",
                "regular expressions",
                "email campaigns"
            ],
            "author": [
                "Paul Prasse",
                "Christoph Sawade",
                "Niels Landwehr",
                "Tobias Scheffer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/prasse15a/prasse15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Order-Independent Constraint-Based Causal Structure Learning",
            "abstract": [
                "We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI-and CCD-algorithms ("
            ],
            "keywords": [
                "directed acyclic graph",
                "PC-algorithm",
                "FCI-algorithm",
                "CCD-algorithm",
                "orderdependence",
                "consistency",
                "high-dimensional data"
            ],
            "author": [
                "Diego Colombo",
                "Marloes H Maathuis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/colombo14a/colombo14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Modular Proximal Optimization for Multidimensional Total-Variation Regularizatioń",
            "abstract": [
                "We study TV regularization, a widely used technique for eliciting structured sparsity. In particular, we propose efficient algorithms for computing prox-operators for p-norm TV. The most important among these is-norm TV, for whose prox-operator we present a new geometric analysis which unveils a hitherto unknown connection to taut-string methods. This connection turns out to be remarkably useful as it shows how our geometry guided implementation results in efficient weighted and unweighted 1D-TV solvers, surpassing stateof-the-art methods. Our 1D-TV solvers provide the backbone for building more complex (two or higher-dimensional) TV solvers within a modular proximal optimization approach. We review the literature for an array of methods exploiting this strategy, and illustrate the benefits of our modular design through extensive suite of experiments on (i) image denoising, (ii) image deconvolution, (iii) four variants of fused-lasso, and (iv) video denoising. To underscore our claims and permit easy reproducibility, we provide all the reviewed and our new TV solvers in an easy to use multi-threaded C++, Matlab and Python library."
            ],
            "keywords": [
                "proximal optimization",
                "total variation",
                "regularized learning",
                "sparsity",
                "nonsmooth optimization"
            ],
            "author": [
                "Alvaro Barbero",
                "Suvrit Sra"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/13-538/13-538.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Family of Additive Online Algorithms for Category Ranking",
            "abstract": [
                "We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stem from recent advances in online learning algorithms. The algorithms are simple to implement and are also time and memory efficient. We provide a unified analysis of the family of algorithms in the mistake bound model. We then discuss experiments with the proposed family of topic-ranking algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora, the algorithms we present achieve state-of-the-art results and outperforms topic-ranking adaptations of Rocchio's algorithm and of the Perceptron algorithm."
            ],
            "keywords": [],
            "author": [
                "Koby Crammer",
                "Jaz Kandola",
                "Thomas Hofmann",
                "Tomaso Poggio",
                "John Shawe-Taylor"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/crammer03b/crammer03b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Necessity of Irrelevant Variables",
            "abstract": [
                "This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant."
            ],
            "keywords": [
                "feature selection",
                "generalization",
                "learning theory"
            ],
            "author": [
                "David P Helmbold",
                "Philip M Long"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/helmbold12a/helmbold12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear Dimensionality Reduction: Survey, Insights, and Generalizations",
            "abstract": [
                "Linear dimensionality reduction methods are a cornerstone of analyzing high dimensional data, due to their simple geometric interpretations and typically attractive computational properties. These methods capture many data features of interest, such as covariance, dynamical structure, correlation between data sets, input-output relationships, and margin between data classes. Methods have been developed with a variety of names and motivations in many fields, and perhaps as a result the connections between all these methods have not been highlighted. Here we survey methods from this disparate literature as optimization programs over matrix manifolds. We discuss principal component analysis, factor analysis, linear multidimensional scaling, Fisher's linear discriminant analysis, canonical correlations analysis, maximum autocorrelation factors, slow feature analysis, sufficient dimensionality reduction, undercomplete independent component analysis, linear regression, distance metric learning, and more. This optimization framework gives insight to some rarely discussed shortcomings of well-known methods, such as the suboptimality of certain eigenvector solutions. Modern techniques for optimization over matrix manifolds enable a generic linear dimensionality reduction solver, which accepts as input data and an objective to be optimized, and returns, as output, an optimal low-dimensional projection of the data. This simple optimization framework further allows straightforward generalizations and novel variants of classical methods, which we demonstrate here by creating an orthogonal-projection canonical correlations analysis. More broadly, this survey and generic solver suggest that linear dimensionality reduction can move toward becoming a blackbox, objective-agnostic numerical technology."
            ],
            "keywords": [
                "dimensionality reduction",
                "eigenvector problems",
                "matrix manifolds"
            ],
            "author": [
                "John P Cunningham",
                "Zoubin Ghahramani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/cunningham15a/cunningham15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Library for Locally Weighted Projection Regression",
            "abstract": [
                "In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages."
            ],
            "keywords": [
                "regression",
                "local learning",
                "online learning",
                "C",
                "C ++",
                "Matlab",
                "Octave",
                "Python"
            ],
            "author": [
                "Stefan Klanke",
                "Stefan Schaal"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/klanke08a/klanke08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models",
            "abstract": [
                "Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian field, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model's partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP's local matching of moments. Through the expansion, we see that EP is correct to first order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution."
            ],
            "keywords": [
                "expectation consistent inference",
                "expectation propagation",
                "perturbation correction",
                "Wick expansions",
                "Ising model",
                "Gaussian process"
            ],
            "author": [
                "Manfred Opper",
                "Ulrich Paquet",
                "Ole Winther"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/opper13a/opper13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Expectation Propagation for Neural Networks with Sparsity-Promoting Priors",
            "abstract": [
                "We propose a novel approach for nonlinear regression using a two-layer neural network (NN) model structure with sparsity-favoring hierarchical priors on the network weights. We present an expectation propagation (EP) approach for approximate integration over the posterior distribution of the weights, the hierarchical scale parameters of the priors, and the residual scale. Using a factorized posterior approximation we derive a computationally efficient algorithm, whose complexity scales similarly to an ensemble of independent sparse linear models. The approach enables flexible definition of weight priors with different sparseness properties such as independent Laplace priors with a common scale parameter or Gaussian automatic relevance determination (ARD) priors with different relevance parameters for all inputs. The approach can be extended beyond standard activation functions and NN model structures to form flexible nonlinear predictors from multiple sparse linear models. The effects of the hierarchical priors and the predictive performance of the algorithm are assessed using both simulated and real-world data. Comparisons are made to two alternative models with ARD priors: a Gaussian process with a NN covariance function and marginal maximum a posteriori estimates of the relevance parameters, and a NN with Markov chain Monte Carlo integration over all the unknown model parameters."
            ],
            "keywords": [
                "expectation propagation",
                "neural network",
                "multilayer perceptron",
                "linear model",
                "sparse prior",
                "automatic relevance determination"
            ],
            "author": [
                "Pasi Jylänki",
                "Aapo Nummenmaa"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/jylanki14a/jylanki14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kriging Prediction with Isotropic Matérn Correlations: Robustness and Experimental Designs",
            "abstract": [
                "This work investigates the prediction performance of the kriging predictors. We derive some error bounds for the prediction error in terms of non-asymptotic probability under the uniform metric and L p metrics when the spectral densities of both the true and the imposed correlation functions decay algebraically. The Matérn family is a prominent class of correlation functions of this kind. Our analysis shows that, when the smoothness of the imposed correlation function exceeds that of the true correlation function, the prediction error becomes more sensitive to the space-filling property of the design points. In particular, the kriging predictor can still reach the optimal rate of convergence, if the experimental design scheme is quasi-uniform. Lower bounds of the kriging prediction error are also derived under the uniform metric and L p metrics. An accurate characterization of this error is obtained, when an oversmoothed correlation function and a space-filling design is used."
            ],
            "keywords": [
                "Computer Experiments",
                "Uncertainty Quantification",
                "Scattered Data Approximation",
                "Space-filling Designs",
                "Bayesian Machine Learning"
            ],
            "author": [
                "Rui Tuo",
                "Wm Michael Barnes '",
                "Wenjia Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-1045/19-1045.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Topology Selection in Graphical Models of Autoregressive Processes",
            "abstract": [
                "An algorithm is presented for topology selection in graphical models of autoregressive Gaussian time series. The graph topology of the model represents the sparsity pattern of the inverse spectrum of the time series and characterizes conditional independence relations between the variables. The method proposed in the paper is based on an ℓ 1-type nonsmooth regularization of the conditional maximum likelihood estimation problem. We show that this reduces to a convex optimization problem and describe a large-scale algorithm that solves the dual problem via the gradient projection method. Results of experiments with randomly generated and real data sets are also included."
            ],
            "keywords": [
                "graphical models",
                "time series",
                "topology selection",
                "convex optimization"
            ],
            "author": [
                "Jitkomut Songsiri"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/songsiri10a/songsiri10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Sufficient Dimension Reduction Through Sliced Inverse Regression",
            "abstract": [
                "Sliced inverse regression is an effective paradigm that achieves the goal of dimension reduction through replacing high dimensional covariates with a small number of linear combinations. It does not impose parametric assumptions on the dependence structure. More importantly, such a reduction of dimension is sufficient in that it does not cause loss of information. In this paper, we adapt the stationary sliced inverse regression to cope with the rapidly changing environments. We propose to implement sliced inverse regression in an online fashion. This online learner consists of two steps. In the first step we construct an online estimate for the kernel matrix; in the second step we propose two online algorithms, one is motivated by the perturbation method and the other is originated from the gradient descent optimization, to perform online singular value decomposition. The theoretical properties of this online learner are established. We demonstrate the numerical performance of this online learner through simulations and real world applications. All numerical studies confirm that this online learner performs as well as the batch learner."
            ],
            "keywords": [
                "Dimension reduction",
                "online learning",
                "perturbation",
                "singular value decomposition",
                "sliced inverse regression",
                "gradient descent"
            ],
            "author": [
                "Zhanrui Cai",
                "Runze Li",
                "Liping Zhu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-567/18-567.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "OLPS: A Toolbox for On-Line Portfolio Selection",
            "abstract": [
                "On-line portfolio selection is a practical financial engineering problem, which aims to sequentially allocate capital among a set of assets in order to maximize long-term return. In recent years, a variety of machine learning algorithms have been proposed to address this challenging problem, but no comprehensive open-source toolbox has been released for various reasons. This article presents the first open-source toolbox for \"On-Line Portfolio Selection\" (OLPS), which implements a collection of classical and state-of-the-art strategies powered by machine learning algorithms. We hope that OLPS can facilitate the development of new learning methods and enable the performance benchmarking and comparisons of different strategies."
            ],
            "keywords": [
                "On-line portfolio selection",
                "online learning",
                "trading system",
                "simulation"
            ],
            "author": [
                "Bin Li",
                "Steven C H Hoi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-317/15-317.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels *",
            "abstract": [
                "We consider the problem of improving the efficiency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large data sets. These approximate feature maps arise as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo (QMC) approximations instead, where the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy based on theoretical characterizations of the integration error with respect to a given sequence. We then propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization. Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of classical and adaptive QMC techniques for this problem."
            ],
            "keywords": [],
            "author": [
                "Haim Avron",
                "Jiyan Yang",
                "Stanford Edu",
                "Michael W Mahoney",
                "Vikas Sindhwani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-538/14-538.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "In Search of Coherence and Consensus: Measuring the Interpretability of Statistical Topics",
            "abstract": [
                "Topic modeling is an important tool in natural language processing. Topic models provide two forms of output. The first is a predictive model. This type of model has the ability to predict unseen documents (e.g., their categories). When topic models are used in this way, there are ample measures to assess their performance. The second output of these models is the topics themselves. Topics are lists of keywords that describe the top words pertaining to each topic. Often, these lists of keywords are presented to a human subject who then assesses the meaning of the topic, which is ultimately subjective. One of the fundamental problems of topic models lies in assessing the quality of the topics from the perspective of human interpretability. Naturally, human subjects need to be employed to evaluate interpretability of a topic. Lately, crowdsourcing approaches are widely used to serve the role of human subjects in evaluation. In this work we study measures of interpretability and propose to measure topic interpretability from two perspectives: topic coherence and topic consensus. We start with an existing measure for topic coherence-model precision. It evaluates coherence of a topic by introducing an intruded word and measuring how well a human subject or a crowdsourcing approach could identify the intruded word: if it is easy to identify, the topic is coherent. We then investigate how we can measure coherence comprehensively by examining dimensions of topic coherence. For the second perspective of topic interpretability, we suggest topic consensus that measures how well the results of a crowdsourcing approach matches those given categories of topics. Good topics should lead to good categories, thus, high topic consensus. Therefore, if there is low topic consensus in terms of categories, topics could be of low interpretability. We then further discuss how topic coherence and topic consensus assess different aspects of topic interpretability and hope that this work can pave way for comprehensive measures of topic interpretability."
            ],
            "keywords": [],
            "author": [
                "Fred Morstatter",
                "Huan Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-069/17-069.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MFE: Towards reproducible meta-feature extraction",
            "abstract": [
                "Automated recommendation of machine learning algorithms is receiving a large deal of attention, not only because they can recommend the most suitable algorithms for a new task, but also because they can support efficient hyper-parameter tuning, leading to better machine learning solutions. The automated recommendation can be implemented using meta-learning, learning from previous learning experiences, to create a meta-model able to associate a data set to the predictive performance of machine learning algorithms. Although a large number of publications report the use of meta-learning, reproduction and comparison of meta-learning experiments is a difficult task. The literature lacks extensive and comprehensive public tools that enable the reproducible investigation of the different meta-learning approaches. An alternative to deal with this difficulty is to develop a meta-feature extractor package with the main characterization measures, following uniform guidelines that facilitate the use and inclusion of new meta-features. In this paper, we propose two Meta-Feature Extractor (MFE) packages, written in both Python and R, to fill this lack. The packages follow recent frameworks for meta-feature extraction, aiming to facilitate the reproducibility of meta-learning experiments."
            ],
            "keywords": [
                "Machine Learning",
                "AutoML",
                "Meta-Learning",
                "Meta-Features"
            ],
            "author": [
                "Edesio Alcobaça",
                "Felipe Siqueira",
                "Luís P F Garcia",
                "Jefferson T Oliva",
                "André C P L F De Carvalho",
                "Edesio ©2020",
                "Felipe Alcobaça",
                "Adriano Siqueira",
                "Luís P F Rivolli",
                "Jefferson T Garcia",
                "André C P L F Oliva",
                "Carvalho De"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-348/19-348.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An ∞ Eigenvector Perturbation Bound and Its Application to Robust Covariance Estimation",
            "abstract": [
                "In statistics and machine learning, we are interested in the eigenvectors (or singular vectors) of certain matrices (e.g. covariance matrices, data matrices, etc). However, those matrices are usually perturbed by noises or statistical errors, either from random sampling or structural patterns. The Davis-Kahan sin θ theorem is often used to bound the difference between the eigenvectors of a matrix A and those of a perturbed matrix A = A + E, in terms of 2 norm. In this paper, we prove that when A is a low-rank and incoherent matrix, the ∞ norm perturbation bound of singular vectors (or eigenvectors in the symmetric case) is smaller by a factor of √ d 1 or √ d 2 for left and right vectors, where d 1 and d 2 are the matrix dimensions. The power of this new perturbation result is shown in robust covariance estimation, particularly when random variables have heavy tails. There, we propose new robust covariance estimators and establish their asymptotic properties using the newly developed perturbation bound. Our theoretical results are verified through extensive numerical experiments."
            ],
            "keywords": [
                "Matrix perturbation theory",
                "Incoherence",
                "Low-rank matrices",
                "Sparsity",
                "Approximate factor model"
            ],
            "author": [
                "Jianqing Fan",
                "Weichen Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-140/16-140.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Submodular Minimization",
            "abstract": [
                "We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efficient and are Hannan-consistent in both the full information and partial feedback settings."
            ],
            "keywords": [
                "submodular optimization",
                "online learning",
                "regret minimization"
            ],
            "author": [
                "Elad Hazan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/hazan12a/hazan12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Joint Harmonic Functions and Their Supervised Connections",
            "abstract": [
                "The cluster assumption had a significant impact on the reasoning behind semi-supervised classification methods in graph-based learning. The literature includes numerous applications where harmonic functions provided estimates that conformed to data satisfying this well-known assumption, but the relationship between this assumption and harmonic functions is not as well-understood theoretically. We investigate these matters from the perspective of supervised kernel classification and provide concrete answers to two fundamental questions. (i) Under what conditions do semisupervised harmonic approaches satisfy this assumption? (ii) If such an assumption is satisfied then why precisely would an observation sacrifice its own supervised estimate in favor of the cluster? First, a harmonic function is guaranteed to assign labels to data in harmony with the cluster assumption if a specific condition on the boundary of the harmonic function is satisfied. Second, it is shown that any harmonic function estimate within the interior is a probability weighted average of supervised estimates, where the weight is focused on supervised kernel estimates near labeled cases. We demonstrate that the uniqueness criterion for harmonic estimators is sensitive when the graph is sparse or the size of the boundary is relatively small. This sets the stage for a third contribution, a new regularized joint harmonic function for semi-supervised learning based on a joint optimization criterion. Mathematical properties of this estimator, such as its uniqueness even when the graph is sparse or the size of the boundary is relatively small, are proven. A main selling point is its ability to operate in circumstances where the cluster assumption may not be fully satisfied on real data by compromising between the purely harmonic and purely supervised estimators. The competitive stature of the new regularized joint harmonic approach is established."
            ],
            "keywords": [
                "harmonic function",
                "joint training",
                "cluster assumption",
                "semi-supervised learning"
            ],
            "author": [
                "Mark Vere Culp",
                "Kenneth Joseph Ryan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/culp13a/culp13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Generalized Bellman Equations and Temporal-Difference Learning",
            "abstract": [
                "We consider off-policy temporal-difference (TD) learning in discounted Markov decision processes, where the goal is to evaluate a policy in a model-free way by using observations of a state process generated without executing the policy. To curb the high variance issue in off-policy TD learning, we propose a new scheme of setting the λ-parameters of TD, based on generalized Bellman equations. Our scheme is to set λ according to the eligibility trace iterates calculated in TD, thereby easily keeping these traces in a desired bounded range. Compared with prior work, this scheme is more direct and flexible, and allows much larger λ values for off-policy TD learning with bounded traces. As to its soundness, using Markov chain theory, we prove the ergodicity of the joint state-trace process under nonrestrictive conditions, and we show that associated with our scheme is a generalized Bellman equation (for the policy to be evaluated) that depends on both the evolution of λ and the unique invariant probability measure of the state-trace process. These results not only lead immediately to a characterization of the convergence behavior of least-squares based implementation of our scheme, but also prepare the ground for further analysis of gradient-based implementations."
            ],
            "keywords": [
                "Markov decision process",
                "approximate policy evaluation",
                "generalized Bellman equation",
                "reinforcement learning",
                "temporal-difference method",
                "Markov chain",
                "randomized stopping time"
            ],
            "author": [
                "Huizhen Yu",
                "A Rupam Mahmood",
                "Richard S Sutton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-283/17-283.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "auDeep: Unsupervised Learning of Representations from Audio with Deep Recurrent Neural Networks",
            "abstract": [
                "auDeep is a Python toolkit for deep unsupervised representation learning from acoustic data. It is based on a recurrent sequence to sequence autoencoder approach which can learn representations of time series data by taking into account their temporal dynamics. We provide an extensive command line interface in addition to a Python API for users and developers, both of which are comprehensively documented and publicly available at https: //github.com/auDeep/auDeep. Experimental results indicate that auDeep features are competitive with state-of-the art audio classification."
            ],
            "keywords": [
                "deep feature learning",
                "sequence to sequence learning",
                "recurrent neural networks",
                "autoencoders",
                "audio processing"
            ],
            "author": [
                "Michael Freitag",
                "Nicholas Cummins"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-406/17-406.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lasso Screening Rules via Dual Polytope Projection",
            "abstract": [
                "Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose an efficient and effective screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there is currently no exact screening rule for group Lasso. We have evaluated our screening rule using synthetic and real data sets. Results show that our rule is more effective in identifying inactive predictors than existing state-of-the-art screening rules for Lasso."
            ],
            "keywords": [
                "lasso",
                "safe screening",
                "sparse regularization",
                "polytope projection",
                "dual formulation",
                "large-scale optimization"
            ],
            "author": [
                "Jie Wang",
                "Peter Wonka",
                "Jieping Ye"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/wang15a/wang15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Similarity-based Clustering by Left-Stochastic Matrix Factorization",
            "abstract": [
                "For similarity-based clustering, we propose modeling the entries of a given similarity matrix as the inner products of the unknown cluster probabilities. To estimate the cluster probabilities from the given similarity matrix, we introduce a left-stochastic non-negative matrix factorization problem. A rotation-based algorithm is proposed for the matrix factorization. Conditions for unique matrix factorizations and clusterings are given, and an error bound is provided. The algorithm is particularly efficient for the case of two clusters, which motivates a hierarchical variant for cases where the number of desired clusters is large. Experiments show that the proposed left-stochastic decomposition clustering model produces relatively high within-cluster similarity on most data sets and can match given class labels, and that the efficient hierarchical variant performs surprisingly well."
            ],
            "keywords": [
                "clustering",
                "non-negative matrix factorization",
                "rotation",
                "indefinite kernel",
                "similarity",
                "completely positive"
            ],
            "author": [
                "Raman Arora",
                "Maya R Gupta",
                "Amol Kapila",
                "Maryam Fazel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/arora13a/arora13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hinge-Minimax Learner for the Ensemble of Hyperplanes",
            "abstract": [
                "In this work we consider non-linear classifiers that comprise intersections of hyperplanes. We learn these classifiers by minimizing the \"minimax\" bound over the negative training examples and the hinge type loss of the positive training examples. These classifiers fit typical real-life datasets that consist of a small number of positive data points and a large number of negative data points. Such an approach is computationally appealing since the majority of training examples (belonging to the negative class) are represented by the statistics of their distribution, which is used in a single constraint on the empirical risk, as opposed to SVM, in which the number of variables is equal to the size of the training set. We first focus on intersection of K hyperplanes, for which we provide empirical risk bounds. We show that these bounds are dimensionally independent and decay as K/ √ m for m samples. We then extend the K-hyperplane mixed risk to the latent mixed risk for training a union of C K-hyperplane models, which can form an arbitrary complex, piecewise linear boundaries. We propose efficient algorithms for training the proposed models. Finally, we show how to combine hinge-minimax training with deep architectures and extend it to multi-class settings using transfer learning. The empirical evaluation of the proposed models shows their advantage over the existing methods in a small training labeled data regime."
            ],
            "keywords": [
                "Minimiax",
                "Imbalanced Classification",
                "Intersection of K Hyperplanes",
                "Transfer Learning 1. Input normalization is a standard procedure",
                "applied for faster learning"
            ],
            "author": [
                "Dolev Raviv",
                "Tamir Hazan",
                "Margarita Osadchy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-402/17-402.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Axioms for Graph Clustering Quality Functions",
            "abstract": [
                "We investigate properties that intuitively ought to be satisfied by graph clustering quality functions, that is, functions that assign a score to a clustering of a graph. Graph clustering, also known as network community detection, is often performed by optimizing such a function. Two axioms tailored for graph clustering quality functions are introduced, and the four axioms introduced in previous work on distance based clustering are reformulated and generalized for the graph setting. We show that modularity, a standard quality function for graph clustering, does not satisfy all of these six properties. This motivates the derivation of a new family of quality functions, adaptive scale modularity, which does satisfy the proposed axioms. Adaptive scale modularity has two parameters, which give greater flexibility in the kinds of clusterings that can be found. Standard graph clustering quality functions, such as normalized cut and unnormalized cut, are obtained as special cases of adaptive scale modularity. In general, the results of our investigation indicate that the considered axiomatic framework covers existing 'good' quality functions for graph clustering, and can be used to derive an interesting new family of quality functions."
            ],
            "keywords": [
                "graph clustering",
                "modularity",
                "axiomatic framework"
            ],
            "author": [
                "Twan Van Laarhoven",
                "Elena Marchiori"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/vanlaarhoven14a/vanlaarhoven14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning with Fenchel-Young Losses",
            "abstract": [
                "Over the past decades, numerous loss functions have been been proposed for a variety of supervised learning tasks, including regression, classification, ranking, and more generally structured prediction. Understanding the core principles and theoretical properties underpinning these losses is key to choose the right loss for the right problem, as well as to create new losses which combine their strengths. In this paper, we introduce Fenchel-Young losses, a generic way to construct a convex loss function for a regularized prediction function. We provide an in-depth study of their properties in a very broad setting, covering all the aforementioned supervised learning tasks, and revealing new connections between sparsity, generalized entropies, and separation margins. We show that Fenchel-Young losses unify many well-known loss functions and allow to create useful new ones easily. Finally, we derive efficient predictive and training algorithms, making Fenchel-Young losses appealing both in theory and practice."
            ],
            "keywords": [
                "loss functions",
                "output regularization",
                "convex duality",
                "structured prediction"
            ],
            "author": [
                "Mathieu Blondel",
                "Vlad Niculae"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-021/19-021.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the properties of variational approximations of Gibbs posteriors",
            "abstract": [
                "The PAC-Bayesian approach is a powerful set of techniques to derive non-asymptotic risk bounds for random estimators. The corresponding optimal distribution of estimators, usually called the Gibbs posterior, is unfortunately often intractable. One may sample from it using Markov chain Monte Carlo, but this is usually too slow for big datasets. We consider instead variational approximations of the Gibbs posterior, which are fast to compute. We undertake a general study of the properties of such approximations. Our main finding is that such a variational approximation has often the same rate of convergence as the original PAC-Bayesian procedure it approximates. In addition, we show that, when the risk function is convex, a variational approximation can be obtained in polynomial time using a convex solver. We give finite sample oracle inequalities for the corresponding estimator. We specialize our results to several learning tasks (classification, ranking, matrix completion), discuss how to implement a variational approximation in each case, and illustrate the good properties of said approximation on real datasets."
            ],
            "keywords": [],
            "author": [
                "Pierre Alquier",
                "James Ridgway",
                "Nicolas Chopin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-290/15-290.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty",
            "abstract": [
                "Clustering analysis is widely used in many fields. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classification and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classification and regression, such as model selection criteria to select the number of clusters, a difficult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method's promising performance."
            ],
            "keywords": [
                "generalized degrees of freedom",
                "grouping",
                "K-means clustering",
                "Lasso",
                "penalized regression",
                "truncated Lasso penalty (TLP)"
            ],
            "author": [
                "Wei Pan",
                "Xiaotong Shen",
                "Binghui Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/pan13a/pan13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Nonparametric Statistical Approach to Clustering via Mode Identification",
            "abstract": [
                "A new clustering approach based on mode identification is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efficiently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model fitting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Specifically, a pairwise separability measure for clusters is defined using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difficulty for parametric mixture modeling."
            ],
            "keywords": [
                "modal clustering",
                "mode-based clustering",
                "mixture modeling",
                "modal EM",
                "ridgeline EM",
                "nonparametric density"
            ],
            "author": [
                "Jia Li",
                "Bruce G Lindsay"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/li07a/li07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Rates for Total Variation Image Denoising",
            "abstract": [
                "We study the theoretical properties of image denoising via total variation penalized leastsquares. We define the total vatiation in terms of the two-dimensional total discrete derivative of the image and show that it gives rise to denoised images that are piecewise constant on rectangular sets. We prove that, if the true image is piecewise constant on just a few rectangular sets, the denoised image converges to the true image at a parametric rate, up to a log factor. More generally, we show that the denoised image enjoys oracle properties, that is, it is almost as good as if some aspects of the true image were known. In other words, image denoising with total variation regularization leads to an adaptive reconstruction of the true image."
            ],
            "keywords": [
                "total variation",
                "image denoising",
                "fused Lasso",
                "oracle inequalities"
            ],
            "author": [
                "Francesco Ortelli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-301/20-301.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Unsupervised Similarity-Based Risk Stratification for Cardiovascular Events Using Long-Term Time-Series Data",
            "abstract": [
                "In medicine, one often bases decisions upon a comparative analysis of patient data. In this paper, we build upon this observation and describe similarity-based algorithms to risk stratify patients for major adverse cardiac events. We evolve the traditional approach of comparing patient data in two ways. First, we propose similarity-based algorithms that compare patients in terms of their long-term physiological monitoring data. Symbolic mismatch identifies functional units in longterm signals and measures changes in the morphology and frequency of these units across patients. Second, we describe similarity-based algorithms that are unsupervised and do not require comparisons to patients with known outcomes for risk stratification. This is achieved by using an anomaly detection framework to identify patients who are unlike other patients in a population and may potentially be at an elevated risk. We demonstrate the potential utility of our approach by showing how symbolic mismatch-based algorithms can be used to classify patients as being at high or low risk of major adverse cardiac events by comparing their long-term electrocardiograms to that of a large population. We describe how symbolic mismatch can be used in three different existing methods: one-class support vector machines, nearest neighbor analysis, and hierarchical clustering. When evaluated on a population of 686 patients with available long-term electrocardiographic data, symbolic mismatch-based comparative approaches were able to identify patients at roughly a twofold increased risk of major adverse cardiac events in the 90 days following acute coronary syndrome. These results were consistent even after adjusting for other clinical risk variables."
            ],
            "keywords": [
                "risk stratification",
                "cardiovascular disease",
                "time-series comparison",
                "symbolic analysis",
                "anomaly detection"
            ],
            "author": [
                "Zeeshan Syed",
                "John Guttag"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/syed11a/syed11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Active Learning of Halfspaces: An Aggressive Approach",
            "abstract": [
                "We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efficient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches. Our efficient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in significantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings."
            ],
            "keywords": [
                "active learning",
                "linear classifiers",
                "margin",
                "adaptive sub-modularity"
            ],
            "author": [
                "Alon Gonen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/gonen13a/gonen13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Iterative Hessian Sketch: Fast and Accurate Solution Approximation for Constrained Least-Squares",
            "abstract": [
                "We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including 1-regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment."
            ],
            "keywords": [
                "Convex optimization",
                "Random Projection",
                "Lasso",
                "Low-rank Approximation",
                "Information Theory"
            ],
            "author": [
                "Mert Pilanci",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-460/14-460.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Low-Rank Doubly Stochastic Matrix Decomposition for Cluster Analysis",
            "abstract": [
                "Cluster analysis by nonnegative low-rank approximations has experienced a remarkable progress in the past decade. However, the majority of such approximation approaches are still restricted to nonnegative matrix factorization (NMF) and suffer from the following two drawbacks: 1) they are unable to produce balanced partitions for large-scale manifold data which are common in real-world clustering tasks; 2) most existing NMF-type clustering methods cannot automatically determine the number of clusters. We propose a new low-rank learning method to address these two problems, which is beyond matrix factorization. Our method approximately decomposes a sparse input similarity in a normalized way and its objective can be used to learn both cluster assignments and the number of clusters. For efficient optimization, we use a relaxed formulation based on Data-Cluster-Data random walk, which is also shown to be equivalent to low-rank factorization of the doublystochastically normalized cluster incidence matrix. The probabilistic cluster assignments can thus be learned with a multiplicative majorization-minimization algorithm. Experimental results show that the new method is more accurate both in terms of clustering large-scale manifold data sets and of selecting the number of clusters."
            ],
            "keywords": [
                "cluster analysis",
                "probabilistic relaxation",
                "doubly stochastic matrix",
                "manifold",
                "multiplicative updates"
            ],
            "author": [
                "Zhirong Yang",
                "Jukka Corander",
                "Erkki Oja"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-549/15-549.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Iterative Regularization for Learning with Convex Loss Functions",
            "abstract": [
                "We consider the problem of supervised learning with convex loss functions and propose a new form of iterative regularization based on the subgradient method. Unlike other regularization approaches, in iterative regularization no constraint or penalization is considered, and generalization is achieved by (early) stopping an empirical iteration. We consider a nonparametric setting, in the framework of reproducing kernel Hilbert spaces, and prove consistency and finite sample bounds on the excess risk under general regularity conditions. Our study provides a new class of efficient regularized learning algorithms and gives insights on the interplay between statistics and optimization in machine learning."
            ],
            "keywords": [],
            "author": [
                "Junhong Lin",
                "Lorenzo Rosasco",
                "Ding-Xuan Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-115/15-115.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Clustering with Hidden Markov Model on Variable Blocks",
            "abstract": [
                "Large-scale data containing multiple important rare clusters, even at moderately high dimensions, pose challenges for existing clustering methods. To address this issue, we propose a new mixture model called Hidden Markov Model on Variable Blocks (HMM-VB) and a new mode search algorithm called Modal Baum-Welch (MBW) for mode-association clustering. HMM-VB leverages prior information about chain-like dependence among groups of variables to achieve the effect of dimension reduction. In case such a dependence structure is unknown or assumed merely for the sake of parsimonious modeling, we develop a recursive search algorithm based on BIC to optimize the formation of ordered variable blocks. The MBW algorithm ensures the feasibility of clustering via mode association, achieving linear complexity in terms of the number of variable blocks despite the exponentially growing number of possible state sequences in HMM-VB. In addition, we provide theoretical investigations about the identifiability of HMM-VB as well as the consistency of our approach to search for the block partition of variables in a special case. Experiments on simulated and real data show that our proposed method outperforms other widely used methods."
            ],
            "keywords": [
                "Gaussian mixture model",
                "hidden Markov model",
                "modal Baum-Welch algorithm",
                "modal clustering"
            ],
            "author": [
                "Lin Lin",
                "Jia Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-342/16-342.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large Scale Multiple Kernel Learning",
            "abstract": [
                "While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun."
            ],
            "keywords": [
                "multiple kernel learning",
                "string kernels",
                "large scale optimization",
                "support vector machines",
                "support vector regression",
                "column generation",
                "semi-infinite linear programming"
            ],
            "author": [
                "Sören Sonnenburg",
                "Gunnar Rätsch",
                "Christin Schäfer",
                "Bernhard Schölkopf",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/sonnenburg06a/sonnenburg06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Identifiability of 1 -minimization Dictionary Learning: a Sufficient and Almost Necessary Condition",
            "abstract": [
                "We study the theoretical properties of learning a dictionary from N signals x i ∈ R K for i = 1,. .. , N via 1-minimization. We assume that x i 's are i.i.d. random linear combinations of the K columns from a complete (i.e., square and invertible) reference dictionary D 0 ∈ R K×K. Here, the random linear coefficients are generated from either the s-sparse Gaussian model or the Bernoulli-Gaussian model. First, for the population case, we establish a sufficient and almost necessary condition for the reference dictionary D 0 to be locally identifiable, i.e., a strict local minimum of the expected-norm objective function. Our condition covers both sparse and dense cases of the random linear coefficients and significantly improves the sufficient condition by Gribonval and Schnass (2010). In addition, we show that for a complete µ-coherent reference dictionary, i.e., a dictionary with absolute pairwise column inner-product at most µ ∈ [0, 1), local identifiability holds even when the random linear coefficient vector has up to O(µ −2) nonzero entries. Moreover, our local identifiability results also translate to the finite sample case with high probability provided that the number of signals N scales as O(K log K)."
            ],
            "keywords": [
                "dictionary learning",
                "1 -minimization",
                "local minimum",
                "non-convex optimization",
                "sparse decomposition"
            ],
            "author": [
                "Siqi Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-119/16-119.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions",
            "abstract": [
                "A Boolean function f is correlation immune if each input variable is independent of the output, under the uniform distribution on inputs. For example, the parity function is correlation immune. We consider the problem of identifying relevant variables of a correlation immune function, in the presence of irrelevant variables. We address this problem in two different contexts. First, we analyze Skewing, a heuristic method that was developed to improve the ability of greedy decision tree algorithms to identify relevant variables of correlation immune Boolean functions, given examples drawn from the uniform distribution (Page and Ray, 2003). We present theoretical results revealing both the capabilities and limitations of skewing. Second, we explore the problem of identifying relevant variables in the Product Distribution Choice (PDC) learning model, a model in which the learner can choose product distributions and obtain examples from them. We prove a lemma establishing a property of Boolean functions that may be of independent interest. Using this lemma, we give two new algorithms for finding relevant variables of correlation immune functions in the PDC model."
            ],
            "keywords": [
                "correlation immune functions",
                "skewing",
                "relevant variables",
                "Boolean functions",
                "product distributions"
            ],
            "author": [
                "Lisa Hellerstein",
                "Bernard Rosell",
                "Eric Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/hellerstein09a/hellerstein09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Projection Oblique Randomer Forests",
            "abstract": [
                "Decision forests, including Random Forests and Gradient Boosting Trees, have recently demonstrated state-of-the-art performance in a variety of machine learning settings. Decision forests are typically ensembles of axis-aligned decision trees; that is, trees that split only along feature dimensions. In contrast, many recent extensions to decision forests are based on axis-oblique splits. Unfortunately, these extensions forfeit one or more of the favorable properties of decision forests based on axis-aligned splits, such as robustness to many noise dimensions, interpretability, or computational efficiency. We introduce yet another decision forest, called \"Sparse Projection Oblique Randomer Forests\" (SPORF). SPORF uses very sparse random projections, i.e., linear combinations of a small subset of features. SPORF significantly improves accuracy over existing state-of-the-art algorithms on a standard benchmark suite for classification with > 100 problems of varying dimension, sample size, and number of classes. To illustrate how SPORF addresses the limitations of both axis-aligned and existing oblique decision forest methods, we conduct extensive simulated experiments. SPORF typically yields improved performance over existing decision forests, while mitigating computational efficiency and scalability and maintaining interpretability. Very sparse random projections can be incorporated into gradient boosted trees to obtain potentially similar gains."
            ],
            "keywords": [
                "Ensemble Learning",
                "Random Forests",
                "Decision Trees",
                "Random Projections",
                "Classification",
                "Regression",
                "Feature Extraction",
                "Sparse Learning"
            ],
            "author": [
                "Tyler M Tomita",
                "James Browne",
                "Cencheng Shen",
                "Jaewon Chung",
                "Jesse L Patsolic",
                "Benjamin Falk",
                "Jason Yim",
                "Randal Burns",
                "Mauro Maggioni",
                "Joshua T Vogelstein",
                "Carey E Priebe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-664/18-664.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Network Granger Causality with Inherent Grouping Structure",
            "abstract": [
                "The problem of estimating high-dimensional network models arises naturally in the analysis of many biological and socioeconomic systems. In this work, we aim to learn a network structure from temporal panel data, employing the framework of Granger causal models under the assumptions of sparsity of its edges and inherent grouping structure among its nodes. To that end, we introduce a group lasso regression regularization framework, and also examine a thresholded variant to address the issue of group misspecification. Further, the norm consistency and variable selection consistency of the estimates are established, the latter under the novel concept of direction consistency. The performance of the proposed methodology is assessed through an extensive set of simulation studies and comparisons with existing techniques. The study is illustrated on two motivating examples coming from functional genomics and financial econometrics."
            ],
            "keywords": [
                "Granger causality",
                "high dimensional networks",
                "panel vector autoregression model",
                "group lasso",
                "thresholding"
            ],
            "author": [
                "Sumanta Basu",
                "Ali Shojaie",
                "George Michailidis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/basu15a/basu15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Model Selection Consistency of Lasso",
            "abstract": [
                "Sparsity or parsimony of statistical models is crucial for their proper interpretations, as in sciences and social sciences. Model selection is a commonly used method to find such models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani, 1996) is now being used as a computationally feasible alternative to model selection. Therefore it is important to study Lasso for model selection purposes. In this paper, we prove that a single condition, which we call the Irrepresentable Condition, is almost necessary and sufficient for Lasso to select the true model both in the classical fixed p setting and in the large p setting as the sample size n gets large. Based on these results, sufficient conditions that are verifiable in practice are given to relate to previous works and help applications of Lasso for feature selection and sparse representation. This Irrepresentable Condition, which depends mainly on the covariance of the predictor variables, states that Lasso selects the true model consistently if and (almost) only if the predictors that are not in the true model are \"irrepresentable\" (in a sense to be clarified) by predictors that are in the true model. Furthermore, simulations are carried out to provide insights and understanding of this result."
            ],
            "keywords": [
                "Lasso",
                "regularization",
                "sparsity",
                "model selection",
                "consistency"
            ],
            "author": [
                "Peng Zhao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/zhao06a/zhao06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Online Prediction by Following the Perturbed Leader",
            "abstract": [
                "When applying aggregating strategies to Prediction with Expert Advice (PEA), the learning rate must be adaptively tuned. The natural choice of √ complexity/current loss renders the analysis of Weighted Majority (WM) derivatives quite complicated. In particular, for arbitrary weights there have been no results proven so far. The analysis of the alternative Follow the Perturbed Leader (FPL) algorithm from Kalai and Vempala (2003) based on Hannan's algorithm is easier. We derive loss bounds for adaptive learning rate and both finite expert classes with uniform weights and countable expert classes with arbitrary weights. For the former setup, our loss bounds match the best known results so far, while for the latter our results are new."
            ],
            "keywords": [
                "prediction with expert advice",
                "follow the perturbed leader",
                "general weights",
                "adaptive learning rate",
                "adaptive adversary",
                "hierarchy of experts",
                "expected and high probability bounds",
                "general alphabet and loss",
                "online sequential prediction"
            ],
            "author": [
                "Marcus Hutter",
                "Jan Poland"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/hutter05a/hutter05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Optimal Matching Methods for Causal Inference",
            "abstract": [
                "We develop an encompassing framework for matching, covariate balancing, and doublyrobust methods for causal inference from observational data called generalized optimal matching (GOM). The framework is given by generalizing a new functional-analytical formulation of optimal matching, giving rise to the class of GOM methods, for which we provide a single unified theory to analyze tractability and consistency. Many commonly used existing methods are included in GOM and, using their GOM interpretation, can be extended to optimally and automatically trade off balance for variance and outperform their standard counterparts. As a subclass, GOM gives rise to kernel optimal matching (KOM), which, as supported by new theoretical and empirical results, is notable for combining many of the positive properties of other methods in one. KOM, which is solved as a linearly-constrained convex-quadratic optimization problem, inherits both the interpretability and model-free consistency of matching but can also achieve the √ n-consistency of well-specified regression and the bias reduction and robustness of doubly robust methods. In settings of limited overlap, KOM enables a very transparent method for interval estimation for partial identification and robust coverage. We demonstrate this in examples with both synthetic and real data."
            ],
            "keywords": [
                "Causal inference",
                "optimal covariate balance",
                "embeddings",
                "matching",
                "convex optimization"
            ],
            "author": [
                "Nathan Kallus"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-120/19-120.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Complete Identification Methods for the Causal Hierarchy",
            "abstract": [
                "We consider a hierarchy of queries about causal relationships in graphical models, where each level in the hierarchy requires more detailed information than the one below. The hierarchy consists of three levels: associative relationships, derived from a joint distribution over the observable variables; cause-effect relationships, derived from distributions resulting from external interventions; and counterfactuals, derived from distributions that span multiple \"parallel worlds\" and resulting from simultaneous, possibly conflicting observations and interventions. We completely characterize cases where a given causal query can be computed from information lower in the hierarchy, and provide algorithms that accomplish this computation. Specifically, we show when effects of interventions can be computed from observational studies, and when probabilities of counterfactuals can be computed from experimental studies. We also provide a graphical characterization of those queries which cannot be computed (by any method) from queries at a lower layer of the hierarchy."
            ],
            "keywords": [
                "causality",
                "graphical causal models",
                "identification"
            ],
            "author": [
                "Ilya Shpitser"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/shpitser08a/shpitser08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sharp Oracle Inequalities for Square Root Regularization",
            "abstract": [
                "We study a set of regularization methods for high-dimensional linear regression models. These penalized estimators have the square root of the residual sum of squared errors as loss function, and any weakly decomposable norm as penalty function. This fit measure is chosen because of its property that the estimator does not depend on the unknown standard deviation of the noise. On the other hand, a generalized weakly decomposable norm penalty is very useful in being able to deal with different underlying sparsity structures. We can choose a different sparsity inducing norm depending on how we want to interpret the unknown parameter vector β. Structured sparsity norms, as defined in Micchelli et al. (2010), are special cases of weakly decomposable norms, therefore we also include the square root LASSO (Belloni et al., 2011), the group square root LASSO (Bunea et al., 2014) and a new method called the square root SLOPE (in a similar fashion to the SLOPE from Bogdan et al. 2015). For this collection of estimators our results provide sharp oracle inequalities with the Karush-Kuhn-Tucker conditions. We discuss some examples of estimators. Based on a simulation we illustrate some advantages of the square root SLOPE."
            ],
            "keywords": [
                "Square root LASSO",
                "structured sparsity",
                "Karush-Kuhn-Tucker",
                "sharp oracale inequality",
                "weak decomposability"
            ],
            "author": [
                "Benjamin Stucky"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-482/15-482.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Polynomial-Delay Enumeration of Monotonic Graph Classes",
            "abstract": [
                "Algorithms that list graphs such that no two listed graphs are isomorphic, are important building blocks of systems for mining and learning in graphs. Algorithms are already known that solve this problem efficiently for many classes of graphs of restricted topology, such as trees. In this article we introduce the concept of a dense augmentation schema, and introduce an algorithm that can be used to enumerate any class of graphs with polynomial delay, as long as the class of graphs can be described using a monotonic predicate operating on a dense augmentation schema. In practice this means that this is the first enumeration algorithm that can be applied theoretically efficiently in any frequent subgraph mining algorithm, and that this algorithm generalizes to situations beyond the standard frequent subgraph mining setting."
            ],
            "keywords": [
                "graph mining",
                "enumeration",
                "monotonic graph classes"
            ],
            "author": [
                "Jan Ramon",
                "Siegfried Nijssen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/ramon09a/ramon09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Eigenwords: Spectral Word Embeddings",
            "abstract": [
                "Spectral learning algorithms have recently become popular in data-rich domains, driven in part by recent advances in large scale randomized SVD, and in spectral estimation of Hidden Markov Models. Extensions of these methods lead to statistical estimation algorithms which are not only fast, scalable, and useful on real data sets, but are also provably correct. Following this line of research, we propose four fast and scalable spectral algorithms for learning word embeddings-low dimensional real vectors (called Eigenwords) that capture the \"meaning\" of words from their context. All the proposed algorithms harness the multi-view nature of text data i.e. the left and right context of each word, are fast to train and have strong theoretical properties. Some of the variants also have lower sample complexity and hence higher statistical power for rare words. We provide theory which establishes relationships between these algorithms and optimality criteria for the estimates they provide. We also perform thorough qualitative and quantitative evaluation of Eigenwords showing that simple linear approaches give performance comparable to or superior than the state-of-the-art non-linear deep learning based methods."
            ],
            "keywords": [
                "spectral learning",
                "CCA",
                "word embeddings",
                "NLP"
            ],
            "author": [
                "Paramveer S Dhillon",
                "Dean P Foster",
                "Lyle H Ungar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/dhillon15a/dhillon15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Uncovering Causality from Multivariate Hawkes Integrated Cumulants",
            "abstract": [
                "We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each node of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. As a consequence, it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the process. A theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we show, on numerical experiments, that our approach is indeed very robust with respect to the shape of the kernels and gives appealing results on the MemeTracker database and on financial order book data."
            ],
            "keywords": [
                "Hawkes Process",
                "Causality Inference",
                "Cumulants",
                "Generalized Method of Moments"
            ],
            "author": [
                "Massil Achab",
                "Emmanuel Bacry",
                "Iacopo Mastromatteo",
                "Jean-François Muzy",
                "Edoardo M Airoldi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-284/17-284.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Community Extraction in Multilayer Networks with Heterogeneous Community Structure",
            "abstract": [
                "Multilayer networks are a useful way to capture and model multiple, binary or weighted relationships among a fixed group of objects. While community detection has proven to be a useful exploratory technique for the analysis of single-layer networks, the development of community detection methods for multilayer networks is still in its infancy. We propose and investigate a procedure, called Multilayer Extraction, that identifies densely connected vertex-layer sets in multilayer networks. Multilayer Extraction makes use of a significance based score that quantifies the connectivity of an observed vertex-layer set through comparison with a fixed degree random graph model. Multilayer Extraction directly handles networks with heterogeneous layers where community structure may be different from layer to layer. The procedure can capture overlapping communities, as well as background vertex-layer pairs that do not belong to any community. We establish consistency of the vertex-layer set optimizer of our proposed multilayer score under the multilayer stochastic block model. We investigate the performance of Multilayer Extraction on three applications and a test bed of simulations. Our theoretical and numerical evaluations suggest that Multilayer Extraction is an effective exploratory tool for analyzing complex multilayer networks. Publicly available code is available at https://github.com/jdwilson4/MultilayerExtraction."
            ],
            "keywords": [
                "community detection",
                "clustering",
                "multiplex networks",
                "score based methods",
                "modularity"
            ],
            "author": [
                "James D Wilson",
                "John Palowitch",
                "Shankar Bhamidi",
                "Andrew B Nobel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-645/16-645.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Shallow Parsing using Noisy and Non-Stationary Training Material",
            "abstract": [
                "Shallow parsers are usually assumed to be trained on noise-free material, drawn from the same distribution as the testing material. However, when either the training set is noisy or else drawn from a different distributions, performance may be degraded. Using the parsed Wall Street Journal, we investigate the performance of four shallow parsers (maximum entropy, memory-based learning, N-grams and ensemble learning) trained using various types of artificially noisy material. Our first set of results show that shallow parsers are surprisingly robust to synthetic noise, with performance gradually decreasing as the rate of noise increases. Further results show that no single shallow parser performs best in all noise situations. Final results show that simple, parser-specific extensions can improve noise-tolerance. Our second set of results addresses the question of whether naturally occurring disfluencies undermines performance more than does a change in distribution. Results using the parsed Switchboard corpus suggest that, although naturally occurring disfluencies might harm performance, differences in distribution between the training set and the testing set are more significant."
            ],
            "keywords": [],
            "author": [
                "Miles Osborne",
                "James Hammerton",
                "Susan Armstrong",
                "Walter Daelemans"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/osborne02a/osborne02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Separating a Real-Life Nonlinear Image Mixture",
            "abstract": [
                "When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front-and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation. This paper addresses a difficult version of this problem, corresponding to the use of \"onion skin\" paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement."
            ],
            "keywords": [
                "ICA",
                "blind source separation",
                "nonlinear mixtures",
                "nonlinear separation",
                "image mixture",
                "image separation"
            ],
            "author": [
                "Luís B Almeida"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/almeida05a/almeida05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Construction of Approximation Spaces for Reinforcement Learning",
            "abstract": [
                "Linear reinforcement learning (RL) algorithms like least-squares temporal difference learning (LSTD) require basis functions that span approximation spaces of potential value functions. This article investigates methods to construct these bases from samples. We hypothesize that an ideal approximation spaces should encode diffusion distances and that slow feature analysis (SFA) constructs such spaces. To validate our hypothesis we provide theoretical statements about the LSTD value approximation error and induced metric of approximation spaces constructed by SFA and the state-of-the-art methods Krylov bases and proto-value functions (PVF). In particular, we prove that SFA minimizes the average (over all tasks in the same environment) bound on the above approximation error. Compared to other methods, SFA is very sensitive to sampling and can sometimes fail to encode the whole state space. We derive a novel importance sampling modification to compensate for this effect. Finally, the LSTD and least squares policy iteration (LSPI) performance of approximation spaces constructed by Krylov bases, PVF, SFA and PCA is compared in benchmark tasks and a visual robot navigation experiment (both in a realistic simulation and with a robot). The results support our hypothesis and suggest that (i) SFA provides subspace-invariant features for MDPs with self-adjoint transition operators, which allows strong guarantees on the approximation error, (ii) the modified SFA algorithm is best suited for LSPI in both discrete and continuous state spaces and (iii) approximation spaces encoding diffusion distances facilitate LSPI performance."
            ],
            "keywords": [
                "reinforcement learning",
                "diffusion distance",
                "proto value functions",
                "slow feature analysis",
                "least-squares policy iteration",
                "visual robot navigation"
            ],
            "author": [
                "Wendelin Böhmer",
                "Steffen Grünewälder",
                "Yun Shen",
                "Marek Musial",
                "Klaus Obermayer",
                "GRÜNEWÄLDER, SHEN, MUSIAL AND OBERMAYER Klaus Obermayer Böhmer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/boehmer13a/boehmer13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Geometric Multiscale Approximations for Intrinsically Low-dimensional Data",
            "abstract": [
                "We consider the problem of efficiently approximating and encoding high-dimensional data sampled from a probability distribution ρ in R D , that is nearly supported on a d-dimensional set M-for example supported on a d-dimensional manifold. Geometric Multi-Resolution Analysis (GMRA) provides a robust and computationally efficient procedure to construct low-dimensional geometric approximations of M at varying resolutions. We introduce GMRA approximations that adapt to the unknown regularity of M, by introducing a thresholding algorithm on the geometric wavelet coefficients. We show that these datadriven, empirical geometric approximations perform well, when the threshold is chosen as a suitable universal function of the number of samples n, on a large class of measures ρ, that are allowed to exhibit different regularity at different scales and locations, thereby efficiently encoding data from more complex measures than those supported on manifolds. These GMRA approximations are associated to a dictionary, together with a fast transform mapping data to d-dimensional coefficients, and an inverse of such a map, all of which are data-driven. The algorithms for both the dictionary construction and the transforms have complexity CDn log n with the constant C exponential in d. Our work therefore establishes Adaptive GMRA as a fast dictionary learning algorithm, with approximation guarantees, for intrinsically low-dimensional data. We include several numerical experiments on both synthetic and real data, confirming our theoretical results and demonstrating the effectiveness of Adaptive GMRA."
            ],
            "keywords": [
                "Dictionary Learning",
                "Multi-Resolution Analysis",
                "Adaptive Approximation",
                "Manifold Learning",
                "Compression"
            ],
            "author": [
                "Wenjing Liao",
                "Mauro Maggioni"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-252/17-252.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Cross-Validation via Sequential Testing",
            "abstract": [
                "With the increasing size of today's data sets, finding the right parameter configuration in model selection via cross-validation can be an extremely time-consuming task. In this paper we propose an improved cross-validation procedure which uses nonparametric testing coupled with sequential analysis to determine the best parameter set on linearly increasing subsets of the data. By eliminating underperforming candidates quickly and keeping promising candidates as long as possible, the method speeds up the computation while preserving the power of the full cross-validation. Theoretical considerations underline the statistical power of our procedure. The experimental evaluation shows that our method reduces the computation time by a factor of up to 120 compared to a full cross-validation with a negligible impact on the accuracy."
            ],
            "keywords": [
                "cross-validation",
                "statistical testing",
                "nonparametric methods"
            ],
            "author": [
                "Tammo Krueger",
                "Danny Panknin",
                "Mikio Braun"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/krueger15a/krueger15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Confidence-Weighted Linear Classification for Text Categorization",
            "abstract": [
                "Confidence-weighted online learning is a generalization of margin-based learning of linear classifiers in which the margin constraint is replaced by a probabilistic constraint based on a distribution over classifier weights that is updated online as examples are observed. The distribution captures a notion of confidence on classifier weights, and in some cases it can also be interpreted as replacing a single learning rate by adaptive per-weight rates. Confidence-weighted learning was motivated by the statistical properties of natural-language classification tasks, where most of the informative features are relatively rare. We investigate several versions of confidence-weighted learning that use a Gaussian distribution over weight vectors, updated at each observed example to achieve high probability of correct classification for the example. Empirical evaluation on a range of textcategorization tasks show that our algorithms improve over other state-of-the-art online and batch methods, learn faster in the online setting, and lead to better classifier combination for a type of distributed training commonly used in cloud computing."
            ],
            "keywords": [
                "online learning",
                "confidence prediction",
                "text categorization"
            ],
            "author": [
                "Koby Crammer",
                "Mark Dredze",
                "Fernando Pereira"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/crammer12a/crammer12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria",
            "abstract": [
                "In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account risk, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile riskconstrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application."
            ],
            "keywords": [
                "Markov Decision Process",
                "Reinforcement Learning",
                "Conditional Value-at-Risk",
                "Chance-Constrained Optimization",
                "Policy Gradient Algorithms",
                "Actor-Critic Algorithms"
            ],
            "author": [
                "Yinlam Chow",
                "Mohammad Ghavamzadeh",
                "Lucas Janson",
                "Stanford Edu",
                "Marco Pavone"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-636/15-636.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Moment Bound for Multi-hinge Classifiers",
            "abstract": [
                "The success of support vector machines in binary classification relies on the fact that hinge loss employed in the risk minimization targets the Bayes rule. Recent research explores some extensions of this large margin based method to the multicategory case. We show a moment bound for the socalled multi-hinge loss minimizers based on two kinds of complexity constraints: entropy with bracketing and empirical entropy. Obtaining such a result based on the latter is harder than finding one based on the former. We obtain fast rates of convergence that adapt to the unknown margin."
            ],
            "keywords": [
                "multi-hinge classification",
                "all-at-once",
                "moment bound",
                "fast rate",
                "entropy"
            ],
            "author": [
                "Bernadetta Tarigan",
                "Sara A Van De Geer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/tarigan08a/tarigan08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "RL-Glue: Language-Independent Software for Reinforcement-Learning Experiments",
            "abstract": [
                "RL-Glue is a standard, language-independent software package for reinforcement-learning experiments. The standardization provided by RL-Glue facilitates code sharing and collaboration. Code sharing reduces the need to re-engineer tasks and experimental apparatus, both common barriers to comparatively evaluating new ideas in the context of the literature. Our software features a minimalist interface and works with several languages and computing platforms. RL-Glue compatibility can be extended to any programming language that supports network socket communication. RL-Glue has been used to teach classes, to run international competitions, and is currently used by several other open-source software and hardware projects."
            ],
            "keywords": [
                "reinforcement learning",
                "empirical evaluation",
                "standardization",
                "open source"
            ],
            "author": [
                "Brian Tanner",
                "Adam White",
                "Mikio L Braun"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/tanner09a/tanner09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gradient Tree Boosting for Training Conditional Random Fields",
            "abstract": [
                "Conditional random fields (CRFs) provide a flexible and powerful model for sequence labeling problems. However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features and feature combinations. This paper describes a new algorithm for training CRFs via gradient tree boosting. In tree boosting, the CRF potential functions are represented as weighted sums of regression trees, which provide compact representations of feature interactions. So the algorithm does not explicitly consider the potentially large parameter space. As a result, gradient tree boosting scales linearly in the order of the Markov model and in the order of the feature interactions, rather than exponentially as in previous algorithms based on iterative scaling and gradient descent. Gradient tree boosting also makes it possible to use instance weighting (as in C4.5) and surrogate splitting (as in CART) to handle missing values. Experimental studies of the effectiveness of these two methods (as well as standard imputation and indicator feature methods) show that instance weighting is the best method in most cases when feature values are missing at random."
            ],
            "keywords": [
                "sequential supervised learning",
                "conditional random fields",
                "functional gradient",
                "gradient tree boosting",
                "missing values"
            ],
            "author": [
                "Thomas G Dietterich",
                "Guohua Hao",
                "Adam Ashenfelter"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/dietterich08a/dietterich08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Planar Ising Models",
            "abstract": [
                "Inference and learning of graphical models are both well-studied problems in statistics and machine learning that have found many applications in science and engineering. However, exact inference is intractable in general graphical models, which suggests the problem of seeking the best approximation to a collection of random variables within some tractable family of graphical models. In this paper, we focus on the class of planar Ising models, for which exact inference is tractable using techniques of statistical physics. Based on these techniques and recent methods for planarity testing and planar embedding, we propose a greedy algorithm for learning the best planar Ising model to approximate an arbitrary collection of binary random variables (possibly from sample data). Given the set of all pairwise correlations among variables, we select a planar graph and optimal planar Ising model defined on this graph to best approximate that set of correlations. We demonstrate our method in simulations and for two applications: modeling senate voting records and identifying geo-chemical depth trends from Mars rover data."
            ],
            "keywords": [],
            "author": [
                "Jason K Johnson",
                "Diane Oyen",
                "Michael Chertkov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-579/15-579.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Conditional Independencies under the Algorithmic Independence of Conditionals",
            "abstract": [
                "In this paper we analyze the relationship between faithfulness and the more recent condition of algorithmic Independence of Conditionals (IC) with respect to the Conditional Independencies (CIs) they allow. Both conditions have been extensively used for causal inference by refuting factorizations for which the condition does not hold. Violation of faithfulness happens when there are CIs that do not follow from the Markov condition. For those CIs, non-trivial constraints among some parameters of the Conditional Probability Distributions (CPDs) must hold. When such a constraint is defined over parameters of different CPDs, we prove that IC is also violated unless the parameters have a simple description. To understand which non-Markovian CIs are permitted we define a new condition closely related to IC: the Independence from Product Constraints (IPC). The condition reflects that CIs might be the result of specific parameterizations of individual CPDs but not from constraints on parameters of different CPDs. In that sense it is more restrictive than IC: parameters may have a simple description. On the other hand, IC also excludes other forms of algorithmic dependencies between CPDs. Finally, we prove that on top of the CIs permitted by the Markov condition (faithfulness), IPC allows non-minimality, deterministic relations and what we called proportional CPDs. These are the only cases in which a CI follows from a specific parameterization of a single CPD."
            ],
            "keywords": [
                "faithfulness",
                "causality",
                "independence of conditionals",
                "Kolmogorov complexity"
            ],
            "author": [
                "Jan Lemeire",
                "Isabelle Guyon",
                "Alexander Statnikov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-450/14-450.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active-set Methods for Submodular Minimization Problems",
            "abstract": [
                "We consider the submodular function minimization (SFM) and the quadratic minimization problems regularized by the Lovász extension of the submodular function. These optimization problems are intimately related; for example, min-cut problems and total variation denoising problems, where the cut function is submodular and its Lovász extension is given by the associated total variation. When a quadratic loss is regularized by the total variation of a cut function, it thus becomes a total variation denoising problem and we use the same terminology in this paper for \"general\" submodular functions. We propose a new active-set algorithm for total variation denoising with the assumption of an oracle that solves the corresponding SFM problem. This can be seen as local descent algorithm over ordered partitions with explicit convergence guarantees. It is more flexible than the existing algorithms with the ability for warm-restarts using the solution of a closely related problem. Further, we also consider the case when a submodular function can be decomposed into the sum of two submodular functions F 1 and F 2 and assume SFM oracles for these two functions. We propose a new active-set algorithm for total variation denoising (and hence SFM by thresholding the solution at zero). This algorithm also performs local descent over ordered partitions and its ability to warm start considerably improves the performance of the algorithm. In the experiments, we compare the performance of the proposed algorithms with state-of-the-art algorithms, showing that it reduces the calls to SFM oracles."
            ],
            "keywords": [
                "discrete optimization",
                "submodular function minimization",
                "convex optimization",
                "cut functions",
                "total variation denoising"
            ],
            "author": [
                "K S Sesh Kumar",
                "Francis Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-101/17-101.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Importance Weighting Without Importance Weights: An Efficient Algorithm for Combinatorial Semi-Bandits",
            "abstract": [
                "We propose a sample-efficient alternative for importance weighting for situations where one only has sample access to the probability distribution that generates the observations. Our new method, called Geometric Resampling (GR), is described and analyzed in the context of online combinatorial optimization under semi-bandit feedback, where a learner sequentially selects its actions from a combinatorial decision set so as to minimize its cumulative loss. In particular, we show that the well-known Follow-the-Perturbed-Leader (FPL) prediction method coupled with Geometric Resampling yields the first computationally efficient reduction from offline to online optimization in this setting. We provide a thorough theoretical analysis for the resulting algorithm, showing that its performance is on par with previous, inefficient solutions. Our main contribution is showing that, despite the relatively large variance induced by the GR procedure, our performance guarantees hold with high probability rather than only in expectation. As a side result, we also improve the best known regret bounds for FPL in online combinatorial optimization with full feedback, closing the perceived performance gap between FPL and exponential weights in this setting."
            ],
            "keywords": [
                "online learning",
                "combinatorial optimization",
                "bandit problems",
                "semi-bandit feedback",
                "follow the perturbed leader",
                "importance weighting"
            ],
            "author": [
                "Gergely Neu",
                "Gábor Bartók"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-091/15-091.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Characterization of Linkage-Based Hierarchical Clustering",
            "abstract": [
                "The class of linkage-based algorithms is perhaps the most popular class of hierarchical algorithms. We identify two properties of hierarchical algorithms, and prove that linkagebased algorithms are the only ones that satisfy both of these properties. Our characterization clearly delineates the difference between linkage-based algorithms and other hierarchical methods. We formulate an intuitive notion of locality of a hierarchical algorithm that distinguishes between linkage-based and \"global\" hierarchical algorithms like bisecting k-means, and prove that popular divisive hierarchical algorithms produce clusterings that cannot be produced by any linkage-based algorithm."
            ],
            "keywords": [],
            "author": [
                "Margareta Ackerman",
                "Shai Ben-David"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/11-198/11-198.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Exact Inference in Graphical Models",
            "abstract": [
                "Many algorithms and applications involve repeatedly solving variations of the same inference problem, for example to introduce new evidence to the model or to change conditional dependencies. As the model is updated, the goal of adaptive inference is to take advantage of previously computed quantities to perform inference more rapidly than from scratch. In this paper, we present algorithms for adaptive exact inference on general graphs that can be used to efficiently compute marginals and update MAP configurations under arbitrary changes to the input factor graph and its associated elimination tree. After a linear time preprocessing step, our approach enables updates to the model and the computation of any marginal in time that is logarithmic in the size of the input model. Moreover, in contrast to max-product our approach can also be used to update MAP configurations in time that is roughly proportional to the number of updated entries, rather than the size of the input model. To evaluate the practical effectiveness of our algorithms, we implement and test them using synthetic data as well as for two real-world computational biology applications. Our experiments show that adaptive inference can achieve substantial speedups over performing complete inference as the model undergoes small changes over time."
            ],
            "keywords": [
                "exact inference",
                "factor graphs",
                "factor elimination",
                "marginalization",
                "dynamic programming",
                "MAP computation",
                "model updates",
                "parallel tree contraction"
            ],
            "author": [
                "Ozgür Sümer",
                "Umut A Acar",
                "Alexander T Ihler",
                "Donald Bren",
                "Ramgopal R Mettu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/sumer11a/sumer11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graphical Models via Univariate Exponential Family Distributions",
            "abstract": [
                "Undirected graphical models, or Markov networks, are a popular class of statistical models, used in a wide variety of applications. Popular instances of this class include Gaussian graphical models and Ising models. In many settings, however, it might not be clear which subclass of graphical models to use, particularly for non-Gaussian and non-categorical data. In this paper, we consider a general sub-class of graphical models where the node-wise conditional distributions arise from exponential families. This allows us to derive multivariate graphical model distributions from univariate exponential family distributions, such as the Poisson, negative binomial, and exponential distributions. Our key contributions include a class of M-estimators to fit these graphical model distributions; and rigorous statistical analysis showing that these M-estimators recover the true graphical model structure exactly, with high probability. We provide examples of genomic and proteomic networks learned via instances of our class of graphical models derived from Poisson and exponential distributions."
            ],
            "keywords": [
                "graphical models",
                "model selection",
                "sparse estimation"
            ],
            "author": [
                "Eunho Yang",
                "Pradeep Ravikumar",
                "Genevera I Allen",
                "Zhandong Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/yang15a/yang15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification",
            "abstract": [
                "Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere S p−1. Such representations should then be modeled using spherical distributions. However, the difficulty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead-as if the data were represented in R p rather than S p−1. This opens the question to whether the classification results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases (which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to define optimal classifiers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classifiers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classification of images of objects, gene expression sequences, and text data."
            ],
            "keywords": [
                "directional data",
                "spherical distributions",
                "normal distributions",
                "norm normalization",
                "linear and non-linear classifiers",
                "computer vision"
            ],
            "author": [
                "Onur C Hamsici",
                "Aleix M Martinez"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/hamsici07a/hamsici07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning from Examples as an Inverse Problem",
            "abstract": [
                "Many works related learning from examples to regularization techniques for inverse problems, emphasizing the strong algorithmic and conceptual analogy of certain learning algorithms with regularization algorithms. In particular it is well known that regularization schemes such as Tikhonov regularization can be effectively used in the context of learning and are closely related to algorithms such as support vector machines. Nevertheless the connection with inverse problem was considered only for the discrete (finite sample) problem and the probabilistic aspects of learning from examples were not taken into account. In this paper we provide a natural extension of such analysis to the continuous (population) case and study the interplay between the discrete and continuous problems. From a theoretical point of view, this allows to draw a clear connection between the consistency approach in learning theory and the stability convergence property in ill-posed inverse problems. The main mathematical result of the paper is a new probabilistic bound for the regularized least-squares algorithm. By means of standard results on the approximation term, the consistency of the algorithm easily follows."
            ],
            "keywords": [
                "statistical learning",
                "inverse problems",
                "regularization theory",
                "consistency"
            ],
            "author": [
                "Ernesto De",
                "Andrea Caponnetto",
                "Umberto De Giovannini",
                "Francesca Odone",
                "De Vito",
                "Lorenzo Rosasco"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/devito05a/devito05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Topic Modeling Toolbox Using Belief Propagation",
            "abstract": [
                "Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. TMBP toolbox is implemented by MEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existing topic modeling packages, the novelty of this toolbox lies in the BP algorithms for learning LDA-based topic models. The current version includes BP algorithms for latent Dirichlet allocation (LDA), authortopic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more BP-based algorithms for various topic models will be added in the near future. Interested users may also extend BP algorithms for learning more complicated topic models."
            ],
            "keywords": [
                "topic models",
                "belief propagation",
                "variational Bayes",
                "Gibbs sampling"
            ],
            "author": [
                "Jia Zeng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/zeng12a/zeng12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss",
            "abstract": [
                "Many performance metrics have been introduced in the literature for the evaluation of classification performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into refinement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassification costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: fixed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the refinement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibration in choosing the threshold choice method."
            ],
            "keywords": [
                "classification performance metrics",
                "cost-sensitive evaluation",
                "operating condition",
                "Brier score",
                "area under the ROC curve (AUC)",
                "calibration loss",
                "refinement loss"
            ],
            "author": [
                "José Hernández-Orallo",
                "Peter Flach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/hernandez-orallo12a/hernandez-orallo12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Universal Approximation Results for the Temporal Restricted Boltzmann Machine and the Recurrent Temporal Restricted Boltzmann Machine",
            "abstract": [
                "The Restricted Boltzmann Machine (RBM) has proved to be a powerful tool in machine learning, both on its own and as the building block for Deep Belief Networks (multi-layer generative graphical models). The RBM and Deep Belief Network have been shown to be universal approximators for probability distributions on binary vectors. In this paper we prove several similar universal approximation results for two variations of the Restricted Boltzmann Machine with time dependence, the Temporal Restricted Boltzmann Machine (TRBM) and the Recurrent Temporal Restricted Boltzmann Machine (RTRBM). We show that the TRBM is a universal approximator for Markov chains and generalize the theorem to sequences with longer time dependence. We then prove that the RTRBM is a universal approximator for stochastic processes with finite time dependence. We conclude with a discussion on efficiency and how the constructions developed could explain some previous experimental results."
            ],
            "keywords": [
                "TRBM",
                "RTRBM",
                "machine learning",
                "universal approximation"
            ],
            "author": [
                "Simon Odense",
                "Roderick Edwards"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-478/15-478.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Towards More Efficient SPSD Matrix Approximation and CUR Matrix Decomposition",
            "abstract": [
                "Symmetric positive semi-definite (SPSD) matrix approximation methods have been extensively used to speed up large-scale eigenvalue computation and kernel learning methods. The standard sketch based method, which we call the prototype model, produces relatively accurate approximations, but is inefficient on large square matrices. The Nyström method is highly efficient, but can only achieve low accuracy. In this paper we propose a novel model that we call the fast SPSD matrix approximation model. The fast model is nearly as efficient as the Nyström method and as accurate as the prototype model. We show that the fast model can potentially solve eigenvalue problems and kernel learning problems in linear time with respect to the matrix size n to achieve 1 + relative-error, whereas both the prototype model and the Nyström method cost at least quadratic time to attain comparable error bound. Empirical comparisons among the prototype model, the Nyström method, and our fast model demonstrate the superiority of the fast model. We also contribute new understandings of the Nyström method. The Nyström method is a special instance of our fast model and is approximation to the prototype model. Our technique can be straightforwardly applied to make the CUR matrix decomposition more efficiently computed without much affecting the accuracy."
            ],
            "keywords": [
                "Kernel approximation",
                "matrix factorization",
                "the Nyström method",
                "CUR matrix decomposition"
            ],
            "author": [
                "Shusen Wang",
                "Zhihua Zhang",
                "Tong Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-190/15-190.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graphical Methods for Efficient Likelihood Inference in Gaussian Covariance Models",
            "abstract": [
                "In graphical modelling, a bi-directed graph encodes marginal independences among random variables that are identified with the vertices of the graph. We show how to transform a bi-directed graph into a maximal ancestral graph that (i) represents the same independence structure as the original bi-directed graph, and (ii) minimizes the number of arrowheads among all ancestral graphs satisfying (i). Here the number of arrowheads of an ancestral graph is the number of directed edges plus twice the number of bi-directed edges. In Gaussian models, this construction can be used for more efficient iterative maximization of the likelihood function and to determine when maximum likelihood estimates are equal to empirical counterparts."
            ],
            "keywords": [
                "ancestral graph",
                "covariance graph",
                "graphical model",
                "marginal independence",
                "maximum likelihood estimation",
                "multivariate normal distribution"
            ],
            "author": [
                "Mathias Drton",
                "Thomas S Richardson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/drton08a/drton08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Coevolutionary Learning of Deterministic Finite Automata",
            "abstract": [
                "This paper describes an active learning approach to the problem of grammatical inference, specifically the inference of deterministic finite automata (DFAs). We refer to the algorithm as the estimation-exploration algorithm (EEA). This approach differs from previous passive and active learning approaches to grammatical inference in that training data is actively proposed by the algorithm, rather than passively receiving training data from some external teacher. Here we show that this algorithm outperforms one version of the most powerful set of algorithms for grammatical inference, evidence driven state merging (EDSM), on randomly-generated DFAs. The performance increase is due to the fact that the EDSM algorithm only works well for DFAs with specific balances (percentage of positive labelings), while the EEA is more consistent over a wider range of balances. Based on this finding we propose a more general method for generating DFAs to be used in the development of future grammatical inference algorithms."
            ],
            "keywords": [
                "grammatical inference",
                "evolutionary computation",
                "deterministic finite automata",
                "active learning",
                "system identification"
            ],
            "author": [
                "Josh Bongard",
                "Hod Lipson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/bongard05a/bongard05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The On-Line Shortest Path Problem Under Partial Monitoring",
            "abstract": [
                "The on-line shortest path problem is considered under various models of partial monitoring. Given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. In a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. For this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/ √ n and depends only polynomially on the number of edges of the graph. The algorithm can be implemented with complexity that is linear in the number of rounds n (i.e., the average complexity per round is constant) and in the number of edges. An extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m n time instances. Another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. A version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. Applications to routing in packet switched networks along with simulation results are also presented."
            ],
            "keywords": [
                "on-line learning",
                "shortest path problem",
                "multi-armed bandit problem"
            ],
            "author": [
                "András György"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/gyoergy07a/gyoergy07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Policy Programming",
            "abstract": [
                "In this paper, we propose a novel policy iteration method, called dynamic policy programming (DPP), to estimate the optimal policy in the infinite-horizon Markov decision processes. DPP is an incremental algorithm that forces a gradual change in policy update. This allows us to prove finite-iteration and asymptotic ℓ ∞-norm performance-loss bounds in the presence of approximation/estimation error which depend on the average accumulated error as opposed to the standard bounds which are expressed in terms of the supremum of the errors. The dependency on the average error is important in problems with limited number of samples per iteration, for which the average of the errors can be significantly smaller in size than the supremum of the errors. Based on these theoretical results, we prove that a sampling-based variant of DPP (DPP-RL) asymptotically converges to the optimal policy. Finally, we illustrate numerically the applicability of these results on some benchmark problems and compare the performance of the approximate variants of DPP with some existing reinforcement learning (RL) methods."
            ],
            "keywords": [
                "approximate dynamic programming",
                "reinforcement learning",
                "Markov decision processes",
                "Monte-Carlo methods",
                "function approximation"
            ],
            "author": [
                "Mohammad Gheshlaghi Azar",
                "Hilbert J Kappen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/azar12a/azar12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Variational Inference",
            "abstract": [
                "We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets."
            ],
            "keywords": [
                "Bayesian inference",
                "variational inference",
                "stochastic optimization",
                "topic models",
                "Bayesian nonparametrics"
            ],
            "author": [
                "Matthew D Hoffman",
                "David M Blei",
                "Chong Wang",
                "John Paisley"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/hoffman13a/hoffman13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-task Sparse Structure Learning with Gaussian Copula Models",
            "abstract": [
                "Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously. While sometimes the underlying task relationship structure is known, often the structure needs to be estimated from data at hand. In this paper, we present a novel family of models for MTL, applicable to regression and classification problems, capable of learning the structure of tasks relationship. In particular, we consider a joint estimation problem of the tasks relationship structure and the individual task parameters, which is solved using alternating minimization. The task relationship revealed by structure learning is founded on recent advances in Gaussian graphical models endowed with sparse estimators of the precision (inverse covariance) matrix. An extension to include flexible Gaussian copula models that relaxes the Gaussian marginal assumption is also proposed. We illustrate the effectiveness of the proposed model on a variety of synthetic and benchmark data sets for regression and classification. We also consider the problem of combining Earth System Model (ESM) outputs for better projections of future climate, with focus on projections of temperature by combining ESMs in South and North America, and show that the proposed model outperforms several existing methods for the problem."
            ],
            "keywords": [
                "multi-task learning",
                "structure learning",
                "Gaussian copula",
                "probabilistic graphical model",
                "sparse modeling"
            ],
            "author": [
                "André R Gonçalves",
                "Fernando J Von Zuben",
                "Arindam Banerjee",
                "Urun Dogan",
                "Marius Kloft",
                "Francesco Orabona",
                "Tatiana Tommasi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-215/15-215.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Training Gaussian Mixture Models at Scale via Coresets",
            "abstract": [
                "How can we train a statistical mixture model on a massive data set? In this work we show how to construct coresets for mixtures of Gaussians. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension and the number of mixture components, while being independent of the data set size. Hence, one can harness computationally intensive algorithms to compute a good approximation on a significantly smaller data set. More importantly, such coresets can be efficiently constructed both in distributed and streaming settings and do not impose restrictions on the data generating process. Our results rely on a novel reduction of statistical estimation to problems in computational geometry and new combinatorial complexity results for mixtures of Gaussians. Empirical evaluation on several real-world data sets suggests that our coreset-based approach enables significant reduction in training-time with negligible approximation error."
            ],
            "keywords": [
                "Gaussian mixture models",
                "coresets",
                "streaming and distributed computation"
            ],
            "author": [
                "Mario Lucic",
                "Matthew Faulkner",
                "Andreas Krause",
                "Dan Feldman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-506/15-506.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Spectral Learning",
            "abstract": [
                "In this paper, we study the problem of learning a matrix W from a set of linear measurements. Our formulation consists in solving an optimization problem which involves regularization with a spectral penalty term. That is, the penalty term is a function of the spectrum of the covariance of W. Instances of this problem in machine learning include multi-task learning, collaborative filtering and multi-view learning, among others. Our goal is to elucidate the form of the optimal solution of spectral learning. The theory of spectral learning relies on the von Neumann characterization of orthogonally invariant norms and their association with symmetric gauge functions. Using this tool we formulate a representer theorem for spectral regularization and specify it to several useful example, such as Schatten p−norms, trace norm and spectral norm, which should proved useful in applications."
            ],
            "keywords": [
                "kernel methods",
                "matrix learning",
                "minimal norm interpolation",
                "multi-task learning",
                "orthogonally invariant norms",
                "regularization"
            ],
            "author": [
                "Andreas Argyriou",
                "Charles A Micchelli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/argyriou10a/argyriou10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Use of the Zero-Norm with Linear Models and Kernel Methods",
            "abstract": [
                "We explore the use of the so-called zero-norm of the parameters of linear models in learning. Minimization of such a quantity has many uses in a machine learning context: for variable or feature selection, minimizing training error and ensuring sparsity in solutions. We derive a simple but practical method for achieving these goals and discuss its relationship to existing techniques of minimizing the zero-norm. The method boils down to implementing a simple modification of vanilla SVM, namely via an iterative multiplicative rescaling of the training data. Applications we investigate which aid our discussion include variable and feature selection on biological microarray data, and multicategory classification."
            ],
            "keywords": [],
            "author": [
                "Jason Weston",
                "Bernhard Schölkopf",
                "Leslie Pack"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/weston03a/weston03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "One-Class Novelty Detection for Seizure Analysis from Intracranial EEG",
            "abstract": [
                "This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a \"one-shot\" manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1% sensitivity, a mean detection latency of These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training."
            ],
            "keywords": [
                "seizure detection",
                "novelty detection",
                "one-class SVM",
                "epilepsy",
                "unsupervised learning"
            ],
            "author": [
                "Andrew B ©2005",
                "Abba M Gardner",
                "George Krieger",
                "Brian Vachtsevanos",
                "Andrew B Gardner",
                "Abba M Krieger",
                "George Vachtsevanos",
                "Brian Litt"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/gardner06a/gardner06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Balls of Strings from Edit Corrections *",
            "abstract": [
                "When facing the question of learning languages in realistic settings, one has to tackle several problems that do not admit simple solutions. On the one hand, languages are usually defined by complex grammatical mechanisms for which the learning results are predominantly negative, as the few algorithms are not really able to cope with noise. On the other hand, the learning settings themselves rely either on too simple information (text) or on unattainable one (query systems that do not exist in practice, nor can be simulated). We consider simple but sound classes of languages defined via the widely used edit distance: the balls of strings. We propose to learn them with the help of a new sort of queries, called the correction queries: when a string is submitted to the Oracle, either she accepts it if it belongs to the target language, or she proposes a correction, that is, a string of the language close to the query with respect to the edit distance. We show that even if the good balls are not learnable in Angluin's MAT model, they can be learned from a polynomial number of correction queries. Moreover, experimental evidence simulating a human Expert shows that this algorithm is resistant to approximate answers."
            ],
            "keywords": [
                "grammatical inference",
                "oracle learning",
                "correction queries",
                "edit distance",
                "balls of strings"
            ],
            "author": [
                "Leonor Becerra-Bonache",
                "Yale Edu",
                "Jean-Christophe Janodet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/becerra-bonache08a/becerra-bonache08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Assortment Optimization with Changing Contextual Information *",
            "abstract": [
                "In this paper, we study the dynamic assortment optimization problem over a finite selling season of length T. At each time period, the seller offers an arriving customer an assortment of substitutable products under a cardinality constraint, and the customer makes the purchase among offered products according to a discrete choice model. Most existing work associates each product with a real-valued fixed mean utility and assumes a multinomial logit choice (MNL) model. In many practical applications, feature/contextual information of products is readily available. In this paper, we incorporate the feature information by assuming a linear relationship between the mean utility and the feature. In addition, we allow the feature information of products to change over time so that the underlying choice model can also be non-stationary. To solve the dynamic assortment optimization under this changing contextual MNL model, we need to simultaneously learn the underlying unknown coefficient and make the decision on the assortment. To this end, we develop an upper confidence bound (UCB) based policy and establish the regret bound on the order of O(d √ T), where d is the dimension of the feature and O suppresses logarithmic dependence. We further establish a lower bound Ω(d √ T /K), where K is the cardinality constraint of an offered assortment, which is usually small. When K is a constant, our policy is optimal up to logarithmic factors. In the exploitation phase of the UCB algorithm, we need to solve a combinatorial optimization problem for assortment optimization based on the learned information. We further develop an approximation algorithm and an efficient greedy heuristic. The effectiveness of the proposed policy is further demonstrated by our numerical studies."
            ],
            "keywords": [
                "Dynamic assortment optimization",
                "regret analysis",
                "contextual information",
                "bandit learning",
                "upper confidence bounds"
            ],
            "author": [
                "Xi Chen",
                "Leonard N Stern",
                "Yining Wang",
                "Yuan Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-1054/19-1054.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition",
            "abstract": [
                "We consider the problem of parsing human poses and recognizing their actions in static images with part-based models. Most previous work in part-based models only considers rigid parts (e.g., torso, head, half limbs) guided by human anatomy. We argue that this representation of parts is not necessarily appropriate. In this paper, we introduce hierarchical poselets-a new representation for modeling the pose configuration of human bodies. Hierarchical poselets can be rigid parts, but they can also be parts that cover large portions of human bodies (e.g., torso + left arm). In the extreme case, they can be the whole bodies. The hierarchical poselets are organized in a hierarchical way via a structured model. Human parsing can be achieved by inferring the optimal labeling of this hierarchical model. The pose information captured by this hierarchical model can also be used as a intermediate representation for other high-level tasks. We demonstrate it in action recognition from static images."
            ],
            "keywords": [
                "human parsing",
                "action recognition",
                "part-based models",
                "hierarchical poselets",
                "maxmargin structured learning"
            ],
            "author": [
                "Yang Wang",
                "Duan Tran",
                "Zicheng Liao",
                "David Forsyth",
                "Isabelle Guyon",
                "Vassilis Athitsos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/wang12a/wang12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Functional Martingale Residual Process for High-Dimensional Cox Regression with Model Averaging",
            "abstract": [
                "Regularization methods for the Cox proportional hazards regression with high-dimensional survival data have been studied extensively in the literature. However, if the model is misspecified, this would result in misleading statistical inference and prediction. To enhance the prediction accuracy for the relative risk and the survival probability, we propose three model averaging approaches for the high-dimensional Cox proportional hazards regression. Based on the martingale residual process, we define the delete-one cross-validation (CV) process, and further propose three novel CV functionals, including the end-time CV, integrated CV, and supremum CV, to achieve more accurate prediction for the risk quantities of clinical interest. The optimal weights for candidate models, without the constraint of summing up to one, can be obtained by minimizing these functionals, respectively. The proposed model averaging approach can attain the lowest possible prediction loss asymptotically. Furthermore, we develop a greedy model averaging algorithm to overcome the computational obstacle when the dimension is high. The performances of the proposed model averaging procedures are evaluated via extensive simulation studies, demonstrating that our methods achieve superior prediction accuracy over the existing regularization methods. As an illustration, we apply the proposed methods to the mantle cell lymphoma study."
            ],
            "keywords": [
                "Asymptotic Optimality",
                "Censored Data",
                "Cross Validation",
                "Greedy Algorithm",
                "Martingale Residual Process",
                "Prediction",
                "Survival Analysis"
            ],
            "author": [
                "Yanyan Liu",
                "Guosheng, Yin Yuanshan Wu",
                "Guosheng Yin",
                "Xingqiu Zhao",
                "Baihua He",
                "Zhao Xingqiu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-829/19-829.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Space-Time Partitioning by Sampling and Pruning Spanning Trees",
            "abstract": [
                "A typical problem in spatial data analysis is regionalization or spatially constrained clustering, which consists of aggregating small geographical areas into larger regions. A major challenge when partitioning a map is the huge number of possible partitions that compose the search space. This is compounded if we are partitioning spatio-temporal data rather than purely spatial data. We introduce a spatio-temporal product partition model that deals with the regionalization problem in a probabilistic way. Random spanning trees are used as a tool to tackle the problem of searching the space of possible partitions making feasible this exploration. Based on this framework, we propose an efficient Gibbs sampler algorithm to sample from the posterior distribution of the parameters, specially the random partition. The proposed Gibbs sampler scheme carries out a random walk on the space of the spanning trees and the partitions induced by deleting tree edges. In the purely spatial situation, we compare our proposed model with other state-of-art regionalization techniques to partition maps using simulated and real social and health data. To illustrate how the temporal component is handled by the algorithm and to show how the spatial clusters vary along the time we presented an application using human development index data. The analysis shows that our proposed model is better than state-of-art alternatives. Another appealing feature of the method is that the prior distribution for the partition is interpretable with a trivial coin flipping mechanism allowing its easy elicitation."
            ],
            "keywords": [
                "Spatial Clustering",
                "Product Partition Models",
                "Random Spanning Trees",
                "Bayesian Clustering"
            ],
            "author": [
                "Leonardo V Teixeira",
                "Renato M Assunção",
                "Rosangela H Loschi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/16-615/16-615.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Provably Correct Algorithms for Matrix Column Subset Selection with Selectively Sampled Data",
            "abstract": [
                "We consider the problem of matrix column subset selection, which selects a subset of columns from an input matrix such that the input can be well approximated by the span of the selected columns. Column subset selection has been applied to numerous real-world data applications such as population genetics summarization, electronic circuits testing and recommendation systems. In many applications the complete data matrix is unavailable and one needs to select representative columns by inspecting only a small portion of the input matrix. In this paper we propose the first provably correct column subset selection algorithms for partially observed data matrices. Our proposed algorithms exhibit different merits and limitations in terms of statistical accuracy, computational efficiency, sample complexity and sampling schemes, which provides a nice exploration of the tradeoff between these desired properties for column subset selection. The proposed methods employ the idea of feedback driven sampling and are inspired by several sampling schemes previously introduced for low-rank matrix approximation tasks ("
            ],
            "keywords": [
                "Column subset selection",
                "active learning",
                "leverage scores"
            ],
            "author": [
                "Yining Wang",
                "Aarti Singh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-233/15-233.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distance Dependent Chinese Restaurant Processes",
            "abstract": [
                "We develop the distance dependent Chinese restaurant process, a flexible class of distributions over partitions that allows for dependencies between the elements. This class can be used to model many kinds of dependencies between data in infinite clustering models, including dependencies arising from time, space, and network connectivity. We examine the properties of the distance dependent CRP, discuss its connections to Bayesian nonparametric mixture models, and derive a Gibbs sampler for both fully observed and latent mixture settings. We study its empirical performance with three text corpora. We show that relaxing the assumption of exchangeability with distance dependent CRPs can provide a better fit to sequential data and network data. We also show that the distance dependent CRP representation of the traditional CRP mixture leads to a faster-mixing Gibbs sampling algorithm than the one based on the original formulation."
            ],
            "keywords": [],
            "author": [
                "David M Blei",
                "Peter I Frazier"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/blei11a/blei11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence of an Alternating Maximization Procedure",
            "abstract": [
                "We derive two convergence results for a sequential alternating maximization procedure to approximate the maximizer of random functionals such as the realized log likelihood in MLE estimation. We manage to show that the sequence attains the same deviation properties as shown for the profile M-estimator by Andresen and Spokoiny (2013), that means a finite sample Wilks and Fisher theorem. Further under slightly stronger smoothness constraints on the random functional we can show nearly linear convergence to the global maximizer if the starting point for the procedure is well chosen."
            ],
            "keywords": [
                "alternating maximization",
                "alternating minimization",
                "profile maximum likelihood",
                "EM-algorithm",
                "M-estimation",
                "local linear approximation",
                "local concentration",
                "semiparametric"
            ],
            "author": [
                "Andreas Andresen",
                "Vladimir Spokoiny"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-392/15-392.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Probabilistic Preference Learning with the Mallows Rank Model",
            "abstract": [
                "Ranking and comparing items is crucial for collecting information about preferences in many areas, from marketing to politics. The Mallows rank model is among the most successful approaches to analyze rank data, but its computational complexity has limited its use to a particular form based on Kendall distance. We develop new computationally tractable methods for Bayesian inference in Mallows models that work with any right-invariant distance. Our method performs inference on the consensus ranking of the items, also when based on partial rankings, such as top-k items or pairwise comparisons. We prove that items that none of the assessors has ranked do not influence the maximum a posteriori consensus ranking, and can therefore be ignored. When assessors are many or heterogeneous, we propose a mixture model for clustering them in homogeneous subgroups, with clusterspecific consensus rankings. We develop approximate stochastic algorithms that allow a fully probabilistic analysis, leading to coherent quantifications of uncertainties. We make probabilistic predictions on the class membership of assessors based on their ranking of just some items, and predict missing individual preferences, as needed in recommendation systems. We test our approach using several experimental and benchmark data sets."
            ],
            "keywords": [
                "Incomplete rankings",
                "Pairwise comparisons",
                "Preference learning with uncertainty",
                "Recommendation systems",
                "Markov Chain Monte Carlo"
            ],
            "author": [
                "Valeria Vitelli",
                "Øystein Sørensen",
                "Marta Crispino",
                "Arnoldo Frigessi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-481/15-481.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Model Selection with Graph Structured Sparsity",
            "abstract": [
                "We propose a general algorithmic framework for Bayesian model selection. A spike-and-slab Laplacian prior is introduced to model the underlying structural assumption. Using the notion of effective resistance, we derive an EM-type algorithm with closed-form iterations to efficiently explore possible candidates for Bayesian model selection. The deterministic nature of the proposed algorithm makes it more scalable to large-scale and high-dimensional data sets compared with existing stochastic search algorithms. When applied to sparse linear regression, our framework recovers the EMVS algorithm (Ročková and George, 2014) as a special case. We also discuss extensions of our framework using tools from graph algebra to incorporate complex Bayesian models such as biclustering and submatrix localization. Extensive simulation studies and real data applications are conducted to demonstrate the superior performance of our methods over its frequentist competitors such as 0 or 1 penalization."
            ],
            "keywords": [
                "spike-and-slab prior",
                "graph laplacian",
                "variational inference",
                "expectation maximization",
                "sparse linear regression",
                "biclustering"
            ],
            "author": [
                "Youngseok Kim",
                "Chao Gao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-123/19-123.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimal Nonlinear Distortion Principle for Nonlinear Independent Component Analysis",
            "abstract": [
                "It is well known that solutions to the nonlinear independent component analysis (ICA) problem are highly non-unique. In this paper we propose the \"minimal nonlinear distortion\" (MND) principle for tackling the ill-posedness of nonlinear ICA problems. MND prefers the nonlinear ICA solution with the estimated mixing procedure as close as possible to linear, among all possible solutions. It also helps to avoid local optima in the solutions. To achieve MND, we exploit a regularization term to minimize the mean square error between the nonlinear mixing mapping and the best-fitting linear one. The effect of MND on the inherent trivial and non-trivial indeterminacies in nonlinear ICA solutions is investigated. Moreover, we show that local MND is closely related to the smoothness regularizer penalizing large curvature, which provides another useful regularization condition for nonlinear ICA. Experiments on synthetic data show the usefulness of the MND principle for separating various nonlinear mixtures. Finally, as an application, we use nonlinear ICA with MND to separate daily returns of a set of stocks in Hong Kong, and the linear causal relations among them are successfully discovered. The resulting causal relations give some interesting insights into the stock market. Such a result can not be achieved by linear ICA. Simulation studies also verify that when doing causality discovery, sometimes one should not ignore the nonlinear distortion in the data generation procedure, even if it is weak."
            ],
            "keywords": [
                "nonlinear ICA",
                "regularization",
                "minimal nonlinear distortion",
                "mean square error",
                "best linear reconstruction"
            ],
            "author": [
                "Kun Zhang",
                "Laiwan Chan",
                "Hong Kong",
                "Aapo Hyvärinen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/zhang08b/zhang08b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Matching Words and Pictures",
            "abstract": [
                "We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real"
            ],
            "keywords": [],
            "author": [
                "Kobus Barnard",
                "David Forsyth",
                "David M Blei",
                "Michael I Jordan",
                "Jaz Kandola",
                "Thomas Hofmann",
                "Tomaso Poggio",
                "John Shawe-Taylor"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/barnard03a/barnard03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exploration of the (Non-)Asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics",
            "abstract": [
                "Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data sets is computationally infeasible. The recently proposed stochastic gradient Langevin dynamics (SGLD) method circumvents this problem in three ways: it generates proposed moves using only a subset of the data, it skips the Metropolis-Hastings accept-reject step, and it uses sequences of decreasing step sizes. In Teh et al. (2014), we provided the mathematical foundations for the decreasing step size SGLD, including consistency and a central limit theorem. However, in practice the SGLD is run for a relatively small number of iterations, and its step size is not decreased to zero. The present article investigates the behaviour of the SGLD with fixed step size. In particular we characterise the asymptotic bias explicitly, along with its dependence on the step size and the variance of the stochastic gradient. On that basis a modified SGLD which removes the asymptotic bias due to the variance of the stochastic gradients up to first order in the step size is derived. Moreover, we are able to obtain bounds on the finite-time bias, variance and mean squared error (MSE). The theory is illustrated with a Gaussian toy model for which the bias and the MSE for the estimation of moments can be obtained explicitly. For this toy model we study the gain of the SGLD over the standard Euler method in the limit of large data sets."
            ],
            "keywords": [
                "Markov Chain Monte Carlo",
                "Langevin dynamics",
                "big data",
                "fixed step size"
            ],
            "author": [
                "Sebastian J Vollmer",
                "Konstantinos C Zygalakis",
                "Yee Whye Teh",
                "Yee Whye"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-494/15-494.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Matrix Completion from Noisy Entries",
            "abstract": [
                "Given a matrix M of low-rank, we consider the problem of reconstructing it from noisy observations of a small, random subset of its entries. The problem arises in a variety of applications, from collaborative filtering (the 'Netflix problem') to structure-from-motion and positioning. We study a low complexity algorithm introduced by Keshavan, Montanari, and Oh (2010), based on a combination of spectral techniques and manifold optimization, that we call here OPTSPACE. We prove performance guarantees that are order-optimal in a number of circumstances."
            ],
            "keywords": [
                "matrix completion",
                "low-rank matrices",
                "spectral methods",
                "manifold optimization"
            ],
            "author": [
                "Raghunandan H Keshavan",
                "Andrea Montanari",
                "Stanford Edu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/keshavan10a/keshavan10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Streamwise Feature Selection",
            "abstract": [
                "In streamwise feature selection, new features are sequentially considered for addition to a predictive model. When the space of potential features is large, streamwise feature selection offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overfitting can be controlled by dynamically adjusting the threshold for adding features to the model. In contrast to traditional forward feature selection algorithms such as stepwise regression in which at each step all possible features are evaluated and the best one is selected, streamwise feature selection only evaluates each feature once when it is generated. We describe information-investing and α-investing, two adaptive complexity penalty methods for streamwise feature selection which dynamically adjust the threshold on the error reduction required for adding a new feature. These two methods give false discovery rate style guarantees against overfitting. They differ from standard penalty methods such as AIC, BIC and RIC, which always drastically over-or under-fit in the limit of infinite numbers of non-predictive features. Empirical results show that streamwise regression is competitive with (on small data sets) and superior to (on large data sets) much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with millions of potential features."
            ],
            "keywords": [
                "classification",
                "stepwise regression",
                "multiple regression",
                "feature selection",
                "false discovery rate"
            ],
            "author": [
                "Jing Zhou",
                "Robert A Stine",
                "Lyle H Ungar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/zhou06a/zhou06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models",
            "abstract": [
                "Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude."
            ],
            "keywords": [
                "intractable likelihood",
                "latent variables",
                "Bayesian inference",
                "approximate Bayesian computation",
                "computational efficiency"
            ],
            "author": [
                "Michael U Gutmann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-017/15-017.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Particle Gibbs with Ancestor Sampling",
            "abstract": [
                "Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combining the two main tools used for Monte Carlo statistical inference: sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). We present a new PMCMC algorithm that we refer to as particle Gibbs with ancestor sampling (PGAS). PGAS provides the data analyst with an off-the-shelf class of Markov kernels that can be used to simulate, for instance, the typically high-dimensional and highly autocorrelated state trajectory in a state-space model. The ancestor sampling procedure enables fast mixing of the PGAS kernel even when using seemingly few particles in the underlying SMC sampler. This is important as it can significantly reduce the computational burden that is typically associated with using SMC. PGAS is conceptually similar to the existing PG with backward simulation (PGBS) procedure. Instead of using separate forward and backward sweeps as in PGBS, however, we achieve the same effect in a single forward sweep. This makes PGAS well suited for addressing inference problems not only in state-space models, but also in models with more complex dependencies, such as non-Markovian, Bayesian nonparametric, and general probabilistic graphical models."
            ],
            "keywords": [
                "particle Markov chain Monte Carlo",
                "sequential Monte Carlo",
                "Bayesian inference",
                "non-Markovian models",
                "state-space models"
            ],
            "author": [
                "Fredrik Lindsten",
                "Michael I Jordan",
                "Thomas B Schön"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/lindsten14a/lindsten14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning with Mixtures of Trees",
            "abstract": [
                "This paper describes the mixtures-of-trees model, a probabilistic model for discrete multidimensional domains. Mixtures-of-trees generalize the probabilistic trees of Chow and Liu (1968) in a different and complementary direction to that of Bayesian networks. We present efficient algorithms for learning mixtures-of-trees models in maximum likelihood and Bayesian frameworks. We also discuss additional efficiencies that can be obtained when data are \"sparse,\" and we present data structures and algorithms that exploit such sparseness. Experimental results demonstrate the performance of the model for both density estimation and classification. We also discuss the sense in which tree-based classifiers perform an implicit form of feature selection, and demonstrate a resulting insensitivity to irrelevant attributes."
            ],
            "keywords": [],
            "author": [
                "Marina Meilȃ",
                "Michael I Jordan"
            ],
            "ref": "http://www.jmlr.org/papers/volume1/meila00a/meila00a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimising Kernel Parameters and Regularisation Coefficients for Non-linear Discriminant Analysis",
            "abstract": [
                "In this paper we consider a novel Bayesian interpretation of Fisher's discriminant analysis. We relate Rayleigh's coefficient to a noise model that minimises a cost based on the most probable class centres and that abandons the 'regression to the labels' assumption used by other algorithms. Optimisation of the noise model yields a direction of discrimination equivalent to Fisher's discriminant, and with the incorporation of a prior we can apply Bayes' rule to infer the posterior distribution of the direction of discrimination. Nonetheless, we argue that an additional constraining distribution has to be included if sensible results are to be obtained. Going further, with the use of a Gaussian process prior we show the equivalence of our model to a regularised kernel Fisher's discriminant. A key advantage of our approach is the facility to determine kernel parameters and the regularisation coefficient through the optimisation of the marginal log-likelihood of the data. An added bonus of the new formulation is that it enables us to link the regularisation coefficient with the generalisation error."
            ],
            "keywords": [],
            "author": [
                "Tonatiuh Peña Centeno",
                "Neil D Lawrence"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/centeno06a/centeno06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Iterated Learning in Dynamic Social Networks",
            "abstract": [
                "A classic finding by (Kalish et al., 2007) shows that no language can be learned iteratively by rational agents in a self-sustained manner. In other words, if A teaches a foreign language to B, who then teaches what she learned to C, and so on, the language will quickly get lost and agents will wind up teaching their own common native language. If so, how can linguistic novelty ever be sustained? We address this apparent paradox by considering the case of iterated learning in a social network: we show that by varying the lengths of the learning sessions over time or by keeping the networks dynamic, it is possible for iterated learning to endure forever with arbitrarily small loss."
            ],
            "keywords": [],
            "author": [
                "Bernard Chazelle",
                "Chu Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-539/18-539.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structure Spaces",
            "abstract": [
                "Finite structures such as point patterns, strings, trees, and graphs occur as \"natural\" representations of structured data in different application areas of machine learning. We develop the theory of structure spaces and derive geometrical and analytical concepts such as the angle between structures and the derivative of functions on structures. In particular, we show that the gradient of a differentiable structural function is a well-defined structure pointing in the direction of steepest ascent. Exploiting the properties of structure spaces, it will turn out that a number of problems in structural pattern recognition such as central clustering or learning in structured output spaces can be formulated as optimization problems with cost functions that are locally Lipschitz. Hence, methods from nonsmooth analysis are applicable to optimize those cost functions."
            ],
            "keywords": [
                "graphs",
                "graph matching",
                "learning in structured domains",
                "nonsmooth optimization"
            ],
            "author": [
                "Brijnesh J Jain",
                "Klaus Obermayer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/jain09a/jain09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Network Learning via Topological Order",
            "abstract": [
                "We propose a mixed integer programming (MIP) model and iterative algorithms based on topological orders to solve optimization problems with acyclic constraints on a directed graph. The proposed MIP model has a significantly lower number of constraints compared to popular MIP models based on cycle elimination constraints and triangular inequalities. The proposed iterative algorithms use gradient descent and iterative reordering approaches, respectively, for searching topological orders. A computational experiment is presented for the Gaussian Bayesian network learning problem, an optimization problem minimizing the sum of squared errors of regression models with L1 penalty over a feature network with application of gene network inference in bioinformatics."
            ],
            "keywords": [
                "Bayesian networks",
                "topological orders",
                "Gaussian Bayesian network",
                "directed acyclic graphs"
            ],
            "author": [
                "Young Woong Park",
                "Diego Klabjan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-033/17-033.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On The Power of Membership Queries in Agnostic Learning *",
            "abstract": [
                "We study the properties of the agnostic learning framework of Haussler (1992) and Kearns, Schapire, and Sellie (1994). In particular, we address the question: is there any situation in which membership queries are useful in agnostic learning? Our results show that the answer is negative for distribution-independent agnostic learning and positive for agnostic learning with respect to a specific marginal distribution. Namely, we give a simple proof that any concept class learnable agnostically by a distribution-independent algorithm with access to membership queries is also learnable agnostically without membership queries. This resolves an open problem posed by Kearns et al. (1994). For agnostic learning with respect to the uniform distribution over {0, 1} n we show a concept class that is learnable with membership queries but computationally hard to learn from random examples alone (assuming that one-way functions exist)."
            ],
            "keywords": [
                "agnostic learning",
                "membership query",
                "separation",
                "PAC learning"
            ],
            "author": [
                "Vitaly Feldman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/feldman09a/feldman09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernels on Prolog Proof Trees: Statistical Learning in the ILP Setting *",
            "abstract": [
                "We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets."
            ],
            "keywords": [
                "kernel methods",
                "inductive logic programming",
                "Prolog",
                "learning from program traces"
            ],
            "author": [
                "Andrea Passerini",
                "Paolo Frasconi",
                "Luc De Raedt",
                "Roland Olsson",
                "Ute Schmid"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/passerini06a/passerini06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence guarantees for a class of non-convex and non-smooth optimization problems",
            "abstract": [
                "We consider the problem of finding critical points of functions that are non-convex and nonsmooth. Studying a fairly broad class of such problems, we analyze the behavior of three gradient-based methods (gradient descent, proximal update, and Frank-Wolfe update). For each of these methods, we establish rates of convergence for general problems, and also prove faster rates for continuous sub-analytic functions. We also show that our algorithms can escape strict saddle points for a class of non-smooth functions, thereby generalizing known results for smooth functions. Our analysis leads to a simplification of the popular CCCP algorithm, used for optimizing functions that can be written as a difference of two convex functions. Our simplified algorithm retains all the convergence properties of CCCP, along with a significantly lower cost per iteration. We illustrate our methods and theory via applications to the problems of best subset selection, robust estimation, mixture density estimation, and shape-from-shading reconstruction."
            ],
            "keywords": [],
            "author": [
                "Koulik Khamaru",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-762/18-762.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis *",
            "abstract": [
                "Reducing the dimensionality of data without losing intrinsic information is an important preprocessing step in high-dimensional data analysis. Fisher discriminant analysis (FDA) is a traditional technique for supervised dimensionality reduction, but it tends to give undesired results if samples in a class are multimodal. An unsupervised dimensionality reduction method called localitypreserving projection (LPP) can work well with multimodal data due to its locality preserving property. However, since LPP does not take the label information into account, it is not necessarily useful in supervised learning scenarios. In this paper, we propose a new linear supervised dimensionality reduction method called local Fisher discriminant analysis (LFDA), which effectively combines the ideas of FDA and LPP. LFDA has an analytic form of the embedding transformation and the solution can be easily computed just by solving a generalized eigenvalue problem. We demonstrate the practical usefulness and high scalability of the LFDA method in data visualization and classification tasks through extensive simulation studies. We also show that LFDA can be extended to non-linear dimensionality reduction scenarios by applying the kernel trick."
            ],
            "keywords": [
                "dimensionality reduction",
                "supervised learning",
                "Fisher discriminant analysis",
                "locality preserving projection",
                "affinity matrix"
            ],
            "author": [
                "Masashi Sugiyama"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/sugiyama07b/sugiyama07b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning from Binary Multiway Data: Probabilistic Tensor Decomposition and its Statistical Optimality",
            "abstract": [
                "We consider the problem of decomposing a higher-order tensor with binary entries. Such data problems arise frequently in applications such as neuroimaging, recommendation system, topic modeling, and sensor network localization. We propose a multilinear Bernoulli model, develop a rank-constrained likelihood-based estimation method, and obtain the theoretical accuracy guarantees. In contrast to continuous-valued problems, the binary tensor problem exhibits an interesting phase transition phenomenon according to the signal-tonoise ratio. The error bound for the parameter tensor estimation is established, and we show that the obtained rate is minimax optimal under the considered model. Furthermore, we develop an alternating optimization algorithm with convergence guarantees. The efficacy of our approach is demonstrated through both simulations and analyses of multiple data sets on the tasks of tensor completion and clustering."
            ],
            "keywords": [
                "binary tensor",
                "CANDECOMP/PARAFAC tensor decomposition",
                "constrained maximum likelihood estimation",
                "diverging dimensionality",
                "generalized linear model"
            ],
            "author": [
                "Miaoyan Wang",
                "Lexin Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-766/18-766.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs",
            "abstract": [
                "We present a class of graphical models for directly representing the joint cumulative distribution function (CDF) of many random variables, called cumulative distribution networks (CDNs). Unlike graphs for probability density and mass functions, for CDFs the marginal probabilities for any subset of variables are obtained by computing limits of functions in the model, and conditional probabilities correspond to computing mixed derivatives. We will show that the conditional independence properties in a CDN are distinct from the conditional independence properties of directed, undirected and factor graphs, but include the conditional independence properties of bi-directed graphs. In order to perform inference in such models, we describe the 'derivative-sum-product' (DSP) message-passing algorithm in which messages correspond to derivatives of the joint CDF. We will then apply CDNs to the problem of learning to rank players in multiplayer team-based games and suggest several future directions for research."
            ],
            "keywords": [
                "graphical models",
                "cumulative distribution function",
                "message-passing algorithm",
                "inference"
            ],
            "author": [
                "Jim C Huang",
                "Brendan J Frey"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/huang11a/huang11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Interplay of Optimization and Machine Learning Research",
            "abstract": [
                "The fields of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semidefinite, and semi-infinite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms."
            ],
            "keywords": [
                "machine learning",
                "mathematical programming",
                "convex optimization"
            ],
            "author": [
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/MLOPT-intro06a/MLOPT-intro06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees",
            "abstract": [
                "The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP fixed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefficients via Monte Carlo techniques and damped updates. We prove that the SOSMP iterates converge to a δ-neighborhood of the unique BP fixed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefficients as a function of the desired approximation accuracy δ and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical flow estimation."
            ],
            "keywords": [
                "graphical models",
                "sum-product for continuous state spaces",
                "low-complexity belief propagation",
                "stochastic approximation",
                "Monte Carlo methods",
                "orthogonal basis expansion"
            ],
            "author": [
                "Nima Noorshams",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/noorshams13a/noorshams13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Internal Regret with Partial Monitoring: Calibration-Based Optimal Algorithms",
            "abstract": [
                "We provide consistent random algorithms for sequential decision under partial monitoring, when the decision maker does not observe the outcomes but receives instead random feedback signals. Those algorithms have no internal regret in the sense that, on the set of stages where the decision maker chose his action according to a given law, the average payoff could not have been improved in average by using any other fixed law. They are based on a generalization of calibration, no longer defined in terms of a Voronoï diagram but instead of a Laguerre diagram (a more general concept). This allows us to bound, for the first time in this general framework, the expected average internal, as well as the usual external, regret at stage n by O(n −1/3), which is known to be optimal."
            ],
            "keywords": [
                "repeated games",
                "on-line learning",
                "regret",
                "partial monitoring",
                "calibration",
                "Voronoï and Laguerre diagrams"
            ],
            "author": [
                "Vianney Perchet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/perchet11a/perchet11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Computation of Gaussian Process Regression for Large Spatial Data Sets by Patching Local Gaussian Processes",
            "abstract": [
                "This paper develops an efficient computational method for solving a Gaussian process (GP) regression for large spatial data sets using a collection of suitably defined local GP regressions. The conventional local GP approach first partitions a domain into multiple non-overlapping local regions, and then fits an independent GP regression for each local region using the training data belonging to the region. Two key issues with the local GP are (1) the prediction around the boundary of a local region is not as accurate as the prediction at interior of the local region, and (2) two local GP regressions for two neighboring local regions produce different predictions at the boundary of the two regions, creating undesirable discontinuity in the prediction. We address these issues by constraining the predictions of local GP regressions sharing a common boundary to satisfy the same boundary constraints, which in turn are estimated by the data. The boundary constrained local GP regressions are solved by a finite element method. Our approach shows competitive performance when compared with several state-of-the-art methods using two synthetic data sets and three real data sets."
            ],
            "keywords": [
                "constrained Gaussian process regression",
                "kriging",
                "local regression",
                "boundary value problem",
                "spatial prediction",
                "variational problem"
            ],
            "author": [
                "Chiwoo Park",
                "Jianhua Z Huang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-327/15-327.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Comparison of the Lasso and Marginal Regression",
            "abstract": [
                "The lasso is an important method for sparse, high-dimensional regression problems, with efficient algorithms available, a long history of practical success, and a large body of theoretical results supporting and explaining its performance. But even with the best available algorithms, finding the lasso solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points. Marginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than the lasso solutions. The question that remains is how the statistical performance of the method compares to that of the lasso in these cases. In this paper, we study the relative statistical performance of the lasso and marginal regression for sparse, high-dimensional regression problems. We consider the problem of learning which coefficients are non-zero. Our main results are as follows: (i) we compare the conditions under which the lasso and marginal regression guarantee exact recovery in the fixed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the fixed design, noise free, random coefficients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefficients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric. In light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study."
            ],
            "keywords": [
                "high-dimensional regression",
                "lasso",
                "phase diagram",
                "regularization"
            ],
            "author": [
                "Christopher R Genovese",
                "Jiashun Jin",
                "Larry Wasserman",
                "Zhigang Yao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/genovese12b/genovese12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Scalable Deep Kernels with Recurrent Structure",
            "abstract": [
                "Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure, and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable."
            ],
            "keywords": [],
            "author": [
                "Maruan Al-Shedivat",
                "Andrew Gordon Wilson",
                "Yunus Saatchi",
                "Zhiting Hu",
                "Eric P Xing"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-498/16-498.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Search for Additive Nonlinear Time Series Causal Models",
            "abstract": [
                "Pointwise consistent, feasible procedures for estimating contemporaneous linear causal structure from time series data have been developed using multiple conditional independence tests, but no such procedures are available for non-linear systems. We describe a feasible procedure for learning a class of non-linear time series structures, which we call additive non-linear time series. We show that for data generated from stationary models of this type, two classes of conditional independence relations among time series variables and their lags can be tested efficiently and consistently using tests based on additive model regression. Combining results of statistical tests for these two classes of conditional independence relations and the temporal structure of time series data, a new consistent model specification procedure is able to extract relatively detailed causal information. We investigate the finite sample behavior of the procedure through simulation, and illustrate the application of this method through analysis of the possible causal connections among four ocean indices. Several variants of the procedure are also discussed."
            ],
            "keywords": [
                "conditional independence test",
                "contemporaneous causation",
                "additive model regression",
                "Granger causality",
                "ocean indices"
            ],
            "author": [
                "Tianjiao Chu",
                "Clark Glymour"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/chu08a/chu08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Algebraic Combinatorial Approach for Low-Rank Matrix Completion",
            "abstract": [
                "We present a novel algebraic combinatorial view on low-rank matrix completion based on studying relations between a few entries with tools from algebraic geometry and matroid theory. The intrinsic locality of the approach allows for the treatment of single entries in a closed theoretical and practical framework. More specifically, apart from introducing an algebraic combinatorial theory of low-rank matrix completion, we present probabilityone algorithms to decide whether a particular entry of the matrix can be completed. We also describe methods to complete that entry from a few others, and to estimate the error which is incurred by any method completing that entry. Furthermore, we show how known results on matrix completion and their sampling assumptions can be related to our new perspective and interpreted in terms of a completability phase transition."
            ],
            "keywords": [
                "Low-rank matrix completion",
                "entry-wise completion",
                "matrix reconstruction",
                "algebraic combinatorics"
            ],
            "author": [
                "Franz J Király",
                "Louis Theran"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/kiraly15a/kiraly15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Short-term Sparse Portfolio Optimization Based on Alternating Direction Method of Multipliers",
            "abstract": [
                "We propose a short-term sparse portfolio optimization (SSPO) system based on alternating direction method of multipliers (ADMM). Although some existing strategies have also exploited sparsity, they either constrain the quantity of the portfolio change or aim at the long-term portfolio optimization. Very few of them are dedicated to constructing sparse portfolios for the short-term portfolio optimization, which will be complemented by the proposed SSPO. SSPO concentrates wealth on a small proportion of assets that have good increasing potential according to some empirical financial principles, so as to maximize the cumulative wealth for the whole investment. We also propose a solving algorithm based on ADMM to handle the 1-regularization term and the self-financing constraint simultaneously. As a significant improvement in the proposed ADMM, we have proven that its augmented Lagrangian has a saddle point, which is the foundation of the iterative formulae of ADMM but is seldom addressed by other sparsity strategies. Extensive experiments on 5 benchmark data sets from real-world stock markets show that SSPO outperforms other state-of-the-art systems in thorough evaluations, withstands reasonable transaction costs and runs fast. Thus it is suitable for real-world financial environments."
            ],
            "keywords": [
                "short-term portfolio optimization",
                "sparse portfolio",
                "alternating direction method of multipliers"
            ],
            "author": [
                "Zhao-Rong Lai",
                "Pei-Yi Yang",
                "Xiaotian Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-558/17-558.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Concentration Bounds for Unigram Language Models",
            "abstract": [
                "We show several high-probability concentration bounds for learning unigram language models. One interesting quantity is the probability of all words appearing exactly k times in a sample of size m. A standard estimator for this quantity is the Good-Turing estimator. The existing analysis on its error shows a high-probability bound of approximately O k √ m. We improve its dependency on k to O 4 √ k √ m + k m. We also analyze the empirical frequencies estimator, showing that with high probability its error is bounded by approximately O k + √ k m. We derive a combined estimator, which has an error of approximately O m − 2 5 , for any k. A standard measure for the quality of a learning algorithm is its expected per-word log-loss. The leave-one-out method can be used for estimating the log-loss of the unigram model. We show that its error has a high-probability bound of approximately O 1 √ m , for any underlying distribution. We also bound the log-loss a priori, as a function of various parameters of the distribution."
            ],
            "keywords": [
                "Good-Turing estimators",
                "logarithmic loss",
                "leave-one-out estimation",
                "Chernoff bounds"
            ],
            "author": [
                "Evgeny Drukh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/drukh05a/drukh05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Equivalence Relationships Between Classification and Ranking Algorithms Ş eyda Ertekin",
            "abstract": [
                "We demonstrate that there are machine learning algorithms that can achieve success for two separate tasks simultaneously, namely the tasks of classification and bipartite ranking. This means that advantages gained from solving one task can be carried over to the other task, such as the ability to obtain conditional density estimates, and an order-of-magnitude reduction in computational time for training the algorithm. It also means that some algorithms are robust to the choice of evaluation metric used; they can theoretically perform well when performance is measured either by a misclassification error or by a statistic of the ROC curve (such as the area under the curve). Specifically, we provide such an equivalence relationship between a generalization of Freund et al.'s RankBoost algorithm, called the \"P-Norm Push,\" and a particular cost-sensitive classification algorithm that generalizes AdaBoost, which we call \"P-Classification.\" We discuss and validate the potential benefits of this equivalence relationship, and perform controlled experiments to understand P-Classification's empirical performance. There is no established equivalence relationship for logistic regression and its ranking counterpart, so we introduce a logistic-regression-style algorithm that aims in between classification and ranking, and has promising experimental performance with respect to both tasks."
            ],
            "keywords": [
                "supervised classification",
                "bipartite ranking",
                "area under the curve",
                "rank statistics",
                "boosting",
                "logistic regression"
            ],
            "author": [
                "Cynthia Rudin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/ertekin11a/ertekin11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ranking Individuals by Group Comparisons",
            "abstract": [
                "This paper proposes new approaches to rank individuals from their group comparison results. Many real-world problems are of this type. For example, ranking players from team comparisons is important in some sports. In machine learning, a closely related application is classification using coding matrices. Group comparison results are usually in two types: binary indicator outcomes (wins/losses) or measured outcomes (scores). For each type of results, we propose new models for estimating individuals' abilities, and hence a ranking of individuals. The estimation is carried out by solving convex minimization problems, for which we develop easy and efficient solution procedures. Experiments on real bridge records and multi-class classification demonstrate the viability of the proposed models."
            ],
            "keywords": [
                "ranking",
                "group comparison",
                "binary/scored outcomes",
                "Bradley-Terry model",
                "multiclass classification"
            ],
            "author": [
                "Tzu-Kuo Huang",
                "Chih-Jen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/huang08a/huang08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local algorithms for interactive clustering",
            "abstract": [
                "We study the design of interactive clustering algorithms. The user supervision that we consider is in the form of cluster split/merge requests; such feedback is easy for users to provide because it only requires a high-level understanding of the clusters. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable properties in many applications. Local changes are desirable because in practice edits of other parts of the clustering are considered churn-changes that are perceived as quality-neutral or quality-negative. We show that in this framework we can still design provably correct algorithms given that our data satisfies natural separability properties. We also show that our framework works well in practice."
            ],
            "keywords": [],
            "author": [
                "Pranjal Awasthi",
                "Konstantin Voevodski"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-085/15-085.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Matrix Exponentiated Gradient Updates for On-line Learning and Bregman Projection",
            "abstract": [
                "We address the problem of learning a symmetric positive definite matrix. The central issue is to design parameter updates that preserve positive definiteness. Our updates are motivated with the von Neumann divergence. Rather than treating the most general case, we focus on two key applications that exemplify our methods: on-line learning with a simple square loss, and finding a symmetric positive definite matrix subject to linear constraints. The updates generalize the exponentiated gradient (EG) update and AdaBoost, respectively: the parameter is now a symmetric positive definite matrix of trace one instead of a probability vector (which in this context is a diagonal positive definite matrix with trace one). The generalized updates use matrix logarithms and exponentials to preserve positive definiteness. Most importantly, we show how the derivation and the analyses of the original EG update and AdaBoost generalize to the non-diagonal case. We apply the resulting matrix exponentiated gradient (MEG) update and DefiniteBoost to the problem of learning a kernel matrix from distance measurements."
            ],
            "keywords": [],
            "author": [
                "Koji Tsuda",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/tsuda05a/tsuda05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Very Fast Learning Method for Neural Networks Based on Sensitivity Analysis",
            "abstract": [
                "This paper introduces a learning method for two-layer feedforward neural networks based on sensitivity analysis, which uses a linear training algorithm for each of the two layers. First, random values are assigned to the outputs of the first layer; later, these initial values are updated based on sensitivity formulas, which use the weights in each of the layers; the process is repeated until convergence. Since these weights are learnt solving a linear system of equations, there is an important saving in computational time. The method also gives the local sensitivities of the least square errors with respect to input and output data, with no extra computational cost, because the necessary information becomes available without extra calculations. This method, called the Sensitivity-Based Linear Learning Method, can also be used to provide an initial set of weights, which significantly improves the behavior of other learning algorithms. The theoretical basis for the method is given and its performance is illustrated by its application to several examples in which it is compared with several learning algorithms and well known data sets. The results have shown a learning speed generally faster than other existing methods. In addition, it can be used as an initialization tool for other well known methods with significant improvements."
            ],
            "keywords": [
                "supervised learning",
                "neural networks",
                "linear optimization",
                "least-squares",
                "initialization method",
                "sensitivity analysis"
            ],
            "author": [
                "Enrique Castillo",
                "Oscar Fontenla-Romero",
                "Amparo Alonso-Betanzos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/castillo06a/castillo06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Influence of Momentum Acceleration on Online Learning",
            "abstract": [
                "The article examines in some detail the convergence rate and mean-square-error performance of momentum stochastic gradient methods in the constant step-size and slow adaptation regime. The results establish that momentum methods are equivalent to the standard stochastic gradient method with a re-scaled (larger) step-size value. The size of the re-scaling is determined by the value of the momentum parameter. The equivalence result is established for all time instants and not only in steady-state. The analysis is carried out for general strongly convex and smooth risk functions, and is not limited to quadratic risks. One notable conclusion is that the well-known benefits of momentum constructions for deterministic optimization problems do not necessarily carry over to the adaptive online setting when small constant step-sizes are used to enable continuous adaptation and learning in the presence of persistent gradient noise. From simulations, the equivalence between momentum and standard stochastic gradient methods is also observed for non-differentiable and non-convex problems."
            ],
            "keywords": [
                "Online Learning",
                "Stochastic Gradient",
                "Momentum Acceleration",
                "Heavy-ball Method",
                "Nesterov's Method",
                "Mean-Square-Error Analysis",
                "Convergence Rate"
            ],
            "author": [
                "Kun Yuan",
                "Ali H Sayed"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-157/16-157.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem",
            "abstract": [
                "A sequence x 1 ,. .. , x n ,. .. of discrete-valued observations is generated according to some unknown probabilistic law (measure) µ. After observing each outcome, one is required to give conditional probabilities of the next observation. The realizable case is when the measure µ belongs to an arbitrary but known class C of process measures. The non-realizable case is when µ is completely arbitrary, but the prediction performance is measured with respect to a given set C of process measures. We are interested in the relations between these problems and between their solutions, as well as in characterizing the cases when a solution exists and finding these solutions. We show that if the quality of prediction is measured using the total variation distance, then these problems coincide, while if it is measured using the expected average KL divergence, then they are different. For some of the formalizations we also show that when a solution exists it can be obtained as a Bayes mixture over a countable subset of C. We also obtain several characterization of those sets C for which solutions to the considered problems exist. As an illustration to the general results obtained, we show that a solution to the non-realizable case of the sequence prediction problem exists for the set of all finite-memory processes, but does not exist for the set of all stationary processes. It should be emphasized that the framework is completely general: the processes measures considered are not required to be i.i.d., mixing, stationary, or to belong to any parametric family."
            ],
            "keywords": [
                "sequence prediction",
                "time series",
                "online prediction",
                "realizable sequence prediction",
                "non-realizable sequence prediction"
            ],
            "author": [
                "Daniil Ryabko"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/ryabko11a/ryabko11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Message-passing for Graph-structured Linear Programs: Proximal Methods and Rounding Schemes",
            "abstract": [
                "The problem of computing a maximum a posteriori (MAP) configuration is a central computational challenge associated with Markov random fields. There has been some focus on \"tree-based\" linear programming (LP) relaxations for the MAP problem. This paper develops a family of super-linearly convergent algorithms for solving these LPs, based on proximal minimization schemes using Bregman divergences. As with standard message-passing on graphs, the algorithms are distributed and exploit the underlying graphical structure, and so scale well to large problems. Our algorithms have a double-loop character, with the outer loop corresponding to the proximal sequence, and an inner loop of cyclic Bregman projections used to compute each proximal update. We establish convergence guarantees for our algorithms, and illustrate their performance via some simulations. We also develop two classes of rounding schemes, deterministic and randomized, for obtaining integral configurations from the LP solutions. Our deterministic rounding schemes use a \"re-parameterization\" property of our algorithms so that when the LP solution is integral, the MAP solution can be obtained even before the LP-solver converges to the optimum. We also propose graph-structured randomized rounding schemes applicable to iterative LP-solving algorithms in general. We analyze the performance of and report simulations comparing these rounding schemes."
            ],
            "keywords": [
                "graphical models",
                "MAP Estimation",
                "LP relaxation",
                "proximal minimization",
                "rounding schemes"
            ],
            "author": [
                "Pradeep Ravikumar",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/ravikumar10a/ravikumar10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient and Effective Visual Codebook Generation Using Additive Kernels",
            "abstract": [
                "Common visual codebook generation methods used in a bag of visual words model, for example, k-means or Gaussian Mixture Model, use the Euclidean distance to cluster features into visual code words. However, most popular visual descriptors are histograms of image measurements. It has been shown that with histogram features, the Histogram Intersection Kernel (HIK) is more effective than the Euclidean distance in supervised learning tasks. In this paper, we demonstrate that HIK can be used in an unsupervised manner to significantly improve the generation of visual codebooks. We propose a histogram kernel k-means algorithm which is easy to implement and runs almost as fast as the standard k-means. The HIK codebooks have consistently higher recognition accuracy over k-means codebooks by 2-4% in several benchmark object and scene recognition data sets. The algorithm is also generalized to arbitrary additive kernels. Its speed is thousands of times faster than a naive implementation of the kernel k-means algorithm. In addition, we propose a one-class SVM formulation to create more effective visual code words. Finally, we show that the standard kmedian clustering method can be used for visual codebook generation and can act as a compromise between the HIK / additive kernel and the k-means approaches."
            ],
            "keywords": [
                "visual codebook",
                "additive kernel",
                "histogram intersection kernel"
            ],
            "author": [
                "Jianxin Wu",
                "James M Rehg"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/wu11b/wu11b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-parametric Policy Search with Limited Information Loss Herke van Hoof",
            "abstract": [
                "Learning complex control policies from non-linear and redundant sensory input is an important challenge for reinforcement learning algorithms. Non-parametric methods that approximate values functions or transition models can address this problem, by adapting to the complexity of the data set. Yet, many current non-parametric approaches rely on unstable greedy maximization of approximate value functions, which might lead to poor convergence or oscillations in the policy update. A more robust policy update can be obtained by limiting the information loss between successive state-action distributions. In this paper, we develop a policy search algorithm with policy updates that are both robust and non-parametric. Our method can learn non-parametric control policies for infinite horizon continuous Markov decision processes with non-linear and redundant sensory representations. We investigate how we can use approximations of the kernel function to reduce the time requirements of the demanding non-parametric computations. In our experiments, we show the strong performance of the proposed method, and how it can be approximated efficiently. Finally, we show that our algorithm can learn a real-robot under-powered swing-up task directly from image data."
            ],
            "keywords": [
                "Reinforcement Learning",
                "Kernel Methods",
                "Policy Search",
                "Robotics"
            ],
            "author": [
                "Gerhard Neumann",
                "Jan Peters"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-142/16-142.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Supervised Descriptive Rule Discovery: A Unifying Survey of Contrast Set, Emerging Pattern and Subgroup Mining",
            "abstract": [
                "This paper gives a survey of contrast set mining (CSM), emerging pattern mining (EPM), and subgroup discovery (SD) in a unifying framework named supervised descriptive rule discovery. While all these research areas aim at discovering patterns in the form of rules induced from labeled data, they use different terminology and task definitions, claim to have different goals, claim to use different rule learning heuristics, and use different means for selecting subsets of induced patterns. This paper contributes a novel understanding of these subareas of data mining by presenting a unified terminology, by explaining the apparent differences between the learning tasks as variants of a unique supervised descriptive rule discovery task and by exploring the apparent differences between the approaches. It also shows that various rule learning heuristics used in CSM, EPM and SD algorithms all aim at optimizing a trade off between rule coverage and precision. The commonalities (and differences) between the approaches are showcased on a selection of best known variants of CSM, EPM and SD algorithms. The paper also provides a critical survey of existing supervised descriptive rule discovery visualization methods."
            ],
            "keywords": [
                "descriptive rules",
                "rule learning",
                "contrast set mining",
                "emerging patterns",
                "subgroup discovery"
            ],
            "author": [
                "Petra Kralj Novak",
                "Nada Lavrač",
                "Geoffrey I Webb"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/kralj-novak09a/kralj-novak09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Differentially Private Data Releasing for Smooth Queries",
            "abstract": [
                "In the past few years, differential privacy has become a standard concept in the area of privacy. One of the most important problems in this field is to answer queries while preserving differential privacy. In spite of extensive studies, most existing work on differentially private query answering assumes the data are discrete (i.e., in {0, 1} d) and focuses on queries induced by Boolean functions. In real applications however, continuous data are at least as common as binary data. Thus, in this work we explore a less studied topic, namely, differential privately query answering for continuous data with continuous function. As a first step on L ∞-approximation of (transformed) smooth functions by low-degree even trigonometric polynomials with uniformly bounded coefficients. We also develop practically efficient variants of the mechanisms with promising experimental results."
            ],
            "keywords": [
                "differential privacy",
                "smooth queries",
                "synthetic dataset"
            ],
            "author": [
                "Ziteng Wang",
                "Chi Jin",
                "Kai Fan",
                "Jiaqi Zhang",
                "Junliang Huang",
                "Yiqiao Zhong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-388/14-388.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Off-policy Learning With Eligibility Traces: A Survey",
            "abstract": [
                "In the framework of Markov Decision Processes, we consider linear off-policy learning, that is the problem of learning a linear approximation of the value function of some fixed policy from one trajectory possibly generated by some other policy. We briefly review on-policy learning algorithms of the literature (gradient-based and least-squares-based), adopting a unified algorithmic view. Then, we highlight a systematic approach for adapting them to off-policy learning with eligibility traces. This leads to some known algorithmsoff-policy LSTD(λ), LSPE(λ), TD(λ), TDC/GQ(λ)-and suggests new extensions-offpolicy FPKF(λ), BRM(λ), gBRM(λ), GTD2(λ). We describe a comprehensive algorithmic derivation of all algorithms in a recursive and memory-efficent form, discuss their known convergence properties and illustrate their relative empirical behavior on Garnet problems. Our experiments suggest that the most standard algorithms on and off-policy LSTD(λ)/LSPE(λ)-and TD(λ) if the feature space dimension is too large for a leastsquares approach-perform the best."
            ],
            "keywords": [
                "reinforcement learning",
                "value function estimation",
                "off-policy learning",
                "eligibility traces"
            ],
            "author": [
                "Matthieu Geist",
                "Bruno Scherrer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/geist14a/geist14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Streaming Principal Component Analysis From Incomplete Data",
            "abstract": [
                "Linear subspace models are pervasive in computational sciences and particularly used for large datasets which are often incomplete due to privacy issues or sampling constraints. Therefore, a critical problem is developing an efficient algorithm for detecting low-dimensional linear structure from incomplete data efficiently, in terms of both computational complexity and storage. In this paper we propose a streaming subspace estimation algorithm called Subspace Navigation via Interpolation from Partial Entries (SNIPE) that efficiently processes blocks of incomplete data to estimate the underlying subspace model. In every iteration, SNIPE finds the subspace that best fits the new data block but remains close to the previous estimate. We show that SNIPE is a streaming solver for the underlying nonconvex matrix completion problem, that it converges globally to a stationary point of this program regardless of initialization, and that the convergence is locally linear with high probability. We also find that SNIPE shows state-of-the-art performance in our numerical simulations."
            ],
            "keywords": [
                "Principal component analysis",
                "Subspace identification",
                "Matrix completion",
                "Streaming algorithms",
                "Nonconvex optimization",
                "Global convergence"
            ],
            "author": [
                "Armin Eftekhari",
                "Gregory Ongie",
                "Laura Balzano",
                "Michael B Wakin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/16-627/16-627.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Discriminative Learning of Max-Sum Classifiers Vojtěch Franc *",
            "abstract": [
                "The max-sum classifier predicts n-tuple of labels from n-tuple of observable variables by maximizing a sum of quality functions defined over neighbouring pairs of labels and observable variables. Predicting labels as MAP assignments of a Random Markov Field is a particular example of the max-sum classifier. Learning parameters of the max-sum classifier is a challenging problem because even computing the response of such classifier is NP-complete in general. Estimating parameters using the Maximum Likelihood approach is feasible only for a subclass of max-sum classifiers with an acyclic structure of neighbouring pairs. Recently, the discriminative methods represented by the perceptron and the Support Vector Machines, originally designed for binary linear classifiers, have been extended for learning some subclasses of the max-sum classifier. Besides the max-sum classifiers with the acyclic neighbouring structure, it has been shown that the discriminative learning is possible even with arbitrary neighbouring structure provided the quality functions fulfill some additional constraints. In this article, we extend the discriminative approach to other three classes of max-sum classifiers with an arbitrary neighbourhood structure. We derive learning algorithms for two subclasses of max-sum classifiers whose response can be computed in polynomial time: (i) the max-sum classifiers with supermodular quality functions and (ii) the max-sum classifiers whose response can be computed exactly by a linear programming relaxation. Moreover, we show that the learning problem can be approximately solved even for a general max-sum classifier."
            ],
            "keywords": [
                "max-xum classifier",
                "hidden Markov networks",
                "support vector machines"
            ],
            "author": [
                "Bogdan Savchynskyy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/franc08a/franc08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Subspace Information Criterion for Infinite Dimensional Hypothesis Spaces",
            "abstract": [
                "A central problem in learning is selection of an appropriate model. This is typically done by estimating the unknown generalization errors of a set of models to be selected from and then choosing the model with minimal generalization error estimate. In this article, we discuss the problem of model selection and generalization error estimation in the context of kernel regression models, e.g., kernel ridge regression, kernel subset regression or Gaussian process regression. Previously, a non-asymptotic generalization error estimator called the subspace information criterion (SIC) was proposed, that could be successfully applied to finite dimensional subspace models. SIC is an unbiased estimator of the generalization error for the finite sample case under the conditions that the learning target function belongs to a specified reproducing kernel Hilbert space (RKHS) H and the reproducing kernels centered on training sample points span the whole space H. These conditions hold only if dim H ≤ , where (< ∞) is the number of training examples. Therefore, SIC could be applied only to finite dimensional RKHSs. In this paper, we extend the range of applicability of SIC, and show that even if the reproducing kernels centered on training sample points do not span the whole space H, SIC is an unbiased estimator of an essential part of the generalization error. Our extension allows the use of any RKHSs including infinite dimensional ones, i.e., richer function classes commonly used in Gaussian processes, support vector machines or boosting. We further show that when the kernel matrix is invertible, SIC can be expressed in a much simpler form, making its computation highly efficient. In computer simulations on ridge parameter selection with real and artificial data sets, SIC is compared favorably with other standard model selection techniques for instance leave-one-out cross-validation or an empirical Bayesian method."
            ],
            "keywords": [
                "Generalization error",
                "model selection",
                "subspace information criterion",
                "crossvalidation",
                "kernel regression",
                "reproducing kernel Hilbert space",
                "finite sample statistics",
                "Gaussian processes",
                "unbiased estimators"
            ],
            "author": [
                "Masashi Sugiyama",
                "Klaus- Robert Müller"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/sugiyama02a/sugiyama02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Efficient Two Step Algorithm for High Dimensional Change Point Regression Models Without Grid Search",
            "abstract": [
                "We propose a two step algorithm based on 1 / 0 regularization for the detection and estimation of parameters of a high dimensional change point regression model and provide the corresponding rates of convergence for the change point as well as the regression parameter estimates. Importantly, the computational cost of our estimator is only 2•Lasso(n, p), where Lasso(n, p) represents the computational burden of one Lasso optimization in a model of size (n, p). In comparison, existing grid search based approaches to this problem require a computational cost of at least n • Lasso(n, p) optimizations. Additionally, the proposed method is shown to be able to consistently detect the case of 'no change', i.e., where no finite change point exists in the model. We allow the true change point parameter τ 0 to possibly move to the boundaries of its parametric space, and the jump size β 0 − γ 0 2 to possibly diverge as n increases. We then characterize the corresponding effects on the rates of convergence of the change point and regression estimates. In particular, we show that, while an increasing jump size may have a beneficial effect on the change point estimate, however the optimal rate of regression parameter estimates are preserved only upto a certain rate of the increasing jump size. This behavior in the rate of regression parameter estimates is unique to high dimensional change point regression models only. Simulations are performed to empirically evaluate performance of the proposed estimators. The methodology is applied to community level socioeconomic data of the U.S., collected from the 1990 U.S. census and other sources."
            ],
            "keywords": [
                "Change point regression",
                "High dimensional models",
                "1",
                "0 regularization",
                "Rate of convergence",
                "Two phase regression"
            ],
            "author": [
                "Abhishek Kaul",
                "Venkata K Jandhyala"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-460/18-460.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear Fitted-Q Iteration with Multiple Reward Functions",
            "abstract": [
                "We present a general and detailed development of an algorithm for finite-horizon fitted-Q iteration with an arbitrary number of reward signals and linear value function approximation using an arbitrary number of state features. This includes a detailed treatment of the 3-reward function case using triangulation primitives from computational geometry and a method for identifying globally dominated actions. We also present an example of how our methods can be used to construct a realworld decision aid by considering symptom reduction, weight gain, and quality of life in sequential treatments for schizophrenia. Finally, we discuss future directions in which to take this work that will further enable our methods to make a positive impact on the field of evidence-based clinical decision support."
            ],
            "keywords": [
                "reinforcement learning",
                "dynamic programming",
                "decision making",
                "linear regression",
                "preference elicitation"
            ],
            "author": [
                "Daniel J Lizotte",
                "David R Cheriton",
                "Susan A Murphy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/lizotte12a/lizotte12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Target Regression with Rule Ensembles",
            "abstract": [
                "Methods for learning decision rules are being successfully applied to many problem domains, in particular when understanding and interpretation of the learned model is necessary. In many real life problems, we would like to predict multiple related (nominal or numeric) target attributes simultaneously. While several methods for learning rules that predict multiple targets at once exist, they are all based on the covering algorithm, which does not work well for regression problems. A better solution for regression is the rule ensemble approach that transcribes an ensemble of decision trees into a large collection of rules. An optimization procedure is then used to select the best (and much smaller) subset of these rules and to determine their respective weights. We introduce the FIRE algorithm for solving multi-target regression problems, which employs the rule ensembles approach. We improve the accuracy of the algorithm by adding simple linear functions to the ensemble. We also extensively evaluate the algorithm with and without linear functions. The results show that the accuracy of multi-target regression rule ensembles is high. They are more accurate than, for instance, multi-target regression trees, but not quite as accurate as multi-target random forests. The rule ensembles are significantly more concise than random forests, and it is also possible to create compact rule sets that are smaller than a single regression tree but still comparable in accuracy."
            ],
            "keywords": [
                "multi-target prediction",
                "rule learning",
                "rule ensembles",
                "regression"
            ],
            "author": [
                "Timo Aho"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/aho12a/aho12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization",
            "abstract": [
                "We present a general approach for collaborative filtering (CF) using spectral regularization to learn linear operators mapping a set of \"users\" to a set of possibly desired \"objects\". In particular, several recent low-rank type matrix-completion methods for CF are shown to be special cases of our proposed framework. Unlike existing regularization-based CF, our approach can be used to incorporate additional information such as attributes of the users/objects-a feature currently lacking in existing regularization-based CF approaches-using popular and well-known kernel methods. We provide novel representer theorems that we use to develop new estimation methods. We then provide learning algorithms based on low-rank decompositions and test them on a standard CF data set. The experiments indicate the advantages of generalizing the existing regularization-based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach."
            ],
            "keywords": [
                "collaborative filtering",
                "matrix completion",
                "kernel methods",
                "spectral regularization"
            ],
            "author": [
                "Jacob Abernethy",
                "Francis Bach",
                "Jean-Philippe Vert",
                "Mines Paristech"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/abernethy09a/abernethy09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularized Principal Manifolds",
            "abstract": [
                "Many settings of unsupervised learning can be viewed as quantization problems-the minimization of the expected quantization error subject to some restrictions. This allows the use of tools such as regularization from the theory of (supervised) risk minimization for unsupervised learning. This setting turns out to be closely related to principal curves, the generative topographic map, and robust coding. We explore this connection in two ways: (1) we propose an algorithm for finding principal manifolds that can be regularized in a variety of ways; and (2) we derive uniform convergence bounds and hence bounds on the learning rates of the algorithm. In particular, we give bounds on the covering numbers which allows us to obtain nearly optimal learning rates for certain types of regularization operators. Experimental results demonstrate the feasibility of the approach."
            ],
            "keywords": [
                "Regularization",
                "Uniform Convergence",
                "Kernels",
                "Entropy Numbers",
                "Principal Curves",
                "Clustering",
                "generative topographic map",
                "Support Vector Machines",
                "Kernel PCA"
            ],
            "author": [
                "Alexander J Smola",
                "Sebastian Mika",
                "Bernhard Schölkopf",
                "Robert C Williamson",
                "Mika Smola",
                "Williamson Schölkopf"
            ],
            "ref": "http://www.jmlr.org/papers/volume1/smola01a/smola01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Change Surfaces for Expressive Multidimensional Changepoints and Counterfactual Prediction",
            "abstract": [
                "Identifying changes in model parameters is fundamental in machine learning and statistics. However, standard changepoint models are limited in expressiveness, often addressing unidimensional problems and assuming instantaneous changes. We introduce change surfaces as a multidimensional and highly expressive generalization of changepoints. We provide a model-agnostic formalization of change surfaces, illustrating how they can provide variable, heterogeneous, and non-monotonic rates of change across multiple dimensions. Additionally, we show how change surfaces can be used for counterfactual prediction. As a concrete instantiation of the change surface framework, we develop Gaussian Process Change Surfaces (GPCS). We demonstrate counterfactual prediction with Bayesian posterior mean and credible sets, as well as massive scalability by introducing novel methods for additive nonseparable kernels. Using two large spatio-temporal datasets we employ GPCS to discover and characterize complex changes that can provide scientific and policy relevant insights. Specifically, we analyze twentieth century measles incidence across the United States and discover previously unknown heterogeneous changes after the introduction of the measles vaccine. Additionally, we apply the model to requests for lead testing kits in New York City, discovering distinct spatial and demographic patterns."
            ],
            "keywords": [
                "Change surface",
                "changepoint",
                "counterfactual",
                "Gaussian process",
                "scalable inference",
                "kernel method"
            ],
            "author": [
                "William Herlands",
                "Daniel B Neill",
                "Hannes Nickisch",
                "Andrew Gordon Wilson",
                "Andrew Gordon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-352/17-352.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Complete Graphical Characterization and Construction of Adjustment Sets in Markov Equivalence Classes of Ancestral Graphs",
            "abstract": [
                "We present a graphical criterion for covariate adjustment that is sound and complete for four different classes of causal graphical models: directed acyclic graphs (DAGs), maximal ancestral graphs (MAGs), completed partially directed acyclic graphs (CPDAGs), and partial ancestral graphs (PAGs). Our criterion unifies covariate adjustment for a large set of graph classes. Moreover, we define an explicit set that satisfies our criterion, if there is any set that satisfies our criterion. We also give efficient algorithms for constructing all sets that fulfill our criterion, implemented in the R package dagitty. Finally, we discuss the relationship between our criterion and other criteria for adjustment, and we provide new soundness and completeness proofs for the adjustment criterion for DAGs."
            ],
            "keywords": [
                "causal effects",
                "graphical models",
                "covariate adjustment",
                "latent variables",
                "confounding"
            ],
            "author": [
                "Emilija Perković",
                "Johannes Textor",
                "Markus Kalisch",
                "Marloes H Maathuis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-319/16-319.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "BayesPy: Variational Bayesian Inference in Python",
            "abstract": [
                "BayesPy is an open-source Python software package for performing variational Bayesian inference. It is based on the variational message passing framework and supports conjugate exponential family models. By removing the tedious task of implementing the variational Bayesian update equations, the user can construct models faster and in a less error-prone way. Simple syntax, flexible model construction and efficient inference make BayesPy suitable for both average and expert Bayesian users. It also supports some advanced methods such as stochastic and collapsed variational inference."
            ],
            "keywords": [
                "variational Bayes",
                "probabilistic programming",
                "Python"
            ],
            "author": [
                "Jaakko Luttinen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/luttinen16a/luttinen16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Kernel Ridge Regression with Communications",
            "abstract": [
                "This paper focuses on generalization performance analysis for distributed algorithms in the framework of learning theory. Taking distributed kernel ridge regression (DKRR) for example, we succeed in deriving its optimal learning rates in expectation and providing theoretically optimal ranges of the number of local processors. Due to the gap between theory and experiments, we also deduce optimal learning rates for DKRR in probability to essentially reflect the generalization performance and limitations of DKRR. Furthermore, we propose a communication strategy to improve the learning performance of DKRR and demonstrate the power of communications in DKRR via both theoretical assessments and numerical experiments."
            ],
            "keywords": [
                "learning theory",
                "distributed learning",
                "kernel ridge regression",
                "communication"
            ],
            "author": [
                "Shao-Bo Lin",
                "Di Wang",
                "Ding-Xuan Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-592/19-592.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Exponential Model for Infinite Rankings",
            "abstract": [
                "This paper presents a statistical model for expressing preferences through rankings, when the number of alternatives (items to rank) is large. A human ranker will then typically rank only the most preferred items, and may not even examine the whole set of items, or know how many they are. Similarly, a user presented with the ranked output of a search engine, will only consider the highest ranked items. We model such situations by introducing a stagewise ranking model that operates with finite ordered lists called top-t orderings over an infinite space of items. We give algorithms to estimate this model from data, and demonstrate that it has sufficient statistics, being thus an exponential family model with continuous and discrete parameters. We describe its conjugate prior and other statistical properties. Then, we extend the estimation problem to multimodal data by introducing an Exponential-Blurring-Mean-Shift nonparametric clustering algorithm. The experiments highlight the properties of our model and demonstrate that infinite models over permutations can be simple, elegant and practical."
            ],
            "keywords": [
                "permutations",
                "partial orderings",
                "Mallows model",
                "distance based ranking model",
                "exponential family",
                "non-parametric clustering",
                "branch-and-bound"
            ],
            "author": [
                "Marina Meilȃ",
                "Le Bao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/meila10a/meila10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Predicting a Switching Sequence of Graph Labelings",
            "abstract": [
                "We study the problem of predicting online the labeling of a graph. We consider a novel setting for this problem in which, in addition to observing vertices and labels on the graph, we also observe a sequence of just vertices on a second graph. A latent labeling of the second graph selects one of K labelings to be active on the first graph. We propose a polynomial time algorithm for online prediction in this setting and derive a mistake bound for the algorithm. The bound is controlled by the geometric cut of the observed and latent labelings, as well as the resistance diameters of the graphs. When specialized to multitask prediction and online switching problems the bound gives new and sharper results under certain conditions."
            ],
            "keywords": [
                "online learning over graphs",
                "kernel methods",
                "matrix winnow",
                "switching"
            ],
            "author": [
                "Mark Herbster",
                "Stephen Pasteris",
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/herbster15a/herbster15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Working Set Selection Using Second Order Information for Training Support Vector Machines",
            "abstract": [
                "Working set selection is an important step in decomposition methods for training support vector machines (SVMs). This paper develops a new technique for working set selection in SMO-type decomposition methods. It uses second order information to achieve fast convergence. Theoretical properties such as linear convergence are established. Experiments demonstrate that the proposed method is faster than existing selection methods using first order information."
            ],
            "keywords": [
                "support vector machines",
                "decomposition methods",
                "sequential minimal optimization",
                "working set selection"
            ],
            "author": [
                "Rong-En Fan",
                "Pai-Hsuen Chen",
                "Chih-Jen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/fan05a/fan05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Locally Weighted Bag of Words Framework for Document Representation",
            "abstract": [
                "The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efficient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks."
            ],
            "keywords": [],
            "author": [
                "Guy Lebanon",
                "Yi Mao",
                "Joshua Dillon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/lebanon07a/lebanon07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Learning and Planning with Compressed Predictive States",
            "abstract": [
                "Predictive state representations (PSRs) offer an expressive framework for modelling partially observable systems. By compactly representing systems as functions of observable quantities, the PSR learning approach avoids using local-minima prone expectationmaximization and instead employs a globally optimal moment-based algorithm. Moreover, since PSRs do not require a predetermined latent state structure as an input, they offer an attractive framework for model-based reinforcement learning when agents must plan without a priori access to a system model. Unfortunately, the expressiveness of PSRs comes with significant computational cost, and this cost is a major factor inhibiting the use of PSRs in applications. In order to alleviate this shortcoming, we introduce the notion of compressed PSRs (CPSRs). The CPSR learning approach combines recent advancements in dimensionality reduction, incremental matrix decomposition, and compressed sensing. We show how this approach provides a principled avenue for learning accurate approximations of PSRs, drastically reducing the computational costs associated with learning while also providing effective regularization. Going further, we propose a planning framework which exploits these learned models. And we show that this approach facilitates model-learning and planning in large complex partially observable domains, a task that is infeasible without the principled use of compression. 1"
            ],
            "keywords": [
                "predictive state representation",
                "reinforcement learning",
                "dimensionality reduction",
                "random projections"
            ],
            "author": [
                "William Hamilton",
                "Mahdi Milani Fard",
                "Joelle Pineau"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/hamilton14a/hamilton14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Introducing CURRENNT: The Munich Open-Source CUDA RecurREnt Neural Network Toolkit",
            "abstract": [
                "In this article, we introduce CURRENNT, an open-source parallel implementation of deep recurrent neural networks (RNNs) supporting graphics processing units (GPUs) through NVIDIA's Computed Unified Device Architecture (CUDA). CURRENNT supports uni-and bidirectional RNNs with Long Short-Term Memory (LSTM) memory cells which overcome the vanishing gradient problem. To our knowledge, CURRENNT is the first publicly available parallel implementation of deep LSTM-RNNs. Benchmarks are given on a noisy speech recognition task from the 2013 2nd CHiME Speech Separation and Recognition Challenge, where LSTM-RNNs have been shown to deliver best performance. In the result, double digit speedups in bidirectional LSTM training are achieved with respect to a reference single-threaded CPU implementation."
            ],
            "keywords": [
                "parallel computing",
                "deep neural networks",
                "recurrent neural networks",
                "Long Short-Term Memory"
            ],
            "author": [
                "Felix Weninger",
                "Johannes Bergmann",
                "* B Schuller"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/weninger15a/weninger15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Message-Passing Algorithms for Quadratic Minimization",
            "abstract": [
                "Gaussian belief propagation (GaBP) is an iterative algorithm for computing the mean (and variances) of a multivariate Gaussian distribution, or equivalently, the minimum of a multivariate positive definite quadratic function. Sufficient conditions, such as walk-summability, that guarantee the convergence and correctness of GaBP are known, but GaBP may fail to converge to the correct solution given an arbitrary positive definite covariance matrix. As was observed by Malioutov et al. (2006), the GaBP algorithm fails to converge if the computation trees produced by the algorithm are not positive definite. In this work, we will show that the failure modes of the GaBP algorithm can be understood via graph covers, and we prove that a parameterized generalization of the min-sum algorithm can be used to ensure that the computation trees remain positive definite whenever the input matrix is positive definite. We demonstrate that the resulting algorithm is closely related to other iterative schemes for quadratic minimization such as the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically, that there always exists a choice of parameters such that the above generalization of the GaBP algorithm converges."
            ],
            "keywords": [
                "belief propagation",
                "Gaussian graphical models",
                "graph covers"
            ],
            "author": [
                "Nicholas Ruozzi",
                "Sekhar Tatikonda"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/ruozzi13a/ruozzi13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Accelerating Cross-Validation in Multinomial Logistic Regression with 1 -Regularization",
            "abstract": [
                "We develop an approximate formula for evaluating a cross-validation estimator of predictive likelihood for multinomial logistic regression regularized by an 1-norm. This allows us to avoid repeated optimizations required for literally conducting cross-validation; hence, the computational time can be significantly reduced. The formula is derived through a perturbative approach employing the largeness of the data size and the model dimensionality. An extension to the elastic net regularization is also addressed. The usefulness of the approximate formula is demonstrated on simulated data and the ISOLET dataset from the UCI machine learning repository. MATLAB and python codes implementing the approximate formula are distributed in (Obuchi, 2017; Takahashi and Obuchi, 2017)."
            ],
            "keywords": [
                "classification",
                "multinomial logistic regression",
                "cross-validation",
                "linear perturbation",
                "self-averaging approximation"
            ],
            "author": [
                "Tomoyuki Obuchi",
                "Yoshiyuki Kabashima"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-684/17-684.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Rates for General Unbounded Loss Functions: From ERM to Generalized Bayes",
            "abstract": [
                "We present new excess risk bounds for general unbounded loss functions including log loss and squared loss, where the distribution of the losses may be heavy-tailed. The bounds hold for general estimators, but they are optimized when applied to η-generalized Bayesian, MDL, and empirical risk minimization estimators. In the case of log loss, the bounds imply convergence rates for generalized Bayesian inference under misspecification in terms of a generalization of the Hellinger metric as long as the learning rate η is set correctly. For general loss functions, our bounds rely on two separate conditions: the v-GRIP (generalized reversed information projection) conditions, which control the lower tail of the excess loss; and the newly introduced witness condition, which controls the upper tail. The parameter v in the v-GRIP conditions determines the achievable rate and is akin to the exponent in the Tsybakov margin condition and the Bernstein condition for bounded losses, which the v-GRIP conditions generalize; favorable v in combination with small model complexity leads toÕ(1 n) rates. The witness condition allows us to connect the excess risk to an \"annealed\" version thereof, by which we generalize several previous results connecting Hellinger and Rényi divergence to KL divergence."
            ],
            "keywords": [
                "Statistical Learning Theory",
                "Fast Rates",
                "PAC-Bayes",
                "Misspecification",
                "Generalized Bayes"
            ],
            "author": [
                "Peter D Grünwald",
                "Nishant A Mehta"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-488/18-488.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Estimation of Kernel Mean Embeddings",
            "abstract": [
                "In this paper, we study the minimax estimation of the Bochner integral µ k (P) := X k(•, x) dP (x), also called as the kernel mean embedding, based on random samples drawn i.i.d. from P , where k : X × X → R is a positive definite kernel. Various estimators (including the empirical estimator),θ n of µ k (P) are studied in the literature wherein all of them satisfy θ n − µ k (P) H k = O P (n −1/2) with H k being the reproducing kernel Hilbert space induced by k. The main contribution of the paper is in showing that the above mentioned rate of n −1/2 is minimax in • H k and • L 2 (R d)-norms over the class of discrete measures and the class of measures that has an infinitely differentiable density, with k being a continuous translation-invariant kernel on R d. The interesting aspect of this result is that the minimax rate is independent of the smoothness of the kernel and the density of P (if it exists)."
            ],
            "keywords": [
                "Bochner integral",
                "Bochner's theorem",
                "kernel mean embeddings",
                "minimax lower bounds",
                "reproducing kernel Hilbert space",
                "translation invariant kernel"
            ],
            "author": [
                "Ilya Tolstikhin",
                "Krikamol Muandet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-032/17-032.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bipartite Ranking: a Risk-Theoretic Perspective",
            "abstract": [
                "We present a systematic study of the bipartite ranking problem, with the aim of explicating its connections to the class-probability estimation problem. Our study focuses on the properties of the statistical risk for bipartite ranking with general losses, which is closely related to a generalised notion of the area under the ROC curve: we establish alternate representations of this risk, relate the Bayes-optimal risk to a class of probability divergences, and characterise the set of Bayes-optimal scorers for the risk. We further study properties of a generalised class of bipartite risks, based on the-norm push of Rudin (2009). Our analysis is based on the rich framework of proper losses, which are the central tool in the study of class-probability estimation. We show how this analytic tool makes transparent the generalisations of several existing results, such as the equivalence of the minimisers for four seemingly disparate risks from bipartite ranking and class-probability estimation. A novel practical implication of our analysis is the design of new families of losses for scenarios where accuracy at the head of ranked list is paramount, with comparable empirical performance to the-norm push."
            ],
            "keywords": [
                "bipartite ranking",
                "class-probability estimation",
                "proper losses",
                "Bayes-optimality",
                "ranking the best"
            ],
            "author": [
                "Aditya Krishna Menon",
                "Robert C Williamson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-265/14-265.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Submodular Observation Selection",
            "abstract": [
                "In many applications, one has to actively select among a set of expensive observations before making an informed decision. For example, in environmental monitoring, we want to select locations to measure in order to most effectively predict spatial phenomena. Often, we want to select observations which are robust against a number of possible objective functions. Examples include minimizing the maximum posterior variance in Gaussian Process regression, robust experimental design, and sensor placement for outbreak detection. In this paper, we present the Submodular Saturation algorithm, a simple and efficient algorithm with strong theoretical approximation guarantees for cases where the possible objective functions exhibit submodularity, an intuitive diminishing returns property. Moreover, we prove that better approximation algorithms do not exist unless NP-complete problems admit efficient algorithms. We show how our algorithm can be extended to handle complex cost functions (incorporating non-unit observation cost or communication and path costs). We also show how the algorithm can be used to near-optimally trade off expected-case (e.g., the Mean Square Prediction Error in Gaussian Process regression) and worst-case (e.g., maximum predictive variance) performance. We show that many important machine learning problems fit our robust submodular observation selection formalism, and provide extensive empirical evaluation on several real-world problems. For Gaussian Process regression, our algorithm compares favorably with state-of-the-art heuristics described in the geostatistics literature, while being simpler, faster and providing theoretical guarantees. For robust experimental design, our algorithm performs favorably compared to SDP-based algorithms."
            ],
            "keywords": [
                "observation selection",
                "experimental design",
                "active learning",
                "submodular functions",
                "Gaussian processes"
            ],
            "author": [
                "Andreas Krause",
                "H Brendan Mcmahan",
                "Carlos Guestrin",
                "Anupam Gupta",
                "MCMAHAN, GUESTRIN AND GUPTA Krause"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/krause08b/krause08b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data",
            "abstract": [
                "Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classifier achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classifier achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassifications."
            ],
            "keywords": [
                "set covering machines",
                "sample-compression",
                "loss bounds"
            ],
            "author": [
                "Zakria Hussain",
                "François Laviolette",
                "Mario Marchand",
                "John Shawe-Taylor",
                "Spencer Charles Brubaker",
                "Matthew D Mullin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/hussain07a/hussain07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Concordance-assisted Learning for Optimal Treatment Decision",
            "abstract": [
                "To find optimal decision rule, Fan et al. (2016) proposed an innovative concordance-assisted learning algorithm which is based on maximum rank correlation estimator. It makes better use of the available information through pairwise comparison. However the objective function is discontinuous and computationally hard to optimize. In this paper, we consider a convex surrogate loss function to solve this problem. In addition, our algorithm ensures sparsity of decision rule and renders easy interpretation. We derive the L 2 error bound of the estimated coefficients under ultra-high dimension. Simulation results of various settings and application to STAR*D both illustrate that the proposed method can still estimate optimal treatment regime successfully when the number of covariates is large."
            ],
            "keywords": [
                "concordance-assisted learning",
                "optimal treatment regime",
                "L 1 norm",
                "support vector machine",
                "variable selection"
            ],
            "author": [
                "Shuhan Liang",
                "Wenbin Lu",
                "Rui Song",
                "Lan Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-159/17-159.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improving Variational Methods via Pairwise Linear Response Identities",
            "abstract": [
                "Inference methods are often formulated as variational approximations: these approximations allow easy evaluation of statistics by marginalization or linear response, but these estimates can be inconsistent. We show that by introducing constraints on covariance, one can ensure consistency of linear response with the variational parameters, and in so doing inference of marginal probability distributions is improved. For the Bethe approximation and its generalizations, improvements are achieved with simple choices of the constraints. The approximations are presented as variational frameworks; iterative procedures related to message passing are provided for finding the minima."
            ],
            "keywords": [
                "variational inference",
                "graphical models",
                "message passing algorithms",
                "statistical physics",
                "linear response"
            ],
            "author": [
                "Jack Raymond",
                "Federico Ricci-Tersenghi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-070/16-070.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Topological Data Analysis using Persistence Landscapes",
            "abstract": [
                "We define a new topological summary for data that we call the persistence landscape. Since this summary lies in a vector space, it is easy to combine with tools from statistics and machine learning, in contrast to the standard topological summaries. Viewed as a random variable with values in a Banach space, this summary obeys a strong law of large numbers and a central limit theorem. We show how a number of standard statistical tests can be used for statistical inference using this summary. We also prove that this summary is stable and that it can be used to provide lower bounds for the bottleneck and Wasserstein distances."
            ],
            "keywords": [
                "topological data analysis",
                "statistical topology",
                "persistent homology",
                "topological summary",
                "persistence landscape"
            ],
            "author": [
                "Peter Bubenik"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/bubenik15a/bubenik15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
            "abstract": [
                "Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more efficiently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient \"turnkey\" samplers."
            ],
            "keywords": [
                "Markov chain Monte Carlo",
                "Hamiltonian Monte Carlo",
                "Bayesian inference",
                "adaptive Monte Carlo",
                "dual averaging"
            ],
            "author": [
                "Matthew D Hoffman",
                "Andrew Gelman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/hoffman14a/hoffman14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptivity of Averaged Stochastic Gradient Descent to Local Strong Convexity for Logistic Regression",
            "abstract": [
                "In this paper, we consider supervised learning problems such as logistic regression and study the stochastic gradient method with averaging, in the usual stochastic approximation setting where observations are used only once. We show that after N iterations, with a constant step-size proportional to 1/R 2 √ N where N is the number of observations and R is the maximum norm of the observations, the convergence rate is always of order O(1/ √ N), and improves to O(R 2 /µN) where µ is the lowest eigenvalue of the Hessian at the global optimum (when this eigenvalue is greater than R 2 / √ N). Since µ does not need to be known in advance, this shows that averaged stochastic gradient is adaptive to unknown local strong convexity of the objective function. Our proof relies on the generalized selfconcordance properties of the logistic loss and thus extends to all generalized linear models with uniformly bounded features."
            ],
            "keywords": [
                "stochastic approximation",
                "logistic regression",
                "self-concordance"
            ],
            "author": [
                "Francis Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/bach14a/bach14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model Selection in Kernel Based Regression using the Influence Function",
            "abstract": [
                "Recent results about the robustness of kernel methods involve the analysis of influence functions. By definition the influence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the influence function is used in a similar way to analyze the statistical efficiency of a method. Links between both worlds are explored. The influence function is related to the first term of a Taylor expansion. Higher order influence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate influence functions at a specific sample distribution to obtain an approximation of the leave-one-out error. A specific implementation is proposed using a L 1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data."
            ],
            "keywords": [
                "kernel based regression",
                "robustness",
                "stability",
                "influence function",
                "model selection"
            ],
            "author": [
                "Michiel Debruyne",
                "Mia Hubert",
                "Johan A K Suykens"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/debruyne08a/debruyne08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Multi-modal Similarity",
            "abstract": [
                "In many applications involving multi-media data, the definition of similarity between items is integral to several key tasks, including nearest-neighbor retrieval, classification, and recommendation. Data in such regimes typically exhibits multiple modalities, such as acoustic and visual content of video. Integrating such heterogeneous data to form a holistic similarity space is therefore a key challenge to be overcome in many real-world applications. We present a novel multiple kernel learning technique for integrating heterogeneous data into a single, unified similarity space. Our algorithm learns an optimal ensemble of kernel transformations which conform to measurements of human perceptual similarity, as expressed by relative comparisons. To cope with the ubiquitous problems of subjectivity and inconsistency in multimedia similarity, we develop graph-based techniques to filter similarity measurements, resulting in a simplified and robust training procedure."
            ],
            "keywords": [
                "multiple kernel learning",
                "metric learning",
                "similarity"
            ],
            "author": [
                "Brian Mcfee",
                "Gert Lanckriet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/mcfee11a/mcfee11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiclass Boosting for Weak Classifiers",
            "abstract": [
                "AdaBoost.M2 is a boosting algorithm designed for multiclass problems with weak base classifiers. The algorithm is designed to minimize a very loose bound on the training error. We propose two alternative boosting algorithms which also minimize bounds on performance measures. These performance measures are not as strongly connected to the expected error as the training error, but the derived bounds are tighter than the bound on the training error of AdaBoost.M2. In experiments the methods have roughly the same performance in minimizing the training and test error rates. The new algorithms have the advantage that the base classifier should minimize the confidence-rated error, whereas for AdaBoost.M2 the base classifier should minimize the pseudo-loss. This makes them more easily applicable to already existing base classifiers. The new algorithms also tend to converge faster than AdaBoost.M2."
            ],
            "keywords": [
                "boosting",
                "multiclass",
                "ensemble",
                "classification",
                "decision stumps"
            ],
            "author": [
                "Günther Eibl",
                "Karl-Peter Pfeiffer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/eibl05a/eibl05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "STORE: Sparse Tensor Response Regression and Neuroimaging Analysis",
            "abstract": [
                "Motivated by applications in neuroimaging analysis, we propose a new regression model, Sparse TensOr REsponse regression (STORE), with a tensor response and a vector predictor. STORE embeds two key sparse structures: element-wise sparsity and low-rankness. It can handle both a non-symmetric and a symmetric tensor response, and thus is applicable to both structural and functional neuroimaging data. We formulate the parameter estimation as a non-convex optimization problem, and develop an efficient alternating updating algorithm. We establish a non-asymptotic estimation error bound for the actual estimator obtained from the proposed algorithm. This error bound reveals an interesting interaction between the computational efficiency and the statistical rate of convergence. When the distribution of the error tensor is Gaussian, we further obtain a fast estimation error rate which allows the tensor dimension to grow exponentially with the sample size. We illustrate the efficacy of our model through intensive simulations and an analysis of the Autism spectrum disorder neuroimaging data."
            ],
            "keywords": [
                "Functional connectivity analysis",
                "High-dimensional statistical learning",
                "Magnetic resonance imaging",
                "Non-asymptotic error bound",
                "Tensor decomposition"
            ],
            "author": [
                "Will Wei Sun",
                "Lexin Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-203/17-203.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Accurate Error Bounds for the Eigenvalues of the Kernel Matrix",
            "abstract": [
                "The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to infinity. We derive probabilistic finite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reflecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a significant improvement over existing non-scaling bounds."
            ],
            "keywords": [
                "kernel matrix",
                "eigenvalues",
                "relative perturbation bounds"
            ],
            "author": [
                "Mikio L Braun"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/braun06a/braun06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Image Denoising with Kernels Based on Natural Image Relations",
            "abstract": [
                "A successful class of image denoising methods is based on Bayesian approaches working in wavelet representations. The performance of these methods improves when relations among the local frequency coefficients are explicitly included. However, in these techniques, analytical estimates can be obtained only for particular combinations of analytical models of signal and noise, thus precluding its straightforward extension to deal with other arbitrary noise sources."
            ],
            "keywords": [],
            "author": [
                "Valero Laparra",
                "Jesús Malo"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/laparra10a/laparra10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Nearest-Neighbor Learning in Metric Spaces",
            "abstract": [
                "We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearestneighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. MARMANN is based on a generalized sample compression scheme, and a new label-efficient active model-selection procedure."
            ],
            "keywords": [
                "Nearest-neighbors",
                "active learning",
                "metric spaces",
                "non-parametric learning"
            ],
            "author": [
                "Aryeh Kontorovich",
                "Sivan Sabato",
                "Ruth Urner"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-499/16-499.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Mutual Information Based Matching for Causal Inference with Observational Data",
            "abstract": [
                "This paper presents an information theory-driven matching methodology for making causal inference from observational data. The paper adopts a \"potential outcomes framework\" view on evaluating the strength of cause-effect relationships: the population-wide average effects of binary treatments are estimated by comparing two groups of units-the treated and untreated (control). To reduce the bias in such treatment effect estimation, one has to compose a control group in such a way that across the compared groups of units, treatment is independent of the units' covariates. This requirement gives rise to a subset selection / matching problem. This paper presents the models and algorithms that solve the matching problem by minimizing the mutual information (MI) between the covariates and the treatment variable. Such a formulation becomes tractable thanks to the derived optimality conditions that tackle the non-linearity of the sample-based MI function. Computational experiments with mixed integer-programming formulations and four matching algorithms demonstrate the utility of MI based matching for causal inference studies. The algorithmic developments culminate in a matching heuristic that allows for balancing the compared groups in polynomial (close to linear) time, thus allowing for treatment effect estimation with large data sets."
            ],
            "keywords": [
                "Observational Causal Inference",
                "Mutual Information",
                "Matching",
                "Subset Selection",
                "Optimization"
            ],
            "author": [
                "Lei Sun",
                "Alexander G Nikolaev"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-420/15-420.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Instance Learning with Any Hypothesis Class",
            "abstract": [
                "In the supervised learning setting termed Multiple-Instance Learning (MIL), the examples are bags of instances, and the bag label is a function of the labels of its instances. Typically, this function is the Boolean OR. The learner observes a sample of bags and the bag labels, but not the instance labels that determine the bag labels. The learner is then required to emit a classification rule for bags based on the sample. MIL has numerous applications, and many heuristic algorithms have been used successfully on this problem, each adapted to specific settings or applications. In this work we provide a unified theoretical analysis for MIL, which holds for any underlying hypothesis class, regardless of a specific application or problem domain. We show that the sample complexity of MIL is only poly-logarithmically dependent on the size of the bag, for any underlying hypothesis class. In addition, we introduce a new PAC-learning algorithm for MIL, which uses a regular supervised learning algorithm as an oracle. We prove that efficient PAC-learning for MIL can be generated from any efficient non-MIL supervised learning algorithm that handles one-sided error. The computational complexity of the resulting algorithm is only polynomially dependent on the bag size."
            ],
            "keywords": [
                "multiple-instance learning",
                "learning theory",
                "sample complexity",
                "PAC learning",
                "supervised classification"
            ],
            "author": [
                "Sivan Sabato",
                "Naftali Tishby"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/sabato12a/sabato12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structured Prediction via Output Space Search",
            "abstract": [
                "We consider a framework for structured prediction based on search in the space of complete structured outputs. Given a structured input, an output is produced by running a timebounded search procedure guided by a learned cost function, and then returning the least cost output uncovered during the search. This framework can be instantiated for a wide range of search spaces and search procedures, and easily incorporates arbitrary structuredprediction loss functions. In this paper, we make two main technical contributions. First, we describe a novel approach to automatically defining an effective search space over structured outputs, which is able to leverage the availability of powerful classification learning algorithms. In particular, we define the limited-discrepancy search space and relate the quality of that space to the quality of learned classifiers. We also define a sparse version of the search space to improve the efficiency of our overall approach. Second, we give a generic cost function learning approach that is applicable to a wide range of search procedures. The key idea is to learn a cost function that attempts to mimic the behavior of conducting searches guided by the true loss function. Our experiments on six benchmark domains show that a small amount of search in limited discrepancy search space is often sufficient for significantly improving on state-of-the-art structured-prediction performance. We also demonstrate significant speed improvements for our approach using sparse search spaces with little or no loss in accuracy."
            ],
            "keywords": [
                "structured prediction",
                "state space search",
                "imitation learning",
                "cost function"
            ],
            "author": [
                "Janardhan Rao Doppa",
                "Alan Fern",
                "Prasad Tadepalli",
                "S V N Vishwanathan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/doppa14a/doppa14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Apache Mahout: Machine Learning on Distributed Dataflow Systems",
            "abstract": [
                "Apache Mahout is a library for scalable machine learning (ML) on distributed dataflow systems, offering various implementations of classification, clustering, dimensionality reduction and recommendation algorithms. Mahout was a pioneer in large-scale machine learning in 2008, when it started and targeted MapReduce, which was the predominant abstraction for scalable computing in industry at that time. Mahout has been widely used by leading web companies and is part of several commercial cloud offerings. In recent years, Mahout migrated to a general framework enabling a mix of dataflow programming and linear algebraic computations on backends such as Apache Spark and Apache Flink. This design allows users to execute data preprocessing and model training in a single, unified dataflow system, instead of requiring a complex integration of several specialized systems. Mahout is maintained as a community-driven open source project at the Apache Software Foundation, and is available under https://mahout.apache.org."
            ],
            "keywords": [],
            "author": [
                "Robin Anil",
                "Ted Dunning",
                "Ellen Friedman",
                "Trevor Grant",
                "Shannon Quinn",
                "Paritosh Ranjan",
                "Sebastian Schelter",
                "Ozgür Yılmazel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-800/18-800.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Graphical Models for Multivariate Functional Data",
            "abstract": [
                "Graphical models express conditional independence relationships among variables. Although methods for vector-valued data are well established, functional data graphical models remain underdeveloped. By functional data, we refer to data that are realizations of random functions varying over a continuum (e.g., images, signals). We introduce a notion of conditional independence between random functions, and construct a framework for Bayesian inference of undirected, decomposable graphs in the multivariate functional data context. This framework is based on extending Markov distributions and hyper Markov laws from random variables to random processes, providing a principled alternative to naive application of multivariate methods to discretized functional data. Markov properties facilitate the composition of likelihoods and priors according to the decomposition of a graph. Our focus is on Gaussian process graphical models using orthogonal basis expansions. We propose a hyper-inverse-Wishart-process prior for the covariance kernels of the infinite coefficient sequences of the basis expansion, and establish its existence and uniqueness. We also prove the strong hyper Markov property and the conjugacy of this prior under a finite rank condition of the prior kernel parameter. Stochastic search Markov chain Monte Carlo algorithms are developed for posterior inference, assessed through simulations, and applied to a study of brain activity and alcoholism."
            ],
            "keywords": [
                "graphical model",
                "functional data analysis",
                "gaussian process",
                "model uncertainty",
                "stochastic search"
            ],
            "author": [
                "Hongxiao Zhu",
                "Nate Strawn",
                "David B Dunson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-164/16-164.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos",
            "abstract": [
                "We propose the novel approach of dynamic affine-invariant shape-appearance model (Aff-SAM) and employ it for handshape classification and sign recognition in sign language (SL) videos. Aff-SAM offers a compact and descriptive representation of hand configurations as well as regularized model-fitting, assisting hand tracking and extracting handshape features. We construct SA images representing the hand's shape and appearance without landmark points. We model the variation of the images by linear combinations of eigenimages followed by affine transformations, accounting for 3D hand pose changes and improving model's compactness. We also incorporate static and dynamic handshape priors, offering robustness in occlusions, which occur often in signing. The approach includes an affine signer adaptation component at the visual level, without requiring training from scratch a new singer-specific model. We rather employ a short development data set to adapt the models for a new signer. Experiments on the Boston-University-400 continuous SL corpus demonstrate improvements on handshape classification when compared to other feature extraction approaches. Supplementary evaluations of sign recognition experiments, are conducted on a multi-signer, 100-sign data set, from the Greek sign language lemmas corpus. These explore the fusion with movement cues as well as signer adaptation of Aff-SAM to multiple signers providing promising results."
            ],
            "keywords": [
                "affine-invariant shape-appearance model",
                "landmarks-free shape representation",
                "static and dynamic priors",
                "feature extraction",
                "handshape classification"
            ],
            "author": [
                "Anastasios Roussos",
                "Queen Mary",
                "Stavros Theodorakis",
                "Isabelle Guyon",
                "Vassilis Athitsos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/roussos13a/roussos13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-Dimensional Learning of Linear Causal Networks via Inverse Covariance Estimation",
            "abstract": [
                "We establish a new framework for statistical estimation of directed acyclic graphs (DAGs) when data are generated from a linear, possibly non-Gaussian structural equation model. Our framework consists of two parts: (1) inferring the moralized graph from the support of the inverse covariance matrix; and (2) selecting the best-scoring graph amongst DAGs that are consistent with the moralized graph. We show that when the error variances are known or estimated to close enough precision, the true DAG is the unique minimizer of the score computed using the reweighted squared 2-loss. Our population-level results have implications for the identifiability of linear SEMs when the error covariances are specified up to a constant multiple. On the statistical side, we establish rigorous conditions for highdimensional consistency of our two-part algorithm, defined in terms of a \"gap\" between the true DAG and the next best candidate. Finally, we demonstrate that dynamic programming may be used to select the optimal DAG in linear time when the treewidth of the moralized graph is bounded."
            ],
            "keywords": [
                "causal inference",
                "dynamic programming",
                "identifiability",
                "inverse covariance matrix estimation",
                "linear structural equation models"
            ],
            "author": [
                "Po-Ling Loh",
                "Peter Bühlmann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/loh14a/loh14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DSA: Decentralized Double Stochastic Averaging Gradient Algorithm",
            "abstract": [
                "This paper considers optimization problems where nodes of a network have access to summands of a global objective. Each of these local objectives is further assumed to be an average of a finite set of functions. The motivation for this setup is to solve large scale machine learning problems where elements of the training set are distributed to multiple computational elements. The decentralized double stochastic averaging gradient (DSA) algorithm is proposed as a solution alternative that relies on: (i) The use of local stochastic averaging gradients. (ii) Determination of descent steps as differences of consecutive stochastic averaging gradients. Strong convexity of local functions and Lipschitz continuity of local gradients is shown to guarantee linear convergence of the sequence generated by DSA in expectation. Local iterates are further shown to approach the optimal argument for almost all realizations. The expected linear convergence of DSA is in contrast to the sublinear rate characteristic of existing methods for decentralized stochastic optimization. Numerical experiments on a logistic regression problem illustrate reductions in convergence time and number of feature vectors processed until convergence relative to these other alternatives."
            ],
            "keywords": [
                "decentralized optimization",
                "stochastic optimization",
                "stochastic averaging gradient",
                "linear convergence",
                "large-scale optimization",
                "logistic regression"
            ],
            "author": [
                "Aryan Mokhtari",
                "Alejandro Ribeiro"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-292/15-292.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Value Regularization and Fenchel Duality",
            "abstract": [
                "Regularization is an approach to function learning that balances fit and smoothness. In practice, we search for a function f with a finite representation f = ∑ i c i φ i (•). In most treatments, the c i are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning."
            ],
            "keywords": [
                "kernel machines",
                "duality",
                "optimization",
                "convex analysis",
                "kernel learning"
            ],
            "author": [
                "Ryan M Rifkin",
                "Ross A Lippert"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/rifkin07a/rifkin07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Data Collection For Informative Rankings Expose Well-Connected Graphs",
            "abstract": [
                "Given a graph where vertices represent alternatives and arcs represent pairwise comparison data, the statistical ranking problem is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with the pairwise comparisons. Our goal in this paper is to develop a method for collecting data for which the least squares estimator for the ranking problem has maximal Fisher information. Our approach, based on experimental design, is to view data collection as a bi-level optimization problem where the inner problem is the ranking problem and the outer problem is to identify data which maximizes the informativeness of the ranking. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding multigraphs with large algebraic connectivity. This reduction of the data collection problem to graph-theoretic questions is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating data set and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking. As another application, we study the 2011-12 NCAA football schedule and propose schedules with the same number of games which are significantly more informative. Using spectral clustering methods to identify highly-connected communities within the division, we argue that the NCAA could improve its notoriously poor rankings by simply scheduling more out-of-conference games."
            ],
            "keywords": [
                "ranking",
                "active learning",
                "scheduling",
                "optimal experimental design",
                "graph synthesis",
                "algebraic connectivity"
            ],
            "author": [
                "Braxton Osting",
                "Christoph Brune",
                "Stanley J Osher"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/osting14a/osting14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Reward Maximization Under Uncertainty: Leveraging Side-Observations on Networks",
            "abstract": [
                "We study the stochastic multi-armed bandit (MAB) problem in the presence of sideobservations across actions that occur as a result of an underlying network structure. In our model, a bipartite graph captures the relationship between actions and a common set of unknowns such that choosing an action reveals observations for the unknowns that it is connected to. This models a common scenario in online social networks where users respond to their friends' activity, thus providing side information about each other's preferences. Our contributions are as follows: 1) We derive an asymptotic lower bound (with respect to time) as a function of the bi-partite network structure on the regret of any uniformly good policy that achieves the maximum long-term average reward. 2) We propose two policiesa randomized policy; and a policy based on the well-known upper confidence bound (UCB) policies-both of which explore each action at a rate that is a function of its network position. We show, under mild assumptions, that these policies achieve the asymptotic lower bound on the regret up to a multiplicative factor, independent of the network structure. Finally, we use numerical examples on a real-world social network and a routing example network to demonstrate the benefits obtained by our policies over other existing policies."
            ],
            "keywords": [
                "Multi-armed Bandits",
                "Side Observations",
                "Bipartite Graph",
                "Regret Bounds"
            ],
            "author": [
                "Swapna Buccapatnam",
                "Fang Liu",
                "Ness B Shroff"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-340/16-340.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "To Tune or Not to Tune the Number of Trees in Random Forest",
            "abstract": [
                "The number of trees T in the random forest (RF) algorithm for supervised learning has to be set by the user. It is unclear whether T should simply be set to the largest computationally manageable value or whether a smaller T may be sufficient or in some cases even better. While the principle underlying bagging is that more trees are better, in practice the classification error rate sometimes reaches a minimum before increasing again for increasing number of trees. The goal of this paper is four-fold: (i) providing theoretical results showing that the expected error rate may be a non-monotonous function of the number of trees and explaining under which circumstances this happens; (ii) providing theoretical results showing that such non-monotonous patterns cannot be observed for other performance measures such as the Brier score and the logarithmic loss (for classification) and the mean squared error (for regression); (iii) illustrating the extent of the problem through an application to a large number (n = 306) of datasets from the public database OpenML; (iv) finally arguing in favor of setting T to a computationally feasible large number as long as classical error measures based on average loss are considered."
            ],
            "keywords": [
                "Random forest",
                "number of trees",
                "bagging",
                "out-of-bag",
                "error rate"
            ],
            "author": [
                "Philipp Probst",
                "Anne-Laure Boulesteix"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-269/17-269.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning",
            "abstract": [
                "We extend the well-known BFGS quasi-Newton method and its memory-limited variant LBFGS to the optimization of nonsmooth convex objectives. This is done in a rigorous fashion by generalizing three components of BFGS to subdifferentials: the local quadratic model, the identification of a descent direction, and the Wolfe line search conditions. We prove that under some technical conditions, the resulting subBFGS algorithm is globally convergent in objective function value. We apply its memory-limited variant (subLBFGS) to L 2-regularized risk minimization with the binary hinge loss. To extend our algorithm to the multiclass and multilabel settings, we develop a new, efficient, exact line search algorithm. We prove its worst-case time complexity bounds, and show that our line search can also be used to extend a recently developed bundle method to the multiclass and multilabel settings. We also apply the direction-finding component of our algorithm to L 1-regularized risk minimization with logistic loss. In all these contexts our methods perform comparable to or better than specialized state-of-the-art solvers on a number of publicly available data sets. An open source implementation of our algorithms is freely available."
            ],
            "keywords": [
                "BFGS",
                "variable metric methods",
                "Wolfe conditions",
                "subgradient",
                "risk minimization",
                "hinge loss",
                "multiclass",
                "multilabel",
                "bundle methods",
                "BMRM",
                "OCAS",
                "OWL-QN"
            ],
            "author": [
                "Jin Yu",
                "S V N Vishwanathan",
                "Simon Günter",
                "Nicol N Schraudolph"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/yu10a/yu10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bahadur Representation of the Linear Support Vector Machine",
            "abstract": [
                "The support vector machine has been successful in a variety of applications. Also on the theoretical front, statistical properties of the support vector machine have been studied quite extensively with a particular attention to its Bayes risk consistency under some conditions. In this paper, we study somewhat basic statistical properties of the support vector machine yet to be investigated, namely the asymptotic behavior of the coefficients of the linear support vector machine. A Bahadur type representation of the coefficients is established under appropriate conditions, and their asymptotic normality and statistical variability are derived on the basis of the representation. These asymptotic results do not only help further our understanding of the support vector machine, but also they can be useful for related statistical inferences."
            ],
            "keywords": [
                "asymptotic normality",
                "Bahadur representation",
                "classification",
                "convexity lemma",
                "Radon transform"
            ],
            "author": [
                "Ja-Yong Koo",
                "Yuwon Kim",
                "Changyi Park"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/koo08a/koo08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning to Construct Fast Signal Processing Implementations",
            "abstract": [
                "A single signal processing algorithm can be represented by many mathematically equivalent formulas. However, when these formulas are implemented in code and run on real machines, they have very different runtimes. Unfortunately, it is extremely difficult to model this broad performance range. Further, the space of formulas for real signal transforms is so large that it is impossible to search it exhaustively for fast implementations. We approach this search question as a control learning problem. We present a new method for learning to generate fast formulas, allowing us to intelligently search through only the most promising formulas. Our approach incorporates signal processing knowledge, hardware features, and formula performance data to learn to construct fast formulas. Our method learns from performance data for a few formulas of one size and then can construct formulas that will have the fastest runtimes possible across many sizes."
            ],
            "keywords": [
                "Signal processing optimization",
                "regression trees",
                "decision trees",
                "reinforcement learning"
            ],
            "author": [
                "Bryan Singer",
                "Manuela Veloso",
                "Carla E Brodley",
                "Andrea Danyluk"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/singer02a/singer02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dependent relevance determination for smooth and structured sparse regression",
            "abstract": [
                "In many problem settings, parameter vectors are not merely sparse but dependent in such a way that non-zero coefficients tend to cluster together. We refer to this form of dependency as \"region sparsity.\" Classical sparse regression methods, such as the lasso and automatic relevance determination (ARD), which model parameters as independent a priori, and therefore do not exploit such dependencies. Here we introduce a hierarchical model for smooth, region-sparse weight vectors and tensors in a linear regression setting. Our approach represents a hierarchical extension of the relevance determination framework, where we add a transformed Gaussian process to model the dependencies between the prior variances of regression weights. We combine this with a structured model of the prior variances of Fourier coefficients, which eliminates unnecessary high frequencies. The resulting prior encourages weights to be region-sparse in two different bases simultaneously. We develop Laplace approximation and Monte Carlo Markov Chain (MCMC) sampling to provide efficient inference for the posterior. Furthermore, a two-stage convex relaxation of the Laplace approximation approach is also provided to relax the inevitable non-convexity during the optimization. We finally show substantial improvements over comparable methods for both simulated and real datasets from brain imaging."
            ],
            "keywords": [
                "Bayesian nonparametric",
                "Sparsity",
                "Structure learning",
                "Gaussian Process",
                "fMRI"
            ],
            "author": [
                "Anqi Wu",
                "Oluwasanmi Koyejo",
                "Jonathan Pillow"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-757/17-757.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "(1 + ε)-class Classification: an Anomaly Detection Method for Highly Imbalanced or Incomplete Data Sets",
            "abstract": [
                "Anomaly detection is not an easy problem since distribution of anomalous samples is unknown a priori. We explore a novel method that gives a trade-off possibility between oneclass and two-class approaches, and leads to a better performance on anomaly detection problems with small or non-representative anomalous samples. The method is evaluated using several data sets and compared to a set of conventional one-class and two-class approaches."
            ],
            "keywords": [
                "Anomaly Detection",
                "Imbalanced Data Sets",
                "Neural Networks",
                "One-class Classification",
                "Regularization"
            ],
            "author": [
                "Maxim Borisyak",
                "Artem Ryzhikov",
                "Denis Derkach",
                "Fedor Ratnikov",
                "Olga Mineeva",
                "Andrey Ustyuzhanin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-514/19-514.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Primal-Dual Convergence Analysis of Boosting",
            "abstract": [
                "Boosting combines weak learners into a predictor with low empirical risk. Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated. This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing: • Weak learnability aids the whole loss family: for any ε > 0, O(ln(1/ε)) iterations suffice to produce a predictor with empirical risk ε-close to the infimum; • The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O(ln(1/ε)); • Arbitrary instances may be decomposed into the above two, granting rate O(1/ε), with a matching lower bound provided for the logistic loss."
            ],
            "keywords": [
                "boosting",
                "convex analysis",
                "weak learnability",
                "coordinate descent",
                "maximum entropy"
            ],
            "author": [
                "Matus Telgarsky"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/telgarsky12a/telgarsky12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Latent Dirichlet Allocation",
            "abstract": [
                "We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model."
            ],
            "keywords": [],
            "author": [
                "David M Blei",
                "Andrew Y Ng",
                "Michael I Jordan"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Low Complexity Algorithm with O( √ T ) Regret and O(1) Constraint Violations for Online Convex Optimization with Long Term Constraints",
            "abstract": [
                "This paper considers online convex optimization over a complicated constraint set, which typically consists of multiple functional constraints and a set constraint. The conventional online projection algorithm (Zinkevich, 2003) can be difficult to implement due to the potentially high computation complexity of the projection operation. In this paper, we relax the functional constraints by allowing them to be violated at each round but still requiring them to be satisfied in the long term. This type of relaxed online convex optimization (with long term constraints) was first considered in Mahdavi et al. (2012). That prior work proposes an algorithm to achieve O(√ T) regret and O(T 3/4) constraint violations for general problems and another algorithm to achieve an O(T 2/3) bound for both regret and constraint violations when the constraint set can be described by a finite number of linear constraints. A recent extension in Jenatton et al. (2016) can achieve O(T max{θ,1−θ}) regret and O(T 1−θ/2) constraint violations where θ ∈ (0, 1). The current paper proposes a new simple algorithm that yields improved performance in comparison to prior works. The new algorithm achieves an O(√ T) regret bound with O(1) constraint violations."
            ],
            "keywords": [
                "online convex optimization",
                "long term constraints",
                "regret bounds",
                "constraint violation bounds",
                "low complexity"
            ],
            "author": [
                "Hao Yu",
                "Michael J Neely"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/16-494/16-494.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Neural Empirical Bayes",
            "abstract": [
                "We unify kernel density estimation and empirical Bayes and address a set of problems in unsupervised machine learning with a geometric interpretation of those methods, rooted in the concentration of measure phenomenon. Kernel density is viewed symbolically as X Y where the random variable X is smoothed to Y = X + N (0, σ 2 I d), and empirical Bayes is the machinery to denoise in a least-squares sense, which we express as X Y. A learning objective is derived by combining these two, symbolically captured by X Y. Crucially, instead of using the original nonparametric estimators, we parametrize the energy function with a neural network denoted by φ; at optimality, ∇φ ≈ −∇ log f where f is the density of Y. The optimization problem is abstracted as interactions of high-dimensional spheres which emerge due to the concentration of isotropic Gaussians. We introduce two algorithmic frameworks based on this machinery: (i) a \"walk-jump\" sampling scheme that combines Langevin MCMC (walks) and empirical Bayes (jumps), and (ii) a probabilistic framework for associative memory, called NEBULA, definedà la Hopfield by the gradient flow of the learned energy to a set of attractors. We finish the paper by reporting the emergence of very rich \"creative memories\" as attractors of NEBULA for highly-overlapping spheres."
            ],
            "keywords": [
                "empirical Bayes",
                "unnormalized densities",
                "concentration of measure",
                "Langevin MCMC",
                "associative memory"
            ],
            "author": [
                "Saeed Saremi",
                "Aapo Hyvärinen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-216/19-216.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Effective Sampling and Learning for Mallows Models with Pairwise-Preference Data",
            "abstract": [
                "Learning preference distributions is a critical problem in many areas (e.g., recommender systems, IR, social choice). However, many existing learning and inference methods impose restrictive assumptions on the form of user preferences that can be admitted as evidence. We relax these restrictions by considering as data arbitrary pairwise comparisons of alternatives, which represent the fundamental building blocks of ordinal rankings. We develop the first algorithms for learning Mallows models (and mixtures thereof) from pairwise comparison data. At the heart of our technique is a new algorithm, the generalized repeated insertion model (GRIM), which allows sampling from arbitrary ranking distributions, and conditional Mallows models in particular. While we show that sampling from a Mallows model with pairwise evidence is computationally difficult in general, we develop approximate samplers that are exact for many important special cases-and have provable bounds with pairwise evidence-and derive algorithms for evaluating log-likelihood, learning Mallows mixtures, and non-parametric estimation. Experiments on real-world data sets demonstrate the effectiveness of our approach."
            ],
            "keywords": [
                "preference learning",
                "ranking",
                "incomplete data",
                "Mallows models",
                "mixture models"
            ],
            "author": [
                "Tyler Lu",
                "Craig Boutilier"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/lu14a/lu14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consensus-Based Distributed Support Vector Machines",
            "abstract": [
                "This paper develops algorithms to train support vector machines when training data are distributed across different nodes, and their communication to a centralized processing unit is prohibited due to, for example, communication complexity, scalability, or privacy reasons. To accomplish this goal, the centralized linear SVM problem is cast as a set of decentralized convex optimization subproblems (one per node) with consensus constraints on the wanted classifier parameters. Using the alternating direction method of multipliers, fully distributed training algorithms are obtained without exchanging training data among nodes. Different from existing incremental approaches, the overhead associated with inter-node communications is fixed and solely dependent on the network topology rather than the size of the training sets available per node. Important generalizations to train nonlinear SVMs in a distributed fashion are also developed along with sequential variants capable of online processing. Simulated tests illustrate the performance of the novel algorithms. 1"
            ],
            "keywords": [
                "support vector machine",
                "distributed optimization",
                "distributed data mining",
                "distributed learning",
                "sensor networks"
            ],
            "author": [
                "Pedro A Forero",
                "Alfonso Cano"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/forero10a/forero10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Input Output Kernel Regression: Supervised and Semi-Supervised Structured Output Prediction with Operator-Valued Kernels",
            "abstract": [
                "In this paper, we introduce a novel approach, called Input Output Kernel Regression (IOKR), for learning mappings between structured inputs and structured outputs. The approach belongs to the family of Output Kernel Regression methods devoted to regression in feature space endowed with some output kernel. In order to take into account structure in input data and benefit from kernels in the input space as well, we use the Reproducing Kernel Hilbert Space theory for vector-valued functions. We first recall the ridge solution for supervised learning and then study the regularized hinge loss-based solution used in Maximum Margin Regression. Both models are also developed in the context of semi-supervised setting. In addition we derive an extension of Generalized Cross Validation for model selection in the case of the least-square model. Finally we show the versatility of the IOKR framework on two different problems: link prediction seen as a structured output problem and multi-task regression seen as a multiple and interdependent output problem. Eventually, we present a set of detailed numerical results that shows the relevance of the method on these two tasks."
            ],
            "keywords": [
                "structured output prediction",
                "output kernel regression",
                "vector-valued RKHS",
                "operator-valued kernel",
                "semi-supervised learning c Céline Brouard",
                "Marie Szafranski and Florence d'Alché-Buc"
            ],
            "author": [
                "Céline Brouard",
                "Marie Szafranski",
                "Florence D'alché-Buc"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-602/15-602.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Likelihood for Gaussian Process Classification and Generalized Linear Mixed Models under Case-Control Sampling",
            "abstract": [
                "Modern data sets in various domains often include units that were sampled non-randomly from the population and have a latent correlation structure. Here we investigate a common form of this setting, where every unit is associated with a latent variable, all latent variables are correlated, and the probability of sampling a unit depends on its response. Such settings often arise in case-control studies, where the sampled units are correlated due to spatial proximity, family relations, or other sources of relatedness. Maximum likelihood estimation in such settings is challenging from both a computational and statistical perspective, necessitating approximations that take the sampling scheme into account. We propose a family of approximate likelihood approaches which combine composite likelihood and expectation propagation. We demonstrate the efficacy of our solutions via extensive simulations. We utilize them to investigate the genetic architecture of several complex disorders collected in case-control genetic association studies, where hundreds of thousands of genetic variants are measured for every individual, and the underlying disease liabilities of individuals are correlated due to genetic similarity. Our work is the first to provide a tractable likelihood-based solution for case-control data with complex dependency structures."
            ],
            "keywords": [
                "Gaussian Processes",
                "Expectation Propagation",
                "Composite Likelihood",
                "Selection Bias",
                "Linear Mixed Models"
            ],
            "author": [
                "Omer Weissbrod",
                "Shachar Kaufman",
                "David Golan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-298/18-298.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation",
            "abstract": [
                "Co-clustering, or simultaneous clustering of rows and columns of a two-dimensional data matrix, is rapidly becoming a powerful data analysis technique. Co-clustering has enjoyed wide success in varied application domains such as text clustering, gene-microarray analysis, natural language processing and image, speech and video analysis. In this paper, we introduce a partitional co-clustering formulation that is driven by the search for a good matrix approximation-every co-clustering is associated with an approximation of the original data matrix and the quality of co-clustering is determined by the approximation error. We allow the approximation error to be measured using a large class of loss functions called Bregman divergences that include squared Euclidean distance and KL-divergence as special cases. In addition, we permit multiple structurally different co-clustering schemes that preserve various linear statistics of the original data matrix. To accomplish the above tasks, we introduce a new minimum Bregman information (MBI) principle that simultaneously generalizes the maximum entropy and standard least squares principles, and leads to a matrix approximation that is optimal among all generalized additive models in a certain natural parameter space. Analysis based on this principle yields an elegant meta algorithm, special cases of which include most previously known alternate minimization based clustering algorithms such as kmeans and co-clustering algorithms such as information theoretic (Dhillon et al., 2003b) and minimum sum-squared residue co-clustering (Cho et al., 2004). To demonstrate the generality and flexibility of our co-clustering framework, we provide examples and empirical evidence on a vari"
            ],
            "keywords": [
                "co-clustering",
                "matrix approximation",
                "Bregman divergences",
                "Bregman information",
                "maximum entropy"
            ],
            "author": [
                "Arindam Banerjee",
                "Inderjit Dhillon",
                "Srujana Merugu",
                "Dharmendra S Modha",
                "S Dhillon",
                "Joydeep Ghosh",
                "DHILLON, GHOSH, MERUGU AND MODHA Banerjee"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/banerjee07a/banerjee07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SPSD Matrix Approximation vis Column Selection: Theories, Algorithms, and Extensions",
            "abstract": [
                "Symmetric positive semidefinite (SPSD) matrix approximation is an important problem with applications in kernel methods. However, existing SPSD matrix approximation methods such as the Nyström method only have weak error bounds. In this paper we conduct in-depth studies of an SPSD matrix approximation model and establish strong relative-error bounds. We call it the prototype model for it has more efficient and effective extensions, and some of its extensions have high scalability. Though the prototype model itself is not suitable for large-scale data, it is still useful to study its properties, on which the analysis of its extensions relies. This paper offers novel theoretical analysis, efficient algorithms, and a highly accurate extension. First, we establish a lower error bound for the prototype model and improve the error bound of an existing column selection algorithm to match the lower bound. In this way, we obtain the first optimal column selection algorithm for the prototype model. We also prove that the prototype model is exact under certain conditions. Second, we develop a simple column selection algorithm with a provable error bound. Third, we propose a so-called spectral shifting model to make the approximation more accurate when the eigenvalues of the matrix decay slowly, and the improvement is theoretically quantified. The spectral shifting method can also be applied to improve other SPSD matrix approximation models."
            ],
            "keywords": [
                "Matrix approximation",
                "matrix factorization",
                "kernel methods",
                "the Nyström method",
                "spectral shifting"
            ],
            "author": [
                "Shusen Wang",
                "Luo Luo",
                "Zhihua Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-199/14-199.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Evolutionary Function Approximation for Reinforcement Learning",
            "abstract": [
                "Temporal difference methods are theoretically grounded and empirically effective methods for addressing reinforcement learning problems. In most real-world reinforcement learning tasks, TD methods require a function approximator to represent the value function. However, using function approximators requires manually making crucial representational decisions. This paper investigates evolutionary function approximation, a novel approach to automatically selecting function approximator representations that enable efficient individual learning. This method evolves individuals that are better able to learn. We present a fully implemented instantiation of evolutionary function approximation which combines NEAT, a neuroevolutionary optimization technique, with Q-learning, a popular TD method. The resulting NEAT+Q algorithm automatically discovers effective representations for neural network function approximators. This paper also presents on-line evolutionary computation, which improves the on-line performance of evolutionary computation by borrowing selection mechanisms used in TD methods to choose individual actions and using them in evolutionary computation to select policies for evaluation. We evaluate these contributions with extended empirical studies in two domains: 1) the mountain car task, a standard reinforcement learning benchmark on which neural network function approximators have previously performed poorly and 2) server job scheduling, a large probabilistic domain drawn from the field of autonomic computing. The results demonstrate that evolutionary function approximation can significantly improve the performance of TD methods and on-line evolutionary computation can significantly improve evolutionary methods. This paper also presents additional tests that offer insight into what factors can make neural network function approximation difficult in practice."
            ],
            "keywords": [
                "reinforcement learning",
                "temporal difference methods",
                "evolutionary computation",
                "neuroevolution",
                "on-line learning"
            ],
            "author": [
                "Shimon Whiteson",
                "Peter Stone"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/whiteson06a/whiteson06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bridging Viterbi and Posterior Decoding: A Generalized Risk Approach to Hidden Path Inference Based on Hidden Markov Models",
            "abstract": [
                "Motivated by the unceasing interest in hidden Markov models (HMMs), this paper reexamines hidden path inference in these models, using primarily a risk-based framework. While the most common maximum a posteriori (MAP), or Viterbi, path estimator and the minimum error, or Posterior Decoder (PD) have long been around, other path estimators, or decoders, have been either only hinted at or applied more recently and in dedicated applications generally unfamiliar to the statistical learning community. Over a decade ago, however, a family of algorithmically defined decoders aiming to hybridize the two standard ones was proposed elsewhere. The present paper gives a careful analysis of this hybridization approach, identifies several problems and issues with it and other previously proposed approaches, and proposes practical resolutions of those. Furthermore, simple modifications of the classical criteria for hidden path recognition are shown to lead to a new class of decoders. Dynamic programming algorithms to compute these decoders in the usual forward-backward manner are presented. A particularly interesting subclass of such estimators can be also viewed as hybrids of the MAP and PD estimators. Similar to previously proposed MAP-PD hybrids, the new class is parameterized by a small number of tunable parameters. Unlike their algorithmic predecessors, the new risk-based decoders are more clearly interpretable, and, most importantly, work \"out-of-the box\" in practice, which is demonstrated on some real bioinformatics tasks and data. Some further generalizations and applications are discussed in the conclusion."
            ],
            "keywords": [
                "admissible path",
                "decoder",
                "HMM",
                "hybrid",
                "interpolation",
                "MAP sequence",
                "minimum error",
                "optimal accuracy",
                "power transform",
                "risk",
                "segmental classification",
                "symbol-bysymbol",
                "posterior decoding",
                "Viterbi algorithm"
            ],
            "author": [
                "Jüri Lember",
                "Alexey A Koloydenko"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/lember14a/lember14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Quadratic Discriminant Analysis",
            "abstract": [
                "Quadratic discriminant analysis is a common tool for classification, but estimation of the Gaussian parameters can be ill-posed. This paper contains theoretical and algorithmic contributions to Bayesian estimation for quadratic discriminant analysis. A distribution-based Bayesian classifier is derived using information geometry. Using a calculus of variations approach to define a functional Bregman divergence for distributions, it is shown that the Bayesian distribution-based classifier that minimizes the expected Bregman divergence of each class conditional distribution also minimizes the expected misclassification cost. A series approximation is used to relate regularized discriminant analysis to Bayesian discriminant analysis. A new Bayesian quadratic discriminant analysis classifier is proposed where the prior is defined using a coarse estimate of the covariance based on the training data; this classifier is termed BDA7. Results on benchmark data sets and simulations show that BDA7 performance is competitive with, and in some cases significantly better than, regularized quadratic discriminant analysis and the cross-validated Bayesian quadratic discriminant analysis classifier Quadratic Bayes."
            ],
            "keywords": [
                "quadratic discriminant analysis",
                "regularized quadratic discriminant analysis",
                "Bregman divergence",
                "data-dependent prior",
                "eigenvalue decomposition",
                "Wishart",
                "functional analysis"
            ],
            "author": [
                "Santosh Srivastava",
                "Maya R Gupta",
                "Béla A Frigyik"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/srivastava07a/srivastava07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SimpleDet: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition",
            "abstract": [
                "Object detection and instance recognition play a central role in many AI applications like autonomous driving, video surveillance and medical image analysis. However, training object detection models on large scale datasets remains computationally expensive and time consuming. This paper presents an efficient and open source object detection framework called SimpleDet which enables the training of state-of-the-art detection models on consumer grade hardware at large scale. SimpleDet covers a wide range of models including both high-performance and high-speed ones. SimpleDet is well-optimized for both low precision training and distributed training and achieves 70% higher throughput for the Mask R-CNN detector compared with existing frameworks. Codes, examples and documents of SimpleDet can be found at https://github.com/tusimple/simpledet."
            ],
            "keywords": [
                "Object Detection",
                "Instance Recognition",
                "Distributed Training",
                "Mixed Precision Training"
            ],
            "author": [
                "Yuntao Chen",
                "Chenxia Han",
                "Yanghao Li",
                "Zehao Huang",
                "Yi Jiang",
                "Naiyan Wang",
                "Zhaoxiang Zhang",
                "Chen Yuntao",
                "Han Chenxia",
                "Li Yanghao",
                "Huang Zehao",
                "Jiang Yi",
                "Wang Naiyan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-205/19-205.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structured Variable Selection with Sparsity-Inducing Norms",
            "abstract": [
                "We consider the empirical risk minimization problem for linear supervised learning, with regularization by structured sparsity-inducing norms. These are defined as sums of Euclidean norms on certain subsets of variables, extending the usual ℓ-norm and the group ℓ 1-norm by allowing the subsets to overlap. This leads to a specific set of allowed nonzero patterns for the solutions of such problems. We first explore the relationship between the groups defining the norm and the resulting nonzero patterns, providing both forward and backward algorithms to go back and forth from groups to patterns. This allows the design of norms adapted to specific prior knowledge expressed in terms of nonzero patterns. We also present an efficient active set algorithm, and analyze the consistency of variable selection for least-squares linear regression in low and high-dimensional settings."
            ],
            "keywords": [
                "sparsity",
                "consistency",
                "variable selection",
                "convex optimization",
                "active set algorithm"
            ],
            "author": [
                "Rodolphe Jenatton",
                "Jean-Yves Audibert",
                "Francis Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/jenatton11b/jenatton11b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Uniform Object Generation for Optimizing One-class Classifiers",
            "abstract": [
                "In one-class classification, one class of data, called the target class, has to be distinguished from the rest of the feature space. It is assumed that only examples of the target class are available. This classifier has to be constructed such that objects not originating from the target set, by definition outlier objects, are not classified as target objects. In previous research the support vector data description (SVDD) is proposed to solve the problem of one-class classification. It models a hypersphere around the target set, and by the introduction of kernel functions, more flexible descriptions are obtained. In the original optimization of the SVDD, two parameters have to be given beforehand by the user. To automatically optimize the values for these parameters, the error on both the target and outlier data has to be estimated. Because no outlier examples are available, we propose a method for generating artificial outliers, uniformly distributed in a hypersphere. An (relative) efficient estimate for the volume covered by the one-class classifiers is obtained, and so an estimate for the outlier error. Results are shown for artificial data and for real world data."
            ],
            "keywords": [
                "Support vector classifiers",
                "one-class classification",
                "novelty detection",
                "outlier detection"
            ],
            "author": [
                "Robert P W Duin"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/tax01a/tax01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ultra-Scalable and Efficient Methods for Hybrid Observational and Experimental Local Causal Pathway Discovery",
            "abstract": [
                "Discovery of causal relations from data is a fundamental objective of several scientific disciplines. Most causal discovery algorithms that use observational data can infer causality only up to a statistical equivalency class, thus leaving many causal relations undetermined. In general, complete identification of causal relations requires experimentation to augment discoveries from observational data. This has led to the recent development of several methods for active learning of causal networks that utilize both observational and experimental data in order to discover causal networks. In this work, we focus on the problem of discovering local causal pathways that contain only direct causes and direct effects of the target variable of interest and propose new discovery methods that aim to minimize the number of required experiments, relax common sufficient discovery assumptions in order to increase discovery accuracy, and scale to high-dimensional data with thousands of variables. We conduct a comprehensive evaluation of new and existing methods with data of dimensionality up to 1,000,000 variables. We use both artificially simulated networks and in-silico gene transcriptional networks that model the characteristics of real gene expression data."
            ],
            "keywords": [
                "causality",
                "large-scale experimental design",
                "local causal pathway discovery",
                "observational data",
                "experimental data",
                "randomized experiments"
            ],
            "author": [
                "Alexander Statnikov",
                "Sisi Ma",
                "Mikael Henaff",
                "Nikita Lytkin",
                "Constantin F Aliferis",
                "Efstratios Efstathiadis",
                "Eric R Peskin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/statnikov15a/statnikov15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Collaborative Filtering Approaches for Large Recommender Systems",
            "abstract": [
                "The collaborative filtering (CF) using known user ratings of items has proved to be effective for predicting user preferences in item selection. This thriving subfield of machine learning became popular in the late 1990s with the spread of online services that use recommender systems, such as Amazon, Yahoo! Music, and Netflix. CF approaches are usually designed to work on very large data sets. Therefore the scalability of the methods is crucial. In this work, we propose various scalable solutions that are validated against the Netflix Prize data set, currently the largest publicly available collection. First, we propose various matrix factorization (MF) based techniques. Second, a neighbor correction method for MF is outlined, which alloys the global perspective of MF and the localized property of neighbor based approaches efficiently. In the experimentation section, we first report on some implementation issues, and we suggest on how parameter optimization can be performed efficiently for MFs. We then show that the proposed scalable approaches compare favorably with existing ones in terms of prediction accuracy and/or required training time. Finally, we report on some experiments performed on MovieLens and Jester data sets."
            ],
            "keywords": [
                "collaborative filtering",
                "recommender systems",
                "matrix factorization",
                "neighbor based correction",
                "Netflix prize"
            ],
            "author": [
                "Gábor Takács",
                "Paolo Frasconi",
                "Kristian Kersting",
                "Hannu Toivonen",
                "Koji Tsuda"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/takacs09a/takacs09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic Differentiation",
            "abstract": [
                "Optimization on manifolds is a class of methods for optimization of an objective function, subject to constraints which are smooth, in the sense that the set of points which satisfy the constraints admits the structure of a differentiable manifold. While many optimization problems are of the described form, technicalities of differential geometry and the laborious calculation of derivatives pose a significant barrier for experimenting with these methods. We introduce Pymanopt (available at pymanopt.github.io), a toolbox for optimization on manifolds, implemented in Python, that-similarly to the Manopt 1 Matlab toolbox-implements several manifold geometries and optimization algorithms. Moreover, we lower the barriers to users further by using automated differentiation 2 for calculating derivative information, saving users time and saving them from potential calculation and implementation errors."
            ],
            "keywords": [
                "Riemannian optimization",
                "non-convex optimization",
                "manifold optimization",
                "projection matrices",
                "symmetric matrices",
                "rotation matrices",
                "positive definite matrices"
            ],
            "author": [
                "James Townsend",
                "Sebastian Weichwald"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-177/16-177.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Novel M-Estimator for Robust PCA",
            "abstract": [
                "We study the basic problem of robust subspace recovery. That is, we assume a data set that some of its points are sampled around a fixed subspace and the rest of them are spread in the whole ambient space, and we aim to recover the fixed underlying subspace. We first estimate \"robust inverse sample covariance\" by solving a convex minimization procedure; we then recover the subspace by the bottom eigenvectors of this matrix (their number correspond to the number of eigenvalues close to 0). We guarantee exact subspace recovery under some conditions on the underlying data. Furthermore, we propose a fast iterative algorithm, which linearly converges to the matrix minimizing the convex problem. We also quantify the effect of noise and regularization and discuss many other practical and theoretical issues for improving the subspace recovery in various settings. When replacing the sum of terms in the convex energy function (that we minimize) with the sum of squares of terms, we obtain that the new minimizer is a scaled version of the inverse sample covariance (when exists). We thus interpret our minimizer and its subspace (spanned by its bottom eigenvectors) as robust versions of the empirical inverse covariance and the PCA subspace respectively. We compare our method with many other algorithms for robust PCA on synthetic and real data sets and demonstrate state-of-the-art speed and accuracy."
            ],
            "keywords": [
                "principal components analysis",
                "robust statistics",
                "M-estimator",
                "iteratively re-weighted least squares",
                "convex relaxation"
            ],
            "author": [
                "Teng Zhang",
                "Gilad Lerman",
                "* Gilad Lerman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/zhang14a/zhang14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Learn Neural Networks",
            "abstract": [
                "In this paper, we propose a novel model for high-dimensional data, called the Hybrid Orthogonal Projection and Estimation (HOPE) model, which combines a linear orthogonal projection and a finite mixture model under a unified generative modeling framework. The HOPE model itself can be learned unsupervised from unlabelled data based on the maximum likelihood estimation as well as discriminatively from labelled data. More interestingly, we have shown the proposed HOPE models are closely related to neural networks (NNs) in a sense that each hidden layer can be reformulated as a HOPE model. As a result, the HOPE framework can be used as a novel tool to probe why and how NNs work, more importantly, to learn NNs in either supervised or unsupervised ways. In this work, we have investigated the HOPE framework to learn NNs for several standard tasks, including image recognition on MNIST and speech recognition on TIMIT. Experimental results have shown that the HOPE framework yields significant performance gains over the current state-ofthe-art methods in various types of NN learning problems, including unsupervised feature learning, supervised or semi-supervised learning."
            ],
            "keywords": [
                "Orthogonal Projection",
                "PCA",
                "Mixture Models",
                "Neural Networks",
                "Unsupervised Learning",
                "von Mises-Fisher (vMF) model"
            ],
            "author": [
                "Shiliang Zhang",
                "Hui Jiang",
                "Lirong Dai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-335/15-335.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Asynchronous Stochastic Gradient-Push: Asymptotically Optimal and Network-Independent Performance for Strongly Convex Functions",
            "abstract": [
                "We consider the standard model of distributed optimization of a sum of functions F (z) = n i=1 f i (z), where node i in a network holds the function f i (z). We allow for a harsh network model characterized by asynchronous updates, message delays, unpredictable message losses, and directed communication among nodes. In this setting, we analyze a modification of the Gradient-Push method for distributed optimization, assuming that (i) node i is capable of generating gradients of its function f i (z) corrupted by zero-mean boundedsupport additive noise at each step, (ii) F (z) is strongly convex, and (iii) each f i (z) has Lipschitz gradients. We show that our proposed method asymptotically performs as well as the best bounds on centralized gradient descent that takes steps in the direction of the sum of the noisy gradients of all the functions f 1 (z),. .. , f n (z) at each step."
            ],
            "keywords": [],
            "author": [
                "Artin Spiridonoff",
                "Alex Olshevsky"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-813/18-813.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Algorithmic Luckiness",
            "abstract": [
                "Classical statistical learning theory studies the generalisation performance of machine learning algorithms rather indirectly. One of the main detours is that algorithms are studied in terms of the hypothesis class that they draw their hypotheses from. In this paper, motivated by the luckiness framework of Shawe-Taylor et al. (1998), we study learning algorithms more directly and in a way that allows us to exploit the serendipity of the training sample. The main difference to previous approaches lies in the complexity measure; rather than covering all hypotheses in a given hypothesis space it is only necessary to cover the functions which could have been learned using the fixed learning algorithm. We show how the resulting framework relates to the VC, luckiness and compression frameworks. Finally, we present an application of this framework to the maximum margin algorithm for linear classifiers which results in a bound that exploits the margin, the sparsity of the resultant weight vector, and the degree of clustering of the training data in feature space."
            ],
            "keywords": [],
            "author": [
                "Ralf Herbrich",
                "Robert C Williamson",
                "David A Cohn"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/herbrich02a/herbrich02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics",
            "abstract": [
                "We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities."
            ],
            "keywords": [
                "unnormalized models",
                "partition function",
                "computation",
                "estimation",
                "natural image statistics"
            ],
            "author": [
                "Michael U Gutmann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/gutmann12a/gutmann12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gradient Estimation with Simultaneous Perturbation and Compressive Sensing",
            "abstract": [
                "We propose a scheme for finding a \"good\" estimator for the gradient of a function on a high-dimensional space with few function evaluations, for applications where function evaluations are expensive and the function under consideration is not sensitive in all coordinates locally, making its gradient almost sparse. Exploiting the latter aspect, our method combines ideas from Spall's Simultaneous Perturbation Stochastic Approximation with compressive sensing. We theoretically justify its computational advantages and illustrate them empirically by numerical experiments. In particular, applications to estimating gradient outer product matrix as well as standard optimization problems are illustrated via simulations."
            ],
            "keywords": [
                "Gradient estimation",
                "Compressive sensing",
                "Sparsity",
                "Gradient descent",
                "Gradient outer product matrix"
            ],
            "author": [
                "Vivek S Borkar",
                "Neeraja Sahasrabudhe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-592/15-592.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-Sparse Multiple Kernel Fisher Discriminant Analysis",
            "abstract": [
                "Sparsity-inducing multiple kernel Fisher discriminant analysis (MK-FDA) has been studied in the literature. Building on recent advances in non-sparse multiple kernel learning (MKL), we propose a non-sparse version of MK-FDA, which imposes a general ℓ p norm regularisation on the kernel weights. We formulate the associated optimisation problem as a semi-infinite program (SIP), and adapt an iterative wrapper algorithm to solve it. We then discuss, in light of latest advances in MKL optimisation techniques, several reformulations and optimisation strategies that can potentially lead to significant improvements in the efficiency and scalability of MK-FDA. We carry out extensive experiments on six datasets from various application areas, and compare closely the performance of ℓ p MK-FDA, fixed norm MK-FDA, and several variants of SVM-based MKL (MK-SVM). Our results demonstrate that ℓ p MK-FDA improves upon sparse MK-FDA in many practical situations. The results also show that on image categorisation problems, ℓ p MK-FDA tends to outperform its SVM counterpart. Finally, we also discuss the connection between (MK-)FDA and (MK-)SVM, under the unified framework of regularised kernel machines."
            ],
            "keywords": [
                "multiple kernel learning",
                "kernel fisher discriminant analysis",
                "regularised least squares",
                "support vector machines"
            ],
            "author": [
                "Fei Yan",
                "Josef Kittler",
                "Krystian Mikolajczyk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/yan12a/yan12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gibbs Max-margin Topic Models with Data Augmentation",
            "abstract": [
                "Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max-margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restrictive assumptions and no need to solve SVM subproblems. Furthermore, each step of the \"augment-and-collapse\" Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results on several medium-sized and large-scale data sets demonstrate significant improvements on time efficiency. The classification performance is also improved over competitors on binary, multi-class and multi-label classification tasks."
            ],
            "keywords": [
                "supervised topic models",
                "max-margin learning",
                "Gibbs classifiers",
                "regularized Bayesian inference",
                "support vector machines"
            ],
            "author": [
                "Jun Zhu",
                "Ning Chen",
                "Hugh Perkins",
                "Bo Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/zhu14a/zhu14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiplicative Multitask Feature Learning",
            "abstract": [
                "We investigate a general framework of multiplicative multitask feature learning which decomposes individual task's model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods can be proved to be special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the task-specific component for all these regularizers, leading to a better understanding of the shrinkage effects of different regularizers. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. An efficient blockwise coordinate descent algorithm is developed suitable for solving the entire family of formulations with rigorous convergence analysis. Simulation studies have identified the statistical properties of data that would be in favor of the new formulations. Extensive empirical studies on various classification and regression benchmark data sets have revealed the relative advantages of the two new c Xin Wang, Jinbo Bi, Shipeng Yu, Jiangwen Sun and Minghu Song. Wang, Bi, Yu, Sun and Song formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks."
            ],
            "keywords": [
                "Multitask learning",
                "regularization",
                "sparse modeling",
                "blockwise coordinate descent"
            ],
            "author": [
                "Xin Wang",
                "Jinbo Bi",
                "Shipeng Yu",
                "Jiangwen Sun",
                "Urun Dogan",
                "Marius Kloft",
                "Francesco Orabona",
                "Tatiana Tommasi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-234/15-234.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Asymptotic Performance of Linear Echo State Neural Networks",
            "abstract": [
                "In this article, a study of the mean-square error (MSE) performance of linear echo-state neural networks is performed, both for training and testing tasks. Considering the realistic setting of noise present at the network nodes, we derive deterministic equivalents for the aforementioned MSE in the limit where the number of input data T and network size n both grow large. Specializing then the network connectivity matrix to specific random settings, we further obtain simple formulas that provide new insights on the performance of such networks."
            ],
            "keywords": [
                "recurrent neural networks",
                "echo state networks",
                "random matrix theory",
                "mean square error",
                "linear networks"
            ],
            "author": [
                "Romain Couillet",
                "Gilles Wainrib",
                "Harry Sevi",
                "Tiomoko Hafiz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-076/16-076.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quadratic Programming Feature Selection",
            "abstract": [
                "Identifying a subset of features that preserves classification accuracy is a problem of growing importance, because of the increasing size and dimensionality of real-world data sets. We propose a new feature selection method, named Quadratic Programming Feature Selection (QPFS), that reduces the task to a quadratic optimization problem. In order to limit the computational complexity of solving the optimization problem, QPFS uses the Nyström method for approximate matrix diagonalization. QPFS is thus capable of dealing with very large data sets, for which the use of other methods is computationally expensive. In experiments with small and medium data sets, the QPFS method leads to classification accuracy similar to that of other successful techniques. For large data sets, QPFS is superior in terms of computational efficiency."
            ],
            "keywords": [
                "feature selection",
                "quadratic programming",
                "Nyström method",
                "large data set",
                "highdimensional data"
            ],
            "author": [
                "Irene Rodriguez-Lujan",
                "Ramon Huerta",
                "Charles Elkan",
                "Carlos Santa Cruz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/rodriguez-lujan10a/rodriguez-lujan10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-Convex Projected Gradient Descent for Generalized Low-Rank Tensor Regression",
            "abstract": [
                "In this paper, we consider the problem of learning high-dimensional tensor regression problems with low-rank structure. One of the core challenges associated with learning highdimensional models is computation since the underlying optimization problems are often non-convex. While convex relaxations could lead to polynomial-time algorithms they are often slow in practice. On the other hand, limited theoretical guarantees exist for nonconvex methods. In this paper we provide a general framework that provides theoretical guarantees for learning high-dimensional tensor regression models under different low-rank structural assumptions using the projected gradient descent algorithm applied to a potentially non-convex constraint set Θ in terms of its localized Gaussian width (due to Gaussian design). We juxtapose our theoretical results for non-convex projected gradient descent algorithms with previous results on regularized convex approaches. The two main differences between the convex and non-convex approach are: (i) from a computational perspective whether the non-convex projection operator is computable and whether the projection has desirable contraction properties and (ii) from a statistical error bound perspective, the non-convex approach has a superior rate for a number of examples. We provide three concrete examples of low-dimensional structure which address these issues and explain the pros and cons for the non-convex and convex approaches. We supplement our theoretical results with simulations which show that, under several common settings of generalized low rank tensor regression, the projected gradient descent approach is superior both in terms of statistical error and run-time provided the step-sizes of the projected descent algorithm are suitably chosen."
            ],
            "keywords": [
                "tensors",
                "non-convex optimization",
                "high-dimensional regression",
                "low-rank;"
            ],
            "author": [
                "Han Chen",
                "Ming Yuan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/16-607/16-607.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "AD : Alternating Directions Dual Decomposition for MAP Inference in Graphical Models *",
            "abstract": [
                "We present AD 3 , a new algorithm for approximate maximum a posteriori (MAP) inference on factor graphs, based on the alternating directions method of multipliers. Like other dual decomposition algorithms, AD 3 has a modular architecture, where local subproblems are solved independently, and their solutions are gathered to compute a global update. The key characteristic of AD 3 is that each local subproblem has a quadratic regularizer, leading to faster convergence, both theoretically and in practice. We provide closed-form solutions for these AD 3 subproblems for binary pairwise factors and factors imposing first-order logic constraints. For arbitrary factors (large or combinatorial), we introduce an active set method which requires only an oracle for computing a local MAP configuration, making AD 3 applicable to a wide range of problems. Experiments on synthetic and real-world problems show that AD compares favorably with the state-of-the-art."
            ],
            "keywords": [
                "MAP inference",
                "graphical models",
                "dual decomposition",
                "alternating directions method of multipliers"
            ],
            "author": [
                "André F T Martins",
                "Mário A T Figueiredo",
                "Pedro M Q Aguiar",
                "Noah A Smith",
                "Eric P Xing"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/martins15a/martins15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric Quantile Estimation",
            "abstract": [
                "In regression, the desired estimate of y|x is not always given by a conditional mean, although this is most common. Sometimes one wants to obtain a good estimate that satisfies the property that a proportion, τ, of y|x, will be below the estimate. For τ = 0.5 this is an estimate of the median. What might be called median regression, is subsumed under the term quantile regression. We present a nonparametric version of a quantile estimator, which can be obtained by solving a simple quadratic programming problem and provide uniform convergence statements and bounds on the quantile property of our estimator. Experimental results show the feasibility of the approach and competitiveness of our method with existing ones. We discuss several types of extensions including an approach to solve the quantile crossing problems, as well as a method to incorporate prior qualitative knowledge such as monotonicity constraints."
            ],
            "keywords": [
                "support vector machines",
                "kernel methods",
                "quantile estimation",
                "nonparametric techniques",
                "estimation with constraints"
            ],
            "author": [
                "Ichiro Takeuchi",
                "Timothy D Sears",
                "Alexander J Smola"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/takeuchi06a/takeuchi06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Recursive Control Programs from Problem Solving",
            "abstract": [
                "In this paper, we propose a new representation for physical control-teleoreactive logic programs-along with an interpreter that uses them to achieve goals. In addition, we present a new learning method that acquires recursive forms of these structures from traces of successful problem solving. We report experiments in three different domains that demonstrate the generality of this approach. In closing, we review related work on learning complex skills and discuss directions for future research on this topic."
            ],
            "keywords": [
                "teleoreactive control",
                "logic programs",
                "problem solving",
                "skill learning"
            ],
            "author": [
                "Pat Langley",
                "Dongkyu Choi",
                "Roland Olsson",
                "Ute Schmid"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/langley06a/langley06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Universal Latent Space Model Fitting for Large Networks with Edge Covariates",
            "abstract": [
                "Latent space models are effective tools for statistical modeling and visualization of network data. Due to their close connection to generalized linear models, it is also natural to incorporate covariate information in them. The current paper presents two universal fitting algorithms for networks with edge covariates: one based on nuclear norm penalization and the other based on projected gradient descent. Both algorithms are motivated by maximizing the likelihood function for an existing class of inner-product models, and we establish their statistical rates of convergence for these models. In addition, the theory informs us that both methods work simultaneously for a wide range of different latent space models that allow latent positions to affect edge formation in flexible ways, such as distance models. Furthermore, the effectiveness of the methods is demonstrated on a number of real world network data sets for different statistical tasks, including community detection with and without edge covariates, and network assisted learning."
            ],
            "keywords": [
                "community detection",
                "network with covariates",
                "non-convex optimization",
                "projected gradient descent"
            ],
            "author": [
                "Zhuang Ma",
                "Zongming Ma",
                "Hongsong Yuan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/17-470/17-470.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Theory Analysis for Association Rules and Sequential Event Prediction",
            "abstract": [
                "We present a theoretical analysis for prediction algorithms based on association rules. As part of this analysis, we introduce a problem for which rules are particularly natural, called \"sequential event prediction.\" In sequential event prediction, events in a sequence are revealed one by one, and the goal is to determine which event will next be revealed. The training set is a collection of past sequences of events. An example application is to predict which item will next be placed into a customer's online shopping cart, given his/her past purchases. In the context of this problem, algorithms based on association rules have distinct advantages over classical statistical and machine learning methods: they look at correlations based on subsets of co-occurring past events (items a and b imply item c), they can be applied to the sequential event prediction problem in a natural way, they can potentially handle the \"cold start\" problem where the training set is small, and they yield interpretable predictions. In this work, we present two algorithms that incorporate association rules. These algorithms can be used both for sequential event prediction and for supervised classification, and they are simple enough that they can possibly be understood by users, customers, patients, managers, etc. We provide generalization guarantees on these algorithms based on algorithmic stability analysis from statistical learning theory. We include a discussion of the strict minimum support threshold often used in association rule mining, and introduce an \"adjusted confidence\" measure that provides a weaker minimum support condition that has advantages over the strict minimum support. The paper brings together ideas from statistical learning theory, association rule mining and Bayesian analysis."
            ],
            "keywords": [
                "statistical learning theory",
                "algorithmic stability",
                "association rules",
                "sequence prediction",
                "associative classification"
            ],
            "author": [
                "Cynthia Rudin",
                "Benjamin Letham",
                "David Madigan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/rudin13a/rudin13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large Margin Hierarchical Classification with Mutually Exclusive Class Membership",
            "abstract": [
                "In hierarchical classification, class labels are structured, that is each label value corresponds to one non-root node in a tree, where the inter-class relationship for classification is specified by directed paths of the tree. In such a situation, the focus has been on how to leverage the interclass relationship to enhance the performance of flat classification, which ignores such dependency. This is critical when the number of classes becomes large relative to the sample size. This paper considers single-path or partial-path hierarchical classification, where only one path is permitted from the root to a leaf node. A large margin method is introduced based on a new concept of generalized margins with respect to hierarchy. For implementation, we consider support vector machines and ψ-learning. Numerical and theoretical analyses suggest that the proposed method achieves the desired objective and compares favorably against strong competitors in the literature, including its flat counterparts. Finally, an application to gene function prediction is discussed."
            ],
            "keywords": [
                "difference convex programming",
                "gene function annotation",
                "margins",
                "multi-class classification",
                "structured learning"
            ],
            "author": [
                "Huixin Wang",
                "Xiaotong Shen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/wang11c/wang11c.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "From External to Internal Regret",
            "abstract": [
                "External regret compares the performance of an online algorithm, selecting among N actions, to the performance of the best of those actions in hindsight. Internal regret compares the loss of an online algorithm to the loss of a modified online algorithm, which consistently replaces one action by another. In this paper we give a simple generic reduction that, given an algorithm for the external regret problem, converts it to an efficient online algorithm for the internal regret problem. We provide methods that work both in the full information model, in which the loss of every action is observed at each time step, and the partial information (bandit) model, where at each time step only the loss of the selected action is observed. The importance of internal regret in game theory is due to the fact that in a general game, if each player has sublinear internal regret, then the empirical frequencies converge to a correlated equilibrium. For external regret we also derive a quantitative regret bound for a very general setting of regret, which includes an arbitrary set of modification rules (that possibly modify the online algorithm) and an arbitrary set of time selection functions (each giving different weight to each time step). The regret for a given time selection and modification rule is the difference between the cost of the online algorithm and the cost of the modified online algorithm, where the costs are weighted by the time selection function. This can be viewed as a generalization of the previously-studied sleeping experts setting."
            ],
            "keywords": [
                "online learning",
                "internal regret",
                "external regret",
                "multi-arm bandit",
                "sleeping experts",
                "reductions"
            ],
            "author": [
                "Avrim Blum"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/blum07a/blum07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robustifying Independent Component Analysis by Adjusting for Group-Wise Stationary Noise",
            "abstract": [
                "We introduce coroICA, confounding-robust independent component analysis, a novel ICA algorithm which decomposes linearly mixed multivariate observations into independent components that are corrupted (and rendered dependent) by hidden group-wise stationary confounding. It extends the ordinary ICA model in a theoretically sound and explicit way to incorporate group-wise (or environment-wise) confounding. We show that our proposed general noise model allows to perform ICA in settings where other noisy ICA procedures fail. Additionally, it can be used for applications with grouped data by adjusting for different stationary noise within each group. Our proposed noise model has a natural relation to causality and we explain how it can be applied in the context of causal inference. In addition to our theoretical framework, we provide an efficient estimation procedure and prove identifiability of the unmixing matrix under mild assumptions. Finally, we illustrate the performance and robustness of our method on simulated data, provide audible and visual examples, and demonstrate the applicability to real-world scenarios by experiments on publicly available Antarctic ice core data as well as two EEG data sets. We provide a scikit-learn compatible pip-installable Python package coroICA as well as R and Matlab implementations accompanied by a documentation at https://sweichwald.de/coroICA/."
            ],
            "keywords": [
                "blind source separation",
                "causal inference",
                "confounding noise",
                "group analysis",
                "heterogeneous data",
                "independent component analysis",
                "non-stationary signal",
                "robustness"
            ],
            "author": [
                "Niklas Pfister",
                "Sebastian Weichwald",
                "Peter Bühlmann",
                "Bernhard Schölkopf"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-399/18-399.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Information Bottleneck for Gaussian Variables",
            "abstract": [
                "The problem of extracting the relevant aspects of data was previously addressed through the information bottleneck (IB) method, through (soft) clustering one variable while preserving information about another-relevance-variable. The current work extends these ideas to obtain continuous representations that preserve relevant information, rather than discrete clusters, for the special case of multivariate Gaussian variables. While the general continuous IB problem is difficult to solve, we provide an analytic solution for the optimal representation and tradeoff between compression and relevance for the this important case. The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized regression matrix Σ x|y Σ −1 x , which is also the basis obtained in canonical correlation analysis. However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector, through a cascade of structural phase transitions. This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level. Our analysis also provides a complete analytic expression of the preserved information as a function of the compression (the \"information-curve\"), in terms of the eigenvalue spectrum of the data. As in the discrete case, the information curve is concave and smooth, though it is made of different analytic segments for each optimal dimension. Finally, we show how the algorithmic theory developed in the IB framework provides an iterative algorithm for obtaining the optimal Gaussian projections."
            ],
            "keywords": [
                "information bottleneck",
                "Gaussian processes",
                "dimensionality reduction",
                "canonical correlation analysis"
            ],
            "author": [
                "Gal Chechik",
                "Amir Globerson",
                "Naftali Tishby",
                "Yair Weiss"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/chechik05a/chechik05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multiscale Strategies for Computing Optimal Transport",
            "abstract": [
                "This paper presents a multiscale approach to efficiently compute approximate optimal transport plans between point sets. It is particularly well-suited for point sets that are in high-dimensions, but are close to being intrinsically low-dimensional. The approach is based on an adaptive multiscale decomposition of the point sets. The multiscale decomposition yields a sequence of optimal transport problems, that are solved in a top-to-bottom fashion from the coarsest to the finest scale. We provide numerical evidence that this multiscale approach scales approximately linearly, in time and memory, in the number of nodes, instead of quadratically or worse for a direct solution. Empirically, the multiscale approach results in less than one percent relative error in the objective function. Furthermore, the multiscale plans constructed are of interest by themselves as they may be used to introduce novel features and notions of distances between point sets. An analysis of sets of brain MRI based on optimal transport distances illustrates the effectiveness of the proposed method on a real world data set. The application demonstrates that multiscale optimal transport distances have the potential to improve on state-of-the-art metrics currently used in computational anatomy."
            ],
            "keywords": [],
            "author": [
                "Samuel Gerber"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-108/16-108.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Learning with Partially Observed Attributes *",
            "abstract": [
                "We investigate three variants of budgeted learning, a setting in which the learner is allowed to access a limited number of attributes from training or test examples. In the \"local budget\" setting, where a constraint is imposed on the number of available attributes per training example, we design and analyze an efficient algorithm for learning linear predictors that actively samples the attributes of each training instance. Our analysis bounds the number of additional examples sufficient to compensate for the lack of full information on the training set. This result is complemented by a general lower bound for the easier \"global budget\" setting, where it is only the overall number of accessible training attributes that is being constrained. In the third, \"prediction on a budget\" setting, when the constraint is on the number of available attributes per test example, we show that there are cases in which there exists a linear predictor with zero error but it is statistically impossible to achieve arbitrary accuracy without full information on test examples. Finally, we run simple experiments on a digit recognition problem that reveal that our algorithm has a good performance against both partial information and full information baselines."
            ],
            "keywords": [
                "budgeted learning",
                "statistical learning",
                "linear predictors",
                "learning with partial information",
                "learning theory"
            ],
            "author": [
                "Nicolò Cesa-Bianchi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/cesa-bianchi11a/cesa-bianchi11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Tight Bounds for the Lasso",
            "abstract": [
                "We present upper and lower bounds for the prediction error of the Lasso. For the case of random Gaussian design, we show that under mild conditions the prediction error of the Lasso is up to smaller order terms dominated by the prediction error of its noiseless counterpart. We then provide exact expressions for the prediction error of the latter, in terms of compatibility constants. Here, we assume the active components of the underlying regression function satisfy some \"betamin\" condition. For the case of fixed design, we provide upper and lower bounds, again in terms of compatibility constants. As an example, we give an up to a logarithmic term tight bound for the least squares estimator with total variation penalty."
            ],
            "keywords": [
                "Compatibility",
                "Lasso",
                "Linear Model",
                "Lower Bound"
            ],
            "author": [
                "Sara Van De Geer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-025/17-025.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA",
            "abstract": [
                "WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA's learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method. Our new package is tightly integrated with WEKA, making it just as accessible to end users as any other learning algorithm."
            ],
            "keywords": [
                "Hyperparameter Optimization",
                "Model Selection",
                "Feature Selection"
            ],
            "author": [
                "Lars Kotthoff",
                "Chris Thornton",
                "Holger H Hoos",
                "Frank Hutter"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-261/16-261.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multicategory Large-Margin Unified Machines",
            "abstract": [
                "Hard and soft classifiers are two important groups of techniques for classification problems. Logistic regression and Support Vector Machines are typical examples of soft and hard classifiers respectively. The essential difference between these two groups is whether one needs to estimate the class conditional probability for the classification task or not. In particular, soft classifiers predict the label based on the obtained class conditional probabilities, while hard classifiers bypass the estimation of probabilities and focus on the decision boundary. In practice, for the goal of accurate classification, it is unclear which one to use in a given situation. To tackle this problem, the Largemargin Unified Machine (LUM) was recently proposed as a unified family to embrace both groups. The LUM family enables one to study the behavior change from soft to hard binary classifiers. For multicategory cases, however, the concept of soft and hard classification becomes less clear. In that case, class probability estimation becomes more involved as it requires estimation of a probability vector. In this paper, we propose a new Multicategory LUM (MLUM) framework to investigate the behavior of soft versus hard classification under multicategory settings. Our theoretical and numerical results help to shed some light on the nature of multicategory classification and its transition behavior from soft to hard classifiers. The numerical results suggest that the proposed tuned MLUM yields very competitive performance."
            ],
            "keywords": [
                "hard classification",
                "large-margin",
                "soft classification",
                "support vector machine"
            ],
            "author": [
                "Chong Zhang",
                "Yufeng Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/liu13a/liu13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Solutions for Sparse Principal Component Analysis Alexandre d'Aspremont",
            "abstract": [
                "Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefficients, with total complexity O(n 3), where n is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in O(n 3) per pattern. We discuss applications in subset selection and sparse recovery and show on artificial examples and biological data that our algorithm does provide globally optimal solutions in many cases."
            ],
            "keywords": [
                "PCA",
                "subset selection",
                "sparse eigenvalues",
                "sparse recovery",
                "lasso"
            ],
            "author": [
                "Francis Bach",
                "Laurent El Ghaoui"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/aspremont08a/aspremont08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Boosted Classification Trees and Class Probability/Quantile Estimation",
            "abstract": [
                "The standard by which binary classifiers are usually judged, misclassification error, assumes equal costs of misclassifying the two classes or, equivalently, classifying at the 1/2 quantile of the conditional class probability function P[y = 1|x]. Boosted classification trees are known to perform quite well for such problems. In this article we consider the use of standard, off-the-shelf boosting for two more general problems: 1) classification with unequal costs or, equivalently, classification at quantiles other than 1/2, and 2) estimation of the conditional class probability function P[y = 1|x]. We first examine whether the latter problem, estimation of P[y = 1|x], can be solved with Logit-Boost, and with AdaBoost when combined with a natural link function. The answer is negative: both approaches are often ineffective because they overfit P[y = 1|x] even though they perform well as classifiers. A major negative point of the present article is the disconnect between class probability estimation and classification. Next we consider the practice of over/under-sampling of the two classes. We present an algorithm that uses AdaBoost in conjunction with Over/Under-Sampling and Jittering of the data (\"JOUS-Boost\"). This algorithm is simple, yet successful, and it preserves the advantage of relative protection against overfitting, but for arbitrary misclassification costs and, equivalently, arbitrary quantile boundaries. We then use collections of classifiers obtained from a grid of quantiles to form estimators of class probabilities. The estimates of the class probabilities compare favorably to those obtained by a variety of methods across both simulated and real data sets."
            ],
            "keywords": [
                "boosting algorithms",
                "LogitBoost",
                "AdaBoost",
                "class probability estimation",
                "over-sampling",
                "under-sampling",
                "stratification",
                "data jittering"
            ],
            "author": [
                "MEASE D@COB.SJSU David Mease",
                "Abraham J Wyner",
                "Andreas Buja"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/mease07a/mease07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Surrogate Modeling and Adaptive Sampling Toolbox for Computer Based Design",
            "abstract": [
                "An exceedingly large number of scientific and engineering fields are confronted with the need for computer simulations to study complex, real world phenomena or solve challenging design problems. However, due to the computational cost of these high fidelity simulations, the use of neural networks, kernel methods, and other surrogate modeling techniques have become indispensable. Surrogate models are compact and cheap to evaluate, and have proven very useful for tasks such as optimization, design space exploration, prototyping, and sensitivity analysis. Consequently, in many fields there is great interest in tools and techniques that facilitate the construction of such regression models, while minimizing the computational cost and maximizing model accuracy. This paper presents a mature, flexible, and adaptive machine learning toolkit for regression modeling and active learning to tackle these issues. The toolkit brings together algorithms for data fitting, model selection, sample selection (active learning), hyperparameter optimization, and distributed computing in order to empower a domain expert to efficiently generate an accurate model for the problem or data at hand."
            ],
            "keywords": [
                "surrogate modeling",
                "metamodeling",
                "function approximation",
                "model selection",
                "adaptive sampling",
                "active learning",
                "distributed computing"
            ],
            "author": [
                "Dirk Gorissen",
                "Karel Crombecq"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/gorissen10a/gorissen10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimation of Non-Normalized Statistical Models by Score Matching",
            "abstract": [
                "One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data."
            ],
            "keywords": [
                "statistical estimation",
                "non-normalized densities",
                "pseudo-likelihood",
                "Markov chain Monte Carlo",
                "contrastive divergence"
            ],
            "author": [
                "Aapo Hyvärinen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/hyvarinen05a/hyvarinen05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "RSG: Beating Subgradient Method without Smoothness and Strong Convexity",
            "abstract": [
                "In this paper, we study the efficiency of a Restarted SubGradient (RSG) method that periodically restarts the standard subgradient method (SG). We show that, when applied to a broad class of convex optimization problems, RSG method can find an-optimal solution with a lower complexity than the SG method. In particular, we first show that RSG can reduce the dependence of SG's iteration complexity on the distance between the initial solution and the optimal set to that between the-level set and the optimal set multiplied by a logarithmic factor. Moreover, we show the advantages of RSG over SG in solving a broad family of problems that satisfy a local error bound condition, and also demonstrate its advantages for three specific families of convex optimization problems with different power constants in the local error bound condition. (a) For the problems whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local quadratic growth property in the-sublevel set, RSG has an O(1 log(1)) iteration complexity. (c) For the problems that admit a local Kurdyka-Lojasiewicz property with a power constant of β ∈ [0, 1), RSG has an O(1 2β log(1)) iteration complexity. The novelty of our analysis lies at exploiting the lower bound of the first-order optimality residual at the-level set. It is this novelty that allows us to explore the local properties of functions (e.g., local quadratic growth property, local Kurdyka-Lojasiewicz property, more generally local error bound conditions) to develop the improved convergence of RSG. We also develop a practical variant of RSG enjoying faster convergence than the SG method, which can be run without knowing the involved parameters in the local error bound condition. We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression, classification and matrix completion."
            ],
            "keywords": [
                "subgradient method",
                "improved convergence",
                "local error bound",
                "machine learning"
            ],
            "author": [
                "Tianbao Yang",
                "Qihang Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-016/17-016.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Some Theory for Generalized Boosting Algorithms",
            "abstract": [
                "We give a review of various aspects of boosting, clarifying the issues through a few simple results, and relate our work and that of others to the minimax paradigm of statistics. We consider the population version of the boosting algorithm and prove its convergence to the Bayes classifier as a corollary of a general result about Gauss-Southwell optimization in Hilbert space. We then investigate the algorithmic convergence of the sample version, and give bounds to the time until perfect separation of the sample. We conclude by some results on the statistical optimality of the L 2 boosting."
            ],
            "keywords": [
                "classification",
                "Gauss-Southwell algorithm",
                "AdaBoost",
                "cross-validation",
                "non-parametric convergence rate"
            ],
            "author": [
                "Peter J Bickel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/bickel06a/bickel06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Binarsity: a penalization for one-hot encoded features in linear supervised learning",
            "abstract": [
                "This paper deals with the problem of large-scale linear supervised learning in settings where a large number of continuous features are available. We propose to combine the well-known trick of one-hot encoding of continuous features with a new penalization called binarsity. In each group of binary features coming from the one-hot encoding of a single raw continuous feature, this penalization uses total-variation regularization together with an extra linear constraint. This induces two interesting properties on the model weights of the one-hot encoded features: they are piecewise constant, and are eventually block sparse. Non-asymptotic oracle inequalities for generalized linear models are proposed. Moreover, under a sparse additive model assumption, we prove that our procedure matches the state-of-the-art in this setting. Numerical experiments illustrate the good performances of our approach on several datasets. It is also noteworthy that our method has a numerical complexity comparable to standard 1 penalization."
            ],
            "keywords": [
                "Supervised learning",
                "Features binarization",
                "Sparse additive modeling",
                "Total-variation",
                "Oracle inequalities",
                "Proximal methods"
            ],
            "author": [
                "Mokhtar Z Alaya",
                "Simon Bussy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-170/17-170.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-scale Online Learning: Theory and Applications to Online Auctions and Pricing",
            "abstract": [
                "We consider revenue maximization in online auction/pricing problems. A seller sells an identical item in each period to a new buyer, or a new set of buyers. For the online pricing problem, both when the arriving buyer bids or only responds to the posted price, we design algorithms whose regret bounds scale with the best fixed price in-hindsight, rather than the range of the values. Under the bidding model, we further show our algorithms achieve a revenue convergence rate that matches the offline sample complexity of the single-item single-buyer auction. We also show regret bounds that are scale free, and match the offline sample complexity, when comparing to a benchmark that requires a lower bound on the market share. We further expand our results beyond pricing to multi-buyer auctions, and obtain online learning algorithms for auctions, with convergence rates matching the known sample complexity upper bound of online single-item multi-buyer auctions. These results are obtained by generalizing the classical learning from experts and multiarmed bandit problems to their multi-scale versions. In this version, the reward of each action is in a different range, and the regret with respect to a given action scales with its own range, rather than the maximum range. We obtain almost optimal multi-scale regret bounds by introducing a new Online Mirror Descent (OMD) algorithm whose mirror map is the multi-scale version of the negative entropy function. We further generalize to the bandit setting by introducing the stochastic variant of this OMD algorithm."
            ],
            "keywords": [
                "online learning",
                "multi-scale learning",
                "auction theory",
                "bandit information",
                "sample complexity. †"
            ],
            "author": [
                "Sébastien Bubeck",
                "Zhiyi Huang",
                "Rad Niazadeh",
                "* Zhiyi Huang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-498/17-498.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Concentration inequalities for empirical processes of linear time series",
            "abstract": [
                "The paper considers suprema of empirical processes for linear time series indexed by functional classes. We derive an upper bound for the tail probability of the suprema under conditions on the size of the function class, the sample size, temporal dependence and the moment conditions of the underlying time series. Due to the dependence and heavy-tailness, our tail probability bound is substantially different from those classical exponential bounds obtained under the independence assumption in that it involves an extra polynomial decaying term. We allow both short-and long-range dependent processes. For empirical processes indexed by half intervals, our tail probability inequality is sharp up to a multiplicative constant."
            ],
            "keywords": [
                "martingale decomposition",
                "tail probability",
                "heavy tail",
                "MA(∞)"
            ],
            "author": [
                "Likai Chen",
                "Wei Biao Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-012/17-012.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Mean Field Variational Approximation for Continuous-Time Bayesian Networks *",
            "abstract": [
                "Continuous-time Bayesian networks is a natural structured representation language for multicomponent stochastic processes that evolve continuously over time. Despite the compact representation provided by this language, inference in such models is intractable even in relatively simple structured networks. We introduce a mean field variational approximation in which we use a product of inhomogeneous Markov processes to approximate a joint distribution over trajectories. This variational approach leads to a globally consistent distribution, which can be efficiently queried. Additionally, it provides a lower bound on the probability of observations, thus making it attractive for learning tasks. Here we describe the theoretical foundations for the approximation, an efficient implementation that exploits the wide range of highly optimized ordinary differential equations (ODE) solvers, experimentally explore characterizations of processes for which this approximation is suitable, and show applications to a large-scale real-world inference problem."
            ],
            "keywords": [
                "continuous time Markov processes",
                "continuous time Bayesian networks",
                "variational approximations",
                "mean field approximation"
            ],
            "author": [
                "Ido Cohn",
                "Nir Friedman",
                "Raz Kupferman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/cohn10a/cohn10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distribution-Matching Embedding for Visual Domain Adaptation",
            "abstract": [
                "Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Distribution-Matching Embedding approach: An unsupervised domain adaptation method that overcomes this issue by mapping the data to a latent space where the distance between the empirical distributions of the source and target examples is minimized. In other words, we seek to extract the information that is invariant across the source and target data. In particular, we study two different distances to compare the source and target distributions: the Maximum Mean Discrepancy and the Hellinger distance. Furthermore, we show that our approach allows us to learn either a linear embedding, or a nonlinear one. We demonstrate the benefits of our approach on the tasks of visual object recognition, text categorization, and WiFi localization."
            ],
            "keywords": [
                "Domain Adaptation",
                "Maximum Mean Discrepancy",
                "Hellinger Distance",
                "Distribution Matching",
                "Domain Invariant Representations"
            ],
            "author": [
                "Mahsa Baktashmotlagh",
                "Mathieu Salzmann",
                "EPFL Cvlab",
                "Urun Dogan",
                "Marius Kloft",
                "Francesco Orabona",
                "Tatiana Tommasi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-207/15-207.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stability and Generalization in Structured Prediction",
            "abstract": [
                "Structured prediction models have been found to learn effectively from a few large examplessometimes even just one. Despite empirical evidence, canonical learning theory cannot guarantee generalization in this setting because the error bounds decrease as a function of the number of examples. We therefore propose new PAC-Bayesian generalization bounds for structured prediction that decrease as a function of both the number of examples and the size of each example. Our analysis hinges on the stability of joint inference and the smoothness of the data distribution. We apply our bounds to several common learning scenarios, including max-margin and soft-max training of Markov random fields. Under certain conditions, the resulting error bounds can be far more optimistic than previous results and can even guarantee generalization from a single large example."
            ],
            "keywords": [
                "structured prediction",
                "learning theory",
                "PAC-Bayes",
                "generalization bounds"
            ],
            "author": [
                "Ben London",
                "Bert Huang",
                "Virginia Tech"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-501/15-501.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Junction Tree Framework for Undirected Graphical Model Selection",
            "abstract": [
                "An undirected graphical model is a joint probability distribution defined on an undirected graph G * , where the vertices in the graph index a collection of random variables and the edges encode conditional independence relationships among random variables. The undirected graphical model selection (UGMS) problem is to estimate the graph G * given observations drawn from the undirected graphical model. This paper proposes a framework for decomposing the UGMS problem into multiple subproblems over clusters and subsets of the separators in a junction tree. The junction tree is constructed using a graph that contains a superset of the edges in G *. We highlight three main properties of using junction trees for UGMS. First, different regularization parameters or different UGMS algorithms can be used to learn different parts of the graph. This is possible since the subproblems we identify can be solved independently of each other. Second, under certain conditions, a junction tree based UGMS algorithm can produce consistent results with fewer observations than the usual requirements of existing algorithms. Third, both our theoretical and experimental results show that the junction tree framework does a significantly better job at finding the weakest edges in a graph than existing methods. This property is a consequence of both the first and second properties. Finally, we note that our framework is independent of the choice of the UGMS algorithm and can be used as a wrapper around standard UGMS algorithms for more accurate graph estimation."
            ],
            "keywords": [
                "Graphical models",
                "Markov random fields",
                "junction trees",
                "model selection",
                "graphical model selection",
                "high-dimensional statistics",
                "graph decomposition"
            ],
            "author": [
                "Divyanshu Vats",
                "Robert D Nowak"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/vats14a/vats14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convolutional Nets and Watershed Cuts for Real-Time Semantic Labeling of RGBD Videos",
            "abstract": [
                "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on handcrafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. Using a frame by frame labeling, we obtain nearly state-of-the-art performance on the NYU-v2 depth data set with an accuracy of 64.5%. We then show that the labeling can be further improved by exploiting the temporal consistency in the video sequence of the scene. To that goal, we present a method producing temporally consistent superpixels from a streaming video. Among the different methods producing superpixel segmentations of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time by using a minimum spanning tree. In a framework exploiting minimum spanning trees all along, we propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real-time applications. We illustrate the labeling of indoor scenes in video sequences that could be processed in real-time using appropriate hardware such as an FPGA."
            ],
            "keywords": [
                "deep learning",
                "optimization",
                "convolutional networks",
                "superpixels",
                "depth information"
            ],
            "author": [
                "Camille Couprie",
                "Laurent Najman",
                "Yann Lecun",
                "Aaron Courville",
                "Rob Fergus",
                "Christopher Manning"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/couprie14a/couprie14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Target-Aware Bayesian Inference: How to Beat Optimal Conventional Estimators",
            "abstract": [
                "Standard approaches for Bayesian inference focus solely on approximating the posterior distribution. Typically, this approximation is, in turn, used to calculate expectations for one or more target functions-a computational pipeline that is inefficient when the target function(s) are known upfront. We address this inefficiency by introducing a framework for target-aware Bayesian inference (TABI) that estimates these expectations directly. While conventional Monte Carlo estimators have a fundamental limit on the error they can achieve for a given sample size, our TABI framework is able to breach this limit; it can theoretically produce arbitrarily accurate estimators using only three samples, while we show empirically that it can also breach this limit in practice. We utilize our TABI framework by combining it with adaptive importance sampling approaches and show both theoretically and empirically that the resulting estimators are capable of converging faster than the standard O(1/N) Monte Carlo rate, potentially producing rates as fast as O(1/N 2). We further combine our TABI framework with amortized inference methods, to produce a method for amortizing the cost of calculating expectations. Finally, we show how TABI can be used to convert any marginal likelihood estimator into a target aware inference scheme and demonstrate the substantial benefits this can yield."
            ],
            "keywords": [
                "Bayesian inference",
                "Monte Carlo methods",
                "importance sampling",
                "adaptive sampling",
                "amortized inference"
            ],
            "author": [
                "Tom Rainforth",
                "Adam Goliński",
                "Frank Wood",
                "Sheheryar Zaidi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-102/19-102.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Boosting Algorithms for Detector Cascade Learning",
            "abstract": [
                "The problem of learning classifier cascades is considered. A new cascade boosting algorithm, fast cascade boosting (FCBoost), is proposed. FCBoost is shown to have a number of interesting properties, namely that it 1) minimizes a Lagrangian risk that jointly accounts for classification accuracy and speed, 2) generalizes adaboost, 3) can be made cost-sensitive to support the design of high detection rate cascades, and 4) is compatible with many predictor structures suitable for sequential decision making. It is shown that a rich family of such structures can be derived recursively from cascade predictors of two stages, denoted cascade generators. Generators are then proposed for two new cascade families, last-stage and multiplicative cascades, that generalize the two most popular cascade architectures in the literature. The concept of neutral predictors is finally introduced, enabling FCBoost to automatically determine the cascade configuration, i.e., number of stages and number of weak learners per stage, for the learned cascades. Experiments on face and pedestrian detection show that the resulting cascades outperform current state-of-the-art methods in both detection accuracy and speed."
            ],
            "keywords": [
                "complexity-constrained learning",
                "detector cascades",
                "sequential decision-making",
                "boosting",
                "ensemble methods",
                "cost-sensitive learning",
                "real-time object detection"
            ],
            "author": [
                "Mohammad Saberian"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/saberian14a/saberian14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel-Based Learning of Hierarchical Multilabel Classification Models *",
            "abstract": [
                "We present a kernel-based algorithm for hierarchical text classification where the documents are allowed to belong to more than one category at a time. The classification model is a variant of the Maximum Margin Markov Network framework, where the classification hierarchy is represented as a Markov tree equipped with an exponential family defined on the edges. We present an efficient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classification hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efficient as training independent SVM-light classifiers for each node. The algorithm's predictive accuracy was found to be competitive with other recently introduced hierarchical multicategory or multilabel classification learning algorithms."
            ],
            "keywords": [
                "kernel methods",
                "hierarchical classification",
                "text categorization",
                "convex optimization",
                "structured outputs"
            ],
            "author": [
                "Juho Rousu",
                "Craig Saunders",
                "Sandor Szedmak",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/rousu06a/rousu06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "New Algorithms for Efficient High-Dimensional Nonparametric Classification",
            "abstract": [
                "This paper is about non-approximate acceleration of high-dimensional nonparametric operations such as k nearest neighbor classifiers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly find the data points close to the query, but merely need to answer questions about the properties of that set of data points. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. This is applicable to many algorithms in nonparametric statistics, memory-based learning and kernel-based learning. But for clarity, this paper concentrates on pure k-NN classification. We introduce new ball-tree algorithms that on real-world data sets give accelerations from 2-fold to 100-fold compared against highly optimized traditional ball-tree-based k-NN. These results include data sets with up to 10 6 dimensions and 10 5 records, and demonstrate non-trivial speed-ups while giving exact answers."
            ],
            "keywords": [],
            "author": [
                "Ting Liu",
                "Andrew W Moore",
                "Alexander Gray"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/liu06a/liu06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Likelihood Estimation for Mixtures of Spherical Gaussians is NP-hard",
            "abstract": [
                "This paper presents NP-hardness and hardness of approximation results for maximum likelihood estimation of mixtures of spherical Gaussians."
            ],
            "keywords": [
                "Mixtures of Gaussians",
                "maximum likelihood",
                "NP-completeness"
            ],
            "author": [
                "Christopher Tosh",
                "Sanjoy Dasgupta"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-657/16-657.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": null,
            "abstract": NaN,
            "keywords": [],
            "author": [],
            "ref": "http://www.jmlr.org/papers/volume2/mendelson01a/mendelson01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Clustering with Bregman Divergences",
            "abstract": [
                "A wide variety of distortion functions, such as squared Euclidean distance, Mahalanobis distance, Itakura-Saito distance and relative entropy, have been used for clustering. In this paper, we propose and analyze parametric hard and soft clustering algorithms based on a large class of distortion functions known as Bregman divergences. The proposed algorithms unify centroid-based parametric clustering approaches, such as classical kmeans, the Linde-Buzo-Gray (LBG) algorithm and information-theoretic clustering, which arise by special choices of the Bregman divergence. The algorithms maintain the simplicity and scalability of the classical kmeans algorithm, while generalizing the method to a large class of clustering loss functions. This is achieved by first posing the hard clustering problem in terms of minimizing the loss in Bregman information, a quantity motivated by rate distortion theory, and then deriving an iterative algorithm that monotonically decreases this loss. In addition, we show that there is a bijection between regular exponential families and a large class of Bregman divergences, that we call regular Bregman divergences. This result enables the development of an alternative interpretation of an efficient EM scheme for learning mixtures of exponential family distributions, and leads to a simple soft clustering algorithm for regular Bregman divergences. Finally, we discuss the connection between rate distortion theory and Bregman clustering and present an information theoretic analysis of Bregman clustering algorithms in terms of a trade-off between compression and loss in Bregman information."
            ],
            "keywords": [
                "clustering",
                "Bregman divergences",
                "Bregman information",
                "exponential families",
                "expectation maximization",
                "information theory"
            ],
            "author": [
                "Arindam Banerjee",
                "Srujana Merugu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/banerjee05b/banerjee05b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced Aggregation of Sparsely Interacting Workers *",
            "abstract": [
                "We consider worker skill estimation for the single-coin Dawid-Skene crowdsourcing model. In practice, skill-estimation is challenging because worker assignments are sparse and irregular due to the arbitrary and uncontrolled availability of workers. We formulate skill estimation as a rank-one correlation-matrix completion problem, where the observed components correspond to observed label correlation between workers. We show that the correlation matrix can be successfully recovered and skills are identifiable if and only if the sampling matrix (observed components) does not have a bipartite connected component. We then propose a projected gradient descent scheme and show that skill estimates converge to the desired global optima for such sampling matrices. Our proof is original and the results are surprising in light of the fact that even the weighted rank-one matrix factorization problem is NP-hard in general. Next, we derive sample complexity bounds in terms of spectral properties of the signless Laplacian of the sampling matrix. Our proposed scheme achieves state-of-art performance on a number of real-world datasets."
            ],
            "keywords": [],
            "author": [
                "Yao Ma",
                "Alex Olshevsky"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-359/19-359.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data",
            "abstract": [
                "We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our first algorithm uses block coordinate descent, and can be interpreted as recursive 1-norm penalized regression. Our second algorithm, based on Nesterov's first order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data."
            ],
            "keywords": [
                "model selection",
                "maximum likelihood estimation",
                "convex optimization",
                "Gaussian graphical model",
                "binary data"
            ],
            "author": [
                "Onureena Banerjee",
                "Laurent El Ghaoui"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/banerjee08a/banerjee08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity",
            "abstract": [
                "The use of convex regularizers allows for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, a popular subclass of-based nonconvex sparsity-inducing and low-rank regularizers is considered. This includes nonconvex variants of lasso, sparse group lasso, tree-structured lasso, nuclear norm and total variation regularizers. We propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex one, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the proximal algorithm, Frank-Wolfe algorithm, alternating direction method of multipliers and stochastic gradient descent). This is further extended to consider cases where the convexified regularizer does not have a closed-form proximal step, and when the loss function is nonconvex nonsmooth. Extensive experiments on a variety of machine learning application scenarios show that optimizing the transformed problem is much faster than running the state-of-the-art on the original problem."
            ],
            "keywords": [
                "Nonconvex optimization",
                "Nonconvex regularization",
                "Proximal algorithm",
                "Frank-Wolfe algorithm",
                "Matrix completion"
            ],
            "author": [
                "Quanming Yao",
                "James T Kwok",
                "Hong Kong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-078/17-078.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Estimation of Sparse Topic Models",
            "abstract": [
                "Topic models have become popular tools for dimension reduction and exploratory analysis of text data which consists in observed frequencies of a vocabulary of p words in n documents, stored in a p × n matrix. The main premise is that the mean of this data matrix can be factorized into a product of two non-negative matrices: a p × K word-topic matrix A and a K × n topic-document matrix W. This paper studies the estimation of A that is possibly element-wise sparse, and the number of topics K is unknown. In this under-explored context, we derive a new minimax lower bound for the estimation of such A and propose a new computationally efficient algorithm for its recovery. We derive a finite sample upper bound for our estimator, and show that it matches the minimax lower bound in many scenarios. Our estimate adapts to the unknown sparsity of A and our analysis is valid for any finite n, p, K and document lengths. Empirical results on both synthetic data and semi-synthetic data show that our proposed estimator is a strong competitor of the existing state-of-the-art algorithms for both non-sparse A and sparse A, and has superior performance is many scenarios of interest."
            ],
            "keywords": [
                "topic models",
                "minimax estimation",
                "sparse estimation",
                "adaptive estimation",
                "high dimensional estimation",
                "non-negative matrix factorization",
                "separability",
                "anchor words"
            ],
            "author": [
                "Xin Bing",
                "Florentina Bunea",
                "Marten Wegkamp"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-079/20-079.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Forest Density Estimation",
            "abstract": [
                "We study graph estimation and density estimation in high dimensions, using a family of density estimators based on forest structured undirected graphical models. For density estimation, we do not assume the true distribution corresponds to a forest; rather, we form kernel density estimates of the bivariate and univariate marginals, and apply Kruskal's algorithm to estimate the optimal forest on held out data. We prove an oracle inequality on the excess risk of the resulting estimator relative to the risk of the best forest. For graph estimation, we consider the problem of estimating forests with restricted tree sizes. We prove that finding a maximum weight spanning forest with restricted tree size is NP-hard, and develop an approximation algorithm for this problem. Viewing the tree size as a complexity parameter, we then select a forest using data splitting, and prove bounds on excess risk and structure selection consistency of the procedure. Experiments with simulated data and microarray data indicate that the methods are a practical alternative to Gaussian graphical models."
            ],
            "keywords": [
                "kernel density estimation",
                "forest structured Markov network",
                "high dimensional inference",
                "risk consistency",
                "structure selection consistency"
            ],
            "author": [
                "Han Liu",
                "Min Xu",
                "Haijie Gu",
                "Anupam Gupta",
                "John Lafferty",
                "Larry Wasserman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/liu11a/liu11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed High-dimensional Regression Under a Quantile Loss Function",
            "abstract": [
                "This paper studies distributed estimation and support recovery for high-dimensional linear regression model with heavy-tailed noise. To deal with heavy-tailed noise whose variance can be infinite, we adopt the quantile regression loss function instead of the commonly used squared loss. However, the non-smooth quantile loss poses new challenges to high-dimensional distributed estimation in both computation and theoretical development. To address the challenge, we transform the response variable and establish a new connection between quantile regression and ordinary linear regression. Then, we provide a distributed estimator that is both computationally and communicationally efficient, where only the gradient information is communicated at each iteration. Theoretically, we show that, after a constant number of iterations, the proposed estimator achieves a near-oracle convergence rate without any restriction on the number of machines. Moreover, we establish the theoretical guarantee for the support recovery. The simulation analysis is provided to demonstrate the effectiveness of our method."
            ],
            "keywords": [
                "Distributed estimation",
                "high-dimensional linear model",
                "quantile loss",
                "robust estimator",
                "support recovery"
            ],
            "author": [
                "Xi Chen",
                "Weidong Liu",
                "Xiaojun Mao",
                "Zhuoyi Yang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-297/20-297.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ultraconservative Online Algorithms for Multiclass Problems",
            "abstract": [
                "In this paper we study a paradigm to generalize online classification algorithms for binary classification problems to multiclass problems. The particular hypotheses we investigate maintain one prototype vector per class. Given an input instance, a multiclass hypothesis computes a similarityscore between each prototype and the input instance and sets the predicted label to be the index of the prototype achieving the highest similarity. To design and analyze the learning algorithms in this paper we introduce the notion of ultraconservativeness. Ultraconservative algorithms are algorithms that update only the prototypes attaining similarity-scores which are higher than the score of the correct label's prototype. We start by describing a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores. We then discuss a specific online algorithm that seeks a set of prototypes which have a small norm. The resulting algorithm, which we term MIRA (for Margin Infused Relaxed Algorithm) is ultraconservative as well. We derive mistake bounds for all the algorithms and provide further analysis of MIRA using a generalized notion of the margin for multiclass problems. We discuss the form the algorithms take in the binary case and show that all the algorithms from the first family reduce to the Perceptron algorithm while MIRA provides a new Perceptron-like algorithm with a margin-dependent learning rate. We then return to multiclass problems and describe an analogous multiplicative family of algorithms with corresponding mistake bounds. We end the formal part by deriving and analyzing a multiclass version of Li and Long's ROMMA algorithm. We conclude with a discussion of experimental results that demonstrate the merits of our algorithms."
            ],
            "keywords": [],
            "author": [
                "Koby Crammer",
                "Manfred K Warmuth"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/crammer03a/crammer03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Convergence for Distributed Learning with Stochastic Gradient Methods and Spectral Algorithms",
            "abstract": [
                "We study generalization properties of distributed algorithms in the setting of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We first investigate distributed stochastic gradient methods (SGM), with mini-batches and multi-passes over the data. We show that optimal generalization error bounds (up to logarithmic factor) can be retained for distributed SGM provided that the partition level is not too large. We then extend our results to spectral algorithms (SA), including kernel ridge regression (KRR), kernel principal component regression, and gradient methods. Our results show that distributed SGM has a smaller theoretical computational complexity, compared with distributed KRR and classic SGM. Moreover, even for a general non-distributed SA, they provide optimal, capacity-dependent convergence rates, for the case that the regression function may not be in the RKHS in the well-conditioned regimes."
            ],
            "keywords": [
                "Kernel Methods",
                "Stochastic Gradient Methods",
                "Regularization",
                "Distributed Learning"
            ],
            "author": [
                "Junhong Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-041/18-041.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Sample Complexity of Learning Linear Predictors with the Squared Loss",
            "abstract": [
                "We provide a tight sample complexity bound for learning bounded-norm linear predictors with respect to the squared loss. Our focus is on an agnostic PAC-style setting, where no assumptions are made on the data distribution beyond boundedness. This contrasts with existing results in the literature, which rely on other distributional assumptions, refer to specific parameter settings, or use other performance measures."
            ],
            "keywords": [
                "sample complexity",
                "squared loss",
                "linear predictors",
                "distribution-free learning"
            ],
            "author": [
                "Ohad Shamir"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/shamir15a/shamir15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Reinforcement Learning for Closed-Loop Propofol Anesthesia: A Study in Human Volunteers",
            "abstract": [
                "Clinical research has demonstrated the efficacy of closed-loop control of anesthesia using the bispectral index of the electroencephalogram as the controlled variable. These controllers have evolved to yield patient-specific anesthesia, which is associated with improved patient outcomes. Despite progress, the problem of patient-specific anesthesia remains unsolved. A variety of factors confound good control, including variations in human physiology, imperfect measures of drug effect, and delayed, hysteretic response to drug delivery. Reinforcement learning (RL) appears to be uniquely equipped to overcome these challenges; however, the literature offers no precedent for RL in anesthesia. To begin exploring the role RL might play in improving anesthetic care, we investigated the method's application in the delivery of patient-specific, propofol-induced hypnosis in human volunteers. When compared to performance metrics reported in the anesthesia literature, RL demonstrated patient-specific control marked by improved accuracy and stability. Furthermore, these results suggest that RL may be considered a viable alternative for solving other difficult closed-loop control problems in medicine. More rigorous clinical study, beyond the confines of controlled human volunteer studies, is needed to substantiate these findings."
            ],
            "keywords": [
                "reinforcement learning",
                "bispectral index",
                "propofol",
                "anesthesia",
                "hypnosis",
                "closed-loop control"
            ],
            "author": [
                "Brett L Moore",
                "Larry D Pyeatt",
                "Vivekanand Kulkarni"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/moore14a/moore14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stability and Generalization",
            "abstract": [
                "We define notions of stability for learning algorithms and show how to use these notions to derive generalization error bounds based on the empirical error and the leave-one-out error. The methods we use can be applied in the regression framework as well as in the classification one when the classifier is obtained by thresholding a real-valued function. We study the stability properties of large classes of learning algorithms such as regularization based algorithms. In particular we focus on Hilbert space regularization and Kullback-Leibler regularization. We demonstrate how to apply the results to SVM for regression and classification."
            ],
            "keywords": [],
            "author": [
                "Olivier Bousquet"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Making Decision Trees Feasible in Ultrahigh Feature and Label Dimensions",
            "abstract": [
                "Due to the non-linear but highly interpretable representations, decision tree (DT) models have significantly attracted a lot of attention of researchers. However, it is difficult to understand and interpret DT models in ultrahigh dimensions and DT models usually suffer from the curse of dimensionality and achieve degenerated performance when there are many noisy features. To address these issues, this paper first presents a novel data-dependent generalization error bound for the perceptron decision tree (PDT), which provides the theoretical justification to learn a sparse linear hyperplane in each decision node and to prune the tree. Following our analysis, we introduce the notion of budget-aware classifier (BAC) with a budget constraint on the weight coefficients, and propose a supervised budgeted tree (SBT) algorithm to achieve non-linear prediction performance. To avoid generating an unstable and complicated decision tree and improve the generalization of the SBT, we present a pruning strategy by learning classifiers to minimize cross-validation errors on each BAC. To deal with ultrahigh label dimensions, based on three important phenomena of real-world data sets from a variety of application domains, we develop a sparse coding tree framework for multi-label annotation problems and provide the theoretical analysis. Extensive empirical studies verify that 1) SBT is easy to understand and interpret in ultrahigh dimensions and is more resilient to noisy features. 2) Compared with state-of-the-art algorithms, our proposed sparse coding tree framework is more efficient, yet accurate in ultrahigh label and feature dimensions."
            ],
            "keywords": [
                "Classification",
                "Ultrahigh Feature Dimensions",
                "Ultrahigh Label Dimensions",
                "Perceptron Decision Tree"
            ],
            "author": [
                "Weiwei Liu",
                "Ivor W Tsang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-466/16-466.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PREA: Personalized Recommendation Algorithms Toolkit",
            "abstract": [
                "Recommendation systems are important business applications with significant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms."
            ],
            "keywords": [
                "recommender systems",
                "collaborative filtering",
                "evaluation metrics"
            ],
            "author": [
                "Joonseok Lee"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/lee12b/lee12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cover Tree Bayesian Reinforcement Learning",
            "abstract": [
                "This paper proposes an online tree-based Bayesian approach for reinforcement learning. For inference, we employ a generalised context tree model. This defines a distribution on multivariate Gaussian piecewise-linear models, which can be updated in closed form. The tree structure itself is constructed using the cover tree method, which remains efficient in high dimensional spaces. We combine the model with Thompson sampling and approximate dynamic programming to obtain effective exploration policies in unknown environments. The flexibility and computational simplicity of the model render it suitable for many reinforcement learning problems in continuous state spaces. We demonstrate this in an experimental comparison with a Gaussian process model, a linear model and simple least squares policy iteration."
            ],
            "keywords": [
                "Bayesian inference",
                "non-parametric statistics",
                "reinforcement learning"
            ],
            "author": [
                "Nikolaos Tziortziotis",
                "Christos Dimitrakakis",
                "Konstantinos Blekas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/tziortziotis14a/tziortziotis14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model Selection in Bayesian Neural Networks via Horseshoe Priors",
            "abstract": [
                "The promise of augmenting accurate predictions provided by modern neural networks with well-calibrated predictive uncertainties has reinvigorated interest in Bayesian neural networks. However, model selection-even choosing the number of nodes-remains an open question. Poor choices can severely affect the quality of the produced uncertainties. In this paper, we explore continuous shrinkage priors, the horseshoe, and the regularized horseshoe distributions, for model selection in Bayesian neural networks. When placed over node pre-activations and coupled with appropriate variational approximations, we find that the strong shrinkage provided by the horseshoe is effective at turning off nodes that do not help explain the data. We demonstrate that our approach finds compact network structures even when the number of nodes required is grossly overestimated. Moreover, the model selection over the number of nodes does not come at the expense of predictive or computational performance; in fact, we learn smaller networks with comparable predictive performance to current approaches. These effects are particularly apparent in sample-limited settings, such as small data sets and reinforcement learning."
            ],
            "keywords": [
                "Bayesian Neural Networks",
                "Model Selection",
                "Horseshoe Priors",
                "Variational Inference",
                "Structured approximations"
            ],
            "author": [
                "Soumya Ghosh",
                "Jiayu Yao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-236/19-236.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion",
            "abstract": [
                "We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed."
            ],
            "keywords": [
                "1-bit matrix completion",
                "low-rank matrix",
                "max-norm",
                "trace-norm",
                "constrained optimization",
                "maximum likelihood estimate",
                "optimal rate of convergence"
            ],
            "author": [
                "Tony Cai",
                "Wen-Xin Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/cai13b/cai13b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Discovery from Heterogeneous/Nonstationary Data",
            "abstract": [
                "It is commonplace to encounter heterogeneous or nonstationary data, of which the underlying generating process changes across domains or over time. Such a distribution shift feature presents both challenges and opportunities for causal discovery. In this paper, we develop a framework for causal discovery from such data, called Constraint-based causal Discovery from heterogeneous/NOnstationary Data (CD-NOD), to find causal skeleton and directions and estimate the properties of mechanism changes. First, we propose an enhanced constraintbased procedure to detect variables whose local mechanisms change and recover the skeleton of the causal structure over observed variables. Second, we present a method to determine causal orientations by making use of independent changes in the data distribution implied by the underlying causal model, benefiting from information carried by changing distributions. After learning the causal structure, next, we investigate how to efficiently estimate the \"driving force\" of the nonstationarity of a causal mechanism. That is, we aim to extract from data a low-dimensional representation of changes. The proposed methods are nonparametric, with no hard restrictions on data distributions and causal mechanisms, and do not rely on window segmentation. Furthermore, we find that data heterogeneity benefits causal structure identification even with particular types of confounders. Finally, we show the connection between heterogeneity/nonstationarity and soft intervention in causal discovery. Experimental results on various synthetic and real-world data sets (task-fMRI and stock market data) are presented to demonstrate the efficacy of the proposed methods."
            ],
            "keywords": [
                "causal discovery",
                "heterogeneous/nonstationary data",
                "independent-change principle",
                "kernel distribution embedding",
                "driving force estimation",
                "confounder * . Equal contribution"
            ],
            "author": [
                "Biwei Huang",
                "Kun Zhang",
                "Jiji Zhang",
                "Joseph Ramsey",
                "Ruben Sanchez-Romero",
                "Clark Glymour",
                "Bernhard Schölkopf",
                "Kun Huang",
                "Jiji Zhang",
                "Joseph Zhang",
                "Ruben Ramsey",
                "Clark Sanchez-Romero",
                "Bernhard Glymour"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-232/19-232.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "openXBOW -Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit",
            "abstract": [
                "We introduce openXBOW, an open-source toolkit for the generation of bag-of-words (BoW) representations from multimodal input. In the BoW principle, word histograms were first used as features in document classification, but the idea was and can easily be adapted to, e. g., acoustic or visual descriptors, introducing a prior step of vector quantisation. The openXBOW toolkit supports arbitrary numeric input features and text input and concatenates computed sub-bags to a final bag. It provides a variety of extensions and options. To our knowledge, openXBOW is the first publicly available toolkit for the generation of crossmodal bags-of-words. The capabilities of the tool have been exemplified in different scenarios: sentiment analysis in tweets, classification of snore sounds, and time-dependent emotion recognition based on acoustic, linguistic, and visual information, where improved results over other feature representations were observed."
            ],
            "keywords": [
                "bag-of-words",
                "multimodal signal processing",
                "histogram feature representations",
                "feature learning"
            ],
            "author": [
                "Maximilian Schmitt",
                "* B Schuller"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-113/17-113.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-layered Gesture Recognition with Kinect",
            "abstract": [
                "This paper proposes a novel multi-layered gesture recognition method with Kinect. We explore the essential linguistic characters of gestures: the components concurrent character and the sequential organization character, in a multi-layered framework, which extracts features from both the segmented semantic units and the whole gesture sequence and then sequentially classifies the motion, location and shape components. In the first layer, an improved principle motion is applied to model the motion component. In the second layer, a particle-based descriptor and a weighted dynamic time warping are proposed for the location component classification. In the last layer, the spatial path warping is further proposed to classify the shape component represented by unclosed shape context. The proposed method can obtain relatively high performance for one-shot learning gesture recognition on the ChaLearn Gesture Dataset comprising more than 50, 000 gesture sequences recorded with Kinect."
            ],
            "keywords": [
                "gesture recognition",
                "Kinect",
                "linguistic characters",
                "multi-layered classification",
                "principle motion",
                "dynamic time warping"
            ],
            "author": [
                "Feng Jiang",
                "Shengping Zhang",
                "Shen Wu",
                "Yang Gao",
                "Debin Zhao",
                "Isabelle Guyon",
                "Vassilis Athitsos",
                "Sergio Escalera"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/jiang15a/jiang15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonlinear Estimators and Tail Bounds for Dimension Reduction in l 1 Using Cauchy Random Projections",
            "abstract": [
                "For 1 dimension reduction in the l 1 norm, the method of Cauchy random projections multiplies the original data matrix A ∈ R n×D with a random matrix R ∈ R D×k (k D) whose entries are i.i.d. samples of the standard Cauchy C(0, 1). Because of the impossibility result, one can not hope to recover the pairwise l 1 distances in A from B = A × R ∈ R n×k , using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O log n ε 2 suffices with the constants explicitly given. Asymptotically (as k → ∞), both the sample median and the geometric mean estimators are about 80% efficient compared to the MLE. We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian."
            ],
            "keywords": [
                "dimension reduction",
                "l 1 norm",
                "Johnson-Lindenstrauss (JL) lemma",
                "Cauchy random projections"
            ],
            "author": [
                "Ping Li",
                "Trevor J Hastie",
                "Stanford Edu",
                "Kenneth W Church"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/li07b/li07b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Need for Open Source Software in Machine Learning",
            "abstract": [
                "Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the field of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be significantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientific community."
            ],
            "keywords": [
                "machine learning",
                "open source",
                "reproducibility",
                "creditability",
                "algorithms",
                "software"
            ],
            "author": [
                "Sören Sonnenburg",
                "Cheng Mikio L Braun",
                "Soon Cheng",
                "Samy Bengio",
                "Leon Bottou",
                "Geoffrey Holmes",
                "Klaus- Robert Robert Müller",
                "Fernando Pereira",
                "Carl Edward Rasmussen",
                "Soon Ong",
                "Yann Lecun",
                "Gunnar Rätsch",
                "Bernhard Schölkopf",
                "Alexander Smola",
                "Pascal Vincent",
                "Jason Weston",
                "Robert C Williamson",
                "BRAUN, ONG A L Sonnenburg",
                "Gmail Com"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/sonnenburg07a/sonnenburg07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "iNNvestigate Neural Networks!",
            "abstract": [
                "In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and predictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this shortcoming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-thebox implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures."
            ],
            "keywords": [
                "Artificial neural networks",
                "deep learning",
                "analyzing classifiers",
                "explaining classifiers",
                "computer vision"
            ],
            "author": [
                "Maximilian Alber",
                "Sebastian Lapuschkin",
                "Philipp Seegerer",
                "Miriam Hägele",
                "Grégoire Montavon",
                "Wojciech Samek",
                "Klaus-Robert Robert Müller",
                "Sven Dähne",
                "Pieter-Jan Kindermans",
                "Kristof T Schütt"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-540/18-540.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Information, Divergence and Risk for Binary Experiments",
            "abstract": [
                "We unify f-divergences, Bregman divergences, surrogate regret bounds, proper scoring rules, cost curves, ROC-curves and statistical information. We do this by systematically studying integral and variational representations of these objects and in so doing identify their representation primitives which all are related to cost-sensitive binary classification. As well as developing relationships between generative and discriminative views of learning, the new machinery leads to tight and more general surrogate regret bounds and generalised Pinsker inequalities relating f-divergences to variational divergence. The new viewpoint also illuminates existing algorithms: it provides a new derivation of Support Vector Machines in terms of divergences and relates maximum mean discrepancy to Fisher linear discriminants."
            ],
            "keywords": [
                "classification",
                "loss functions",
                "divergence",
                "statistical information",
                "regret bounds"
            ],
            "author": [
                "Mark D Reid",
                "Robert C Williamson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/reid11a/reid11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lower Bounds for Learning Distributions under Communication Constraints via Fisher Information",
            "abstract": [
                "We consider the problem of learning high-dimensional, nonparametric and structured (e.g., Gaussian) distributions in distributed networks, where each node in the network observes an independent sample from the underlying distribution and can use k bits to communicate its sample to a central processor. We consider three different models for communication. Under the independent model, each node communicates its sample to a central processor by independently encoding it into k bits. Under the more general sequential or blackboard communication models, nodes can share information interactively but each node is restricted to write at most k bits on the final transcript. We characterize the impact of the communication constraint k on the minimax risk of estimating the underlying distribution under 2 loss. We develop minimax lower bounds that apply in a unified way to many common statistical models and reveal that the impact of the communication constraint can be qualitatively different depending on the tail behavior of the score function associated with each model. A key ingredient in our proofs is a geometric characterization of Fisher information from quantized samples."
            ],
            "keywords": [
                "Fisher information",
                "statistical estimation",
                "communication constraints",
                "learning distributions"
            ],
            "author": [
                "Leighton Pate Barnes",
                "Yanjun Han"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-737/19-737.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Steering Social Activity: A Stochastic Optimal Control Point Of View *",
            "abstract": [
                "User engagement in online social networking depends critically on the level of social activity in the corresponding platform-the number of online actions, such as posts, shares or replies, taken by their users. Can we design data-driven algorithms to increase social activity? At a user level, such algorithms may increase activity by helping users decide when to take an action to be more likely to be noticed by their peers. At a network level, they may increase activity by incentivizing a few influential users to take more actions, which in turn will trigger additional actions by other users."
            ],
            "keywords": [
                "marked temporal point processes",
                "stochastic optimal control",
                "stochastic differential equations with jumps",
                "social networks",
                "information networks"
            ],
            "author": [
                "Ali Zarezade",
                "Abir De",
                "Utkarsh Upadhyay",
                "Hamid R Rabiee",
                "Manuel Gomez-Rodriguez"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-416/17-416.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rank-based Lasso -efficient methods for high-dimensional robust model selection",
            "abstract": [
                "We consider the problem of identifying significant predictors in large data bases, where the response variable depends on the linear combination of explanatory variables through an unknown monotonic link function, corrupted with the noise from the unknown distribution. We utilize the natural, robust and efficient approach, which relies on replacing values of the response variables by their ranks and then identifying significant predictors by using well known Lasso. We provide new consistency results for the proposed procedure (called ,,RankLasso\") and extend the scope of its applications by proposing its thresholded and adaptive versions. Our theoretical results show that these modifications can identify the set of relevant predictors under a wide range of data generating scenarios. Theoretical results are supported by the simulation study and the real data analysis, which show that our methods can properly identify relevant predictors, even when the error terms come from the Cauchy distribution and the link function is nonlinear. They also demonstrate the superiority of the modified versions of RankLasso over its regular version in the case when predictors are substantially correlated. The numerical study shows also that RankLasso performs substantially better in model selection than LADLasso, which is a well established methodology for robust model selection."
            ],
            "keywords": [
                "Lasso",
                "Model Selection",
                "Ranks",
                "Single Index Model",
                "Sparsity",
                "U -statistics"
            ],
            "author": [
                "Wojciech Rejchel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-120/20-120.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Encog: Library of Interchangeable Machine Learning Models for Java and C#",
            "abstract": [
                "This paper introduces the Encog library for Java and C#, a scalable, adaptable, multiplatform machine learning framework that was first released in 2008. Encog allows a variety of machine learning models to be applied to data sets using regression, classification, and clustering. Various supported machine learning models can be used interchangeably with minimal recoding. Encog uses efficient multithreaded code to reduce training time by exploiting modern multicore processors. The current version of Encog can be downloaded from http://www.encog.org."
            ],
            "keywords": [
                "Java",
                "C#",
                "neural network",
                "support vector machine",
                "open source software"
            ],
            "author": [
                "Jeff Heaton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/heaton15a/heaton15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Complexity of Best-Arm Identification in Multi-Armed Bandit Models",
            "abstract": [
                "The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the m best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when m ≥ 1 under general assumptions. In the specific case of two armed-bandits, we derive refined lower bounds in both the fixedconfidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed-budget setting may be smaller than the complexity of the fixed-confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest : a deviation lemma for self-normalized sums (Lemma 7) and a novel change of measure inequality for bandit models (Lemma 1)."
            ],
            "keywords": [
                "multi-armed bandit",
                "best-arm identification",
                "pure exploration",
                "informationtheoretic divergences",
                "sequential testing"
            ],
            "author": [
                "Emilie Kaufmann",
                "Olivier Cappé"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/kaufman16a/kaufman16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Multilabel Classification and Ranking with Bandit Feedback",
            "abstract": [
                "We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show O(T 1/2 log T) regret bounds, which improve in several ways on the existing results. We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on diverse real-world multilabel data sets, often obtaining comparable performance."
            ],
            "keywords": [
                "contextual bandits",
                "structured prediction",
                "ranking",
                "online learning",
                "regret bounds",
                "generalized linear"
            ],
            "author": [
                "Claudio Gentile",
                "Francesco Orabona"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/gentile14a/gentile14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Graph Based Decomposition of Factored MDPs",
            "abstract": [
                "We present Variable Influence Structure Analysis, or VISA, an algorithm that performs hierarchical decomposition of factored Markov decision processes. VISA uses a dynamic Bayesian network model of actions, and constructs a causal graph that captures relationships between state variables. In tasks with sparse causal graphs VISA exploits structure by introducing activities that cause the values of state variables to change. The result is a hierarchy of activities that together represent a solution to the original task. VISA performs state abstraction for each activity by ignoring irrelevant state variables and lower-level activities. In addition, we describe an algorithm for constructing compact models of the activities introduced. State abstraction and compact activity models enable VISA to apply efficient algorithms to solve the stand-alone subtask associated with each activity. Experimental results show that the decomposition introduced by VISA can significantly accelerate construction of an optimal, or near-optimal, policy."
            ],
            "keywords": [
                "Markov decision processes",
                "hierarchical decomposition",
                "state abstraction"
            ],
            "author": [
                "Anders Jonsson",
                "Andrew Barto"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/jonsson06a/jonsson06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Learning of Bayesian Network Classifiers",
            "abstract": [
                "Ever increasing data quantity makes ever more urgent the need for highly scalable learners that have good classification performance. Therefore, an out-of-core learner with excellent time and space complexity, along with high expressivity (that is, capacity to learn very complex multivariate probability distributions) is extremely desirable. This paper presents such a learner. We propose an extension to the k-dependence Bayesian classifier (KDB) that discriminatively selects a sub-model of a full KDB classifier. It requires only one additional pass through the training data, making it a three-pass learner. Our extensive experimental evaluation on 16 large data sets reveals that this out-of-core algorithm achieves competitive classification performance, and substantially better training and classification time than state-of-the-art in-core learners such as random forest and linear and non-linear logistic regression."
            ],
            "keywords": [
                "scalable Bayesian classification",
                "feature selection",
                "out-of-core learning",
                "big data"
            ],
            "author": [
                "Ana M Martínez",
                "Shenglei Chen",
                "Nayyar A Zaidi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/martinez16a/martinez16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Collaborative Multiagent Reinforcement Learning by Payoff Propagation",
            "abstract": [
                "In this article we describe a set of scalable techniques for learning the behavior of a group of agents in a collaborative multiagent setting. As a basis we use the framework of coordination graphs of Guestrin, Koller, and Parr (2002a) which exploits the dependencies between agents to decompose the global payoff function into a sum of local terms. First, we deal with the single-state case and describe a payoff propagation algorithm that computes the individual actions that approximately maximize the global payoff function. The method can be viewed as the decision-making analogue of belief propagation in Bayesian networks. Second, we focus on learning the behavior of the agents in sequential decision-making tasks. We introduce different model-free reinforcementlearning techniques, unitedly called Sparse Cooperative Q-learning, which approximate the global action-value function based on the topology of a coordination graph, and perform updates using the contribution of the individual agents to the maximal global action value. The combined use of an edge-based decomposition of the action-value function and the payoff propagation algorithm for efficient action selection, result in an approach that scales only linearly in the problem size. We provide experimental evidence that our method outperforms related multiagent reinforcement-learning methods based on temporal differences."
            ],
            "keywords": [
                "collaborative multiagent system",
                "coordination graph",
                "reinforcement learning",
                "Qlearning",
                "belief propagation"
            ],
            "author": [
                "Jelle R Kok",
                "Nikos Vlassis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/kok06a/kok06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach",
            "abstract": [
                "The paper addresses the problem of learning a regression model parameterized by a fixed-rank positive semidefinite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of fixedrank positive semidefinite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semidefinite matrix. Good performance is observed on classical benchmarks."
            ],
            "keywords": [
                "linear regression",
                "positive semidefinite matrices",
                "low-rank approximation",
                "Riemannian geometry",
                "gradient-based learning"
            ],
            "author": [
                "Gilles Meyer",
                "Rodolphe Sepulchre"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/meyer11a/meyer11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Inductive Synthesis of Functional Programs: An Explanation Based Generalization Approach",
            "abstract": [
                "We describe an approach to the inductive synthesis of recursive equations from input/outputexamples which is based on the classical two-step approach to induction of functional Lisp programs of Summers (1977). In a first step, I/O-examples are rewritten to traces which explain the outputs given the respective inputs based on a datatype theory. These traces can be integrated into one conditional expression which represents a non-recursive program. In a second step, this initial program term is generalized into recursive equations by searching for syntactical regularities in the term. Our approach extends the classical work in several aspects. The most important extensions are that we are able to induce a set of recursive equations in one synthesizing step, the equations may contain more than one recursive call, and additionally needed parameters are automatically introduced."
            ],
            "keywords": [
                "inductive program synthesis",
                "inductive functional programming",
                "explanation based generalization",
                "recursive program schemes"
            ],
            "author": [
                "Emanuel Kitzelmann",
                "Ute Schmid",
                "Roland Olsson",
                "Leslie Pack"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/kitzelmann06a/kitzelmann06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs",
            "abstract": [
                "This paper develops a fast method for solving linear SVMs with L 2 loss function that is suited for large scale data mining tasks such as text classification. This is done by modifying the finite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVM light , SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modified Huber's loss function and the L 1 loss function, and also for solving ordinal regression."
            ],
            "keywords": [
                "linear SVMs",
                "classification",
                "conjugate gradient"
            ],
            "author": [
                "S Sathiya Keerthi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/keerthi05a/keerthi05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generic Inference in Latent Gaussian Process Models",
            "abstract": [
                "We develop an automated variational method for inference in models with Gaussian process (gp) priors and general likelihoods. The method supports multiple outputs and multiple latent functions and does not require detailed knowledge of the conditional likelihood, only needing its evaluation as a black-box function. Using a mixture of Gaussians as the variational distribution, we show that the evidence lower bound and its gradients can be estimated efficiently using samples from univariate Gaussian distributions. Furthermore, the method is scalable to large datasets which is achieved by using an augmented prior via the inducing-variable approach underpinning most sparse gp approximations, along with parallel computation and stochastic optimization. We evaluate our approach quantitatively and qualitatively with experiments on small datasets, medium-scale datasets and large datasets, showing its competitiveness under different likelihood models and sparsity levels. On the large-scale experiments involving prediction of airline delays and classification of handwritten digits, we show that our method is on par with the state-of-the-art hard-coded approaches for scalable gp regression and classification."
            ],
            "keywords": [
                "Gaussian processes",
                "black-box likelihoods",
                "nonlinear likelihoods",
                "scalable inference",
                "variational inference"
            ],
            "author": [
                "Edwin V Bonilla",
                "Karl Krauth",
                "Amir Dezfouli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/16-437/16-437.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets",
            "abstract": [
                "This paper describes the ecosystem of R add-on packages developed around the infrastructure provided by the package arules. The packages provide comprehensive functionality for analyzing interesting patterns including frequent itemsets, association rules, frequent sequences and for building applications like associative classification. After discussing the ecosystem's design we illustrate the ease of mining and visualizing rules with a short example."
            ],
            "keywords": [
                "frequent itemsets",
                "association rules",
                "frequent sequences",
                "visualization"
            ],
            "author": [
                "Michael Hahsler",
                "Sudheer Chelluboina",
                "Kurt Hornik",
                "Christian Buchta"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/hahsler11a/hahsler11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Switching Regression Models and Causal Inference in the Presence of Discrete Latent Variables",
            "abstract": [
                "Given a response Y and a vector X = (X 1 ,. .. , X d) of d predictors, we investigate the problem of inferring direct causes of Y among the vector X. Models for Y that use all of its causal covariates as predictors enjoy the property of being invariant across different environments or interventional settings. Given data from such environments, this property has been exploited for causal discovery. Here, we extend this inference principle to situations in which some (discrete-valued) direct causes of Y are unobserved. Such cases naturally give rise to switching regression models. We provide sufficient conditions for the existence, consistency and asymptotic normality of the MLE in linear switching regression models with Gaussian noise, and construct a test for the equality of such models. These results allow us to prove that the proposed causal discovery method obtains asymptotic false discovery control under mild conditions. We provide an algorithm, make available code, and test our method on simulated data. It is robust against model violations and outperforms state-of-the-art approaches. We further apply our method to a real data set, where we show that it does not only output causal predictors, but also a process-based clustering of data points, which could be of additional interest to practitioners."
            ],
            "keywords": [
                "causal discovery",
                "invariance",
                "switching regression models",
                "hidden Markov models",
                "latent variables"
            ],
            "author": [
                "Rune Christiansen",
                "Jonas Peters"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-407/19-407.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Gesture Recognition Toolkit",
            "abstract": [
                "The Gesture Recognition Toolkit is a cross-platform open-source C++ library designed to make real-time machine learning and gesture recognition more accessible for non-specialists. Emphasis is placed on ease of use, with a consistent, minimalist design that promotes accessibility while supporting flexibility and customization for advanced users. The toolkit features a broad range of classification and regression algorithms and has extensive support for building real-time systems. This includes algorithms for signal processing, feature extraction and automatic gesture spotting."
            ],
            "keywords": [
                "gesture recognition",
                "machine learning",
                "C++",
                "open source",
                "classification",
                "regression",
                "clustering",
                "gesture spotting",
                "feature extraction",
                "signal processing"
            ],
            "author": [
                "Nicholas Gillian",
                "Joseph A Paradiso"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/gillian14a/gillian14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm",
            "abstract": [
                "We consider the PC-algorithm (Spirtes et al., 2000) for estimating the skeleton and equivalence class of a very high-dimensional directed acyclic graph (DAG) with corresponding Gaussian distribution. The PC-algorithm is computationally feasible and often very fast for sparse problems with many nodes (variables), and it has the attractive property to automatically achieve high computational efficiency as a function of sparseness of the true underlying DAG. We prove uniform consistency of the algorithm for very high-dimensional, sparse DAGs where the number of nodes is allowed to quickly grow with sample size n, as fast as O(n a) for any 0 < a < ∞. The sparseness assumption is rather minimal requiring only that the neighborhoods in the DAG are of lower order than sample size n. We also demonstrate the PC-algorithm for simulated data."
            ],
            "keywords": [
                "asymptotic consistency",
                "DAG",
                "graphical model",
                "PC-algorithm",
                "skeleton"
            ],
            "author": [
                "Markus Kalisch",
                "Peter Bühlmann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/kalisch07a/kalisch07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rearrangement Clustering: Pitfalls, Remedies, and Applications",
            "abstract": [
                "Given a matrix of values in which the rows correspond to objects and the columns correspond to features of the objects, rearrangement clustering is the problem of rearranging the rows of the matrix such that the sum of the similarities between adjacent rows is maximized. Referred to by various names and reinvented several times, this clustering technique has been extensively used in many fields over the last three decades. In this paper, we point out two critical pitfalls that have been previously overlooked. The first pitfall is deleterious when rearrangement clustering is applied to objects that form natural clusters. The second concerns a similarity metric that is commonly used. We present an algorithm that overcomes these pitfalls. This algorithm is based on a variation of the Traveling Salesman Problem. It offers an extra benefit as it automatically determines cluster boundaries. Using this algorithm, we optimally solve four benchmark problems and a 2,467-gene expression data clustering problem. As expected, our new algorithm identifies better clusters than those found by previous approaches in all five cases. Overall, our results demonstrate the benefits of rectifying the pitfalls and exemplify the usefulness of this clustering technique. Our code is available at our websites."
            ],
            "keywords": [
                "clustering",
                "visualization of patterns in data",
                "bond energy algorithm",
                "traveling salesman problem",
                "asymmetric clustering"
            ],
            "author": [
                "Sharlee Climer",
                "Weixiong Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/climer06a/climer06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Waffles: A Machine Learning Toolkit",
            "abstract": [
                "We present a breadth-oriented collection of cross-platform command-line tools for researchers in machine learning called Waffles. The Waffles tools are designed to offer a broad spectrum of functionality in a manner that is friendly for scripted automation. All functionality is also available in a C++ class library. Waffles is available under the GNU Lesser General Public License."
            ],
            "keywords": [
                "machine learning",
                "toolkits",
                "data mining",
                "C++",
                "open source"
            ],
            "author": [
                "Mike Gashler"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/gashler11a/gashler11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Communication-Efficient Distributed Optimization in Networks with Gradient Tracking and Variance Reduction",
            "abstract": [
                "There is growing interest in large-scale machine learning and optimization over decentralized networks, e.g. in the context of multi-agent learning and federated learning. Due to the imminent need to alleviate the communication burden, the investigation of communicationefficient distributed optimization algorithms-particularly for empirical risk minimization-has flourished in recent years. A large fraction of these algorithms have been developed for the master/slave setting, relying on the presence of a central parameter server that can communicate with all agents. This paper focuses on distributed optimization over networks, or decentralized optimization, where each agent is only allowed to aggregate information from its neighbors over a network (namely, no centralized coordination is present). By properly adjusting the global gradient estimate via local averaging in conjunction with proper correction, we develop a communication-efficient approximate Newton-type method, called Network-DANE, which generalizes DANE to accommodate decentralized scenarios. Our key ideas can be applied, in a systematic manner, to obtain decentralized versions of other master/slave distributed algorithms. A notable development is Network-SVRG/SARAH, which employs variance reduction at each agent to further accelerate local computation. We establish linear convergence of Network-DANE and Network-SVRG for strongly convex losses, and Network-SARAH for quadratic losses, which shed light on the impacts of data homogeneity, network connectivity, and local averaging upon the rate of convergence. We further extend Network-DANE to composite optimization by allowing a nonsmooth penalty term. Numerical evidence is provided to demonstrate the appealing performance of our algorithms over competitive baselines, in terms of both communication and computation efficiency. Our work suggests that by performing a judiciously chosen amount of local communication and computation per iteration, the overall efficiency can be substantially improved."
            ],
            "keywords": [
                "decentralized optimization",
                "federated learning",
                "communication efficiency",
                "gradient tracking",
                "variance reduction"
            ],
            "author": [
                "Boyue Li",
                "Shicong Cen",
                "Yuxin Chen",
                "Yuejie Chi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-210/20-210.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Refining the Confidence Level for Optimistic Bandit Strategies",
            "abstract": [
                "This paper introduces the first strategy for stochastic bandits with unit variance Gaussian noise that is simultaneously minimax optimal up to constant factors, asymptotically optimal, and never worse than the classical upper confidence bound strategy up to universal constant factors. Preliminary empirical evidence is also promising. Besides this, a conjecture on the optimal form of the regret is shown to be false and a finite-time lower bound on the regret of any strategy is presented that very nearly matches the finite-time upper bound of the newly proposed strategy."
            ],
            "keywords": [
                "Stochastic bandits",
                "sequential decision making",
                "regret minimisation"
            ],
            "author": [
                "Tor Lattimore"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-513/17-513.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximation Hardness for A Class of Sparse Optimization Problems",
            "abstract": [
                "In this paper, we consider three typical optimization problems with a convex loss function and a nonconvex sparse penalty or constraint. For the sparse penalized problem, we prove that finding an O(n c1 d c2)-optimal solution to an n × d problem is strongly NP-hard for any c 1 , c 2 ∈ [0, 1) such that c + c 2 < 1. For two constrained versions of the sparse optimization problem, we show that it is intractable to approximately compute a solution path associated with increasing values of some tuning parameter. The hardness results apply to a broad class of loss functions and sparse penalties. They suggest that one cannot even approximately solve these three problems in polynomial time, unless P = NP."
            ],
            "keywords": [
                "nonconvex optimization",
                "computational complexity",
                "variable selection",
                "NPhardness",
                "sparsity"
            ],
            "author": [
                "Yichen Chen",
                "Mengdi Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-373/17-373.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Adaptive Estimation of Nonparametric Hidden Markov Models",
            "abstract": [
                "We consider stationary hidden Markov models with finite state space and nonparametric modeling of the emission distributions. It has remained unknown until very recently that such models are identifiable. In this paper, we propose a new penalized least-squares estimator for the emission distributions which is statistically optimal and practically tractable. We prove a non asymptotic oracle inequality for our nonparametric estimator of the emission distributions. A consequence is that this new estimator is rate minimax adaptive up to a logarithmic term. Our methodology is based on projections of the emission distributions onto nested subspaces of increasing complexity. The popular spectral estimators are unable to achieve the optimal rate but may be used as initial points in our procedure. Simulations are given that show the improvement obtained when applying the least-squares minimization consecutively to the spectral estimation."
            ],
            "keywords": [
                "nonparametric estimation",
                "hidden Markov models",
                "minimax adaptive estimation",
                "oracle inequality",
                "penalized least-squares"
            ],
            "author": [
                "Yohann De Castro",
                "Claire Lacour"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-381/15-381.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SnapVX: A Network-Based Convex Optimization Solver",
            "abstract": [
                "SnapVX is a high-performance solver for convex optimization problems defined on networks. For problems of this form, SnapVX provides a fast and scalable solution with guaranteed global convergence. It combines the capabilities of two open source software packages: Snap.py and CVXPY. Snap.py is a large scale graph processing library, and CVXPY provides a general modeling framework for small-scale subproblems. SnapVX offers a customizable yet easy-to-use Python interface with \"out-of-the-box\" functionality. Based on the Alternating Direction Method of Multipliers (ADMM), it is able to efficiently store, analyze, parallelize, and solve large optimization problems from a variety of different applications. Documentation, examples, and more can be found on the SnapVX website at http://snap.stanford.edu/snapvx."
            ],
            "keywords": [
                "convex optimization",
                "network analytics",
                "graphs",
                "data mining",
                "ADMM"
            ],
            "author": [
                "David Hallac",
                "Christopher Wong",
                "Steven Diamond",
                "Rok Sosič",
                "Stephen Boyd"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-492/15-492.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Rates for the Stochastic Gradient Descent Method for Non-Convex Objective Functions",
            "abstract": [
                "We prove the convergence to minima and estimates on the rate of convergence for the stochastic gradient descent method in the case of not necessarily locally convex nor contracting objective functions. In particular, the analysis relies on a quantitative use of mini-batches to control the loss of iterates to non-attracted regions. The applicability of the results to simple objective functions arising in machine learning is shown."
            ],
            "keywords": [
                "stochastic gradient descent",
                "mini-batch algorithm",
                "machine learning",
                "nonconvex optimization"
            ],
            "author": [
                "Benjamin Fehrman",
                "Benjamin Gess",
                "Arnulf Jentzen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-636/19-636.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimating Labels from Label Proportions *",
            "abstract": [
                "Consider the following problem: given sets of unlabeled observations, each set with known label proportions, predict the labels of another set of observations, possibly with known label proportions. This problem occurs in areas like e-commerce, politics, spam filtering and improper content detection. We present consistent estimators which can reconstruct the correct labels with high probability in a uniform convergence sense. Experiments show that our method works well in practice."
            ],
            "keywords": [
                "unsupervised learning",
                "Gaussian processes",
                "classification and prediction",
                "probabilistic models",
                "missing variables"
            ],
            "author": [
                "Novi Quadrianto",
                "Alex J Smola",
                "Tibério S Caetano"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/quadrianto09a/quadrianto09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Saturating Splines and Feature Selection",
            "abstract": [
                "We extend the adaptive regression spline model by incorporating saturation, the natural requirement that a function extend as a constant outside a certain range. We fit saturating splines to data via a convex optimization problem over a space of measures, which we solve using an efficient algorithm based on the conditional gradient method. Unlike many existing approaches, our algorithm solves the original infinite-dimensional (for splines of degree at least two) optimization problem without pre-specified knot locations. We then adapt our algorithm to fit generalized additive models with saturating splines as coordinate functions and show that the saturation requirement allows our model to simultaneously perform feature selection and nonlinear function fitting. Finally, we briefly sketch how the method can be extended to higher order splines and to different requirements on the extension outside the data range."
            ],
            "keywords": [
                "Convex optimization",
                "feature selection",
                "splines",
                "lasso",
                "regression"
            ],
            "author": [
                "Nicholas Boyd",
                "Trevor Hastie",
                "Stephen Boyd",
                "Benjamin Recht",
                "Michael I Jordan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-178/17-178.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GraKeL: A Graph Kernel Library in Python",
            "abstract": [
                "The problem of accurately measuring the similarity between graphs is at the core of many applications in a variety of disciplines. Graph kernels have recently emerged as a promising approach to this problem. There are now many kernels, each focusing on different structural aspects of graphs. Here, we present GraKeL, a library that unifies several graph kernels into a common framework. The library is written in Python and adheres to the scikit-learn interface. It is simple to use and can be naturally combined with scikit-learn's modules to build a complete machine learning pipeline for tasks such as graph classification and clustering."
            ],
            "keywords": [
                "graph similarity",
                "graph kernels",
                "scikit-learn",
                "Python"
            ],
            "author": [
                "Giannis Siglidis",
                "Christos Giatsidis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-370/18-370.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Bayesian Estimation in Random Covariate Design with a Rescaled Gaussian Process Prior",
            "abstract": [
                "In Bayesian nonparametric models, Gaussian processes provide a popular prior choice for regression function estimation. Existing literature on the theoretical investigation of the resulting posterior distribution almost exclusively assume a fixed design for covariates. The only random design result we are aware of (van der Vaart and van Zanten, 2011) assumes the assigned Gaussian process to be supported on the smoothness class specified by the true function with probability one. This is a fairly restrictive assumption as it essentially rules out the Gaussian process prior with a squared exponential kernel when modeling rougher functions. In this article, we show that an appropriate rescaling of the above Gaussian process leads to a rate-optimal posterior distribution even when the covariates are independently realized from a known density on a compact set. The proofs are based on deriving sharp concentration inequalities for frequentist kernel estimators; the results might be of independent interest."
            ],
            "keywords": [
                "Bayesian",
                "convergence rate",
                "Gaussian process",
                "nonparametric regression",
                "random design",
                "rate-optimal"
            ],
            "author": [
                "Debdeep Pati",
                "Anirban Bhattacharya",
                "Guang Cheng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/pati15a/pati15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stationary-Sparse Causality Network Learning Yuejia He",
            "abstract": [
                "Recently, researchers have proposed penalized maximum likelihood to identify network topology underlying a dynamical system modeled by multivariate time series. The time series of interest are assumed to be stationary, but this restriction is never taken into consideration by existing estimation methods. Moreover, practical problems of interest may have ultra-high dimensionality and obvious node collinearity. In addition, none of the available algorithms provides a probabilistic measure of the uncertainty for the obtained network topology which is informative in reliable network identification. The main purpose of this paper is to tackle these challenging issues. We propose the S 2 learning framework, which stands for stationary-sparse network learning. We propose a novel algorithm referred to as the Berhu iterative sparsity pursuit with stationarity (BISPS), where the Berhu regularization can improve the Lasso in detection and estimation. The algorithm is extremely easy to implement, efficient in computation and has a theoretical guarantee to converge to a global optimum. We also incorporate a screening technique into BISPS to tackle ultra-high dimensional problems and enhance computational efficiency. Furthermore, a stationary bootstrap technique is applied to provide connection occurring frequency for reliable topology learning. Experiments show that our method can achieve stationary and sparse causality network learning and is scalable for high-dimensional problems."
            ],
            "keywords": [
                "stationarity",
                "sparsity",
                "Berhu",
                "screening",
                "bootstrap"
            ],
            "author": [
                "Dapeng Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/he13a/he13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Embarrassingly Parallel Inference for Gaussian Processes",
            "abstract": [
                "Training Gaussian process-based models typically involves an O(N 3) computational bottleneck due to inverting the covariance matrix. Popular methods for overcoming this matrix inversion problem cannot adequately model all types of latent functions, and are often not parallelizable. However, judicious choice of model structure can ameliorate this problem. A mixture-of-experts model that uses a mixture of K Gaussian processes offers modeling flexibility and opportunities for scalable inference. Our embarrassingly parallel algorithm combines low-dimensional matrix inversions with importance sampling to yield a flexible, scalable mixture-of-experts model that offers comparable performance to Gaussian process regression at a much lower computational cost."
            ],
            "keywords": [
                "Gaussian process",
                "parallel inference",
                "machine learning",
                "Bayesian non-parametrics"
            ],
            "author": [
                "Michael Minyi Zhang",
                "Sinead A Williamson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-374/18-374.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Libra Toolkit for Probabilistic Models",
            "abstract": [
                "The Libra Toolkit is a collection of algorithms for learning and inference with discrete probabilistic models, including Bayesian networks, Markov networks, dependency networks, and sum-product networks. Compared to other toolkits, Libra places a greater emphasis on learning the structure of tractable models in which exact inference is efficient. It also includes a variety of algorithms for learning graphical models in which inference is potentially intractable, and for performing exact and approximate inference. Libra is released under a 2-clause BSD license to encourage broad use in academia and industry."
            ],
            "keywords": [
                "probabilistic graphical models",
                "structure learning",
                "inference"
            ],
            "author": [
                "Daniel Lowd"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/lowd15a/lowd15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Harry: A Tool for Measuring String Similarity",
            "abstract": [
                "Comparing strings and assessing their similarity is a basic operation in many application domains of machine learning, such as in information retrieval, natural language processing and bioinformatics. The practitioner can choose from a large variety of available similarity measures for this task, each emphasizing different aspects of the string data. In this article, we present Harry, a small tool specifically designed for measuring the similarity of strings. Harry implements over 20 similarity measures, including common string distances and string kernels, such as the Levenshtein distance and the Subsequence kernel. The tool has been designed with efficiency in mind and allows for multi-threaded as well as distributed computing, enabling the analysis of large data sets of strings. Harry supports common data formats and thus can interface with analysis environments, such as Matlab, Pylab and Weka."
            ],
            "keywords": [
                "string kernels",
                "string distances",
                "similarity measures for strings"
            ],
            "author": [
                "Konrad Rieck",
                "Christian Wressnegger"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/rieck16a/rieck16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Feature-Level Domain Adaptation",
            "abstract": [
                "Domain adaptation is the supervised learning setting in which the training and test data are sampled from different distributions: training data is sampled from a source domain, whilst test data is sampled from a target domain. This paper proposes and studies an approach, called feature-level domain adaptation (flda), that models the dependence between the two domains by means of a feature-level transfer model that is trained to describe the transfer from source to target domain. Subsequently, we train a domain-adapted classifier by minimizing the expected loss under the resulting transfer model. For linear classifiers and a large family of loss functions and transfer models, this expected loss can be computed or approximated analytically, and minimized efficiently. Our empirical evaluation of flda focuses on problems comprising binary and count data in which the transfer can be naturally modeled via a dropout distribution, which allows the classifier to adapt to differences in the marginal probability of features in the source and the target domain. Our experiments on several real-world problems show that flda performs on par with state-of-the-art domainadaptation techniques."
            ],
            "keywords": [
                "Domain adaptation",
                "transfer learning",
                "covariate shift",
                "risk minimization"
            ],
            "author": [
                "Wouter M Kouw",
                "Laurens J P Van Der Maaten",
                "Jesse H Krijthe",
                "Marco Loog",
                "Urun Dogan",
                "Marius Kloft",
                "Francesco Orabona",
                "Tatiana Tommasi",
                "M Kouw",
                "Krijthe, Loog Van Kouw"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-206/15-206.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Visualizing Data using t-SNE Laurens van der Maaten",
            "abstract": [
                "We present a new technique called \"t-SNE\" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets."
            ],
            "keywords": [
                "visualization",
                "dimensionality reduction",
                "manifold learning",
                "embedding algorithms",
                "multidimensional scaling"
            ],
            "author": [
                "Lvdmaaten @ Gmail",
                "Geoffrey Hinton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/vandermaaten08a/vandermaaten08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning to Select Features using their Properties",
            "abstract": [
                "Feature selection is the task of choosing a small subset of features that is sufficient to predict the target labels well. Here, instead of trying to directly determine which features are better, we attempt to learn the properties of good features. For this purpose we assume that each feature is represented by a set of properties, referred to as meta-features. This approach enables prediction of the quality of features without measuring their value on the training instances. We use this ability to devise new selection algorithms that can efficiently search for new good features in the presence of a huge number of features, and to dramatically reduce the number of feature measurements needed. We demonstrate our algorithms on a handwritten digit recognition problem and a visual object category recognition problem. In addition, we show how this novel viewpoint enables derivation of better generalization bounds for the joint learning problem of selection and classification, and how it contributes to a better understanding of the problem. Specifically, in the context of object recognition, previous works showed that it is possible to find one set of features which fits most object categories (aka a universal dictionary). Here we use our framework to analyze one such universal dictionary and find that the quality of features in this dictionary can be predicted accurately by its meta-features."
            ],
            "keywords": [
                "feature selection",
                "unobserved features",
                "meta-features"
            ],
            "author": [
                "Eyal Krupka",
                "Amir Navot"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/krupka08b/krupka08b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Kernel Density Estimation",
            "abstract": [
                "We propose a method for nonparametric density estimation that exhibits robustness to contamination of the training sample. This method achieves robustness by combining a traditional kernel density estimator (KDE) with ideas from classical M-estimation. We interpret the KDE based on a positive semi-definite kernel as a sample mean in the associated reproducing kernel Hilbert space. Since the sample mean is sensitive to outliers, we estimate it robustly via M-estimation, yielding a robust kernel density estimator (RKDE). An RKDE can be computed efficiently via a kernelized iteratively re-weighted least squares (IRWLS) algorithm. Necessary and sufficient conditions are given for kernelized IRWLS to converge to the global minimizer of the M-estimator objective function. The robustness of the RKDE is demonstrated with a representer theorem, the influence function, and experimental results for density estimation and anomaly detection."
            ],
            "keywords": [
                "outlier",
                "reproducing kernel Hilbert space",
                "kernel trick",
                "influence function",
                "M-estimation"
            ],
            "author": [
                "Jooseuk Kim",
                "Clayton D Scott"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/kim12b/kim12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Estimation of Derivatives Using Locally Weighted Least Absolute Deviation Regression",
            "abstract": [
                "In nonparametric regression, the derivative estimation has attracted much attention in recent years due to its wide applications. In this paper, we propose a new method for the derivative estimation using the locally weighted least absolute deviation regression. Different from the local polynomial regression, the proposed method does not require a finite variance for the error term and so is robust to the presence of heavy-tailed errors. Meanwhile, it does not require a zero median or a positive density at zero for the error term in comparison with the local median regression. We further show that the proposed estimator with random difference is asymptotically equivalent to the (infinitely) composite quantile regression estimator. In other words, running one regression is equivalent to combining infinitely many quantile regressions. In addition, the proposed method is also extended to estimate the derivatives at the boundaries and to estimate higher-order derivatives. For the equidistant design, we derive theoretical results for the proposed estimators, including the asymptotic bias and variance, consistency, and asymptotic normality. Finally, we conduct simulation studies to demonstrate that the proposed method has better performance than the existing methods in the presence of outliers and heavy-tailed errors, and analyze the Chinese house price data for the past ten years to illustrate the usefulness of the proposed method."
            ],
            "keywords": [
                "composite quantile regression",
                "differenced method",
                "LowLAD",
                "LowLSR",
                "outlier and heavy-tailed error",
                "robust nonparametric derivative estimation"
            ],
            "author": [
                "Wenwu Wang",
                "Ping Yu",
                "Lu Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-340/17-340.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Regret Classifier for Imprecise Class Distributions",
            "abstract": [
                "The design of a minimum risk classifier based on data usually stems from the stationarity assumption that the conditions during training and test are the same: the misclassification costs assumed during training must be in agreement with real costs, and the same statistical process must have generated both training and test data. Unfortunately, in real world applications, these assumptions may not hold. This paper deals with the problem of training a classifier when prior probabilities cannot be reliably induced from training data. Some strategies based on optimizing the worst possible case (conventional minimax) have been proposed previously in the literature, but they may achieve a robust classification at the expense of a severe performance degradation. In this paper we propose a minimax regret (minimax deviation) approach, that seeks to minimize the maximum deviation from the performance of the optimal risk classifier. A neural-based minimax regret classifier for general multi-class decision problems is presented. Experimental results show its robustness and the advantages in relation to other approaches."
            ],
            "keywords": [
                "classification",
                "imprecise class distribution",
                "minimax regret",
                "minimax deviation",
                "neural networks"
            ],
            "author": [
                "Rocío Alaiz-Rodríguez",
                "Alicia Guerrero-Curieses"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/alaiz-rodriguez07a/alaiz-rodriguez07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Using Extended Statistical Queries to Avoid Membership Queries",
            "abstract": [
                "The Kushilevitz-Mansour (KM) algorithm is an algorithm that finds all the \"large\" Fourier coefficients of a Boolean function. It is the main tool for learning decision trees and DNF expressions in the PAC model with respect to the uniform distribution. The algorithm requires access to the membership query (MQ) oracle. The access is often unavailable in learning applications and thus the KM algorithm cannot be used. We significantly weaken this requirement by producing an analogue of the KM algorithm that uses extended statistical queries (SQ) (SQs in which the expectation is taken with respect to a distribution given by a learning algorithm). We restrict a set of distributions that a learning algorithm may use for its statistical queries to be a set of product distributions with each bit being 1 with probability ρ, 1/2 or 1− ρ for a constant 1/2 > ρ > 0 (we denote the resulting model by SQ-D ρ). Our analogue finds all the \"large\" Fourier coefficients of degree lower than c log n (we call it the Bounded Sieve (BS)). We use BS to learn decision trees and by adapting Freund's boosting technique we give an algorithm that learns DNF in SQ-D ρ. An important property of the model is that its algorithms can be simulated by MQs with persistent noise. With some modifications BS can also be simulated by MQs with product attribute noise (i.e., for a query x oracle changes every bit of x with some constant probability and calculates the value of the target function at the resulting point) and classification noise. This implies learnability of decision trees and weak learnability of DNF with this non-trivial noise. In the second part of this paper we develop a characterization for learnability with these extended statistical queries. We show that our characterization when applied to SQ-D ρ is tight in terms of learning parity functions. We extend the result given by Blum et al. by proving that there is a class learnable in the PAC model with random classification noise and not learnable in SQ-D ρ ."
            ],
            "keywords": [],
            "author": [
                "Nader H Bshouty",
                "Vitaly Feldman"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/bshouty02a/bshouty02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Data Efficient and Feasible Level Set Method for Stochastic Convex Optimization with Expectation Constraints",
            "abstract": [
                "Stochastic convex optimization problems with expectation constraints (SOECs) are encountered in statistics and machine learning, business, and engineering. The SOEC objective and constraints contain expectations defined with respect to complex distributions or large data sets, leading to high computational complexity when solved by the algorithms that use exact functions and their gradients. Recent stochastic first order methods exhibit low computational complexity when handling SOECs but guarantee near-feasibility and nearoptimality only at convergence. These methods may thus return highly infeasible solutions when heuristically terminated, as is often the case, due to theoretical convergence criteria being highly conservative. This issue limits the use of first order methods in several applications where the SOEC constraints encode implementation requirements. We design a stochastic feasible level set method (SFLS) for SOECs that has low complexity and emphasizes feasibility before convergence. Specifically, our level-set method solves a root-finding problem by calling a novel first order oracle that computes a stochastic upper bound on the level-set function by extending mirror descent and online validation techniques. We establish that SFLS maintains a high-probability feasible solution at each root-finding iteration and exhibits favorable complexity compared to state-of-the-art deterministic feasible level set and stochastic subgradient methods. Numerical experiments on three diverse applications highlight how SFLS finds feasible solutions with small optimality gaps with lower complexity than the former approaches."
            ],
            "keywords": [
                "constrained stochastic optimization",
                "level set methods",
                "stochastic gradient methods",
                "min-max optimization",
                "online validation"
            ],
            "author": [
                "Qihang Lin",
                "Selvaprabu Nadarajah",
                "Negar Soheili",
                "Tianbao Yang",
                "Nadarajah, Soheili Yang Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-1022/19-1022.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lower Bounds for Testing Graphical Models: Colorings and Antiferromagnetic Ising Models",
            "abstract": [
                "We study the identity testing problem in the context of spin systems or undirected graphical models, where it takes the following form: given the parameter specification of the model M and a sampling oracle for the distribution µ M * of an unknown model M * , can we efficiently determine if the two models M and M * are the same? We consider identity testing for both soft-constraint and hard-constraint systems. In particular, we prove hardness results in two prototypical cases, the Ising model and proper colorings, and explore whether identity testing is any easier than structure learning."
            ],
            "keywords": [
                "distribution testing",
                "structure learning",
                "graphical models",
                "Ising model",
                "colorings"
            ],
            "author": [
                "Ivona Bezáková",
                "Antonio Blanca",
                "Zongchen Chen",
                "Eric Vigoda",
                "Ivona ©2020",
                "Antonio Bezáková",
                "Zongchen Blanca",
                "DanielŠtefankovič Eric Vigoda Chen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-580/19-580.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Algorithms for Learning Kernels Based on Centered Alignment",
            "abstract": [
                "This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difficult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classification and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efficient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classification and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classification and regression."
            ],
            "keywords": [
                "kernel methods",
                "learning kernels",
                "feature selection"
            ],
            "author": [
                "Corinna Cortes",
                "Mehryar Mohri"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/cortes12a/cortes12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Target Propagation in Recurrent Neural Networks",
            "abstract": [
                "Recurrent Neural Networks have been widely used to process sequence data, but have long been criticized for their biological implausibility and training difficulties related to vanishing and exploding gradients. This paper presents a novel algorithm for training recurrent networks, target propagation through time (TPTT), that outperforms standard backpropagation through time (BPTT) on four out of the five problems used for testing. The proposed algorithm is initially tested and compared to BPTT on four synthetic time lag tasks, and its performance is also measured using the sequential MNIST data set. In addition, as TPTT uses target propagation, it allows for discrete nonlinearities and could potentially mitigate the credit assignment problem in more complex recurrent architectures."
            ],
            "keywords": [
                "recurrent neural networks",
                "target propagation",
                "biological plausibility"
            ],
            "author": [
                "Nikolay Manchev"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-141/18-141.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimation of Gradients and Coordinate Covariation in Classification",
            "abstract": [
                "We introduce an algorithm that simultaneously estimates a classification function as well as its gradient in the supervised learning framework. The motivation for the algorithm is to find salient variables and estimate how they covary. An efficient implementation with respect to both memory and time is given. The utility of the algorithm is illustrated on simulated data as well as a gene expression data set. An error analysis is given for the convergence of the estimate of the classification function and its gradient to the true classification function and true gradient."
            ],
            "keywords": [
                "Tikhnonov regularization",
                "variable selection",
                "reproducing kernel Hilbert space",
                "generalization bounds",
                "classification"
            ],
            "author": [
                "Sayan Mukherjee",
                "Qiang Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/mukherjee06b/mukherjee06b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Decision Boundary for Discrete Bayesian Network Classifiers",
            "abstract": [
                "Bayesian network classifiers are a powerful machine learning tool. In order to evaluate the expressive power of these models, we compute families of polynomials that sign-represent decision functions induced by Bayesian network classifiers. We prove that those families are linear combinations of products of Lagrange basis polynomials. In absence of V-structures in the predictor sub-graph, we are also able to prove that this family of polynomials does indeed characterize the specific classifier considered. We then use this representation to bound the number of decision functions representable by Bayesian network classifiers with a given structure."
            ],
            "keywords": [
                "Bayesian networks",
                "supervised classification",
                "decision boundary",
                "polynomial threshold function",
                "Lagrange basis"
            ],
            "author": [
                "Gherardo Varando",
                "Concha Bielza",
                "Pedro Larrañaga"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/varando15a/varando15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High Dimensional Inverse Covariance Matrix Estimation via Linear Programming",
            "abstract": [
                "This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by \"sparse\" matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such \"sparsity\". The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem."
            ],
            "keywords": [
                "covariance selection",
                "Dantzig selector",
                "Gaussian graphical model",
                "inverse covariance matrix",
                "Lasso",
                "linear programming",
                "oracle inequality",
                "sparsity"
            ],
            "author": [
                "Ming Yuan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/yuan10b/yuan10b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rademacher Complexities and Bounding the Excess Risk in Active Learning",
            "abstract": [
                "Sequential algorithms of active learning based on the estimation of the level sets of the empirical risk are discussed in the paper. Localized Rademacher complexities are used in the algorithms to estimate the sample sizes needed to achieve the required accuracy of learning in an adaptive way. Probabilistic bounds on the number of active examples have been proved and several applications to binary classification problems are considered."
            ],
            "keywords": [
                "active learning",
                "excess risk",
                "Rademacher complexities",
                "capacity function",
                "disagreement coefficient"
            ],
            "author": [
                "Vladimir Koltchinskii"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/koltchinskii10a/koltchinskii10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Noisy Sparse Subspace Clustering",
            "abstract": [
                "This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabeled input data points, which are assumed to be in a union of low-dimensional subspaces. We show that a modified version of SSC is provably effective in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to more practical settings and provides justification to the success of SSC in a class of real applications."
            ],
            "keywords": [
                "Subspace clustering",
                "robustness",
                "stability",
                "compressive sensing",
                "sparse"
            ],
            "author": [
                "Yu-Xiang Wang",
                "Huan Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/13-354/13-354.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Unique Sharp Local Minimum in -minimization Complete Dictionary Learning",
            "abstract": [
                "We study the problem of globally recovering a dictionary from a set of signals via 1minimization. We assume that the signals are generated as i.i.d. random linear combinations of the K atoms from a complete reference dictionary D * ∈ R K×K , where the linear combination coefficients are from either a Bernoulli type model or exact sparse model. First, we obtain a necessary and sufficient norm condition for the reference dictionary D * to be a sharp local minimum of the expected 1 objective function. Our result substantially extends that of Wu and Yu (2018) and allows the combination coefficient to be non-negative. Secondly, we obtain an explicit bound on the region within which the objective value of the reference dictionary is minimal. Thirdly, we show that the reference dictionary is the unique sharp local minimum, thus establishing the first known global property of 1-minimization dictionary learning. Motivated by the theoretical results, we introduce a perturbation based test to determine whether a dictionary is a sharp local minimum of the objective function. In addition, we also propose a new dictionary learning algorithm based on Block Coordinate Descent, called DL-BCD, which is guaranteed to decrease the obective function monotonically. Simulation studies show that DL-BCD has competitive performance in terms of recovery rate compared to other state-of-the-art dictionary learning algorithms when the reference dictionary is generated from random Gaussian matrices."
            ],
            "keywords": [
                "dictionary learning",
                "1 -minimization",
                "local and global identifiability",
                "nonconvex optimization",
                "sharp local minimum"
            ],
            "author": [
                "Yu Wang",
                "Siqi Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-169/19-169.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Teaching Dimension of Linear Learners",
            "abstract": [
                "Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization. This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression. We also exhibit optimal training sets that match these teaching dimensions. Our approach generalizes to other linear learners."
            ],
            "keywords": [
                "Optimization based learner",
                "Karush-Kuhn-Tucker conditions",
                "VC-dimension"
            ],
            "author": [
                "Ji Liu",
                "Xiaojin Zhu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-630/15-630.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers",
            "abstract": [
                "We propose a PAC-Bayes theorem for the sample-compression setting where each classifier is described by a compression subset of the training data and a message string of additional information. This setting, which is the appropriate one to describe many learning algorithms, strictly generalizes the usual data-independent setting where classifiers are represented only by data-independent message strings (or parameters taken from a continuous set). The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classifier vanishes. For posteriors having all their weights on a single sample-compressed classifier, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. (2005). Finally, we extend our results to the case where each sample-compressed classifier of a data-dependent ensemble may abstain of predicting a class label."
            ],
            "keywords": [
                "PAC-Bayes",
                "risk bounds",
                "sample-compression",
                "set covering machines",
                "decision list machines"
            ],
            "author": [
                "François Laviolette",
                "Mario Marchand",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/laviolette07a/laviolette07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ranking the Best Instances",
            "abstract": [
                "We formulate a local form of the bipartite ranking problem where the goal is to focus on the best instances. We propose a methodology based on the construction of real-valued scoring functions. We study empirical risk minimization of dedicated statistics which involve empirical quantiles of the scores. We first state the problem of finding the best instances which can be cast as a classification problem with mass constraint. Next, we develop special performance measures for the local ranking problem which extend the Area Under an ROC Curve (AUC) criterion and describe the optimal elements of these new criteria. We also highlight the fact that the goal of ranking the best instances cannot be achieved in a stage-wise manner where first, the best instances would be tentatively identified and then a standard AUC criterion could be applied. Eventually, we state preliminary statistical results for the local ranking problem."
            ],
            "keywords": [
                "ranking",
                "ROC curve and AUC",
                "empirical risk minimization",
                "fast rates"
            ],
            "author": [
                "Stéphan Clémençon",
                "Nicolas Vayatis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/clemencon07a/clemencon07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Representer Theorem for Deep Neural Networks",
            "abstract": [
                "We propose to optimize the activation functions of a deep neural network by adding a corresponding functional regularization to the cost function. We justify the use of a secondorder total-variation criterion. This allows us to derive a general representer theorem for deep neural networks that makes a direct connection with splines and sparsity. Specifically, we show that the optimal network configuration can be achieved with activation functions that are nonuniform linear splines with adaptive knots. The bottom line is that the action of each neuron is encoded by a spline whose parameters (including the number of knots) are optimized during the training procedure. The scheme results in a computational structure that is compatible with existing deep-ReLU, parametric ReLU, APL (adaptive piecewiselinear) and MaxOut architectures. It also suggests novel optimization challenges and makes an explicit link with 1 minimization and sparsity-promoting techniques."
            ],
            "keywords": [
                "splines",
                "regularization",
                "sparsity",
                "learning",
                "deep neural networks",
                "activation functions"
            ],
            "author": [
                "Michael Unser"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-418/18-418.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Subspace Learning with Partial Information",
            "abstract": [
                "The goal of subspace learning is to find a k-dimensional subspace of R d , such that the expected squared distance between instance vectors and the subspace is as small as possible. In this paper we study subspace learning in a partial information setting, in which the learner can only observe r ≤ d attributes from each instance vector. We propose several efficient algorithms for this task, and analyze their sample complexity."
            ],
            "keywords": [
                "principal components analysis",
                "budgeted learning",
                "statistical learning",
                "learning with partial information",
                "learning theory"
            ],
            "author": [
                "Alon Gonen",
                "Dan Rosenbaum",
                "Yonina C Eldar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-443/14-443.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse Factor Analysis for Learning and Content Analytics",
            "abstract": [
                "We develop a new model and algorithms for machine learning-based learning analytics, which estimate a learner's knowledge of the concepts underlying a domain, and content analytics, which estimate the relationships among a collection of questions and those concepts. Our model represents the probability that a learner provides the correct response to a question in terms of three factors: their understanding of a set of underlying concepts, the concepts involved in each question, and each question's intrinsic difficulty. We estimate these factors given the graded responses to a collection of questions. The underlying estimation problem is ill-posed in general, especially when only a subset of the questions are answered. The key observation that enables a well-posed solution is the fact that typical educational domains of interest involve only a small number of key concepts. Leveraging this observation, we develop both a bi-convex maximum-likelihood-based solution and a Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem. We also incorporate user-defined tags on questions to facilitate the interpretability of the estimated factors. Experiments with synthetic and real-world data demonstrate the efficacy of our approach. Finally, we make a connection between SPARFA and noisy, binary-valued (1-bit) dictionary learning that is of independent interest."
            ],
            "keywords": [
                "factor analysis",
                "sparse probit regression",
                "sparse logistic regression",
                "Bayesian latent factor analysis",
                "personalized learning"
            ],
            "author": [
                "Andrew S Lan",
                "Christoph Studer",
                "Richard G Baraniuk",
                "Andrew S ©2014",
                "Andrew E Lan",
                "Christoph Waters",
                "Richard G Studer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/lan14a/lan14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model-free Nonconvex Matrix Completion: Local Minima Analysis and Applications in Memory-efficient Kernel PCA",
            "abstract": [
                "This work studies low-rank approximation of a positive semidefinite matrix from partial entries via nonconvex optimization. We characterized how well local-minimum based lowrank factorization approximates a fixed positive semidefinite matrix without any assumptions on the rank-matching, the condition number or eigenspace incoherence parameter. Furthermore, under certain assumptions on rank-matching and well-boundedness of condition numbers and eigenspace incoherence parameters, a corollary of our main theorem improves the state-of-the-art sampling rate results for nonconvex matrix completion with no spurious local minima in Ge et al. (2016, 2017). In addition, we have investigated when the proposed nonconvex optimization results in accurate low-rank approximations even in presence of large condition numbers, large incoherence parameters, or rank mismatching. We also propose to apply the nonconvex optimization to memory-efficient kernel PCA. Compared to the well-known Nyström methods, numerical experiments indicate that the proposed nonconvex optimization approach yields more stable results in both low-rank approximation and clustering."
            ],
            "keywords": [
                "low-rank approximation",
                "matrix completion",
                "nonconvex optimization",
                "modelfree analysis",
                "local minimum analysis",
                "kernel PCA"
            ],
            "author": [
                "Ji Chen",
                "Xiaodong Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-776/17-776.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Prediction regions through Inverse Regression",
            "abstract": [
                "Predicting a new response from a covariate is a challenging task in regression, which raises new question since the era of high-dimensional data. In this paper, we are interested in the inverse regression method from a theoretical viewpoint. Theoretical results for the well-known Gaussian linear model are well-known, but the curse of dimensionality has increased the interest of practitioners and theoreticians into generalization of those results for various estimators, calibrated for the high-dimension context. We propose to focus on inverse regression. It is known to be a reliable and efficient approach when the number of features exceeds the number of observations. Indeed, under some conditions, dealing with the inverse regression problem associated to a forward regression problem drastically reduces the number of parameters to estimate, makes the problem tractable and allows to consider more general distributions, as elliptical distributions. When both the responses and the covariates are multivariate, estimators constructed by the inverse regression are studied in this paper, the main result being explicit asymptotic prediction regions for the response. The performances of the proposed estimators and prediction regions are also analyzed through a simulation study and compared with usual estimators."
            ],
            "keywords": [
                "Inverse regression",
                "Prediction regions",
                "Confidence regions",
                "High-dimension",
                "Asymptotic distribution"
            ],
            "author": [
                "Emilie Devijver",
                "Emeline Perthame"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-535/19-535.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Divvy: Fast and Intuitive Exploratory Data Analysis",
            "abstract": [
                "Divvy is an application for applying unsupervised machine learning techniques (clustering and dimensionality reduction) to the data analysis process. Divvy provides a novel UI that allows researchers to tighten the action-perception loop of changing algorithm parameters and seeing a visualization of the result. Machine learning researchers can use Divvy to publish easy to use reference implementations of their algorithms, which helps the machine learning field have a greater impact on research practices elsewhere."
            ],
            "keywords": [
                "clustering",
                "dimensionality reduction",
                "open source software",
                "human computer interaction",
                "data visualization"
            ],
            "author": [
                "Joshua M Lewis",
                "Virginia R De Sa",
                "Laurens Van Der Maaten",
                "Mark Reid"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/lewis13a/lewis13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The SHOGUN Machine Learning Toolbox",
            "abstract": [
                "We have developed a machine learning toolbox, called SHOGUN, which is designed for unified large-scale learning for a broad range of feature types and learning settings. It offers a considerable number of machine learning models such as support vector machines, hidden Markov models, multiple kernel learning, linear discriminant analysis, and more. Most of the specific algorithms are able to deal with several different data classes. We have used this toolbox in several applications from computational biology, some of them coming with no less than 50 million training examples and others with 7 billion test examples. With more than a thousand installations worldwide, SHOGUN is already widely adopted in the machine learning community and beyond. SHOGUN is implemented in C++ and interfaces to MATLAB, TM R, Octave, Python, and has a stand-alone command line interface. The source code is freely available under the GNU General Public License, Version 3 at http://www.shogun-toolbox.org."
            ],
            "keywords": [
                "support vector machines",
                "kernels",
                "large-scale learning",
                "Python",
                "Octave",
                "R"
            ],
            "author": [
                "Sören Sonnenburg",
                "Gunnar Rätsch",
                "Sebastian Henschel",
                "Christian Widmer",
                "Jonas Behr",
                "Alexander Zien",
                "Fabio De",
                "Tuebingen Mpg De",
                "Alexander Binder"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/sonnenburg10a/sonnenburg10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Following the Leader and Fast Rates in Online Linear Prediction: Curved Constraint Sets and Other Regularities *",
            "abstract": [
                "Follow the leader (FTL) is a simple online learning algorithm that is known to perform well when the loss functions are convex and positively curved. In this paper we ask whether there are other settings when FTL achieves low regret. In particular, we study the fundamental problem of linear prediction over a convex, compact domain with non-empty interior. Amongst other results, we prove that the curvature of the boundary of the domain can act as if the losses were curved: In this case, we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero, FTL enjoys logarithmic regret, while for polytope domains and stochastic data it enjoys finite expected regret. The former result is also extended to strongly convex domains by establishing an equivalence between the strong convexity of sets and the minimum curvature of their boundary, which may be of independent interest. Building on a previously known meta-algorithm, we also get an algorithm that simultaneously enjoys the worst-case guarantees and the smaller regret of FTL when the data is 'easy'. Finally, we show that such guarantees are achievable directly (e.g., by the follow the regularized leader algorithm or by a shrinkage-based variant of FTL) when the constraint set is an ellipsoid."
            ],
            "keywords": [
                "online linear optimization",
                "follow the leader",
                "logarithmic regret",
                "strongly convex decision set",
                "curvature"
            ],
            "author": [
                "Ruitong Huang",
                "Tor Lattimore",
                "Csaba Szepesvári",
                "† Ruitong Huang",
                "‡ Tor Lattimore"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-079/17-079.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Calibrated Multivariate Regression with Application to Neural Semantic Basis Discovery *",
            "abstract": [
                "We propose a calibrated multivariate regression method named CMR for fitting high dimensional multivariate regression models. Compared with existing methods, CMR calibrates regularization for each regression task with respect to its noise level so that it simultaneously attains improved finite-sample performance and tuning insensitiveness. Theoretically, we provide sufficient conditions under which CMR achieves the optimal rate of convergence in parameter estimation. Computationally, we propose an efficient smoothed proximal gradient algorithm with a worst-case numerical rate of convergence O(1/), where is a pre-specified accuracy of the objective function value. We conduct thorough numerical simulations to illustrate that CMR consistently outperforms other high dimensional multivariate regression methods. We also apply CMR to solve a brain activity prediction problem and find that it is as competitive as a handcrafted model created by human experts."
            ],
            "keywords": [
                "calibration",
                "multivariate regression",
                "high dimension",
                "sparsity",
                "low Rank",
                "brain activity prediction"
            ],
            "author": [
                "Han Liu",
                "Lie Wang",
                "Tuo Zhao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/liu15b/liu15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Efficient Implementation of an Active Set Method for SVMs",
            "abstract": [
                "We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efficient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims' SVM light (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difficult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVM light , the generalization properties of these two approaches are identical and we do not discuss them in the paper."
            ],
            "keywords": [
                "active set methods",
                "support vector machines",
                "quadratic programming"
            ],
            "author": [
                "Katya Scheinberg",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/scheinberg06a/scheinberg06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Volumetric Spanners: An Efficient Exploration Basis for Learning",
            "abstract": [
                "Numerous learning problems that contain exploration, such as experiment design, multiarm bandits, online routing, search result aggregation and many more, have been studied extensively in isolation. In this paper we consider a generic and efficiently computable method for action space exploration based on convex geometry. We define a novel geometric notion of an exploration mechanism with low variance called volumetric spanners, and give efficient algorithms to construct such spanners. We describe applications of this mechanism to the problem of optimal experiment design and the general framework for decision making under uncertainty of bandit linear optimization. For the latter we give efficient and near-optimal regret algorithm over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set. 1"
            ],
            "keywords": [
                "barycentric spanner",
                "volumetric spanner",
                "linear bandits",
                "hard margin linear regression"
            ],
            "author": [
                "Elad Hazan",
                "Zohar Karnin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/hazan16a/hazan16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Neyman-Pearson classification: parametrics and sample size requirement",
            "abstract": [
                "The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers that achieve a minimal type II error while enforcing the prioritized type I error controlled under some user-specified level α. This paradigm serves naturally in applications such as severe disease diagnosis and spam detection, where people have clear priorities among the two error types. Recently, Tong et al. (2018) proposed a nonparametric umbrella algorithm that adapts all scoring-type classification methods (e.g., logistic regression, support vector machines, random forest) to respect the given type I error (i.e., conditional probability of classifying a class 0 observation as class 1 under the 0-1 coding) upper bound α with high probability, without specific distributional assumptions on the features and the responses. Universal the umbrella algorithm is, it demands an explicit minimum sample size requirement on class 0, which is often the more scarce class, such as in rare disease diagnosis applications. In this work, we employ the parametric linear discriminant analysis (LDA) model and propose a new parametric thresholding algorithm, which does not need the minimum sample size requirements on class 0 observations and thus is suitable for small sample applications such as rare disease diagnosis. Leveraging both the existing nonparametric and the newly proposed parametric thresholding rules, we propose four LDA-based NP classifiers, for both low-and high-dimensional settings. On the theoretical front, we prove NP oracle inequalities for one proposed classifier, where the rate for excess type II error benefits from the explicit parametric model assumption. Furthermore, as NP classifiers involve a sample splitting step of class 0 observations, we construct a new adaptive sample splitting scheme that can be applied universally to NP classifiers, and this adaptive strategy reduces the"
            ],
            "keywords": [
                "classification",
                "asymmetric error",
                "Neyman-Pearson (NP) paradigm",
                "NP oracle inequalities",
                "minimum sample size requirement",
                "linear discriminant analysis (LDA)",
                "NP umbrella algorithm",
                "adaptive splitting"
            ],
            "author": [
                "Xin Tong",
                "Jiacheng Wang",
                "Yang Feng",
                "Xia, Wang Feng Tong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-577/18-577.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Submatrix localization via message passing",
            "abstract": [
                "The principal submatrix localization problem deals with recovering a K × K principal submatrix of elevated mean µ in a large n × n symmetric matrix subject to additive standard Gaussian noise, or more generally, mean zero, variance one, subgaussian noise. This problem serves as a prototypical example for community detection, in which the community corresponds to the support of the submatrix. The main result of this paper is that in the regime Ω(√ n) ≤ K ≤ o(n), the support of the submatrix can be weakly recovered (with o(K) misclassification errors on average) by an optimized message passing algorithm if λ = µ 2 K 2 /n, the signal-to-noise ratio, exceeds 1/e. This extends a result by Deshpande and Montanari previously obtained for K = Θ(√ n) and µ = Θ(1). In addition, the algorithm can be combined with a voting procedure to achieve the information-theoretic limit of exact recovery with sharp constants for all K ≥ n log n (1 8e + o(1)). The total running time of the algorithm is O(n log n). Another version of the submatrix localization problem, known as noisy biclustering, aims to recover a K 1 × K 2 submatrix of elevated mean µ in a large n 1 × n 2 Gaussian matrix. The optimized message passing algorithm and its analysis are adapted to the bicluster problem assuming Ω(√ n i) ≤ K i ≤ o(n i) and K 1 K 2. A sharp informationtheoretic condition for the weak recovery of both clusters is also identified."
            ],
            "keywords": [
                "Submatrix localization",
                "biclustering",
                "message passing",
                "spectral algorithms computational complexity",
                "high-dimensional statistics"
            ],
            "author": [
                "Bruce Hajek",
                "Yihong Wu",
                "Jiaming Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-297/17-297.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimum Density Hyperplanes",
            "abstract": [
                "Associating distinct groups of objects (clusters) with contiguous regions of high probability density (high-density clusters), is central to many statistical and machine learning approaches to the classification of unlabelled data. We propose a novel hyperplane classifier for clustering and semi-supervised classification which is motivated by this objective. The proposed minimum density hyperplane minimises the integral of the empirical probability density function along it, thereby avoiding intersection with high density clusters. We show that the minimum density and the maximum margin hyperplanes are asymptotically equivalent, thus linking this approach to maximum margin clustering and semi-supervised support vector classifiers. We propose a projection pursuit formulation of the associated optimisation problem which allows us to find minimum density hyperplanes efficiently in practice, and evaluate its performance on a range of benchmark data sets. The proposed approach is found to be very competitive with state of the art methods for clustering and semi-supervised classification."
            ],
            "keywords": [
                "low-density separation",
                "high-density clusters",
                "clustering",
                "semi-supervised classification",
                "projection pursuit"
            ],
            "author": [
                "Nicos G Pavlidis",
                "David P Hofmeyr"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-307/15-307.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantifying Uncertainty in Online Regression Forests",
            "abstract": [
                "Accurately quantifying uncertainty in predictions is essential for the deployment of machine learning algorithms in critical applications where mistakes are costly. Most approaches to quantifying prediction uncertainty have focused on settings where the data is static, or bounded. In this paper, we investigate methods that quantify the prediction uncertainty in a streaming setting, where the data is potentially unbounded. We propose two meta-algorithms that produce prediction intervals for online regression forests of arbitrary tree models; one based on conformal prediction, and the other based on quantile regression. We show that the approaches are able to maintain specified error rates, with constant computational cost per example and bounded memory usage. We provide empirical evidence that the methods outperform the state-of-the-art in terms of maintaining error guarantees, while being an order of magnitude faster. We also investigate how the algorithms are able to recover from concept drift."
            ],
            "keywords": [
                "Online learning",
                "Uncertainty",
                "Decision Trees",
                "Regression"
            ],
            "author": [
                "Theodore Vasiloudis",
                "Gianmarco De",
                "Francisci Morales",
                "Henrik Boström"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-006/19-006.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximations of the Restless Bandit Problem",
            "abstract": [
                "The multi-armed restless bandit problem is studied in the case where the pay-off distributions are stationary ϕ-mixing. This version of the problem provides a more realistic model for most real-world applications, but cannot be optimally solved in practice, since it is known to be PSPACE-hard. The objective of this paper is to characterize a sub-class of the problem where good approximate solutions can be found using tractable approaches. Specifically, it is shown that under some conditions on the ϕ-mixing coefficients, a modified version of UCB can prove effective. The main challenge is that, unlike in the i.i.d. setting, the distributions of the sampled pay-offs may not have the same characteristics as those of the original bandit arms. In particular, the ϕ-mixing property does not necessarily carry over. This is overcome by carefully controlling the effect of a sampling policy on the pay-off distributions. Some of the proof techniques developed in this paper can be more generally used in the context of online sampling under dependence. Proposed algorithms are accompanied with corresponding regret analysis."
            ],
            "keywords": [],
            "author": [
                "Steffen Grünewälder"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-547/17-547.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Locally Adaptive Factor Processes for Multivariate Time Series",
            "abstract": [
                "In modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process. In particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. If such time-varying smoothness is not accounted for, one can obtain misleading inferences and predictions, with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. This can lead to mis-calibration of predictive intervals, which can be substantially too narrow or wide depending on the time. We propose a locally adaptive factor process for characterizing multivariate mean-covariance changes in continuous time, allowing locally varying smoothness in both the mean and covariance matrix. This process is constructed utilizing latent dictionary functions evolving in time through nested Gaussian processes and linearly related to the observed data with a sparse mapping. Using a di↵erential equation representation, we bypass usual computational bottlenecks in obtaining MCMC and online algorithms for approximate Bayesian inference. The performance is assessed in simulations and illustrated in a financial application."
            ],
            "keywords": [
                "Bayesian nonparametrics",
                "locally varying smoothness",
                "multivariate time series",
                "nested Gaussian process",
                "stochastic volatility"
            ],
            "author": [
                "Daniele Durante",
                "Bruno Scarpa",
                "David B Dunson",
                "Robert E Mcculloch"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/durante14a/durante14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Random Matrix Analysis and Improvement of Semi-Supervised Learning for Large Dimensional Data",
            "abstract": [
                "This article provides an original understanding of the behavior of a class of graph-oriented semi-supervised learning algorithms in the limit of large and numerous data. It is demonstrated that the intuition at the root of these methods collapses in this limit and that, as a result, most of them become inconsistent. Corrective measures and a new data-driven parametrization scheme are proposed along with a theoretical analysis of the asymptotic performances of the resulting approach. A surprisingly close behavior between theoretical performances on Gaussian mixture models and on real data sets is also illustrated throughout the article, thereby suggesting the importance of the proposed analysis for dealing with practical data. As a result, significant performance gains are observed on practical data classification using the proposed parametrization."
            ],
            "keywords": [
                "semi-supervised learning",
                "kernel methods",
                "random matrix theory",
                "high dimensional statistics"
            ],
            "author": [
                "Xiaoyi Mai",
                "Romain Couillet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-421/17-421.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Streaming Parallel Decision Tree Algorithm",
            "abstract": [
                "We propose a new algorithm for building decision tree classifiers. The algorithm is executed in a distributed environment and is especially designed for classifying large data sets and streaming data. It is empirically shown to be as accurate as a standard decision tree classifier, while being scalable for processing of streaming data on multiple processors. These findings are supported by a rigorous analysis of the algorithm's accuracy. The essence of the algorithm is to quickly construct histograms at the processors, which compress the data to a fixed amount of memory. A master processor uses this information to find near-optimal split points to terminal tree nodes. Our analysis shows that guarantees on the local accuracy of split points imply guarantees on the overall tree accuracy."
            ],
            "keywords": [
                "decision tree classifiers",
                "distributed computing",
                "streaming data",
                "scalability"
            ],
            "author": [
                "Yael Ben-Haim"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/ben-haim10a/ben-haim10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Stability of Feature Selection Algorithms",
            "abstract": [
                "Feature Selection is central to modern data science, from exploratory data analysis to predictive model-building. The \"stability\" of a feature selection algorithm refers to the robustness of its feature preferences, with respect to data sampling and to its stochastic nature. An algorithm is 'unstable' if a small change in data leads to large changes in the chosen feature subset. Whilst the idea is simple, quantifying this has proven more challenging-we note numerous proposals in the literature, each with different motivation and justification. We present a rigorous statistical treatment for this issue. In particular, with this work we consolidate the literature and provide (1) a deeper understanding of existing work based on a small set of properties, and (2) a clearly justified statistical approach with several novel benefits. This approach serves to identify a stability measure obeying all desirable properties, and (for the first time in the literature) allowing confidence intervals and hypothesis tests on the stability, enabling rigorous experimental comparison of feature selection algorithms."
            ],
            "keywords": [],
            "author": [
                "Sarah Nogueira",
                "Gavin Brown"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-514/17-514.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Particle Gibbs Split-Merge Sampling for Bayesian Inference in Mixture Models",
            "abstract": [
                "This paper presents an original Markov chain Monte Carlo method to sample from the posterior distribution of conjugate mixture models. This algorithm relies on a flexible split-merge procedure built using the particle Gibbs sampler introduced in Andrieu et al. (2009, 2010). The resulting so-called Particle Gibbs Split-Merge sampler does not require the computation of a complex acceptance ratio and can be implemented using existing sequential Monte Carlo libraries. We investigate its performance experimentally on synthetic problems as well as on geolocation data. Our results show that for a given computational budget, the Particle Gibbs Split-Merge sampler empirically outperforms existing split merge methods. The code and instructions allowing to reproduce the experiments is available at https://github.com/aroth85/pgsm."
            ],
            "keywords": [
                "Dirichlet process mixture models",
                "Gibbs sampler",
                "Particle Gibbs sampler",
                "Sequential Monte Carlo"
            ],
            "author": [
                "Alexandre Bouchard-Côté",
                "Arnaud Doucet",
                "Andrew Roth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-397/15-397.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "String and Membrane Gaussian Processes",
            "abstract": [
                "In this paper we introduce a novel framework for making exact nonparametric Bayesian inference on latent functions that is particularly suitable for Big Data tasks. Firstly, we introduce a class of stochastic processes we refer to as string Gaussian processes (string GPs which are not to be mistaken for Gaussian processes operating on text). We construct string GPs so that their finitedimensional marginals exhibit suitable local conditional independence structures, which allow for scalable, distributed, and flexible nonparametric Bayesian inference, without resorting to approximations, and while ensuring some mild global regularity constraints. Furthermore, string GP priors naturally cope with heterogeneous input data, and the gradient of the learned latent function is readily available for explanatory analysis. Secondly, we provide some theoretical results relating our approach to the standard GP paradigm. In particular, we prove that some string GPs are Gaussian processes, which provides a complementary global perspective on our framework. Finally, we derive a scalable and distributed MCMC scheme for supervised learning tasks under string GP priors. The proposed MCMC scheme has computational time complexity O(N) and memory requirement O(dN), where N is the data size and d the dimension of the input space. We illustrate the efficacy of the proposed approach on several synthetic and real-world data sets, including a data set with millions input points and 8 attributes."
            ],
            "keywords": [
                "String Gaussian processes",
                "scalable Bayesian nonparametrics",
                "Gaussian processes",
                "nonstationary kernels",
                "reversible-jump MCMC",
                "point process priors"
            ],
            "author": [
                "Yves-Laurent Kom",
                "Stephen J Roberts"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-382/15-382.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lens Depth Function and k-Relative Neighborhood Graph: Versatile Tools for Ordinal Data Analysis",
            "abstract": [
                "In recent years it has become popular to study machine learning problems in a setting of ordinal distance information rather than numerical distance measurements. By ordinal distance information we refer to binary answers to distance comparisons such as d(A, B) < d(C, D). For many problems in machine learning and statistics it is unclear how to solve them in such a scenario. Up to now, the main approach is to explicitly construct an ordinal embedding of the data points in the Euclidean space, an approach that has a number of drawbacks. In this paper, we propose algorithms for the problems of medoid estimation, outlier identification, classification, and clustering when given only ordinal data. They are based on estimating the lens depth function and the k-relative neighborhood graph on a data set. Our algorithms are simple, are much faster than an ordinal embedding approach and avoid some of its drawbacks, and can easily be parallelized."
            ],
            "keywords": [
                "ordinal data",
                "ordinal distance information",
                "comparison-based algorithms",
                "lens depth function",
                "k-relative neighborhood graph",
                "ordinal embedding",
                "non-metric multidimensional scaling"
            ],
            "author": [
                "Matthäus Kleindessner"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-061/16-061.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Natural Language Processing (Almost) from Scratch",
            "abstract": [
                "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements."
            ],
            "keywords": [],
            "author": [
                "Ronan Collobert",
                "Jason Weston",
                "Michael Karlen",
                "Pavel Kuksa"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/collobert11a/collobert11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Composite Likelihood",
            "abstract": [
                "Maximum likelihood estimators are often of limited practical use due to the intensive computation they require. We propose a family of alternative estimators that maximize a stochastic variation of the composite likelihood function. Each of the estimators resolve the computation-accuracy tradeoff differently, and taken together they span a continuous spectrum of computation-accuracy tradeoff resolutions. We prove the consistency of the estimators, provide formulas for their asymptotic variance, statistical robustness, and computational complexity. We discuss experimental results in the context of Boltzmann machines and conditional random fields. The theoretical and experimental studies demonstrate the effectiveness of the estimators when the computational resources are insufficient. They also demonstrate that in some cases reduced computational complexity is associated with robustness thereby increasing statistical accuracy."
            ],
            "keywords": [
                "Markov random fields",
                "composite likelihood",
                "maximum likelihood estimation"
            ],
            "author": [
                "Joshua V Dillon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/dillon10a/dillon10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Comprehensive Survey on Safe Reinforcement Learning",
            "abstract": [
                "Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted finite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning."
            ],
            "keywords": [
                "reinforcement learning",
                "risk sensitivity",
                "safe exploration",
                "teacher advice"
            ],
            "author": [
                "Javier García",
                "Fernando Fernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/garcia15a/garcia15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Combining Information Extraction Systems Using Voting and Stacked Generalization",
            "abstract": [
                "This article investigates the effectiveness of voting and stacked generalization-also known as stacking-in the context of information extraction (IE). A new stacking framework is proposed that accommodates well-known approaches for IE. The key idea is to perform cross-validation on the base-level data set, which consists of text documents annotated with relevant information, in order to create a meta-level data set that consists of feature vectors. A classifier is then trained using the new vectors. Therefore, base-level IE systems are combined with a common classifier at the metalevel. Various voting schemes are presented for comparing against stacking in various IE domains. Well known IE systems are employed at the base-level, together with a variety of classifiers at the meta-level. Results show that both voting and stacking work better when relying on probabilistic estimates by the base-level systems. Voting proved to be effective in most domains in the experiments. Stacking, on the other hand, proved to be consistently effective over all domains, doing comparably or better than voting and always better than the best base-level systems. Particular emphasis is also given to explaining the results obtained by voting and stacking at the meta-level, with respect to the varying degree of similarity in the output of the base-level systems."
            ],
            "keywords": [
                "stacking",
                "voting",
                "information extraction",
                "cross-validation"
            ],
            "author": [
                "Georgios Sigletos",
                "Georgios Paliouras",
                "Constantine D Spyropoulos",
                "Michalis Hatzopoulos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/sigletos05a/sigletos05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Network That Learns Strassen Multiplication",
            "abstract": [
                "We study neural networks whose only non-linear components are multipliers, to test a new training rule in a context where the precise representation of data is paramount. These networks are challenged to discover the rules of matrix multiplication, given many examples. By limiting the number of multipliers, the network is forced to discover the Strassen multiplication rules. This is the mathematical equivalent of finding low rank decompositions of the n × n matrix multiplication tensor, M n. We train these networks with the conservative learning rule, which makes minimal changes to the weights so as to give the correct output for each input at the time the input-output pair is received. Conservative learning needs a few thousand examples to find the rank 7 decomposition of M 2 , and 10 5 for the rank 23 decomposition of M 3 (the lowest known). High precision is critical, especially for M 3 , to discriminate between true decompositions and \"border approximations\"."
            ],
            "keywords": [
                "sum-product networks",
                "Strassen multiplication",
                "tensor decomposition"
            ],
            "author": [
                "Veit Elser"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-074/16-074.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimation of Sparse Binary Pairwise Markov Networks using Pseudo-likelihoods",
            "abstract": [
                "We consider the problems of estimating the parameters as well as the structure of binary-valued Markov networks. For maximizing the penalized log-likelihood, we implement an approximate procedure based on the pseudo-likelihood of Besag (1975) and generalize it to a fast exact algorithm. The exact algorithm starts with the pseudo-likelihood solution and then adjusts the pseudolikelihood criterion so that each additional iterations moves it closer to the exact solution. Our results show that this procedure is faster than the competing exact method proposed by Lee, Ganapathi, and Koller (2006a). However, we also find that the approximate pseudo-likelihood as well as the approaches of Wainwright et al. (2006), when implemented using the coordinate descent procedure of Friedman, Hastie, and Tibshirani (2008b), are much faster than the exact methods, and only slightly less accurate."
            ],
            "keywords": [
                "Markov networks",
                "logistic regression",
                "L penalty",
                "model selection",
                "Binary variables"
            ],
            "author": [
                "Holger Höfling",
                "Robert Tibshirani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/hoefling09a/hoefling09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Log-concave sampling: Metropolis-Hastings algorithms are fast",
            "abstract": [
                "We study the problem of sampling from a strongly log-concave density supported on R d , and prove a non-asymptotic upper bound on the mixing time of the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by simulating a Markov chain obtained from the discretization of an appropriate Langevin diffusion, combined with an accept-reject step. Relative to known guarantees for the unadjusted Langevin algorithm (ULA), our bounds show that the use of an accept-reject step in MALA leads to an exponentially improved dependence on the error-tolerance. Concretely, in order to obtain samples with TV error at most δ for a density with condition number κ, we show that MALA requires O κd log(1/δ) steps from a warm start, as compared to the O κ 2 d/δ 2 steps established in past work on ULA. We also demonstrate the gains of a modified version of MALA over ULA for weakly log-concave densities. Furthermore, we derive mixing time bounds for the Metropolized random walk (MRW) and obtain O(κ) mixing time slower than MALA. We provide numerical examples that support our theoretical findings, and demonstrate the benefits of Metropolis-Hastings adjustment for Langevin-type sampling algorithms."
            ],
            "keywords": [
                "Log-concave sampling",
                "Langevin algorithms",
                "MCMC algorithms",
                "conductance methods"
            ],
            "author": [
                "Raaz Dwivedi",
                "Yuansi Chen",
                "Martin J Wainwright",
                "Voleon Group"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-306/19-306.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Universal Algorithms for Learning Theory Part I : Piecewise Constant Functions",
            "abstract": [
                "This paper is concerned with the construction and analysis of a universal estimator for the regression problem in supervised learning. Universal means that the estimator does not depend on any a priori assumptions about the regression function to be estimated. The universal estimator studied in this paper consists of a least-square fitting procedure using piecewise constant functions on a partition which depends adaptively on the data. The partition is generated by a splitting procedure which differs from those used in CART algorithms. It is proven that this estimator performs at the optimal convergence rate for a wide class of priors on the regression function. Namely, as will be made precise in the text, if the regression function is in any one of a certain class of approximation spaces (or smoothness spaces of order not exceeding one-a limitation resulting because the estimator uses piecewise constants) measured relative to the marginal measure, then the estimator converges to the regression function (in the least squares sense) with an optimal rate of convergence in terms of the number of samples. The estimator is also numerically feasible and can be implemented on-line."
            ],
            "keywords": [
                "distribution-free learning theory",
                "nonparametric regression",
                "universal algorithms",
                "adaptive approximation",
                "on-line algorithms"
            ],
            "author": [
                "Peter Binev",
                "Albert Cohen",
                "Ronald Devore",
                "Vladimir Temlyakov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/binev05a/binev05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Analysis of Distributed Inference with Vector-Valued Gaussian Belief Propagation",
            "abstract": [
                "This paper considers inference over distributed linear Gaussian models using factor graphs and Gaussian belief propagation (BP). The distributed inference algorithm involves only local computation of the information matrix and of the mean vector, and message passing between neighbors. Under broad conditions, it is shown that the message information matrix converges to a unique positive definite limit matrix for arbitrary positive semidefinite initialization, and it approaches an arbitrarily small neighborhood of this limit matrix at an exponential rate. A necessary and sufficient convergence condition for the belief mean vector to converge to the optimal centralized estimator is provided under the assumption that the message information matrix is initialized as a positive semidefinite matrix. Further, it is shown that Gaussian BP always converges when the underlying factor graph is given by the union of a forest and a single loop. The proposed convergence condition in the setup of distributed linear Gaussian models is shown to be strictly weaker than other existing convergence conditions and requirements, including the Gaussian Markov random field based walk-summability condition, and applicable to a large class of scenarios."
            ],
            "keywords": [
                "Graphical Model",
                "Large-Scale Networks",
                "Linear Gaussian Model",
                "Markov Random Field",
                "Walk-summability"
            ],
            "author": [
                "Jian Du",
                "Shaodan Ma",
                "Yik-Chung Wu",
                "Soummya Kar",
                "José M F Moura"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-556/16-556.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bayesian Mixed-Effects Model to Learn Trajectories of Changes from Repeated Manifold-Valued Observations",
            "abstract": [
                "We propose a generic Bayesian mixed-effects model to estimate the temporal progression of a biological phenomenon from observations obtained at multiple time points for a group of individuals. The progression is modeled by continuous trajectories in the space of measurements. Individual trajectories of progression result from spatiotemporal transformations of an average trajectory. These transformations allow for the quantification of changes in direction and pace at which the trajectories are followed. The framework of Riemannian geometry allows the model to be used with any kind of measurements with smooth constraints. A stochastic version of the Expectation-Maximization algorithm is used to produce maximum a posteriori estimates of the parameters. We evaluated our method using a series of neuropsychological test scores from patients with mild cognitive impairments, later diagnosed with Alzheimer's disease, and simulated evolutions of symmetric positive definite matrices. The data-driven model of impairment of cognitive functions illustrated the variability in the ordering and timing of the decline of these functions in the population. We showed that the estimated spatiotemporal transformations effectively put into correspondence significant events in the progression of individuals."
            ],
            "keywords": [
                "longitudinal model",
                "spatiotemporal analysis",
                "Riemannian geometry",
                "stochastic expectation-maximization algorithm"
            ],
            "author": [
                "Jean-Baptiste Schiratti",
                "Stéphanie Allassonnière",
                "Olivier Colliot",
                "Stanley Durrleman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-197/17-197.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods",
            "abstract": [
                "We propose a highly efficient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the fitting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classification tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work."
            ],
            "keywords": [
                "multi-way classification",
                "kernel logistic regression",
                "hierarchical classification",
                "cross validation optimization",
                "Newton-Raphson optimization"
            ],
            "author": [
                "Matthias W Seeger"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/seeger08b/seeger08b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regression with Comparisons: Escaping the Curse of Dimensionality with Ordinal Information",
            "abstract": [
                "In supervised learning, we typically leverage a fully labeled dataset to design methods for function estimation or prediction. In many practical situations, we are able to obtain alternative feedback, possibly at a low cost. A broad goal is to understand the usefulness of, and to design algorithms to exploit, this alternative feedback. In this paper, we consider a semi-supervised regression setting, where we obtain additional ordinal (or comparison) information for the unlabeled samples. We consider ordinal feedback of varying qualities where we have either a perfect ordering of the samples, a noisy ordering of the samples or noisy pairwise comparisons between the samples. We provide a precise quantification of the usefulness of these types of ordinal feedback in both nonparametric and linear regression, showing that in many cases it is possible to accurately estimate an underlying function with a very small labeled set, effectively escaping the curse of dimensionality. We also present lower bounds, that establish fundamental limits for the task and show that our algorithms are optimal in a variety of settings. Finally, we present extensive experiments on new datasets that demonstrate the efficacy and practicality of our algorithms and investigate their robustness to various sources of noise and model misspecification."
            ],
            "keywords": [
                "Pairwise Comparison",
                "Ranking",
                "Regression",
                "Interactive Learning"
            ],
            "author": [
                "Yichong Xu",
                "Sivaraman Balakrishnan",
                "Aarti Singh",
                "Artur Dubrawski"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-505/19-505.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Inconsistency of Pitman-Yor Process Mixtures for the Number of Components",
            "abstract": [
                "In many applications, a finite mixture is a natural model, but it can be difficult to choose an appropriate number of components. To circumvent this choice, investigators are increasingly turning to Dirichlet process mixtures (DPMs), and Pitman-Yor process mixtures (PYMs), more generally. While these models may be well-suited for Bayesian density estimation, many investigators are using them for inferences about the number of components, by considering the posterior on the number of components represented in the observed data. We show that this posterior is not consistent-that is, on data from a finite mixture, it does not concentrate at the true number of components. This result applies to a large class of nonparametric mixtures, including DPMs and PYMs, over a wide variety of families of component distributions, including essentially all discrete families, as well as continuous exponential families satisfying mild regularity conditions (such as multivariate Gaussians)."
            ],
            "keywords": [
                "consistency",
                "Dirichlet process mixture",
                "number of components",
                "finite mixture",
                "Bayesian nonparametrics"
            ],
            "author": [
                "Jeffrey W Miller",
                "Matthew T Harrison"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/miller14a/miller14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A General Framework for Consistent Structured Prediction with Implicit Loss Embeddings",
            "abstract": [
                "We propose and analyze a novel theoretical and algorithmic framework for structured prediction. While so far the term has referred to discrete output spaces, here we consider more general settings, such as manifolds or spaces of probability measures. We define structured prediction as a problem where the output space lacks a vectorial structure. We identify and study a large class of loss functions that implicitly defines a suitable geometry on the problem. The latter is the key to develop an algorithmic framework amenable to a sharp statistical analysis and yielding efficient computations. When dealing with output spaces with infinite cardinality, a suitable implicit formulation of the estimator is shown to be crucial."
            ],
            "keywords": [
                "Structured Prediction",
                "Statistical Learning Theory",
                "Kernel Methods"
            ],
            "author": [
                "Carlo Ciliberto",
                "Alessandro Rudi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-097/20-097.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Non-linear Causal Inference using Gaussianity Measures",
            "abstract": [
                "We provide theoretical and empirical evidence for a type of asymmetry between causes and effects that is present when these are related via linear models contaminated with additive non-Gaussian noise. Assuming that the causes and the effects have the same distribution, we show that the distribution of the residuals of a linear fit in the anti-causal direction is closer to a Gaussian than the distribution of the residuals in the causal direction. This Gaussianization effect is characterized by reduction of the magnitude of the high-order cumulants and by an increment of the differential entropy of the residuals. The problem of non-linear causal inference is addressed by performing an embedding in an expanded feature space, in which the relation between causes and effects can be assumed to be linear. The effectiveness of a method to discriminate between causes and effects based on this type of asymmetry is illustrated in a variety of experiments using different measures of Gaussianity. The proposed method is shown to be competitive with state-of-the-art techniques for causal inference."
            ],
            "keywords": [
                "causal inference",
                "Gaussianity of the residuals",
                "cause-effect pairs"
            ],
            "author": [
                "Daniel Hernández-Lobato",
                "Pablo Morales-Mombiela",
                "David Lopez-Paz",
                "Alberto Suárez",
                "Isabelle Guyon",
                "Alexander Statnikov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-375/14-375.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimization Techniques for Semi-Supervised Support Vector Machines",
            "abstract": [
                "Due to its wide applicability, the problem of semi-supervised classification is attracting increasing attention in machine learning. Semi-Supervised Support Vector Machines (S 3 VMs) are based on applying the margin maximization principle to both labeled and unlabeled examples. Unlike SVMs, their formulation leads to a non-convex optimization problem. A suite of algorithms have recently been proposed for solving S 3 VMs. This paper reviews key ideas in this literature. The performance and behavior of various S 3 VM algorithms is studied together, under a common experimental setting."
            ],
            "keywords": [
                "semi-supervised learning",
                "support vector machines",
                "non-convex optimization",
                "transductive learning"
            ],
            "author": [
                "Olivier Chapelle"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/chapelle08a/chapelle08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Extension on \"Statistical Comparisons of Classifiers over Multiple Data Sets\" for all Pairwise Comparisons",
            "abstract": [
                "In a recently published paper in JMLR, Demšar (2006) recommends a set of non-parametric statistical tests and procedures which can be safely used for comparing the performance of classifiers over multiple data sets. After studying the paper, we realize that the paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method. However, it does not deal with some advanced topics in depth. Regarding these topics, we focus on more powerful proposals of statistical procedures for comparing n × n classifiers. Moreover, we illustrate an easy way of obtaining adjusted and comparable p-values in multiple comparison procedures."
            ],
            "keywords": [
                "statistical methods",
                "non-parametric test",
                "multiple comparisons tests",
                "adjusted p-values",
                "logically related hypotheses"
            ],
            "author": [
                "Salvador García",
                "Francisco Herrera"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/garcia08a/garcia08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Particle Approximations",
            "abstract": [
                "Approximate inference in high-dimensional, discrete probabilistic models is a central problem in computational statistics and machine learning. This paper describes discrete particle variational inference (DPVI), a new approach that combines key strengths of Monte Carlo, variational and search-based techniques. DPVI is based on a novel family of particle-based variational approximations that can be fit using simple, fast, deterministic search techniques. Like Monte Carlo, DPVI can handle multiple modes, and yields exact results in a well-defined limit. Like unstructured mean-field, DPVI is based on optimizing a lower bound on the partition function; when this quantity is not of intrinsic interest, it facilitates convergence assessment and debugging. Like both Monte Carlo and combinatorial search, DPVI can take advantage of factorization, sequential structure, and custom search operators. This paper defines DPVI particle-based approximation family and partition function lower bounds, along with the sequential DPVI and local DPVI algorithm templates for optimizing them. DPVI is illustrated and evaluated via experiments on lattice Markov Random Fields, nonparametric Bayesian mixtures and block-models, and parametric as well as non-parametric hidden Markov models. Results include applications to real-world spike-sorting and relational modeling problems, and show that DPVI can offer appealing time/accuracy trade-offs as compared to multiple alternatives."
            ],
            "keywords": [
                "Bayesian inference",
                "variational methods",
                "Dirichlet process mixture model",
                "Ising model",
                "hidden Markov model",
                "infinite relational model"
            ],
            "author": [
                "Ardavan Saeedi",
                "Tejas D Kulkarni",
                "Vikash K Mansinghka",
                "Samuel J Gershman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-615/15-615.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Lower and Upper Bounds in Smooth and Strongly Convex Optimization",
            "abstract": [
                "We develop a novel framework to study smooth and strongly convex optimization algorithms. Focusing on quadratic functions we are able to examine optimization algorithms as a recursive application of linear operators. This, in turn, reveals a powerful connection between a class of optimization algorithms and the analytic theory of polynomials whereby new lower and upper bounds are derived. Whereas existing lower bounds for this setting are only valid when the dimensionality scales with the number of iterations, our lower bound holds in the natural regime where the dimensionality is fixed. Lastly, expressing it as an optimal solution for the corresponding optimization problem over polynomials, as formulated by our framework, we present a novel systematic derivation of Nesterov's well-known Accelerated Gradient Descent method. This rather natural interpretation of AGD contrasts with earlier ones which lacked a simple, yet solid, motivation."
            ],
            "keywords": [
                "smooth and strongly convex optimization",
                "full gradient descent",
                "accelerated gradient descent",
                "heavy ball method"
            ],
            "author": [
                "Yossi Arjevani",
                "Shai Shalev-Shwartz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-106/15-106.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Tensor Methods for Learning Latent Variable Models",
            "abstract": [
                "We introduce an online tensor decomposition based approach for two latent variable modeling problems namely, (1) community detection, in which we learn the latent communities that the social actors in social networks belong to, and (2) topic modeling, in which we infer hidden topics of text articles. We consider decomposition of moment tensors using stochastic gradient descent. We conduct optimization of multilinear operations in SGD and avoid directly forming the tensors, to save computational and storage costs. We present optimized algorithm in two platforms. Our GPU-based implementation exploits the parallelism of SIMD architectures to allow for maximum speed-up by a careful optimization of storage and data transfer, whereas our CPU-based implementation uses efficient sparse matrix computations and is suitable for large sparse data sets. For the community detection problem, we demonstrate accuracy and computational efficiency on Facebook, Yelp and DBLP data sets, and for the topic modeling problem, we also demonstrate good performance on the New York Times data set. We compare our results to the state-of-the-art algorithms such as the variational method, and report a gain of accuracy and a gain of several orders of magnitude in the execution time."
            ],
            "keywords": [
                "mixed membership stochastic blockmodel",
                "topic modeling",
                "tensor method",
                "stochastic gradient descent",
                "parallel implementation",
                "large datasets"
            ],
            "author": [
                "Furong Huang",
                "Mohammad Umar Hakeem"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/huang15a/huang15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Marginalizing Stacked Linear Denoising Autoencoders",
            "abstract": [
                "Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. They have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized Stacked Linear Denoising Autoencoder (mSLDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSLDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters-in fact, the linear formulation gives rise to a closed-form solution. Consequently, mSLDA, which can be implemented in only 20 lines of MATLAB TM , is about two orders of magnitude faster than a corresponding SDA. Furthermore, the representations learnt by mSLDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks."
            ],
            "keywords": [
                "domain adaption",
                "fast representation learning",
                "noise marginalization",
                "denoising autoencoders"
            ],
            "author": [
                "Minmin Chen",
                "Kilian Q Weinberger",
                "Sha Fei"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/chen15c/chen15c.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Iterative Reweighted Algorithms for Matrix Rank Minimization",
            "abstract": [
                "The problem of minimizing the rank of a matrix subject to affine constraints has applications in several areas including machine learning, and is known to be NP-hard. A tractable relaxation for this problem is nuclear norm (or trace norm) minimization, which is guaranteed to find the minimum rank matrix under suitable assumptions. In this paper, we propose a family of Iterative Reweighted Least Squares algorithms IRLS-p (with 0 ≤ p ≤ 1), as a computationally efficient way to improve over the performance of nuclear norm minimization. The algorithms can be viewed as (locally) minimizing certain smooth approximations to the rank function. When p = 1, we give theoretical guarantees similar to those for nuclear norm minimization, that is, recovery of low-rank matrices under certain assumptions on the operator defining the constraints. For p < 1, IRLSp shows better empirical performance in terms of recovering low-rank matrices than nuclear norm minimization. We provide an efficient implementation for IRLS-p, and also present a related family of algorithms, sIRLS-p. These algorithms exhibit competitive run times and improved recovery when compared to existing algorithms for random instances of the matrix completion problem, as well as on the MovieLens movie recommendation data set."
            ],
            "keywords": [
                "matrix rank minimization",
                "matrix completion",
                "iterative algorithms",
                "null-space property"
            ],
            "author": [
                "Karthik Mohan",
                "Maryam Fazel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/mohan12a/mohan12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sobolev Norm Learning Rates for Regularized Least-Squares Algorithms",
            "abstract": [
                "Learning rates for least-squares regression are typically expressed in terms of L 2-norms. In this paper we extend these rates to norms stronger than the L 2-norm without requiring the regression function to be contained in the hypothesis space. In the special case of Sobolev reproducing kernel Hilbert spaces used as hypotheses spaces, these stronger norms coincide with fractional Sobolev norms between the used Sobolev space and L 2. As a consequence, not only the target function but also some of its derivatives can be estimated without changing the algorithm. From a technical point of view, we combine the well-known integral operator techniques with an embedding property, which so far has only been used in combination with empirical process arguments. This combination results in new finite sample bounds with respect to the stronger norms. From these finite sample bounds our rates easily follow. Finally, we prove the asymptotic optimality of our results in many cases."
            ],
            "keywords": [
                "statistical learning theory",
                "regularized kernel methods",
                "least-squares regression",
                "interpolation norms",
                "uniform convergence",
                "learning rates"
            ],
            "author": [
                "Simon Fischer",
                "Ingo Steinwart"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-734/19-734.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging",
            "abstract": [
                "We address the statistical and optimization impacts of the classical sketch and Hessian sketch used to approximately solve the Matrix Ridge Regression (MRR) problem. Prior research has quantified the effects of classical sketch on the strictly simpler least squares regression (LSR) problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR: namely, it recovers nearly optimal solutions. By contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the \"mass\" in the responses and the optimal objective value."
            ],
            "keywords": [
                "Randomized Linear Algebra",
                "Matrix Sketching",
                "Ridge Regression"
            ],
            "author": [
                "Shusen Wang",
                "Alex Gittens",
                "Michael W Mahoney"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-313/17-313.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Cross-Corpora Unsupervised Learning of Trajectories in Autism Spectrum Disorders",
            "abstract": [
                "Patients with developmental disorders, such as autism spectrum disorder (ASD), present with symptoms that change with time even if the named diagnosis remains fixed. For example, language impairments may present as delayed speech in a toddler and difficulty reading in a school-age child. Characterizing these trajectories is important for early treatment. However, deriving these trajectories from observational sources is challenging: electronic health records only reflect observations of patients at irregular intervals and only record what factors are clinically relevant at the time of observation. Meanwhile, caretakers discuss daily developments and concerns on social media. In this work, we present a fully unsupervised approach for learning disease trajectories from incomplete medical records and social media posts, including cases in which we have only a single observation of each patient. In particular, we use a dynamic topic model approach which embeds each disease trajectory as a path in R D. A Pólya-gamma augmentation scheme is used to efficiently perform inference as well as incorporate multiple data sources. We learn disease trajectories from the electronic health records of 13,435 patients with ASD and the forum posts of 13,743 caretakers of children with ASD, deriving interesting clinical insights as well as good predictions."
            ],
            "keywords": [],
            "author": [
                "Huseyin Melih Elibol",
                "Vincent Nguyen",
                "Scott Linderman",
                "Matthew Johnson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-431/15-431.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning",
            "abstract": [
                "Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms."
            ],
            "keywords": [
                "energy efficiency",
                "green computing",
                "reinforcement learning",
                "deep learning",
                "climate change"
            ],
            "author": [
                "Peter Henderson",
                "Jieru Hu",
                "Joshua Romoff",
                "Emma Brunskill",
                "Dan Jurafsky",
                "Joelle Pineau"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-312/20-312.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generating Weighted MAX-2-SAT Instances with Frustrated Loops: an RBM Case Study",
            "abstract": [
                "Many optimization problems can be cast into the maximum satisfiability (MAX-SAT) form, and many solvers have been developed for tackling such problems. To evaluate a MAX-SAT solver, it is convenient to generate hard MAX-SAT instances with known solutions. Here, we propose a method of generating weighted MAX-2-SAT instances inspired by the frustrated-loop algorithm used by the quantum annealing community. We extend the algorithm for instances of general bipartite couplings, with the associated optimization problem being the minimization of the restricted Boltzmann machine (RBM) energy over the nodal values, which is useful for effectively pre-training the RBM. The hardness of the generated instances can be tuned through a central parameter known as the frustration index. Two versions of the algorithm are presented: the random-and structured-loop algorithms. For the random-loop algorithm, we provide a thorough theoretical and empirical analysis on its mathematical properties from the perspective of frustration, and observe empirically a double phase transition behavior in the hardness scaling behavior driven by the frustration index. For the structured-loop algorithm, we show that it offers an improvement in hardness over the random-loop algorithm in the regime of high loop density, with the variation of hardness tunable through the concentration of frustrated weights."
            ],
            "keywords": [
                "Maximum Satisfiability",
                "Restricted Boltzmann Machine",
                "Frustration Index",
                "Loop Algorithm",
                "Phase Transition"
            ],
            "author": [
                "Yan Ru",
                "Haik Manukian",
                "Massimiliano Di Ventra"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-368/19-368.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Probabilistic Line Searches for Stochastic Optimization",
            "abstract": [
                "In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent."
            ],
            "keywords": [
                "stochastic optimization",
                "learning rates",
                "line searches",
                "Gaussian processes",
                "Bayesian optimization"
            ],
            "author": [
                "Maren Mahsereci"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-049/17-049.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distinguishing Cause from Effect Using Observational Data: Methods and Benchmarks",
            "abstract": [
                "of the best performing methods overall is the method based on Additive Noise Models that has originally been proposed by Hoyer et al. (2009), which obtains an accuracy of 63 ± 10 % and an AUC of 0.74 ± 0.05 on the real-world benchmark. As the main theoretical contribution of this work we prove the consistency of that method."
            ],
            "keywords": [
                "Causal discovery",
                "additive noise",
                "information-geometric causal inference",
                "cause-effect pairs",
                "benchmarks"
            ],
            "author": [
                "Joris M Mooij",
                "Jonas Peters",
                "Jakob Zscheischler",
                "Bernhard Schölkopf",
                "Isabelle Guyon",
                "Alexander Statnikov",
                "C Joris",
                "Marten Mooij",
                "Dominik Janzing",
                "Peters, Janzing, Zscheischler Schölkopf Mooij"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-518/14-518.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "R-max -A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning",
            "abstract": [
                "R-max is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-max, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-max improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh's E 3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the \"optimism under uncertainty\" bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo."
            ],
            "keywords": [
                "Reinforcement Learning",
                "Learning in Games",
                "Stochastic Games",
                "Markov Decision Processes",
                "Provably Efficient Learning"
            ],
            "author": [
                "Ronen I Brafman",
                "Moshe Tennenholtz"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/brafman02a/brafman02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning and Interpreting Multi-Multi-Instance Learning Networks",
            "abstract": [
                "We introduce an extension of the multi-instance learning problem where examples are organized as nested bags of instances (e.g., a document could be represented as a bag of sentences, which in turn are bags of words). This framework can be useful in various scenarios, such as text and image classification, but also supervised learning over graphs. As a further advantage, multi-multi instance learning enables a particular way of interpreting predictions and the decision function. Our approach is based on a special neural network layer, called bag-layer, whose units aggregate bags of inputs of arbitrary size. We prove theoretically that the associated class of functions contains all Boolean functions over sets of sets of instances and we provide empirical evidence that functions of this kind can be actually learned on semi-synthetic datasets. We finally present experiments on text classification, on citation graphs, and social graph data, which show that our model obtains competitive results with respect to accuracy when compared to other approaches such as convolutional networks on graphs, while at the same time it supports a general approach to interpret the learnt model, as well as explain individual predictions."
            ],
            "keywords": [
                "Multi-multi instance learning",
                "relational learning",
                "deep learning"
            ],
            "author": [
                "Alessandro Tibo",
                "Manfred Jaeger",
                "Paolo Frasconi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-811/18-811.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Transfer Learning Decision Forests for Gesture Recognition",
            "abstract": [
                "Decision forests are an increasingly popular tool in computer vision problems. Their advantages include high computational efficiency, state-of-the-art accuracy and multi-class support. In this paper, we present a novel method for transfer learning which uses decision forests, and we apply it to recognize gestures and characters. We introduce two mechanisms into the decision forest framework in order to transfer knowledge from the source tasks to a given target task. The first one is mixed information gain, which is a databased regularizer. The second one is label propagation, which infers the manifold structure of the feature space. We show that both of them are important to achieve higher accuracy. Our experiments demonstrate improvements over traditional decision forests in the ChaLearn Gesture Challenge and MNIST data set. They also compare favorably against other state-of-the-art classifiers."
            ],
            "keywords": [
                "decision forests",
                "transfer learning",
                "gesture recognition"
            ],
            "author": [
                "Norberto A Goussies",
                "Marta Mejail",
                "Isabelle Guyon",
                "Vassilis Athitsos",
                "Sergio Escalera"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/goussies14a/goussies14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": null,
            "abstract": NaN,
            "keywords": [],
            "author": [],
            "ref": "http://www.jmlr.org/papers/volume1/boyan00a/boyan00a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DALEX: Explainers for Complex Predictive Models in R Przemys law Biecek",
            "abstract": [
                "Predictive modeling is invaded by elastic, yet complex methods such as neural networks or ensembles (model stacking, boosting or bagging). Such methods are usually described by a large number of parameters or hyper parameters-a price that one needs to pay for elasticity. The very number of parameters makes models hard to understand. This paper describes a consistent collection of explainers for predictive models, a.k.a. black boxes. Each explainer is a technique for exploration of a black box model. Presented approaches are model-agnostic, what means that they extract useful information from any predictive method irrespective of its internal structure. Each explainer is linked with a specific aspect of a model. Some are useful in decomposing predictions, some serve better in understanding performance, while others are useful in understanding importance and conditional responses of a particular variable. Every explainer presented here works for a single model or for a collection of models. In the latter case, models can be compared against each other. Such comparison helps to find strengths and weaknesses of different models and gives additional tools for model validation. Presented explainers are implemented in the DALEX package for R. They are based on a uniform standardized grammar of model exploration which may be easily extended."
            ],
            "keywords": [
                "interpretable machine learning",
                "explainable artificial intelligence",
                "predictive modelling",
                "model visualization"
            ],
            "author": [],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-416/18-416.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust and Scalable Bayes via a Median of Subset Posterior Measures",
            "abstract": [
                "We propose a novel approach to Bayesian analysis that is provably robust to outliers in the data and often has computational advantages over standard methods. Our technique is based on splitting the data into non-overlapping subgroups, evaluating the posterior distribution given each independent subgroup, and then combining the resulting measures. The main novelty of our approach is the proposed aggregation step, which is based on the evaluation of a median in the space of probability measures equipped with a suitable collection of distances that can be quickly and efficiently evaluated in practice. We present both theoretical and numerical evidence illustrating the improvements achieved by our method."
            ],
            "keywords": [
                "Big data",
                "geometric median",
                "distributed computing",
                "parallel MCMC",
                "Wasserstein distance"
            ],
            "author": [
                "Stanislav Minsker",
                "Sanvesh Srivastava",
                "Lizhen Lin",
                "David B Dunson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-655/16-655.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Estimation of Causal Direction in Acyclic Structural Equation Models with Individual-specific Confounder Variables and Non-Gaussian Distributions",
            "abstract": [
                "Several existing methods have been shown to consistently estimate causal direction assuming linear or some form of nonlinear relationship and no latent confounders. However, the estimation results could be distorted if either assumption is violated. We develop an approach to determining the possible causal direction between two observed variables when latent confounding variables are present. We first propose a new linear non-Gaussian acyclic structural equation model with individual-specific effects that are sometimes the source of confounding. Thus, modeling individual-specific effects as latent variables allows latent confounding to be considered. We then propose an empirical Bayesian approach for estimating possible causal direction using the new model. We demonstrate the effectiveness of our method using artificial and real-world data."
            ],
            "keywords": [
                "structural equation models",
                "Bayesian networks",
                "estimation of causal direction",
                "latent confounding variables",
                "non-Gaussianity"
            ],
            "author": [
                "Shohei Shimizu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/shimizu14a/shimizu14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Asynchronous Parallel Stochastic Coordinate Descent Algorithm",
            "abstract": [
                "We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate (1/K) on general convex functions. Near-linear speedup on a multicore system can be expected if the number of processors is O(n 1/2) in unconstrained optimization and O(n 1/4) in the separable-constrained case, where n is the number of variables. We describe results from implementation on 40-core processors."
            ],
            "keywords": [],
            "author": [
                "Ji Liu",
                "Stephen J Wright",
                "Christopher Ré",
                "Victor Bittorf",
                "Srikrishna Sridhar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/liu15a/liu15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Experiment Selection for Causal Discovery",
            "abstract": [
                "Randomized controlled experiments are often described as the most reliable tool available to scientists for discovering causal relationships among quantities of interest. However, it is often unclear how many and which different experiments are needed to identify the full (possibly cyclic) causal structure among some given (possibly causally insufficient) set of variables. Recent results in the causal discovery literature have explored various identifiability criteria that depend on the assumptions one is able to make about the underlying causal process, but these criteria are not directly constructive for selecting the optimal set of experiments. Fortunately, many of the needed constructions already exist in the combinatorics literature, albeit under terminology which is unfamiliar to most of the causal discovery community. In this paper we translate the theoretical results and apply them to the concrete problem of experiment selection. For a variety of settings we give explicit constructions of the optimal set of experiments and adapt some of the general combinatorics results to answer questions relating to the problem of experiment selection."
            ],
            "keywords": [
                "causality",
                "randomized experiments",
                "experiment selection",
                "separating systems",
                "completely separating systems",
                "cut-coverings"
            ],
            "author": [
                "Antti Hyttinen",
                "Frederick Eberhardt",
                "Patrik O Hoyer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/hyttinen13a/hyttinen13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning",
            "abstract": [
                "This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Specifically, by introducing a definition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary significantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments confirm the superiority of the normalization scheme motivated by learning theory on artificial and real-world data sets."
            ],
            "keywords": [
                "transductive learning",
                "graph learning",
                "Laplacian regularization",
                "normalization of graph Laplacian"
            ],
            "author": [
                "Rie Johnson",
                "Tong Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/johnson07a/johnson07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Approximate MCMC Algorithms for the Horseshoe Prior",
            "abstract": [
                "The horseshoe prior is frequently employed in Bayesian analysis of high-dimensional models, and has been shown to achieve minimax optimal risk properties when the truth is sparse. While optimization-based algorithms for the extremely popular Lasso and elastic net procedures can scale to dimension in the hundreds of thousands, algorithms for the horseshoe that use Markov chain Monte Carlo (MCMC) for computation are limited to problems an order of magnitude smaller. This is due to high computational cost per step and growth of the variance of time-averaging estimators as a function of dimension. We propose two new MCMC algorithms for computation in these models that have significantly improved performance compared to existing alternatives. One of the algorithms also approximates an expensive matrix product to give orders of magnitude speedup in high-dimensional applications. We prove guarantees for the accuracy of the approximate algorithm, and show that gradually decreasing the approximation error as the chain extends results in an exact algorithm. The scalability of the algorithm is illustrated in simulations with problem size as large as N = 5, 000 observations and p = 50, 000 predictors, and an application to a genome-wide association study with N = 2, 267 and p = 98, 385. The empirical results also show that the new algorithm yields estimates with lower mean squared error, intervals with better coverage, and elucidates features of the posterior that were often missed by previous algorithms in high dimensions, including bimodality of posterior marginals indicating uncertainty about which covariates belong in the model."
            ],
            "keywords": [
                "Bayesian",
                "High dimensional",
                "MCMC approximation",
                "Perturbation theory",
                "Shrinkage prior"
            ],
            "author": [
                "James Johndrow",
                "Paulo Orenstein",
                "Anirban Bhattacharya"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-536/19-536.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Learning with Feedback on Both Features and Instances",
            "abstract": [
                "We extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization. Our experiments on a variety of categorization tasks indicate that there is significant potential in improving classifier performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features. Our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion of the most relevant features (over 50% in our experiments). We find that on average, labeling a feature takes much less time than labeling a document. We devise an algorithm that interleaves labeling features and documents which significantly accelerates standard active learning in our simulation experiments. Feature feedback can complement traditional active learning in applications such as news filtering, e-mail classification, and personalization, where the human teacher can have significant knowledge on the relevance of features."
            ],
            "keywords": [
                "active learning",
                "feature selection",
                "relevance feedback",
                "term feedback",
                "text classification"
            ],
            "author": [
                "Hema Raghavan",
                "Omid Madani",
                "Rosie Jones"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/raghavan06a/raghavan06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistent Algorithms for Clustering Time Series",
            "abstract": [
                "The problem of clustering is considered for the case where every point is a time series. The time series are either given in one batch (offline setting), or they are allowed to grow with time and new time series can be added along the way (online setting). We propose a natural notion of consistency for this problem, and show that there are simple, computationally efficient algorithms that are asymptotically consistent under extremely weak assumptions on the distributions that generate the data. The notion of consistency is as follows. A clustering algorithm is called consistent if it places two time series into the same cluster if and only if the distribution that generates them is the same. In the considered framework the time series are allowed to be highly dependent, and the dependence can have arbitrary form. If the number of clusters is known, the only assumption we make is that the (marginal) distribution of each time series is stationary ergodic. No parametric, memory or mixing assumptions are made. When the number of clusters is unknown, stronger assumptions are provably necessary, but it is still possible to devise nonparametric algorithms that are consistent under very general conditions. The theoretical findings of this work are illustrated with experiments on both synthetic and real data."
            ],
            "keywords": [
                "clustering",
                "time series",
                "ergodicity",
                "unsupervised learning"
            ],
            "author": [
                "Azadeh Khaleghi",
                "Daniil Ryabko",
                "Jérémie Mary"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/khaleghi16a/khaleghi16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Augmentable Gamma Belief Networks",
            "abstract": [
                "To infer multilayer deep representations of high-dimensional discrete and nonnegative real vectors, we propose an augmentable gamma belief network (GBN) that factorizes each of its hidden layers into the product of a sparse connection weight matrix and the nonnegative real hidden units of the next layer. The GBN's hidden layers are jointly trained with an upward-downward Gibbs sampler that solves each layer with the same subroutine. The gamma-negative binomial process combined with a layer-wise training strategy allows inferring the width of each layer given a fixed budget on the width of the first layer. Example results illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the GBN can add more layers to improve its performance in both unsupervisedly extracting features and predicting heldout data. For exploratory data analysis, we extract trees and subnetworks from the learned deep network to visualize how the very specific factors discovered at the first hidden layer and the increasingly more general factors discovered at deeper hidden layers are related to each other, and we generate synthetic data by propagating random variables through the deep network from the top hidden layer back to the bottom data layer."
            ],
            "keywords": [
                "Bayesian nonparametrics",
                "deep learning",
                "multilayer representation",
                "Poisson factor analysis",
                "topic modeling",
                "unsupervised learning"
            ],
            "author": [
                "Mingyuan Zhou",
                "Yulai Cong",
                "Bo Chen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-633/15-633.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Tensor Regression",
            "abstract": [
                "We propose a Bayesian approach to regression with a scalar response on vector and tensor covariates. Vectorization of the tensor prior to analysis fails to exploit the structure, often leading to poor estimation and predictive performance. We introduce a novel class of multiway shrinkage priors for tensor coefficients in the regression setting and present posterior consistency results under mild conditions. A computationally efficient Markov chain Monte Carlo algorithm is developed for posterior computation. Simulation studies illustrate substantial gains over existing tensor regression methods in terms of estimation and parameter inference. Our approach is further illustrated in a neuroimaging application."
            ],
            "keywords": [
                "Multiway Shrinkage Prior",
                "Magnetic Resonance Imaging (MRI)",
                "Parafac Decomposition",
                "Posterior Consistency",
                "Tensor Regression"
            ],
            "author": [
                "Rajarshi Guhaniyogi",
                "Shaan Qamar",
                "David B Dunson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-362/16-362.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Plug-in Approach to Neyman-Pearson Classification",
            "abstract": [
                "The Neyman-Pearson (NP) paradigm in binary classification treats type I and type II errors with different priorities. It seeks classifiers that minimize type II error, subject to a type I error constraint under a user specified level α. In this paper, plug-in classifiers are developed under the NP paradigm. Based on the fundamental Neyman-Pearson Lemma, we propose two related plug-in classifiers which amount to thresholding respectively the class conditional density ratio and the regression function. These two classifiers handle different sampling schemes. This work focuses on theoretical properties of the proposed classifiers; in particular, we derive oracle inequalities that can be viewed as finite sample versions of risk bounds. NP classification can be used to address anomaly detection problems, where asymmetry in errors is an intrinsic property. As opposed to a common practice in anomaly detection that consists of thresholding normal class density, our approach does not assume a specific form for anomaly distributions. Such consideration is particularly necessary when the anomaly class density is far from uniformly distributed."
            ],
            "keywords": [
                "plug-in approach",
                "Neyman-Pearson paradigm",
                "nonparametric statistics",
                "oracle inequality",
                "anomaly detection"
            ],
            "author": [
                "Xin Tong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/tong13a/tong13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Extremal Mechanisms for Local Differential Privacy",
            "abstract": [
                "Local differential privacy has recently surfaced as a strong measure of privacy in contexts where personal information remains private even from data analysts. Working in a setting where both the data providers and data analysts want to maximize the utility of statistical analyses performed on the released data, we study the fundamental trade-off between local differential privacy and utility. This trade-off is formulated as a constrained optimization problem: maximize utility subject to local differential privacy constraints. We introduce a combinatorial family of extremal privatization mechanisms, which we call staircase mechanisms, and show that it contains the optimal privatization mechanisms for a broad class of information theoretic utilities such as mutual information and f-divergences. We further prove that for any utility function and any privacy level, solving the privacy-utility maximization problem is equivalent to solving a finite-dimensional linear program, the outcome of which is the optimal staircase mechanism. However, solving this linear program can be computationally expensive since it has a number of variables that is exponential in the size of the alphabet the data lives in. To account for this, we show that two simple privatization mechanisms, the binary and randomized response mechanisms, are universally optimal in the low and high privacy regimes, and well approximate the intermediate regime."
            ],
            "keywords": [
                "local differential privacy",
                "privacy-preserving machine learning algorithms",
                "information theoretic utilities",
                "f -divergences",
                "mutual information",
                "statistical inference",
                "hypothesis testing",
                "estimation"
            ],
            "author": [
                "Peter Kairouz",
                "Sewoong Oh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-135/15-135.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with Minimax Optimal Rates",
            "abstract": [
                "We study a decomposition-based scalable approach to kernel ridge regression, and show that it achieves minimax optimal convergence rates under relatively mild conditions. The method is simple to describe: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent kernel ridge regression estimator for each subset using a careful choice of the regularization parameter, then averages the local solutions into a global predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing kernel ridge regression on all N samples. Our two main theorems establish that despite the computational speed-up, statistical optimality is retained: as long as m is not too large, the partition-based estimator achieves the statistical minimax rate over all estimators using the set of N samples. As concrete examples, our theory guarantees that the number of subsets m may grow nearly linearly for finite-rank or Gaussian kernels and polynomially in N for Sobolev spaces, which in turn allows for substantial reductions in computational cost. We conclude with experiments on both simulated data and a music-prediction task that complement our theoretical results, exhibiting the computational and statistical benefits of our approach."
            ],
            "keywords": [
                "kernel ridge regression",
                "divide and conquer",
                "computation complexity"
            ],
            "author": [
                "Yuchen Zhang",
                "John Duchi",
                "Martin Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/zhang15d/zhang15d.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Certifiably Optimal Rule Lists for Categorical Data",
            "abstract": [
                "We present the design and implementation of a custom discrete optimization technique for building rule lists over a categorical feature space. Our algorithm produces rule lists with optimal training performance, according to the regularized empirical risk, with a certificate of optimality. By leveraging algorithmic bounds, efficient data structures, and computational reuse, we achieve several orders of magnitude speedup in time and a massive reduction of memory consumption. We demonstrate that our approach produces optimal rule lists on practical problems in seconds. Our results indicate that it is possible to construct optimal sparse rule lists that are approximately as accurate as the COMPAS proprietary risk prediction tool on data from Broward County, Florida, but that are completely interpretable. This framework is a novel alternative to CART and other decision tree methods for interpretable modeling."
            ],
            "keywords": [
                "rule lists",
                "decision trees",
                "optimization",
                "interpretable models",
                "criminal justice applications"
            ],
            "author": [
                "Elaine Angelino",
                "Daniel Alabi",
                "Margo Seltzer",
                "Cynthia Rudin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-716/17-716.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classification Using Geometric Level Sets",
            "abstract": [
                "A variational level set method is developed for the supervised classification problem. Nonlinear classifier decision boundaries are obtained by minimizing an energy functional that is composed of an empirical risk term with a margin-based loss and a geometric regularization term new to machine learning: the surface area of the decision boundary. This geometric level set classifier is analyzed in terms of consistency and complexity through the calculation of its ε-entropy. For multicategory classification, an efficient scheme is developed using a logarithmic number of decision functions in the number of classes rather than the typical linear number of decision functions. Geometric level set classification yields performance results on benchmark data sets that are competitive with well-established methods."
            ],
            "keywords": [
                "level set methods",
                "nonlinear classification",
                "geometric regularization",
                "consistency",
                "complexity"
            ],
            "author": [
                "Kush R Varshney",
                "Alan S Willsky"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/varshney10a/varshney10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classifying With Confidence From Incomplete Information",
            "abstract": [
                "We consider the problem of classifying a test sample given incomplete information. This problem arises naturally when data about a test sample is collected over time, or when costs must be incurred to compute the classification features. For example, in a distributed sensor network only a fraction of the sensors may have reported measurements at a certain time, and additional time, power, and bandwidth is needed to collect the complete data to classify. A practical goal is to assign a class label as soon as enough data is available to make a good decision. We formalize this goal through the notion of reliability-the probability that a label assigned given incomplete data would be the same as the label assigned given the complete data, and we propose a method to classify incomplete data only if some reliability threshold is met. Our approach models the complete data as a random variable whose distribution is dependent on the current incomplete data and the (complete) training data. The method differs from standard imputation strategies in that our focus is on determining the reliability of the classification decision, rather than just the class label. We show that the method provides useful reliability estimates of the correctness of the imputed class labels on a set of experiments on time-series data sets, where the goal is to classify the time-series as early as possible while still guaranteeing that the reliability threshold is met."
            ],
            "keywords": [
                "classification",
                "sensor networks",
                "signals",
                "reliability"
            ],
            "author": [
                "Nathan Parrish",
                "Hyrum S Anderson",
                "Maya R Gupta",
                "Dun Yu Hsiao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/parrish13a/parrish13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems",
            "abstract": [
                "A new class of non-homogeneous state-affine systems is introduced for use in reservoir computing. Sufficient conditions are identified that guarantee first, that the associated reservoir computers with linear readouts are causal, time-invariant, and satisfy the fading memory property and second, that a subset of this class is universal in the category of fading memory filters with stochastic almost surely uniformly bounded inputs. This means that any discrete-time filter that satisfies the fading memory property with random inputs of that type can be uniformly approximated by elements in the non-homogeneous state-affine family."
            ],
            "keywords": [
                "reservoir computing",
                "universality",
                "state-affine systems",
                "SAS",
                "echo state networks",
                "ESN",
                "echo state affine systems",
                "machine learning",
                "fading memory property",
                "linear training",
                "stochastic signal treatment"
            ],
            "author": [
                "Lyudmila Grigoryeva"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-020/18-020.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Improved GLMNET for L1-regularized Logistic Regression",
            "abstract": [
                "Recently, Yuan et al. (2010) conducted a comprehensive comparison on software for L1-regularized classification. They concluded that a carefully designed coordinate descent implementation CDN is the fastest among state-of-the-art solvers. In this paper, we point out that CDN is less competitive on loss functions that are expensive to compute. In particular, CDN for logistic regression is much slower than CDN for SVM because the logistic loss involves expensive exp/log operations. In optimization, Newton methods are known to have fewer iterations although each iteration costs more. Because solving the Newton sub-problem is independent of the loss calculation, this type of methods may surpass CDN under some circumstances. In L1-regularized classification, GLMNET by Friedman et al. is already a Newton-type method, but experiments in Yuan et al. (2010) indicated that the existing GLMNET implementation may face difficulties for some largescale problems. In this paper, we propose an improved GLMNET to address some theoretical and implementation issues. In particular, as a Newton-type method, GLMNET achieves fast local convergence, but may fail to quickly obtain a useful solution. By a careful design to adjust the effort for each iteration, our method is efficient for both loosely or strictly solving the optimization problem. Experiments demonstrate that our improved GLMNET is more efficient than CDN for L1-regularized logistic regression."
            ],
            "keywords": [
                "L1 regularization",
                "linear classification",
                "optimization methods",
                "logistic regression",
                "support vector machines"
            ],
            "author": [
                "Guo-Xun Yuan",
                "Chia-Hua Ho",
                "Chih-Jen Lin",
                "S Sathiya Keerthi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/yuan12a/yuan12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms *",
            "abstract": [
                "We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where subsets of base arms with unknown distributions form super arms. In each round, a super arm is played and the base arms contained in the super arm are played and their outcomes are observed. We further consider the extension in which"
            ],
            "keywords": [
                "combinatorial multi-armed bandit",
                "online learning",
                "upper confidence bound",
                "social influence maximization",
                "online advertising"
            ],
            "author": [
                "Wei Chen",
                "Yang Yuan",
                "Yajun Wang",
                "Qinshi Wang",
                "Wang Yuan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-298/14-298.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Deep Learning the Ising Model Near Criticality",
            "abstract": [
                "It is well established that neural networks with deep architectures perform better than shallow networks for many tasks in machine learning. In statistical physics, while there has been recent interest in representing physical data with generative modelling, the focus has been on shallow neural networks. A natural question to ask is whether deep neural networks hold any advantage over shallow networks in representing such data. We investigate this question by using unsupervised, generative graphical models to learn the probability distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep belief networks, and deep restricted Boltzmann networks are trained on thermal spin configurations from this system, and compared to the shallow architecture of the restricted Boltzmann machine. We benchmark the models, focussing on the accuracy of generating energetic observables near the phase transition, where these quantities are most difficult to approximate. Interestingly, after training the generative networks, we observe that the accuracy essentially depends only on the number of neurons in the first hidden layer of the network, and not on other model details such as network depth or model type. This is evidence that shallow networks are more efficient than deep networks at representing physical probability distributions associated with Ising systems near criticality."
            ],
            "keywords": [
                "deep learning",
                "restricted Boltzmann machine",
                "deep belief network",
                "deep Boltzmann machine"
            ],
            "author": [
                "Alan Morningstar",
                "Roger G Melko"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-527/17-527.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistent Model Selection Criteria on High Dimensions",
            "abstract": [
                "Asymptotic properties of model selection criteria for high-dimensional regression models are studied where the dimension of covariates is much larger than the sample size. Several sufficient conditions for model selection consistency are provided. Non-Gaussian error distributions are considered and it is shown that the maximal number of covariates for model selection consistency depends on the tail behavior of the error distribution. Also, sufficient conditions for model selection consistency are given when the variance of the noise is neither known nor estimated consistently. Results of simulation studies as well as real data analysis are given to illustrate that finite sample performances of consistent model selection criteria can be quite different."
            ],
            "keywords": [
                "model selection consistency",
                "general information criteria",
                "high dimension",
                "regression"
            ],
            "author": [
                "Yongdai Kim",
                "Hosik Choi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/kim12a/kim12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularization on Graphs with Function-adapted Diffusion Processes",
            "abstract": [
                "Harmonic analysis and diffusion on discrete data has been shown to lead to state-of-the-art algorithms for machine learning tasks, especially in the context of semi-supervised and transductive learning. The success of these algorithms rests on the assumption that the function(s) to be studied (learned, interpolated, etc.) are smooth with respect to the geometry of the data. In this paper we present a method for modifying the given geometry so the function(s) to be studied are smoother with respect to the modified geometry, and thus more amenable to treatment using harmonic analysis methods. Among the many possible applications, we consider the problems of image denoising and transductive classification. In both settings, our approach improves on standard diffusion based methods."
            ],
            "keywords": [
                "diffusion processes",
                "diffusion geometry",
                "spectral graph theory",
                "image denoising",
                "transductive learning",
                "semi-supervised learning"
            ],
            "author": [
                "Arthur D Szlam",
                "Mauro Maggioni",
                "Mauro Maggioni@",
                "Duke Edu",
                "Ronald R Coifman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/szlam08a/szlam08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Note on the Sample Complexity of the Er-SpUD Algorithm by Spielman, Wang and Wright for Exact Recovery of Sparsely Used Dictionaries",
            "abstract": [
                "We consider the problem of recovering an invertible n×n matrix A and a sparse n×p random matrix X based on the observation of Y = AX (up to a scaling and permutation of columns of A and rows of X). Using only elementary tools from the theory of empirical processes we show that a version of the Er-SpUD algorithm by Spielman, Wang and Wright with high probability recovers A and X exactly, provided that p ≥ Cn log n, which is optimal up to the constant C."
            ],
            "keywords": [
                "sparse dictionaries",
                "Er-SpUD algorithm",
                "1 minimization",
                "exact recovery",
                "sample complexity"
            ],
            "author": [],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-047/16-047.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tree-Based Batch Mode Reinforcement Learning",
            "abstract": [
                "Reinforcement learning aims to determine an optimal control policy from interaction with a system or from observations gathered from a system. In batch mode, it can be achieved by approximating the so-called Q-function based on a set of four-tuples (x t , u t , r t , x t+1) where x t denotes the system state at time t, u t the control action taken, r t the instantaneous reward obtained and x t+1 the successor state of the system, and by determining the control policy from this Q-function. The Q-function approximation may be obtained from the limit of a sequence of (batch mode) supervised learning problems. Within this framework we describe the use of several classical tree-based supervised learning methods (CART, Kd-tree, tree bagging) and two newly proposed ensemble algorithms, namely extremely and totally randomized trees. We study their performances on several examples and find that the ensemble methods based on regression trees perform well in extracting relevant information about the optimal control policy from sets of four-tuples. In particular, the totally randomized trees give good results while ensuring the convergence of the sequence, whereas by relaxing the convergence constraint even better accuracy results are provided by the extremely randomized trees."
            ],
            "keywords": [
                "batch mode reinforcement learning",
                "regression trees",
                "ensemble methods",
                "supervised learning",
                "fitted value iteration",
                "optimal control"
            ],
            "author": [
                "Damien Ernst",
                "Pierre Geurts",
                "Louis Wehenkel",
                "Michael L Littman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/ernst05a/ernst05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dimension Reduction in Text Classification with Support Vector Machines",
            "abstract": [
                "Support vector machines (SVMs) have been recognized as one of the most successful classification methods for many applications including text classification. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efficiently handle a large number of terms in practical applications of text classification. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classification algorithm and support vector classifiers to handle the classification problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efficiency for both training and testing can be achieved without sacrificing prediction accuracy of text classification even when the dimension of the input space is significantly reduced."
            ],
            "keywords": [
                "dimension reduction",
                "support vector machines",
                "text classification",
                "linear discriminant analysis",
                "centroids"
            ],
            "author": [
                "Hyunsoo Kim",
                "Peg Howland",
                "Haesun Park"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/kim05a/kim05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gains and Losses are Fundamentally Different in Regret Minimization: The Sparse Case",
            "abstract": [
                "We demonstrate that, in the classical non-stochastic regret minimization problem with d decisions, gains and losses to be respectively maximized or minimized are fundamentally different. Indeed, by considering the additional sparsity assumption (at each stage, at most s decisions incur a nonzero outcome), we derive optimal regret bounds of different orders. Specifically, with gains, we obtain an optimal regret guarantee after T stages of order √ T log s, so the classical dependency in the dimension is replaced by the sparsity size. With losses, we provide matching upper and lower bounds of order T s log(d)/d, which is decreasing in d. Eventually, we also study the bandit setting, and obtain an upper bound of order T s log(d/s) when outcomes are losses. This bound is proven to be optimal up to the logarithmic factor log(d/s)."
            ],
            "keywords": [
                "regret minimization",
                "bandit",
                "sparsity"
            ],
            "author": [
                "Joon Kwon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-503/15-503.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Anechoic Blind Source Separation Using Wigner Marginals",
            "abstract": [
                "Blind source separation problems emerge in many applications, where signals can be modeled as superpositions of multiple sources. Many popular applications of blind source separation are based on linear instantaneous mixture models. If specific invariance properties are known about the sources, for example, translation or rotation invariance, the simple linear model can be extended by inclusion of the corresponding transformations. When the sources are invariant against translations (spatial displacements or time shifts) the resulting model is called an anechoic mixing model. We present a new algorithmic framework for the solution of anechoic problems in arbitrary dimensions. This framework is derived from stochastic time-frequency analysis in general, and the marginal properties of the Wigner-Ville spectrum in particular. The method reduces the general anechoic problem to a set of anechoic problems with non-negativity constraints and a phase retrieval problem. The first type of subproblem can be solved by existing algorithms, for example by an appropriate modification of non-negative matrix factorization (NMF). The second subproblem is solved by established phase retrieval methods. We discuss and compare implementations of this new algorithmic framework for several example problems with synthetic and real-world data, including music streams, natural 2D images, human motion trajectories and two-dimensional shapes."
            ],
            "keywords": [
                "blind source separation",
                "anechoic mixtures",
                "time-frequency transformations",
                "linear canonical transform",
                "Wigner-Ville spectrum"
            ],
            "author": [
                "Lars Omlor",
                "Martin A Giese"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/omlor11a/omlor11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Learning via Sequential Complexities",
            "abstract": [
                "We consider the problem of sequential prediction and provide tools to study the minimax value of the associated game. Classical statistical learning theory provides several useful complexity measures to study learning with i.i.d. data. Our proposed sequential complexities can be seen as extensions of these measures to the sequential setting. The developed theory is shown to yield precise learning guarantees for the problem of sequential prediction. In particular, we show necessary and sufficient conditions for online learnability in the setting of supervised learning. Several examples show the utility of our framework: we can establish learnability without having to exhibit an explicit online learning algorithm."
            ],
            "keywords": [
                "online learning",
                "sequential complexities",
                "regret minimization"
            ],
            "author": [
                "Alexander Rakhlin",
                "Karthik Sridharan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/rakhlin15a/rakhlin15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions *",
            "abstract": [
                "We consider the problems of attribute-efficient PAC learning of two well-studied concept classes: parity functions and DNF expressions over {0, 1} n. We show that attribute-efficient learning of parities with respect to the uniform distribution is equivalent to decoding high-rate random linear codes from low number of errors, a long-standing open problem in coding theory. This is the first evidence that attribute-efficient learning of a natural PAC learnable concept class can be computationally hard. An algorithm is said to use membership queries (MQs) non-adaptively if the points at which the algorithm asks MQs do not depend on the target concept. Using a simple non-adaptive parity learning algorithm and a modification of Levin's algorithm for locating a weakly-correlated parity due to Bshouty et al. (1999), we give the first non-adaptive and attribute-efficient algorithm for learning DNF with respect to the uniform distribution. Our algorithm runs in timeÕ(ns 4 /ε) and usesÕ(s 4 • log 2 n/ε) non-adaptive MQs, where s is the number of terms in the shortest DNF representation of the target concept. The algorithm improves on the best previous algorithm for learning DNF (of Bshouty et al., 1999) and can also be easily modified to tolerate random persistent classification noise in MQs."
            ],
            "keywords": [
                "attribute-efficient",
                "non-adaptive",
                "membership query",
                "DNF",
                "parity function",
                "random linear code"
            ],
            "author": [
                "Vitaly Feldman",
                "Eecs Harvard Edu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/feldman07a/feldman07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions",
            "abstract": [
                "We show that kernel-based quadrature rules for computing integrals can be seen as a special case of random feature expansions for positive definite kernels, for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent and identically distributed samples from a specific non-uniform distribution, while the lower bound if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in L 2-and L ∞-norm that match known results for special cases. Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipshitz-continuous losses."
            ],
            "keywords": [
                "Quadrature",
                "positive-definite kernels",
                "integral operators"
            ],
            "author": [
                "Francis Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-178/15-178.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tight Lower Bounds on the VC-dimension of Geometric Set Systems",
            "abstract": [
                "The VC-dimension of a set system is a way to capture its complexity and has been a key parameter studied extensively in machine learning and geometry communities. In this paper, we resolve two longstanding open problems on bounding the VC-dimension of two fundamental set systems: k-fold unions/intersections of half-spaces and the simplices set system. Among other implications, it settles an open question in machine learning that was first studied in the foundational paper of Blumer et al. (1989) as well as by Eisenstat and Angluin (2007) and Johnson (2008)."
            ],
            "keywords": [
                "VC-dimension",
                "union of concepts",
                "intersection of concepts",
                "combinatorial problems",
                "PAC learning"
            ],
            "author": [
                "Mónika Csikós",
                "Nabil H Mustafa",
                "Andrey Kupavskii"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-719/18-719.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Design and Analysis of the NIPS 2016 Review Process",
            "abstract": [
                "Neural Information Processing Systems (NIPS) is a top-tier annual conference in machine learning. The 2016 edition of the conference comprised more than 2,400 paper submissions, 3,000 reviewers, and 8,000 attendees. This represents a growth of nearly 40% in terms of submissions, 96% in terms of reviewers, and over 100% in terms of attendees as compared to the previous year. The massive scale as well as rapid growth of the conference calls for a thorough quality assessment of the peer-review process and novel means of improvement. In this paper, we analyze several aspects of the data collected during the review process, including an experiment investigating the efficacy of collecting ordinal rankings from reviewers. We make a number of key observations, provide suggestions that may be useful for subsequent conferences, and discuss open problems towards the goal of improving peer review."
            ],
            "keywords": [
                "Peer review",
                "post hoc analysis",
                "NIPS",
                "consistency",
                "ordinal"
            ],
            "author": [
                "Nihar B Shah",
                "Behzad Tabibian",
                "Krikamol Muandet",
                "Isabelle Guyon",
                "Ulrike Von Luxburg"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-511/17-511.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Classification in Networked Data: A Toolkit and a Univariate Case Study",
            "abstract": [
                "This paper 1 is about classifying entities that are interlinked with entities for which the class is known. After surveying prior work, we present NetKit, a modular toolkit for classification in networked data, and a case-study of its application to networked data used in prior machine learning research. NetKit is based on a node-centric framework in which classifiers comprise a local classifier, a relational classifier, and a collective inference procedure. Various existing node-centric relational learning algorithms can be instantiated with appropriate choices for these components, and new combinations of components realize new algorithms. The case study focuses on univariate network classification, for which the only information used is the structure of class linkage in the network (i.e., only links and some class labels). To our knowledge, no work previously has evaluated systematically the power of class-linkage alone for classification in machine learning benchmark data sets. The results demonstrate that very simple network-classification models perform quite well-well enough that they should be used regularly as baseline classifiers for studies of learning with networked data. The simplest method (which performs remarkably well) highlights the close correspondence between several existing methods introduced for different purposes-that is, Gaussian-field classifiers, Hopfield networks, and relational-neighbor classifiers. The case study also shows that there are two sets of techniques that are preferable in different situations, namely when few versus many labels are known initially. We also demonstrate that link selection plays an important role similar to traditional feature selection."
            ],
            "keywords": [
                "relational learning",
                "network learning",
                "collective inference",
                "collective classification",
                "networked data",
                "probabilistic relational models",
                "network analysis",
                "network data"
            ],
            "author": [
                "Sofus A Macskassy",
                "Foster Provost"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/macskassy07a/macskassy07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "QUIC: Quadratic Approximation for Sparse Inverse Covariance Estimation",
            "abstract": [
                "The 1-regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program. In contrast to recent state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton's method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We show that our method is superlinearly convergent, and present experimental results using synthetic and real-world application data that demonstrate the considerable improvements in performance of our method when compared to previous methods."
            ],
            "keywords": [
                "covariance",
                "graphical model",
                "regularization",
                "optimization",
                "Gaussian Markov random field"
            ],
            "author": [
                "Cho-Jui Hsieh",
                "Mátyás A Sustik",
                "Inderjit S Dhillon",
                "Pradeep Ravikumar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/hsieh14a/hsieh14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Matrix Completion and Robust Factorization",
            "abstract": [
                "If learning methods are to scale to the massive sizes of modern data sets, it is essential for the field of machine learning to embrace parallel and distributed computing. Inspired by the recent development of matrix factorization methods with rich theory but poor computational complexity and by the relative ease of mapping matrices onto distributed architectures, we introduce a scalable divide-and-conquer framework for noisy matrix factorization. We present a thorough theoretical analysis of this framework in which we characterize the statistical errors introduced by the \"divide\" step and control their magnitude in the \"conquer\" step, so that the overall algorithm enjoys high-probability estimation guarantees comparable to those of its base algorithm. We also present experiments in collaborative filtering and video background modeling that demonstrate the near-linear to superlinear speed-ups attainable with this approach."
            ],
            "keywords": [
                "collaborative filtering",
                "divide-and-conquer",
                "matrix completion",
                "matrix factorization",
                "parallel and distributed algorithms",
                "randomized algorithms",
                "robust matrix factorization",
                "video surveillance"
            ],
            "author": [
                "Lester Mackey",
                "Michael I Jordan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/mackey15a/mackey15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "LIBOL: A Library for Online Learning Algorithms",
            "abstract": [
                "LIBOL is an open-source library for large-scale online learning, which consists of a large family of efficient and scalable state-of-the-art online learning algorithms for large-scale online classification tasks. We have offered easy-to-use command-line tools and examples for users and developers, and also have made comprehensive documents available for both beginners and advanced users. LIBOL is not only a machine learning toolbox, but also a comprehensive experimental platform for conducting online learning research."
            ],
            "keywords": [
                "online learning",
                "massive-scale classification",
                "big data analytics"
            ],
            "author": [
                "Steven C H Hoi",
                "Jialei Wang",
                "Peilin Zhao",
                "Mark Reid"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/hoi14a/hoi14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Nested Variance Reduction for Nonconvex Optimization",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "Dongruo Zhou",
                "Pan Xu",
                "Quanquan Gu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-447/18-447.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MLlib: Machine Learning in Apache Spark",
            "abstract": [
                "Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed."
            ],
            "keywords": [
                "scalable machine learning",
                "distributed algorithms",
                "apache spark"
            ],
            "author": [
                "Xiangrui Meng",
                "Joseph Bradley",
                "Evan Sparks",
                "Shivaram Venkataraman",
                "Davies Liu",
                "Jeremy Freeman",
                "D B Tsai",
                "Manish Amde",
                "Sean Owen",
                "Doris Xin",
                "Reynold Xin",
                "Michael J Franklin",
                "Reza Zadeh",
                "Matei Zaharia"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-237/15-237.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Variational Approach to Path Estimation and Parameter Inference of Hidden Diffusion Processes",
            "abstract": [
                "We consider a hidden Markov model, where the signal process, given by a diffusion, is only indirectly observed through some noisy measurements. The article develops a variational method for approximating the hidden states of the signal process given the full set of observations. This, in particular, leads to systematic approximations of the smoothing densities of the signal process. The paper then demonstrates how an efficient inference scheme, based on this variational approach to the approximation of the hidden states, can be designed to estimate the unknown parameters of stochastic differential equations. Two examples at the end illustrate the efficacy and the accuracy of the presented method."
            ],
            "keywords": [
                "Variational inference",
                "stochastic differential equations",
                "diffusion processes",
                "hidden Markov model",
                "optimal control"
            ],
            "author": [
                "Tobias Sutter",
                "Arnab Ganguly",
                "Heinz Koeppl"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-075/16-075.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "StructED : Risk Minimization in Structured Prediction",
            "abstract": [
                "Structured tasks are distinctive: each task has its own measure of performance, such as the word error rate in speech recognition, the BLEU score in machine translation, the NDCG score in information retrieval, or the intersection-over-union score in visual object segmentation. This paper presents StructED, a software package for learning structured prediction models with training methods that aimed at optimizing the task measure of performance. The package was written in Java and released under the MIT license. It can be downloaded from http://adiyoss.github.io/StructED/."
            ],
            "keywords": [
                "structured prediction",
                "structural SVM",
                "CRF",
                "direct loss minimization"
            ],
            "author": [
                "Yossi Adi",
                "Joseph Keshet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-531/15-531.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey",
            "abstract": [
                "Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research."
            ],
            "keywords": [
                "curriculum learning",
                "reinforcement learning",
                "transfer learning"
            ],
            "author": [
                "Sanmit Narvekar",
                "Bei Peng",
                "Matteo Leonetti",
                "Jivko Sinapov",
                "Matthew E Taylor",
                "Peter Stone",
                "Bei Narvekar",
                "Matteo Peng",
                "Jivko Leonetti",
                "Matthew E Sinapov",
                "Peter Stone Taylor"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-212/20-212.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Inference with Posterior Regularization and Applications to Infinite Latent SVMs",
            "abstract": [
                "Existing Bayesian models, especially nonparametric Bayesian methods, rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations. While priors affect posterior distributions through Bayes' rule, imposing posterior regularization is arguably more direct and in some cases more natural and general. In this paper, we present regularized Bayesian inference (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation. RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks. When the regularization is induced from a linear operator on the posterior distributions, such as the expectation operator, we present a general convex-analysis theorem to characterize the solution of RegBayes. Furthermore, we present two concrete examples of RegBayes, infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark data sets, which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics. Such results contribute to push forward the interface between these two important subfields, which have been largely treated as isolated in the community."
            ],
            "keywords": [
                "Bayesian inference",
                "posterior regularization",
                "Bayesian nonparametrics",
                "large-margin learning",
                "classification",
                "multi-task learning"
            ],
            "author": [
                "Jun Zhu",
                "Ning Chen",
                "Eric P Xing"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/zhu14b/zhu14b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast MCMC Sampling for Markov Jump Processes and Extensions",
            "abstract": [
                "Markov jump processes (or continuous-time Markov chains) are a simple and important class of continuous-time dynamical systems. In this paper, we tackle the problem of simulating from the posterior distribution over paths in these models, given partial and noisy observations. Our approach is an auxiliary variable Gibbs sampler, and is based on the idea of uniformization. This sets up a Markov chain over paths by alternately sampling a finite set of virtual jump times given the current path, and then sampling a new path given the set of extant and virtual jump times. The first step involves simulating a piecewise-constant inhomogeneous Poisson process, while for the second, we use a standard hidden Markov model forward filtering-backward sampling algorithm. Our method is exact and does not involve approximations like time-discretization. We demonstrate how our sampler extends naturally to MJP-based models like Markov-modulated Poisson processes and continuous-time Bayesian networks, and show significant computational benefits over state-ofthe-art MCMC samplers for these models."
            ],
            "keywords": [
                "Markov jump process",
                "MCMC",
                "Gibbs sampler",
                "uniformization",
                "Markov-modulated Poisson process",
                "continuous-time Bayesian network"
            ],
            "author": [
                "Vinayak Rao",
                "Yee Whye Teh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/rao13a/rao13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Interior-Point Method for Large-Scale 1 -Regularized Logistic Regression",
            "abstract": [
                "Logistic regression with 1 regularization has been proposed as a promising method for feature selection in classification problems. In this paper we describe an efficient interior-point method for solving large-scale 1-regularized logistic regression problems. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC; medium sized problems, with tens of thousands of features and examples, can be solved in tens of seconds (assuming some sparsity in the data). A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve very large problems, with a million features and examples (e.g., the 20 Newsgroups data set), in a few minutes, on a PC. Using warm-start techniques, a good approximation of the entire regularization path can be computed much more efficiently than by solving a family of problems independently."
            ],
            "keywords": [
                "logistic regression",
                "feature selection",
                "1 regularization",
                "regularization path",
                "interiorpoint methods"
            ],
            "author": [
                "Kwangmoo Koh",
                "Stephen Boyd",
                "Stanford Edu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/koh07a/koh07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric Estimation of Probability Density Functions of Random Persistence Diagrams",
            "abstract": [
                "Topological data analysis refers to a broad set of techniques that are used to make inferences about the shape of data. A popular topological summary is the persistence diagram. Through the language of random sets, we describe a notion of global probability density function for persistence diagrams that fully characterizes their behavior and in part provides a noise likelihood model. Our approach encapsulates the number of topological features and considers the appearance or disappearance of those near the diagonal in a stable fashion. In particular, the structure of our kernel individually tracks long persistence features, while considering those near the diagonal as a collective unit. The choice to describe short persistence features as a group reduces computation time while simultaneously retaining accuracy. Indeed, we prove that the associated kernel density estimate converges to the true distribution as the number of persistence diagrams increases and the bandwidth shrinks accordingly. We also establish the convergence of the mean absolute deviation estimate, defined according to the bottleneck metric. Lastly, examples of kernel density estimation are presented for typical underlying datasets as well as for virtual electroencephalographic data related to cognition."
            ],
            "keywords": [
                "Topological Data Analysis",
                "Persistence Homology",
                "Finite Set Statistics",
                "Global Distribution of Persistence Diagrams",
                "Kernel Density Estimation",
                "EEG Signals"
            ],
            "author": [
                "Vasileios Maroulas",
                "Joshua L Mike",
                "Christopher Oballe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-618/18-618.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Detecting Click Fraud in Online Advertising: A Data Mining Approach",
            "abstract": [
                "Click fraud-the deliberate clicking on advertisements with no real interest on the product or service offered-is one of the most daunting problems in online advertising. Building an effective fraud detection method is thus pivotal for online advertising businesses. We"
            ],
            "keywords": [
                "ensemble learning",
                "feature engineering",
                "fraud detection",
                "imbalanced classification"
            ],
            "author": [
                "Richard Oentaryo",
                "Ee-Peng Lim",
                "Michael Finegold",
                "David Lo",
                "Feida Zhu",
                "Clifton Phua",
                "Eng-Yeow Cheu",
                "Kelvin Sim",
                "Minh Nhut Nguyen",
                "Kasun Perera",
                "Mustafa Faisal",
                "Zeyar Aung",
                "Wei Lee",
                "Wei Chen",
                "Dhaval Patel",
                "Daniel Berrar",
                "Peng Lim",
                "Ghim- Eng Yap",
                "Bijay Neupane",
                "Wei Lee Woon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/oentaryo14a/oentaryo14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Decontamination of Mutual Contamination Models",
            "abstract": [
                "Many machine learning problems can be characterized by mutual contamination models. In these problems, one observes several random samples from different convex combinations of a set of unknown base distributions and the goal is to infer these base distributions. This paper considers the general setting where the base distributions are defined on arbitrary probability spaces. We examine three popular machine learning problems that arise in this general setting: multiclass classification with label noise, demixing of mixed membership models, and classification with partial labels. In each case, we give sufficient conditions for identifiability and present algorithms for the infinite and finite sample settings, with associated performance guarantees."
            ],
            "keywords": [
                "multiclass classification with label noise",
                "classification with partial labels",
                "mixed membership models",
                "topic modeling",
                "mutual contamination models"
            ],
            "author": [
                "Julian Katz-Samuels",
                "Gilles Blanchard",
                "Clayton Scott"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-576/17-576.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Causal Discovery Toolbox: Uncovering causal relationships in Python",
            "abstract": [
                "This paper presents a new open source Python framework for causal discovery from observational data and domain background knowledge, aimed at causal graph and causal mechanism modeling. The Cdt package implements an end-to-end approach, recovering the direct dependencies (the skeleton of the causal graph) and the causal relationships between variables. It includes algorithms from the 'Bnlearn' (Scutari, 2018) and 'Pcalg' (Kalisch et al., 2018) packages, together with algorithms for pairwise causal discovery such as ANM (Hoyer et al."
            ],
            "keywords": [
                "Causal Discovery",
                "Graph recovery",
                "open source",
                "constraint-based methods",
                "score-based methods",
                "pairwise causality",
                "Markov blanket"
            ],
            "author": [
                "Diviyan Kalainathan",
                "Olivier Goudet",
                "Ritik Dutta"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-187/19-187.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fisher Consistency for Prior Probability Shift",
            "abstract": [
                "We introduce Fisher consistency in the sense of unbiasedness as a desirable property for estimators of class prior probabilities. Lack of Fisher consistency could be used as a criterion to dismiss estimators that are unlikely to deliver precise estimates in test data sets under prior probability and more general data set shift. The usefulness of this unbiasedness concept is demonstrated with three examples of classifiers used for quantification: Adjusted Count, EM-algorithm and CDE-Iterate. We find that Adjusted Count and EM-algorithm are Fisher consistent. A counterexample shows that CDE-Iterate is not Fisher consistent and, therefore, cannot be trusted to deliver reliable estimates of class probabilities."
            ],
            "keywords": [
                "Classification",
                "quantification",
                "class distribution estimation",
                "Fisher consistency",
                "data set shift"
            ],
            "author": [
                "Dirk Tasche"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-048/17-048.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences",
            "abstract": [
                "Jstacs is an object-oriented Java library for analysing and classifying sequence data, which emerged from the need for a standardized implementation of statistical models, learning principles, classifiers, and performance measures. In Jstacs, these components can be used, combined, and extended easily, which allows for a direct comparison of different approaches and fosters the development of new components. Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. Jstacs is freely available at http://www.jstacs.de under the GNU GPL license including an API documentation, a cookbook, and code examples."
            ],
            "keywords": [
                "machine learning",
                "statistical models",
                "Java",
                "bioinformatics",
                "classification"
            ],
            "author": [
                "Jan Grau",
                "Jens Keilwagen",
                "Stefan Posch",
                "Ivo Grosse",
                "André Gohr",
                "Berit Haldemann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/grau12a/grau12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Online Learning of Decision Lists",
            "abstract": [
                "A fundamental open problem in computational learning theory is whether there is an attribute efficient learning algorithm for the concept class of decision lists (Rivest, 1987; Blum, 1996). We consider a weaker problem, where the concept class is restricted to decision lists with D alternations. For this class, we present a novel online algorithm that achieves a mistake bound of O(r D log n), where r is the number of relevant variables, and n is the total number of variables. The algorithm can be viewed as a strict generalization of the famous Winnow algorithm by Littlestone (1988), and improves the O(r 2D log n) mistake bound of Balanced Winnow. Our bound is stronger than a similar PAC-learning result of Dhagat and Hellerstein (1994). A combination of our algorithm with the algorithm suggested by Rivest (1987) might achieve even better bounds."
            ],
            "keywords": [],
            "author": [
                "Ziv Nevo"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/nevo02a/nevo02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles",
            "abstract": [
                "We present a canonical way to turn any smooth parametric family of probability distributions on an arbitrary search space into a continuous-time black-box optimization method on , the information-geometric optimization (IGO) method. Invariance as a major design principle keeps the number of arbitrary choices to a minimum. The resulting IGO flow is the flow of an ordinary differential equation conducting the natural gradient ascent of an adaptive, time-dependent transformation of the objective function. It makes no particular assumptions on the objective function to be optimized."
            ],
            "keywords": [
                "black-box optimization",
                "stochastic optimization",
                "randomized optimization",
                "natural gradient",
                "invariance",
                "evolution strategy",
                "information-geometric optimization"
            ],
            "author": [
                "Yann Ollivier",
                "Ludovic Arnold",
                "Anne Auger",
                "Nikolaus Hansen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/14-467/14-467.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
            "abstract": [
                "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."
            ],
            "keywords": [
                "neural networks",
                "regularization",
                "model combination",
                "deep learning"
            ],
            "author": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/srivastava14a/srivastava14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian group factor analysis with structured sparsity",
            "abstract": [
                "Latent factor models are the canonical statistical tool for exploratory analyses of lowdimensional linear structure for a matrix of p features across n samples. We develop a structured Bayesian group factor analysis model that extends the factor model to multiple coupled observation matrices; in the case of two observations, this reduces to a Bayesian model of canonical correlation analysis. Here, we carefully define a structured Bayesian prior that encourages both element-wise and column-wise shrinkage and leads to desirable behavior on high-dimensional data. In particular, our model puts a structured prior on the joint factor loading matrix, regularizing at three levels, which enables element-wise sparsity and unsupervised recovery of latent factors corresponding to structured variance across arbitrary subsets of the observations. In addition, our structured prior allows for both dense and sparse latent factors so that covariation among either all features or only a subset of features can be recovered. We use fast parameter-expanded expectation-maximization for parameter estimation in this model. We validate our method on simulated data with substantial structure. We show results of our method applied to three high-dimensional data sets, comparing results against a number of state-of-the-art approaches. These results illustrate useful properties of our model, including i) recovering sparse signal in the presence of dense effects; ii) the ability to scale naturally to large numbers of observations; iii) flexible observation-and factor-specific regularization to recover factors with a wide variety of sparsity levels and percentage of variance explained; and iv) tractable inference that scales to modern genomic and text data sizes."
            ],
            "keywords": [
                "Bayesian structured sparsity",
                "canonical correlation analysis",
                "sparse priors",
                "sparse and low-rank matrix decomposition",
                "mixture models",
                "parameter expansion"
            ],
            "author": [
                "Shiwen Zhao",
                "Chuan Gao",
                "Sayan Mukherjee",
                "Barbara E Engelhardt",
                "Gao Zhao",
                "Engelhardt Mukherjee"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-472/14-472.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Orange: Data Mining Toolbox in Python",
            "abstract": [
                "Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the flexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationallyintensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining."
            ],
            "keywords": [
                "Python",
                "data mining",
                "machine learning",
                "toolbox",
                "scripting"
            ],
            "author": [
                "Janez Demšar",
                "Mitar Milutinovič",
                "Martin Možina",
                "Marko Toplak",
                "Lan Umek"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/demsar13a/demsar13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Inverse Reinforcement Learning via Nonparametric Spatio-Temporal Subgoal Modeling",
            "abstract": [
                "Advances in the field of inverse reinforcement learning (IRL) have led to sophisticated inference frameworks that relax the original modeling assumption of observing an agent behavior that reflects only a single intention. Instead of learning a global behavioral model, recent IRL methods divide the demonstration data into parts, to account for the fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. In this work, we go one step further: using the intuitive concept of subgoals, we build upon the premise that even a single trajectory can be explained more efficiently locally within a certain context than globally, enabling a more compact representation of the observed behavior. Based on this assumption, we build an implicit intentional model of the agent's goals to forecast its behavior in unobserved situations. The result is an integrated Bayesian prediction framework that significantly outperforms existing IRL solutions and provides smooth policy estimates consistent with the expert's plan. Most notably, our framework naturally handles situations where the intentions of the agent change over time and classical IRL algorithms fail. In addition, due to its probabilistic nature, the model can be straightforwardly applied in active learning scenarios to guide the demonstration process of the expert."
            ],
            "keywords": [
                "Learning from Demonstration",
                "Inverse Reinforcement Learning",
                "Bayesian Nonparametric Modeling",
                "Subgoal Inference",
                "Graphical Models",
                "Gibbs Sampling Š ošić",
                "Rueckert",
                "Peters",
                "Zoubir and Koeppl"
            ],
            "author": [
                "Abdelhak M Zoubir",
                "Elmar Rueckert",
                "Jan Peters",
                "Heinz Koeppl"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-113/18-113.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations",
            "abstract": [
                "We introduce a method to train Quantized Neural Networks (QNNs)-neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At traintime the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves 51% top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online."
            ],
            "keywords": [
                "deep learning",
                "neural networks compression",
                "energy efficient neural networks",
                "computer vision",
                "language models"
            ],
            "author": [
                "Itay Hubara",
                "Matthieu Courbariaux",
                "Daniel Soudry",
                "Ran El-Yaniv",
                "Yoshua Bengio"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-456/16-456.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sharp Oracle Bounds for Monotone and Convex Regression Through Aggregation",
            "abstract": [
                "We derive oracle inequalities for the problems of isotonic and convex regression using the combination of Q-aggregation procedure and sparsity pattern aggregation. This improves upon the previous results including the oracle inequalities for the constrained least squares estimator. One of the improvements is that our oracle inequalities are sharp, i.e., with leading constant 1. It allows us to obtain bounds for the minimax regret thus accounting for model misspecification, which was not possible based on the previous results. Another improvement is that we obtain oracle inequalities both with high probability and in expectation."
            ],
            "keywords": [
                "aggregation",
                "shape constraints",
                "isotonic regression",
                "convex regression",
                "minimax regret",
                "sharp oracle inequalities",
                "model misspecification"
            ],
            "author": [
                "Pierre C Bellec",
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/bellec15a/bellec15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximate Profile Maximum Likelihood",
            "abstract": [
                "We propose an efficient algorithm for approximate computation of the profile maximum likelihood (PML), a variant of maximum likelihood maximizing the probability of observing a sufficient statistic rather than the empirical sample. The PML has appealing theoretical properties, but is difficult to compute exactly. Inspired by observations gleaned from exactly solvable cases, we look for an approximate PML solution, which, intuitively, clumps comparably frequent symbols into one symbol. This amounts to lower-bounding a certain matrix permanent by summing over a subgroup of the symmetric group rather than the whole group during the computation. We extensively experiment with the approximate solution, and the empirical performance of our approach is competitive and sometimes significantly better than state-of-the-art performances for various estimation problems."
            ],
            "keywords": [
                "Profile maximum likelihood",
                "dynamic programming",
                "sufficient statistic",
                "partition of multi-partite numbers",
                "integer partition"
            ],
            "author": [
                "Dmitri S Pavlichin",
                "Jiantao Jiao",
                "Tsachy Weissman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-075/18-075.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases",
            "abstract": [
                "Microarray gene expressions provide new opportunities for molecular classification of heterogeneous diseases. Although various reported classification schemes show impressive performance, most existing gene selection methods are suboptimal and are not well-matched to the unique charac"
            ],
            "keywords": [],
            "author": [
                "Guoqiang Yu",
                "David J Miller",
                "Jianhua Xuan",
                "Eric P Hoffman",
                "Robert Clarke",
                "Ben Davidson",
                "Ishih @ Jhmi",
                "Yue Wang",
                "Yuanjian Feng",
                "Ie-Ming Shih"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/yu10b/yu10b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistent Distribution-Free K-Sample and Independence Tests for Univariate Random Variables",
            "abstract": [
                "A popular approach for testing if two univariate random variables are statistically independent consists of partitioning the sample space into bins, and evaluating a test statistic on the binned data. The partition size matters, and the optimal partition size is data dependent. While for detecting simple relationships coarse partitions may be best, for detecting complex relationships a great gain in power can be achieved by considering finer partitions. We suggest novel consistent distributionfree tests that are based on summation or maximization aggregation of scores over all partitions of a fixed size. We show that our test statistics based on summation can serve as good estimators of the mutual information. Moreover, we suggest regularized tests that aggregate over all partition sizes, and prove those are consistent too. We provide polynomial-time algorithms, which are critical for computing the suggested test statistics efficiently. We show that the power of the regularized tests is excellent compared to existing tests, and almost as powerful as the tests based on the optimal (yet unknown in practice) partition size, in simulations as well as on a real data example."
            ],
            "keywords": [
                "bivariate distribution",
                "nonparametric test",
                "statistical independence",
                "mutual information",
                "two-sample test",
                "HHG R package"
            ],
            "author": [
                "Ruth Heller",
                "Yair Heller",
                "Shachar Kaufman",
                "Barak Brill",
                "Malka Gorfine"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-441/14-441.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Covariance-based Clustering in Multivariate and Functional Data Analysis",
            "abstract": [
                "In this paper we propose a new algorithm to perform clustering of multivariate and functional data. We study the case of two populations different in their covariances, rather than in their means. The algorithm relies on a proper quantification of distance between the estimated covariance operators of the populations, and subdivides data in two groups maximising the distance between their induced covariances. The naive implementation of such an algorithm is computationally forbidding, so we propose a heuristic formulation with a much lighter complexity and we study its convergence properties, along with its computational cost. We also propose to use an enhanced estimator for the estimation of discrete covariances of functional data, namely a linear shrinkage estimator, in order to improve the precision of the clustering. We establish the effectiveness of our algorithm through applications to both synthetic data and a real data set coming from a biomedical context, showing also how the use of shrinkage estimation may lead to substantially better results."
            ],
            "keywords": [
                "Clustering",
                "covariance operator",
                "operator distance",
                "shrinkage estimation",
                "functional data analysis"
            ],
            "author": [
                "Francesca Ieva",
                "Anna Maria Paganoni",
                "Nicholas Tarabelloni"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-568/15-568.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributions of Angles in Random Packing on Spheres",
            "abstract": [
                "This paper studies the asymptotic behaviors of the pairwise angles among n randomly and uniformly distributed unit vectors in R p as the number of points n → ∞, while the dimension p is either fixed or growing with n. For both settings, we derive the limiting empirical distribution of the random angles and the limiting distributions of the extreme angles. The results reveal interesting differences in the two settings and provide a precise characterization of the folklore that \"all high-dimensional random vectors are almost always nearly orthogonal to each other\". Applications to statistics and machine learning and connections with some open problems in physics and mathematics are also discussed."
            ],
            "keywords": [
                "random angle",
                "uniform distribution on sphere",
                "empirical law",
                "maximum of random variables",
                "minimum of random variables",
                "extreme-value distribution",
                "packing on sphere"
            ],
            "author": [
                "Tony Cai",
                "Tiefeng Jiang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/cai13a/cai13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Regularization Techniques for Learning with Matrices",
            "abstract": [
                "There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate. Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning."
            ],
            "keywords": [
                "regularization",
                "strong convexity",
                "regret bounds",
                "generalization bounds",
                "multi-task learning",
                "multi-class learning",
                "multiple kernel learning"
            ],
            "author": [
                "Sham M Kakade",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/kakade12a/kakade12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributed Feature Screening via Componentwise Debiasing",
            "abstract": [
                "Feature screening is a powerful tool in processing high-dimensional data. When the sample size N and the number of features p are both large, the implementation of classic screening methods can be numerically challenging. In this paper, we propose a distributed screening framework for big data setup. In the spirit of \"divide-and-conquer\", the proposed framework expresses a correlation measure as a function of several component parameters, each of which can be distributively estimated using a natural U-statistic from data segments. With the component estimates aggregated, we obtain a final correlation estimate that can be readily used for screening features. This framework enables distributed storage and parallel computing and thus is computationally attractive. Due to the unbiased distributive estimation of the component parameters, the final aggregated estimate achieves a high accuracy that is insensitive to the number of data segments m. Under mild conditions, we show that the aggregated correlation estimator is as efficient as the centralized estimator in terms of the probability convergence bound and the mean squared error rate; the corresponding screening procedure enjoys sure screening property for a wide range of correlation measures. The promising performances of the new method are supported by extensive numerical examples."
            ],
            "keywords": [
                "Feature screening",
                "Big data",
                "Divide-and-conquer",
                "Aggregated correlation",
                "Sure screening property"
            ],
            "author": [
                "Xingxiang Li",
                "Runze Li",
                "Zhiming Xia",
                "Chen Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-537/19-537.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning",
            "abstract": [
                "Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a lowdimensional manifold of parameter space along which the regularizer is smooth. (When an ℓ 1 regularizer is used to induce sparsity in the solution, for example, this manifold is defined by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a \"local phase\" that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identification property and to illustrate the effectiveness of this approach."
            ],
            "keywords": [
                "regularization",
                "dual averaging",
                "partly smooth manifold",
                "manifold identification"
            ],
            "author": [
                "Sangkyun Lee",
                "Stephen J Wright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/lee12a/lee12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Matrix Completion with the Trace Norm: Learning, Bounding, and Transducing",
            "abstract": [
                "Trace-norm regularization is a widely-used and successful approach for collaborative filtering and matrix completion. However, previous learning guarantees require strong assumptions, such as a uniform distribution over the matrix entries. In this paper, we bridge this gap by providing such guarantees, under much milder assumptions which correspond to matrix completion as performed in practice. In fact, we claim that previous difficulties partially stemmed from a mismatch between the standard learning-theoretic modeling of matrix completion, and its practical application. Our results also shed some light on the issue of matrix completion with bounded models, which enforce predictions to lie within a certain range. In particular, we provide experimental and theoretical evidence that such models lead to a modest yet significant improvement."
            ],
            "keywords": [
                "collaborative filtering",
                "matrix completion",
                "trace-norm regularization",
                "transductive learning",
                "sample complexity"
            ],
            "author": [
                "Ohad Shamir",
                "Shai Shalev-Shwartz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/shamir14a/shamir14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rate Optimal Denoising of Simultaneously Sparse and Low Rank Matrices",
            "abstract": [
                "We study minimax rates for denoising simultaneously sparse and low rank matrices in high dimensions. We show that an iterative thresholding algorithm achieves (near) optimal rates adaptively under mild conditions for a large class of loss functions. Numerical experiments on synthetic datasets also demonstrate the competitive performance of the proposed method."
            ],
            "keywords": [
                "Denoising",
                "High dimensionality",
                "Low rank matrices",
                "Minimax rates",
                "Simultaneously structured matrices",
                "Sparse SVD",
                "Sparsity"
            ],
            "author": [
                "Dan Yang",
                "Zongming Ma",
                "Andreas Buja"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-134/15-134.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "RLScore: Regularized Least-Squares Learners",
            "abstract": [
                "RLScore is a Python open source module for kernel based machine learning. The library provides implementations of several regularized least-squares (RLS) type of learners. RLS methods for regression and classification, ranking, greedy feature selection, multi-task and zero-shot learning, and unsupervised classification are included. Matrix algebra based computational shortcuts are used to ensure efficiency of both training and cross-validation. A simple API and extensive tutorials allow for easy use of RLScore."
            ],
            "keywords": [
                "cross-validation",
                "feature selection",
                "kernel methods",
                "Kronecker product kernel",
                "pair-input learning",
                "python",
                "regularized least-squares"
            ],
            "author": [
                "Tapio Pahikkala"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/16-470/16-470.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Memoryless Sequences for General Losses",
            "abstract": [
                "One way to define the randomness of a fixed individual sequence is to ask how hard it is to predict relative to a given loss function. A sequence is memoryless if, with respect to average loss, no continuous function can predict the next entry of the sequence from a finite window of previous entries better than a constant prediction. For squared loss, memoryless sequences are known to have stochastic attributes analogous to those of truly random sequences. In this paper, we address the question of how changing the loss function changes the set of memoryless sequences, and in particular, the stochastic attributes they possess. For convex differentiable losses we establish that the statistic or property elicited by the loss determines the identity and stochastic attributes of the corresponding memoryless sequences. We generalize these results to convex non-differentiable losses, under additional assumptions, and to non-convex Bregman divergences. In particular, our results show that any Bregman divergence has the same set of memoryless sequences as squared loss. We apply our results to price calibration in prediction markets."
            ],
            "keywords": [
                "algorithmic randomness",
                "property elicitation",
                "prediction markets"
            ],
            "author": [
                "Rafael Frongillo",
                "Andrew Nobel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-042/18-042.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Low Permutation-rank Matrices: Structural Properties and Noisy Completion",
            "abstract": [
                "We consider the problem of noisy matrix completion, in which the goal is to reconstruct a structured matrix whose entries are partially observed in noise. Standard approaches to this underdetermined inverse problem are based on assuming that the underlying matrix has low rank, or is well-approximated by a low rank matrix. In this paper, we propose a richer model based on what we term the \"permutation-rank\" of a matrix. We first describe how the classical non-negative rank model enforces restrictions that may be undesirable in practice, and how and these restrictions can be avoided by using the richer permutation-rank model. Second, we establish the minimax rates of estimation under the new permutation-based model, and prove that surprisingly, the minimax rates are equivalent up to logarithmic factors to those for estimation under the typical low rank model. Third, we analyze a computationally efficient singular-value-thresholding algorithm, known to be optimal for the low-rank setting, and show that it also simultaneously yields a consistent estimator for the low-permutation rank setting. Finally, we present various structural results characterizing the uniqueness of the permutation-rank decomposition, and characterizing convex approximations of the permutation-rank polytope."
            ],
            "keywords": [
                "Non-negative matrix completion",
                "recommender systems",
                "permutation-based model",
                "minimax theory",
                "oracle inequalities"
            ],
            "author": [
                "Nihar B Shah",
                "Sivaraman Balakrishnan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-545/17-545.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large Margin Methods for Structured and Interdependent Output Variables",
            "abstract": [
                "Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach."
            ],
            "keywords": [],
            "author": [
                "Ioannis Tsochantaridis",
                "Thorsten Joachims",
                "Thomas Hofmann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/tsochantaridis05a/tsochantaridis05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models",
            "abstract": [
                "The standard Bayesian Information Criterion (BIC) is derived under regularity conditions which are not always satisfied in the case of graphical models with hidden variables. In this paper we derive the BIC for the binary graphical tree models where all the inner nodes of a tree represent binary hidden variables. This provides an extension of a similar formula given by Rusakov and Geiger for naive Bayes models. The main tool used in this paper is the connection between the growth behavior of marginal likelihood integrals and the real log-canonical threshold."
            ],
            "keywords": [
                "BIC",
                "marginal likelihood",
                "singular models",
                "tree models",
                "Bayesian networks",
                "real logcanonical threshold"
            ],
            "author": [
                "Piotr Zwiernik"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/zwiernik11a/zwiernik11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm",
            "abstract": [
                "We propose an extensive analysis of the behavior of majority votes in binary classification. In particular, we introduce a risk bound for majority votes, called the C-bound, that takes into account the average quality of the voters and their average disagreement. We also propose an extensive PAC-Bayesian analysis that shows how the C-bound can be estimated from various observations contained in the training data. The analysis intends to be selfcontained and can be used as introductory material to PAC-Bayesian statistical learning theory. It starts from a general PAC-Bayesian perspective and ends with uncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback-Leibler divergence and others allow kernel functions to be used as voters (via the sample compression setting). Finally, out of the analysis, we propose the MinCq learning algorithm that basically minimizes the C-bound. MinCq reduces to a simple quadratic program. Aside from being theoretically grounded, MinCq achieves state-of-the-art performance, as shown in our extensive empirical comparison with both AdaBoost and the Support Vector Machine."
            ],
            "keywords": [
                "majority vote",
                "ensemble methods",
                "learning theory",
                "PAC-Bayesian theory",
                "sample compression"
            ],
            "author": [
                "Pascal Germain",
                "Alexandre Lacasse",
                "François Laviolette",
                "Mario Marchand",
                "Jean-Francis Roy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/germain15a/germain15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Analysis and Parameter Selection for Mapper",
            "abstract": [
                "In this article, we study the question of the statistical convergence of the 1-dimensional Mapper to its continuous analogue, the Reeb graph. We show that the Mapper is an optimal estimator of the Reeb graph, which gives, as a byproduct, a method to automatically tune its parameters and compute confidence regions on its topological features, such as its loops and flares. This allows to circumvent the issue of testing a large grid of parameters and keeping the most stable ones in the brute-force setting, which is widely used in visualization, clustering and feature selection with the Mapper."
            ],
            "keywords": [
                "Topological Data Analysis",
                "Mapper",
                "Parameter Selection",
                "Confidence Regions",
                "Extended Persistence"
            ],
            "author": [
                "Mathieu Carrière",
                "Bertrand Michel",
                "Steve Oudot",
                "Kevin Murphy",
                "Bernhard Schölkopf"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-291/17-291.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Algorithms for Conditional Independence Inference",
            "abstract": [
                "The topic of the paper is computer testing of (probabilistic) conditional independence (CI) implications by an algebraic method of structural imsets. The basic idea is to transform (sets of) CI statements into certain integral vectors and to verify by a computer the corresponding algebraic relation between the vectors, called the independence implication. We interpret the previous methods for computer testing of this implication from the point of view of polyhedral geometry. However, the main contribution of the paper is a new method, based on linear programming (LP). The new method overcomes the limitation of former methods to the number of involved variables. We recall/describe the theoretical basis for all four methods involved in our computational experiments, whose aim was to compare the efficiency of the algorithms. The experiments show that the LP method is clearly the fastest one. As an example of possible application of such algorithms we show that testing inclusion of Bayesian network structures or whether a CI statement is encoded in an acyclic directed graph can be done by the algebraic method."
            ],
            "keywords": [],
            "author": [
                "Remco Bouckaert",
                "Raymond Hemmecke",
                "Silvia Lindner",
                "Zentrum Mathematik",
                "Milan Studený"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/bouckaert10b/bouckaert10b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "TensorLy: Tensor Learning in Python",
            "abstract": [
                "Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of traditional machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed TensorLy, a Python library that provides a high-level API for tensor methods and deep tensorized neural networks. Ten-sorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and to seamlessly integrate with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with several libraries such as NumPy or PyTorch to name but a few. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly"
            ],
            "keywords": [],
            "author": [
                "Jean Kossaifi",
                "Maja Pantic"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-277/18-277.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning a Robust Relevance Model for Search Using Kernel Methods",
            "abstract": [
                "This paper points out that many search relevance models in information retrieval, such as the Vector Space Model, BM25 and Language Models for Information Retrieval, can be viewed as a similarity function between pairs of objects of different types, referred to as an S-function. An S-function is specifically defined as the dot product between the images of two objects in a Hilbert space mapped from two different input spaces. One advantage of taking this view is that one can take a unified and principled approach to address the issues with regard to search relevance. The paper then proposes employing a kernel method to learn a robust relevance model as an S-function, which can effectively deal with the term mismatch problem, one of the biggest challenges in search. The kernel method exploits a positive semi-definite kernel referred to as an S-kernel. The paper shows that when using an S-kernel the model learned by the kernel method is guaranteed to be an S-function. The paper then gives more general principles for constructing S-kernels. A specific implementation of the kernel method is proposed using the Ranking SVM techniques and click-through data. The proposed approach is employed to learn a relevance model as an extension of BM25, referred to as Robust BM25. Experimental results on web search and enterprise search data show that Robust BM25 significantly outperforms baseline methods and can successfully tackle the term mismatch problem."
            ],
            "keywords": [
                "search",
                "term mismatch",
                "kernel machines",
                "similarity learning",
                "s-function",
                "s-kernel"
            ],
            "author": [
                "Wei Wu",
                "Jun Xu",
                "Hang Li",
                "Satoshi Oyama"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/wu11a/wu11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Agnostic Learning of Disjunctions on Symmetric Distributions",
            "abstract": [
                "We consider the problem of approximating and learning disjunctions (or equivalently, conjunctions) on symmetric distributions over {0, 1} n. Symmetric distributions are distributions whose PDF is invariant under any permutation of the variables. We prove that for every symmetric distribution D, there exists a set of n O(log (1/)) functions S, such that for every disjunction c, there is function p, expressible as a linear combination of functions in S, such that p-approximates c in 1 distance on D or Ex∼D[|c(x) − p(x)|] ≤. This implies an agnostic learning algorithm for disjunctions on symmetric distributions that runs in time n O(log (1/)). The best known previous bound is n O(1/ 4) and follows from approximation of the more general class of halfspaces (Wimmer, 2010). We also show that there exists a symmetric distribution D, such that the minimum degree of a polynomial that 1/3-approximates the disjunction of all n variables in 1 distance on D is Ω(√ n). Therefore the learning result above cannot be achieved via 1-regression with a polynomial basis used in most other agnostic learning algorithms. Our technique also gives a simple proof that for any product distribution D and every disjunction c, there exists a polynomial p of degree O(log (1/)) such that p-approximates c in 1 distance on D. This was first proved by Blais et al. (2008) via a more involved argument."
            ],
            "keywords": [
                "agnostic learning",
                "symmetric distribution",
                "polynomial approximation",
                "regression",
                "disjunction",
                "conjunction",
                "DNF",
                "decision tree"
            ],
            "author": [
                "Vitaly Feldman",
                "Pravesh Kothari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/feldman15a/feldman15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "DBSCAN: Optimal Rates For Density-Based Cluster Estimation",
            "abstract": [
                "We study the problem of optimal estimation of the density cluster tree under various smoothness assumptions on the underlying density. Inspired by the seminal work of Chaudhuri et al. (2014), we formulate a new notion of clustering consistency which is better suited to smooth densities, and derive minimax rates for cluster tree estimation under Hölder smooth densities of arbitrary degree. We present a computationally efficient, rate optimal cluster tree estimator based on simple extensions of the popular DBSCAN algorithm of Ester et al. (1996). Our procedure relies on kernel density estimators and returns a sequence of nested random geometric graphs whose connected components form a hierarchy of clusters. The resulting optimal rates for cluster tree estimation depend on the degree of smoothness of the underlying density and, interestingly, match the minimax rates for density estimation under the sup-norm loss. Our results complement and extend the analysis of the DBSCAN algorithm in Sriperumbudur and Steinwart (2012). Finally, we consider level set estimation and cluster consistency for densities with jump discontinuities. We demonstrate that the DBSCAN algorithm attains the minimax rate in terms of the jump size and sample size in this setting as well."
            ],
            "keywords": [
                "DBSCAN",
                "density-based clustering",
                "cluster tree",
                "minimax optimality",
                "Hölder smooth density"
            ],
            "author": [
                "Daren Wang",
                "Xinyang Lu",
                "Alessandro Rinaldo"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-470/18-470.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Pólya Urn for Time-Varying Pitman-Yor Processes",
            "abstract": [
                "This article introduces a class of first-order stationary time-varying Pitman-Yor processes. Subsuming our construction of time-varying Dirichlet processes presented in (Caron et al., 2007), these models can be used for time-dynamic density estimation and clustering. Our intuitive and simple construction relies on a generalized Pólya urn scheme. Significantly, this construction yields marginal distributions at each time point that can be explicitly characterized and easily controlled. Inference is performed using Markov chain Monte Carlo and sequential Monte Carlo methods. We demonstrate our models and algorithms on epidemiological and video tracking data."
            ],
            "keywords": [
                "Bayesian nonparametrics",
                "clustering",
                "mixture models",
                "sequential Monte Carlo",
                "particle Markov chain Monte Carlo",
                "dynamic models"
            ],
            "author": [
                "François Caron",
                "Willie Neiswanger",
                "Frank Wood",
                "Arnaud Doucet",
                "Manuel Davy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/10-231/10-231.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimistic Bayesian Sampling in Contextual-Bandit Problems",
            "abstract": [
                "In sequential decision problems in an unknown environment, the decision maker often faces a dilemma over whether to explore to discover more about the environment, or to exploit current knowledge. We address the exploration-exploitation dilemma in a general setting encompassing both standard and contextualised bandit problems. The contextual bandit problem has recently resurfaced in attempts to maximise click-through rates in web based applications, a task with significant commercial interest. In this article we consider an approach of Thompson (1933) which makes use of samples from the posterior distributions for the instantaneous value of each action. We extend the approach by introducing a new algorithm, Optimistic Bayesian Sampling (OBS), in which the probability of playing an action increases with the uncertainty in the estimate of the action value. This results in better directed exploratory behaviour. We prove that, under unrestrictive assumptions, both approaches result in optimal behaviour with respect to the average reward criterion of Yang and Zhu (2002). We implement OBS and measure its performance in simulated Bernoulli bandit and linear regression domains, and also when tested with the task of personalised news article recommendation on a Yahoo! Front Page Today Module data set. We find that OBS performs competitively when compared to recently proposed benchmark algorithms and outperforms Thompson's method throughout."
            ],
            "keywords": [
                "multi-armed bandits",
                "contextual bandits",
                "exploration-exploitation",
                "sequential allocation",
                "Thompson sampling"
            ],
            "author": [
                "Benedict C May",
                "Nathan Korda",
                "Anthony Lee",
                "David S Leslie"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/may12a/may12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Characteristic and Universal Tensor Product Kernels",
            "abstract": [
                "Maximum mean discrepancy (MMD), also called energy distance or N-distance in statistics and Hilbert-Schmidt independence criterion (HSIC), specifically distance covariance in statistics, are among the most popular and successful approaches to quantify the difference and independence of random variables, respectively. Thanks to their kernel-based foundations, MMD and HSIC are applicable on a wide variety of domains. Despite their tremendous success, quite little is known about when HSIC characterizes independence and when MMD with tensor product kernel can discriminate probability distributions. In this paper, we answer these questions by studying various notions of characteristic property of the tensor product kernel."
            ],
            "keywords": [
                "tensor product kernel",
                "kernel mean embedding",
                "characteristic kernel",
                "Icharacteristic kernel",
                "universality",
                "maximum mean discrepancy",
                "Hilbert-Schmidt independence criterion"
            ],
            "author": [
                "Zoltán Szabó"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-492/17-492.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Euclidean Embedding of Co-occurrence Data",
            "abstract": [
                "Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are specified. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semidefinite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and significantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis."
            ],
            "keywords": [
                "embedding algorithms",
                "manifold learning",
                "exponential families",
                "multidimensional scaling",
                "matrix factorization",
                "semidefinite programming"
            ],
            "author": [
                "Amir Globerson",
                "Fernando Pereira"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/globerson07a/globerson07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Linearly Constrained Minimum Variance Beamforming",
            "abstract": [
                "Beamforming is a widely used technique for source localization in signal processing and neuroimaging. A number of vector-beamformers have been introduced to localize neuronal activity by using magnetoencephalography (MEG) data in the literature. However, the existing theoretical analyses on these beamformers have been limited to simple cases, where no more than two sources are allowed in the associated model and the theoretical sensor covariance is also assumed known. The information about the effects of the MEG spatial and temporal dimensions on the consistency of vector-beamforming is incomplete. In the present study, we consider a class of vector-beamformers defined by thresholding the sensor covariance matrix, which include the standard vector-beamformer as a special case. A general asymptotic theory is developed for these vector-beamformers, which shows the extent of effects to which the MEG spatial and temporal dimensions on estimating the neuronal activity index. The performances of the proposed beamformers are assessed by simulation studies. Superior performances of the proposed beamformers are obtained when the signalto-noise ratio is low. We apply the proposed procedure to real MEG data sets derived from five sessions of a human face-perception experiment, finding several highly active areas in the brain. A good agreement between these findings and the known neurophysiology of the MEG response to human face perception is shown."
            ],
            "keywords": [
                "MEG neuroimaging",
                "vector-beamforming",
                "sparse covariance estimation",
                "source localization and reconstruction"
            ],
            "author": [
                "Jian Zhang",
                "Chao Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/zhang15b/zhang15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Prediction With Expert Advice For The Brier Game",
            "abstract": [
                "We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches, with well-known bookmakers playing the role of experts. The theoretical performance guarantee is not excessively loose on the football data set and is rather tight on the tennis data set."
            ],
            "keywords": [
                "Brier game",
                "classification",
                "on-line prediction",
                "strong aggregating algorithm",
                "weighted average algorithm"
            ],
            "author": [
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/vovk09a/vovk09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars",
            "abstract": [
                "Standard models of language learning are concerned with weak learning: the learner, receiving as input only information about the strings in the language, must learn to generalise and to generate the correct, potentially infinite, set of strings generated by some target grammar. Here we define the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees. We formalise this using a modification of Gold's identification in the limit model, requiring convergence to a grammar that is isomorphic to the target grammar. We take as our starting point a simple learning algorithm for substitutable context-free languages, based on principles of distributional learning, and modify it so that it will converge to a canonical grammar for each language. We prove a corresponding strong learning result for a subclass of context-free grammars."
            ],
            "keywords": [
                "context-free grammars",
                "grammatical inference",
                "identification in the limit",
                "structure learning"
            ],
            "author": [
                "Alexander Clark"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/clark13a/clark13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Scoring Function for Learning Bayesian Networks based on Mutual Information and Conditional Independence Tests",
            "abstract": [
                "We propose a new scoring function for learning Bayesian networks from data using score+search algorithms. This is based on the concept of mutual information and exploits some well-known properties of this measure in a novel way. Essentially, a statistical independence test based on the chi-square distribution, associated with the mutual information measure, together with a property of additive decomposition of this measure, are combined in order to measure the degree of interaction between each variable and its parent variables in the network. The result is a non-Bayesian scoring function called MIT (mutual information tests) which belongs to the family of scores based on information theory. The MIT score also represents a penalization of the Kullback-Leibler divergence between the joint probability distributions associated with a candidate network and with the available data set. Detailed results of a complete experimental evaluation of the proposed scoring function and its comparison with the well-known K2, BDeu and BIC/MDL scores are also presented."
            ],
            "keywords": [
                "Bayesian networks",
                "scoring functions",
                "learning",
                "mutual information",
                "conditional independence tests"
            ],
            "author": [
                "Luis M De Campos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/decampos06a/decampos06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Leave-One-Out Cross-Validation Approximations for Gaussian Latent Variable Models",
            "abstract": [
                "The future predictive performance of a Bayesian model can be estimated using Bayesian cross-validation. In this article, we consider Gaussian latent variable models where the integration over the latent values is approximated using the Laplace method or expectation propagation (EP). We study the properties of several Bayesian leave-one-out (LOO) crossvalidation approximations that in most cases can be computed with a small additional cost after forming the posterior approximation given the full data. Our main objective is to assess the accuracy of the approximative LOO cross-validation estimators. That is, for each method (Laplace and EP) we compare the approximate fast computation with the exact brute force LOO computation. Secondarily, we evaluate the accuracy of the Laplace and EP approximations themselves against a ground truth established through extensive Markov chain Monte Carlo simulation. Our empirical results show that the approach based upon a Gaussian approximation to the LOO marginal distribution (the so-called cavity distribution) gives the most accurate and reliable results among the fast methods."
            ],
            "keywords": [
                "predictive performance",
                "leave-one-out cross-validation",
                "Gaussian latent variable model",
                "Laplace approximation",
                "expectation propagation"
            ],
            "author": [
                "Aki Vehtari",
                "Tommi Mononen",
                "Ville Tolvanen",
                "Tuomas Sivula",
                "Ole Winther"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-540/14-540.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Network Learning with Parameter Constraints",
            "abstract": [
                "The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. This paper considers a variety of types of domain knowledge for constraining parameter estimates when learning Bayesian networks. In particular, we consider domain knowledge that constrains the values or relationships among subsets of parameters in a Bayesian network with known structure. We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. The assumptions made in module networks, dynamic Bayes nets and context specific independence models can be viewed as particular cases of such parameter constraints. We present closed form solutions or fast iterative algorithms for estimating parameters subject to several specific classes of parameter constraints, including equalities and inequalities among parameters, constraints on individual parameters, and constraints on sums and ratios of parameters, for discrete and continuous variables. Our methods cover learning from both frequentist and Bayesian points of view, from both complete and incomplete data. We present formal guarantees for our estimators, as well as methods for automatically learning useful parameter constraints from data. To validate our approach, we apply it to the domain of fMRI brain image analysis. Here we demonstrate the ability of our system to first learn useful relationships among parameters, and then to use them to constrain the training of the Bayesian network, resulting in improved cross-validated accuracy of the learned model. Experiments on synthetic data are also presented."
            ],
            "keywords": [
                "Bayesian networks",
                "constrained optimization",
                "domain knowledge"
            ],
            "author": [
                "Radu Stefan Niculescu",
                "Tom M Mitchell",
                "R Bharat Rao",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández",
                "Stefan Niculescu",
                "Bharat Rao",
                "Mitchell And Niculescu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/niculescu06a/niculescu06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction",
            "abstract": [
                "Existing template-independent web data extraction approaches adopt highly ineffective decoupled strategies-attempting to do data record detection and attribute labeling in two separate phases. In this paper, we propose an integrated web data extraction paradigm with hierarchical models. The proposed model is called Dynamic Hierarchical Markov Random Fields (DHMRFs). DHMRFs take structural uncertainty into consideration and define a joint distribution of both model structure and class labels. The joint distribution is an exponential family distribution. As a conditional model, DHMRFs relax the independence assumption as made in directed models. Since exact inference is intractable, a variational method is developed to learn the model's parameters and to find the MAP model structure and label assignments. We apply DHMRFs to a real-world web data extraction task. Experimental results show that: (1) integrated web data extraction models can achieve significant improvements on both record detection and attribute labeling compared to decoupled models; (2) in diverse web data extraction DHMRFs can potentially address the blocky artifact issue which is suffered by fixed-structured hierarchical models."
            ],
            "keywords": [
                "conditional random fields",
                "dynamic hierarchical Markov random fields",
                "integrated web data extraction",
                "statistical hierarchical modeling",
                "blocky artifact issue"
            ],
            "author": [
                "Jun Zhu",
                "Web Search",
                "Mining Group",
                "Bo Zhang",
                "Ji-Rong Wen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/zhu08a/zhu08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Quantum Sample Complexity of Learning Algorithms",
            "abstract": [
                "In learning theory, the VC dimension of a concept class C is the most common way to measure its \"richness.\" A fundamental result says that the number of examples needed to learn an unknown target concept c ∈ C under an unknown distribution D, is tightly determined by the VC dimension d of the concept class C. Specifically, in the PAC model"
            ],
            "keywords": [],
            "author": [
                "Srinivasan Arunachalam"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/18-195/18-195.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Algorithms for Marginal MAP",
            "abstract": [
                "The marginal maximum a posteriori probability (MAP) estimation problem, which calculates the mode of the marginal posterior distribution of a subset of variables with the remaining variables marginalized, is an important inference problem in many models, such as those with hidden variables or uncertain parameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has attracted less attention in the literature compared to the joint MAP (maximization) and marginalization problems. We derive a general dual representation for marginal MAP that naturally integrates the marginalization and maximization operations into a joint variational optimization problem, making it possible to easily extend most or all variational-based algorithms to marginal MAP. In particular, we derive a set of \"mixed-product\" message passing algorithms for marginal MAP, whose form is a hybrid of max-product, sum-product and a novel \"argmax-product\" message updates. We also derive a class of convergent algorithms based on proximal point methods, including one that transforms the marginal MAP problem into a sequence of standard marginalization problems. Theoretically, we provide guarantees under which our algorithms give globally or locally optimal solutions, and provide novel upper bounds on the optimal objectives. Empirically, we demonstrate that our algorithms significantly outperform the existing approaches, including a state-of-the-art algorithm based on local search methods."
            ],
            "keywords": [
                "graphical models",
                "message passing",
                "belief propagation",
                "variational methods",
                "maximum a posteriori",
                "marginal-MAP",
                "hidden variable models"
            ],
            "author": [
                "Qiang Liu",
                "Alexander Ihler",
                "Donald Bren"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/liu13b/liu13b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotic behavior of Support Vector Machine for spiked population model",
            "abstract": [
                "For spiked population model, we investigate the large dimension N and large sample size M asymptotic behavior of the Support Vector Machine (SVM) classification method in the limit of N, M → ∞ at fixed α = M/N. We focus on the generalization performance by analytically evaluating the angle between the normal direction vectors of SVM separating hyperplane and corresponding Bayes optimal separating hyperplane. This is an analogous result to the one shown in Paul (2007) and Nadler (2008) for the angle between the sample eigenvector and the population eigenvector in random matrix theorem. We provide not just bound, but sharp prediction of the asymptotic behavior of SVM that can be determined by a set of nonlinear equations. Based on the analytical results, we propose a new method of selecting tuning parameter which significantly reduces the computational cost. A surprising finding is that SVM achieves its best performance at small value of the tuning parameter under spiked population model. These results are confirmed to be correct by comparing with those of numerical simulations on finite-size systems. We also apply our formulas to an actual dataset of breast cancer and find agreement between analytical derivations and numerical computations based on cross validation."
            ],
            "keywords": [
                "Asymptotic behavior",
                "Spiked population model",
                "Support Vector Machine"
            ],
            "author": [
                "Hanwen Huang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-564/16-564.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Max-margin Classification of Data with Absent Features",
            "abstract": [
                "We consider the problem of learning classifiers in structured domains, where some objects have a subset of features that are inherently absent due to complex relationships between the features. Unlike the case where a feature exists but its value is not observed, here we focus on the case where a feature may not even exist (structurally absent) for some of the samples. The common approach for handling missing features in discriminative models is to first complete their unknown values, and then use a standard classification procedure over the completed data. This paper focuses on features that are known to be non-existing, rather than have an unknown value. We show how incomplete data can be classified directly without any completion of the missing features using a max-margin learning framework. We formulate an objective function, based on the geometric interpretation of the margin, that aims to maximize the margin of each sample in its own relevant subspace. In this formulation, the linearly separable case can be transformed into a binary search over a series of second order cone programs (SOCP), a convex problem that can be solved efficiently. We also describe two approaches for optimizing the general case: an approximation that can be solved as a standard quadratic program (QP) and an iterative approach for solving the exact problem. By avoiding the pre-processing phase in which the data is completed, both of these approaches could offer considerable computational savings. More importantly, we show that the elegant handling of missing values by our approach allows it to both outperform other methods when the missing values have non-trivial structure, and be competitive with other methods when the values are missing at random. We demonstrate our results on several standard benchmarks and two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images."
            ],
            "keywords": [
                "max margin",
                "missing features",
                "network reconstruction",
                "metabolic pathways"
            ],
            "author": [
                "Gal Chechik",
                "Stanford Edu",
                "Pieter Abbeel",
                "Daphne Koller",
                "Ai Stanford Edu",
                "C 2008",
                "Geremy Heitz",
                "Gal Elidan",
                "HEITZ, ELIDAN Abbeel And Chechik"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/chechik08a/chechik08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Provably robust estimation of modulo 1 samples of a smooth function with applications to phase unwrapping",
            "abstract": [
                "Consider an unknown smooth function f : [0, 1] d → R, and assume we are given n noisy mod 1 samples of f , i.e., y i = (f (x i)+η i) mod 1, for x i ∈ [0, 1] d , where η i denotes the noise. Given the samples (x i , y i) n i=1 , our goal is to recover smooth, robust estimates of the clean samples f (x i) mod 1. We formulate a natural approach for solving this problem, which works with angular embeddings of the noisy mod samples over the unit circle, inspired by the angular synchronization framework. This amounts to solving a smoothness regularized least-squares problem-a quadratically constrained quadratic program (QCQP)-where the variables are constrained to lie on the unit circle. Our proposed approach is based on solving its relaxation, which is a trust-region sub-problem and hence solvable efficiently. We provide theoretical guarantees demonstrating its robustness to noise for adversarial, as well as random Gaussian and Bernoulli noise models. To the best of our knowledge, these are the first such theoretical results for this problem. We demonstrate the robustness and efficiency of our proposed approach via extensive numerical simulations on synthetic data, along with a simple least-squares based solution for the unwrapping stage, that recovers the original samples of f (up to a global shift). It is shown to perform well at high levels of noise, when taking as input the denoised modulo 1 samples. Finally, we also consider two other approaches for denoising the modulo 1 samples that leverage tools from Riemannian optimization on manifolds, including a Burer-Monteiro approach for a semidefinite programming relaxation of our formulation. For the twodimensional version of the problem, which has applications in synthetic aperture radar interferometry (InSAR), we are able to solve instances of real-world data with a million sample points in under seconds, on a personal laptop."
            ],
            "keywords": [
                "quadratically constrained quadratic programming (QCQP)",
                "trust-region subproblem",
                "angular embedding",
                "phase unwrapping",
                "semidefinite programming",
                "angular synchronization"
            ],
            "author": [
                "Mihai Cucuringu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-143/18-143.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Empirical Priors for Prediction in Sparse High-dimensional Linear Regression",
            "abstract": [
                "In this paper we adopt the familiar sparse, high-dimensional linear regression model and focus on the important but often overlooked task of prediction. In particular, we consider a new empirical Bayes framework that incorporates data in the prior in two ways: one is to center the prior for the non-zero regression coefficients and the other is to provide some additional regularization. We show that, in certain settings, the asymptotic concentration of the proposed empirical Bayes posterior predictive distribution is very fast, and we establish a Bernstein-von Mises theorem which ensures that the derived empirical Bayes prediction intervals achieve the targeted frequentist coverage probability. The empirical prior has a convenient conjugate form, so posterior computations are relatively simple and fast. Finally, our numerical results demonstrate the proposed method's strong finite-sample performance in terms of prediction accuracy, uncertainty quantification, and computation time compared to existing Bayesian methods."
            ],
            "keywords": [
                "Bayesian inference",
                "data-dependent prior",
                "model averaging",
                "predictive distribution",
                "uncertainty quantification"
            ],
            "author": [
                "Ryan Martin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-152/19-152.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Learning for Matrix Factorization and Sparse Coding",
            "abstract": [
                "Sparse coding-that is, modelling data vectors as sparse linear combinations of basis elements-is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets."
            ],
            "keywords": [
                "basis pursuit",
                "dictionary learning",
                "matrix factorization",
                "online learning",
                "sparse coding",
                "sparse principal component analysis",
                "stochastic approximations",
                "stochastic optimization",
                "nonnegative matrix factorization"
            ],
            "author": [
                "Julien Mairal",
                "Francis Bach",
                "Jean Ponce",
                "Guillermo Sapiro"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/mairal10a/mairal10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Recommender Systems Using Linear Classifiers",
            "abstract": [
                "Recommender systems use historical data on user preferences and other available data on users (for example, demographics) and items (for example, taxonomy) to predict items a new user might like. Applications of these methods include recommending items for purchase and personalizing the browsing experience on a web-site. Collaborative filtering methods have focused on using just the history of user preferences to make the recommendations. These methods have been categorized as memory-based if they operate over the entire data to make predictions and as model-based if they use the data to build a model which is then used for predictions. In this paper, we propose the use of linear classifiers in a model-based recommender system. We compare our method with another model-based method using decision trees and with memory-based methods using data from various domains. Our experimental results indicate that these linear models are well suited for this application. They outperform a commonly proposed memory-based method in accuracy and also have a better tradeoff between off-line and on-line computational requirements."
            ],
            "keywords": [
                "Recommender Systems",
                "Collaborative Filtering",
                "Decision Trees",
                "Linear Models",
                "Unbalanced Data"
            ],
            "author": [
                "Tong Zhang",
                "Vijay S Iyengar"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/zhang02a/zhang02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Direct Method for Building Sparse Kernel Learning Algorithms",
            "abstract": [
                "Many kernel learning algorithms, including support vector machines, result in a kernel machine, such as a kernel classifier, whose key component is a weight vector in a feature space implicitly introduced by a positive definite kernel function. This weight vector is usually obtained by solving a convex optimization problem. Based on this fact we present a direct method to build sparse kernel learning algorithms by adding one more constraint to the original convex optimization problem, such that the sparseness of the resulting kernel machine is explicitly controlled while at the same time performance is kept as high as possible. A gradient based approach is provided to solve this modified optimization problem. Applying this method to the support vectom machine results in a concrete algorithm for building sparse large margin classifiers. These classifiers essentially find a discriminating subspace that can be spanned by a small number of vectors, and in this subspace, the different classes of data are linearly well separated. Experimental results over several classification benchmarks demonstrate the effectiveness of our approach."
            ],
            "keywords": [
                "sparse learning",
                "sparse large margin classifiers",
                "kernel learning algorithms",
                "support vector machine",
                "kernel Fisher discriminant"
            ],
            "author": [
                "Mingrui Wu",
                "Bernhard Schölkopf"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/wu06a/wu06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Machine Learning for Computer Security *",
            "abstract": [
                "The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems."
            ],
            "keywords": [
                "computer security",
                "spam",
                "images with embedded text",
                "malicious executables",
                "network protocols",
                "encrypted traffic"
            ],
            "author": [
                "Philip K Chan",
                "Richard P Lippmann",
                "Richard P Lippman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/MLSEC-intro06a/MLSEC-intro06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Behavioral Shaping for Geometric Concepts",
            "abstract": [
                "In a search problem, an agent uses the membership oracle of a target concept to find a positive example of the concept. In a shaped search problem the agent is aided by a sequence of increasingly restrictive concepts leading to the target concept (analogous to behavioral shaping). The concepts are given by membership oracles, and the agent has to find a positive example of the target concept while minimizing the total number of oracle queries. We show that for the concept class of intervals on the real line an agent using a bounded number of queries per oracle exists. In contrast, for the concept class of unions of two intervals on the real line no agent with a bounded number of queries per oracle exists. We then investigate the (amortized) number of queries per oracle needed for the shaped search problem over other concept classes. We explore the following methods to obtain efficient agents. For axis-parallel rectangles we use a bootstrapping technique to obtain gradually better approximations of the target concept. We show that given rectangles R ⊆ A ⊆ R d one can obtain a rectangle A ⊇ R with vol(A)/vol(R) ≤ 2, using only O(d • vol(A)/vol(R)) random samples from A. For ellipsoids of bounded eccentricity in R d we analyze a deterministic ray-shooting process which uses a sequence of rays to get close to the centroid. Finally, we use algorithms for generating random points in convex bodies (Dyer et al., 1991; Kannan et al., 1997) to give a randomized agent for the concept class of convex bodies. In the final section, we explore connections between our bootstrapping method and active learning. Specifically, we use the bootstrapping technique for axis-parallel rectangles to active learn axis-parallel rectangles under the uniform distribution in O(d ln(1/ε)) labeled samples."
            ],
            "keywords": [
                "computational learning theory",
                "behavioral shaping",
                "active learning"
            ],
            "author": [
                "Manu Chhabra",
                "Robert A Jacobs",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/chhabra07a/chhabra07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Constrained Dantzig Selector with Enhanced Consistency *",
            "abstract": [
                "The Dantzig selector has received popularity for many applications such as compressed sensing and sparse modeling, thanks to its computational efficiency as a linear programming problem and its nice sampling properties. Existing results show that it can recover sparse signals mimicking the accuracy of the ideal procedure, up to a logarithmic factor of the dimensionality. Such a factor has been shown to hold for many regularization methods. An important question is whether this factor can be reduced to a logarithmic factor of the sample size in ultra-high dimensions under mild regularity conditions. To provide an affirmative answer, in this paper we suggest the constrained Dantzig selector, which has more flexible constraints and parameter space. We prove that the suggested method can achieve convergence rates within a logarithmic factor of the sample size of the oracle rates and improved sparsity, under a fairly weak assumption on the signal strength. Such improvement is significant in ultra-high dimensions. This method can be implemented efficiently through sequential linear programming. Numerical studies confirm that the sample size needed for a certain level of accuracy in these problems can be much reduced."
            ],
            "keywords": [
                "Sparse Modeling",
                "Compressed Sensing",
                "Ultra-high Dimensionality",
                "Dantzig Selector",
                "Regularization Methods",
                "Finite Sample"
            ],
            "author": [
                "Yinfei Kong",
                "Zemin Zheng",
                "Jinchi Lv"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-513/14-513.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Search Problem in Mixture Models",
            "abstract": [
                "We consider the task of learning the parameters of a single component of a mixture model, for the case when we are given side information about that component; we call this the \"search problem\" in mixture models. We would like to solve this with computational and sample complexity lower than solving the overall original problem, where one learns parameters of all components. Our main contributions are the development of a simple but general model for the notion of side information, and a corresponding simple matrix-based algorithm for solving the search problem in this general setting. We then specialize this model and algorithm to four common scenarios: Gaussian mixture models, LDA topic models, subspace clustering, and mixed linear regression. For each one of these we show that if (and only if) the side information is informative, we obtain parameter estimates with greater accuracy, and also improved computation complexity than existing moment based mixture model algorithms (e.g. tensor methods). We also illustrate several natural ways one can obtain such side information, for specific problem instances. Our experiments on real data sets (NY Times, Yelp, BSDS500) further demonstrate the practicality of our algorithms showing significant improvement in runtime and accuracy."
            ],
            "keywords": [
                "mixture models",
                "search",
                "side information",
                "semi-supervised",
                "method of moments"
            ],
            "author": [
                "Avik Ray",
                "Joe Neeman",
                "Sujay Sanghavi",
                "Sanjay Shakkottai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-483/16-483.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ThunderSVM: A Fast SVM Library on GPUs and CPUs",
            "abstract": [
                "Support Vector Machines (SVMs) are classic supervised learning models for classification, regression and distribution estimation. A survey conducted by Kaggle in 2017 shows that 26% of the data mining and machine learning practitioners are users of SVMs. However, SVM training and prediction are very expensive computationally for large and complex problems. This paper presents an efficient and open source SVM software toolkit called ThunderSVM which exploits the high-performance of Graphics Processing Units (GPUs) and multi-core CPUs. ThunderSVM supports all the functionalities-including classification (SVC), regression (SVR) and one-class SVMs-of LibSVM and uses identical command line options, such that existing LibSVM users can easily apply our toolkit. ThunderSVM can be used through multiple language interfaces including C/C++, Python, R and MAT-LAB. Our experimental results show that ThunderSVM is generally an order of magnitude faster than LibSVM while producing identical SVMs. In addition to the high efficiency, we design our convex optimization solver in a general way such that SVC, SVR, and one-class SVMs share the same solver for the ease of maintenance. Documentation, examples, and more about ThunderSVM are available at https://github.com/zeyiwen/thundersvm."
            ],
            "keywords": [
                "SVMs",
                "GPUs",
                "multi-core CPUs",
                "efficiency",
                "multiple interfaces"
            ],
            "author": [
                "Zeyi Wen",
                "Jian Chen",
                "Jiashuai Shi",
                "Qinbin Li",
                "Bingsheng He"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-740/17-740.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning",
            "abstract": [
                "In this paper, we consider the supervised learning task which consists in predicting the normalized rank of a numerical variable. We introduce a novel probabilistic approach to estimate the posterior distribution of the target rank conditionally to the predictors. We turn this learning task into a model selection problem. For that, we define a 2D partitioning family obtained by discretizing numerical variables and grouping categorical ones and we derive an analytical criterion to select the partition with the highest posterior probability. We show how these partitions can be used to build univariate predictors and multivariate ones under a naive Bayes assumption. We also propose a new evaluation criterion for probabilistic rank estimators. Based on the logarithmic score, we show that such criterion presents the advantage to be minored, which is not the case of the logarithmic score computed for probabilistic value estimator. A first set of experimentations on synthetic data shows the good properties of the proposed criterion and of our partitioning approach. A second set of experimentations on real data shows competitive performance of the univariate and selective naive Bayes rank estimators projected on the value range compared to methods submitted to a recent challenge on probabilistic metric regression tasks. Our approach is applicable for all regression problems with categorical or numerical predictors. It is particularly interesting for those with a high number of predictors as it automatically detects the variables which contain predictive information. It builds pertinent predictors of the normalized rank of the numerical target from one or several predictors. As the criteria selection is regularized by the presence of a prior and a posterior term, it does not suffer from overfitting."
            ],
            "keywords": [
                "rank regression",
                "probabilistic approach",
                "2D partitioning",
                "non parametric estimation",
                "Bayesian model selection"
            ],
            "author": [
                "Carine Hue",
                "Marc Boullé",
                "France Telecom",
                "Isabelle Guyon",
                "Amir Saffari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/hue07a/hue07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Limitations of Learning Via Embeddings in Euclidean Half Spaces",
            "abstract": [
                "The notion of embedding a class of dichotomies in a class of linear half spaces is central to the support vector machines paradigm. We examine the question of determining the minimal Euclidean dimension and the maximal margin that can be obtained when the embedded class has a finite VC dimension. We show that an overwhelming majority of the family of finite concept classes of any constant VC dimension cannot be embedded in low-dimensional half spaces. (In fact, we show that the Euclidean dimension must be almost as high as the size of the instance space.) We strengthen this result even further by showing that an overwhelming majority of the family of finite concept classes of any constant VC dimension cannot be embedded in half spaces (of arbitrarily high Euclidean dimension) with a large margin. (In fact, the margin cannot be substantially larger than the margin achieved by the trivial embedding.) Furthermore, these bounds are robust in the sense that allowing each image half space to err on a small fraction of the instances does not imply a significant weakening of these dimension and margin bounds. Our results indicate that any universal learning machine, which transforms data into the Euclidean space and then applies linear (or large margin) classification, cannot enjoy any meaningful generalization guarantees that are based on either VC dimension or margins considerations. This failure of generalization bounds applies even to classes for which \"straight forward\" empirical risk minimization does enjoy meaningful generalization guarantees."
            ],
            "keywords": [
                "Concept Learning",
                "Embeddings in Half Spaces",
                "Large Margin Classification"
            ],
            "author": [
                "Shai Ben-David",
                "Nadav Eiron",
                "Hans Ulrich Simon"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/bendavid02a/bendavid02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Lazifying Conditional Gradient Algorithms",
            "abstract": [
                "Conditional gradient algorithms (also often called Frank-Wolfe algorithms) are popular due to their simplicity of only requiring a linear optimization oracle and more recently they also gained significant traction for online learning. While simple in principle, in many cases the actual implementation of the linear optimization oracle is costly. We show a general method to lazify various conditional gradient algorithms, which in actual computations leads to several orders of magnitude of speedup in wall-clock time. This is achieved by using a faster separation oracle instead of a linear optimization oracle, relying only on few linear optimization oracle calls."
            ],
            "keywords": [
                "Frank-Wolfe algorithm",
                "conditional gradient",
                "caching",
                "linear optimization oracle",
                "convex optimization"
            ],
            "author": [
                "Gábor Braun",
                "Sebastian Pokutta",
                "Daniel Zink"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-114/18-114.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Model Monitor (M 2 ): Evaluating, Comparing, and Monitoring Models",
            "abstract": [
                "This paper presents Model Monitor (M 2), a Java toolkit for robustly evaluating machine learning algorithms in the presence of changing data distributions. M 2 provides a simple and intuitive framework in which users can evaluate classifiers under hypothesized shifts in distribution and therefore determine the best model (or models) for their data under a number of potential scenarios. Additionally, M 2 is fully integrated with the WEKA machine learning environment, so that a variety of commodity classifiers can be used if desired."
            ],
            "keywords": [
                "machine learning",
                "open-source software",
                "distribution shift",
                "scenario analysis"
            ],
            "author": [
                "Troy Raeder",
                "Nitesh V Chawla"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/raeder09a/raeder09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GPflow: A Gaussian Process Library using TensorFlow",
            "abstract": [
                "GPf low is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. 1 The distinguishing features of GPf low are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware. 1. GPf low and TensorFlow are available as open source software under the Apache 2.0 license."
            ],
            "keywords": [],
            "author": [
                "Alexander G De",
                "G Matthews",
                "Tom Nickson",
                "Alexis Boukouvalas",
                "James Hensman",
                "G De",
                "Mark Van Der Wilk",
                "Keisuke Fujii",
                "Pablo León-Villagrá",
                "Zoubin Ghahramani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-537/16-537.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "mlr: Machine Learning in R",
            "abstract": [
                "The mlr package provides a generic, object-oriented, and extensible framework for classification, regression, survival analysis and clustering for the R language. It provides a unified interface to more than 160 basic learners and includes meta-algorithms and model selection techniques to improve and extend the functionality of basic learners with, e.g., hyperparameter tuning, feature selection, and ensemble construction. Parallel high-performance computing is natively supported. The package targets practitioners who want to quickly apply machine learning algorithms, as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment."
            ],
            "keywords": [
                "machine learning",
                "hyperparameter tuning",
                "model selection",
                "feature selection",
                "benchmarking",
                "R",
                "visualization",
                "data mining"
            ],
            "author": [
                "Bernd Bischl",
                "Michel Lang",
                "Lars Kotthoff",
                "Julia Schiffner",
                "Jakob Richter",
                "Erich Studerus",
                "Giuseppe Casalicchio",
                "Zachary M Jones"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-066/15-066.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parallel Vector Field Embedding",
            "abstract": [
                "We propose a novel local isometry based dimensionality reduction method from the perspective of vector fields, which is called parallel vector field embedding (PFE). We first give a discussion on local isometry and global isometry to show the intrinsic connection between parallel vector fields and isometry. The problem of finding an isometry turns out to be equivalent to finding orthonormal parallel vector fields on the data manifold. Therefore, we first find orthonormal parallel vector fields by solving a variational problem on the manifold. Then each embedding function can be obtained by requiring its gradient field to be as close to the corresponding parallel vector field as possible. Theoretical results show that our method can precisely recover the manifold if it is isometric to a connected open subset of Euclidean space. Both synthetic and real data examples demonstrate the effectiveness of our method even if there is heavy noise and high curvature."
            ],
            "keywords": [
                "manifold learning",
                "isometry",
                "vector field",
                "covariant derivative",
                "out-of-sample extension"
            ],
            "author": [
                "Binbin Lin",
                "Chiyuan Zhang",
                "Ming Ji"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/lin13a/lin13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Optimal Sample Complexity of PAC Learning",
            "abstract": [
                "This work establishes a new upper bound on the number of samples sufficient for PAC learning in the realizable case. The bound matches known lower bounds up to numerical constant factors. This solves a long-standing open problem on the sample complexity of PAC learning. The technique and analysis build on a recent breakthrough by Hans Simon."
            ],
            "keywords": [
                "sample complexity",
                "PAC learning",
                "statistical learning theory",
                "minimax analysis",
                "learning algorithm"
            ],
            "author": [
                "Steve Hanneke"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-389/15-389.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Invariant Models for Causal Transfer Learning",
            "abstract": [
                "Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set."
            ],
            "keywords": [
                "Transfer learning",
                "Multi-task learning",
                "Causality",
                "Domain adaptation",
                "Domain generalization"
            ],
            "author": [
                "Mateo Rojas-Carulla",
                "Bernhard Schölkopf",
                "Richard Turner",
                "Jonas Peters"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-432/16-432.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Approximate Bilinear Programming for Value Function Approximation",
            "abstract": [
                "Value function approximation methods have been successfully used in many applications, but the prevailing techniques often lack useful a priori error bounds. We propose a new approximate bilinear programming formulation of value function approximation, which employs global optimization. The formulation provides strong a priori guarantees on both robust and expected policy loss by minimizing specific norms of the Bellman residual. Solving a bilinear program optimally is NP-hard, but this worst-case complexity is unavoidable because the Bellman-residual minimization itself is NP-hard. We describe and analyze the formulation as well as a simple approximate algorithm for solving bilinear programs. The analysis shows that this algorithm offers a convergent generalization of approximate policy iteration. We also briefly analyze the behavior of bilinear programming algorithms under incomplete samples. Finally, we demonstrate that the proposed approach can consistently minimize the Bellman residual on simple benchmark problems."
            ],
            "keywords": [
                "value function approximation",
                "approximate dynamic programming",
                "Markov decision processes"
            ],
            "author": [
                "Marek Petrik",
                "Shlomo Zilberstein"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/petrik11a/petrik11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distribution-Dependent Sample Complexity of Large Margin Learning",
            "abstract": [
                "We obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with L 2 regularization: We introduce the margin-adapted dimension, which is a simple function of the second order statistics of the data distribution, and show distribution-specific upper and lower bounds on the sample complexity, both governed by the margin-adapted dimension of the data distribution. The upper bounds are universal, and the lower bounds hold for the rich family of sub-Gaussian distributions with independent features. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. To prove the lower bound, we develop several new tools of independent interest. These include new connections between shattering and hardness of learning, new properties of shattering with linear classifiers, and a new lower bound on the smallest eigenvalue of a random Gram matrix generated by sub-Gaussian variables. Our results can be used to quantitatively compare large margin learning to other learning rules, and to improve the effectiveness of methods that use sample complexity bounds, such as active learning."
            ],
            "keywords": [
                "supervised learning",
                "sample complexity",
                "linear classifiers",
                "distribution-dependence"
            ],
            "author": [
                "Sivan Sabato",
                "Nathan Srebro",
                "Naftali Tishby"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/sabato13a/sabato13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "QuantMiner for Mining Quantitative Association Rules",
            "abstract": [
                "In this paper, we propose QUANTMINER, a mining quantitative association rules system. This system is based on a genetic algorithm that dynamically discovers \"good\" intervals in association rules by optimizing both the support and the confidence. The experiments on real and artificial databases have shown the usefulness of QUANTMINER as an interactive, exploratory data mining tool."
            ],
            "keywords": [
                "association rules",
                "numerical and categorical attributes",
                "unsupervised discretization",
                "genetic algorithm",
                "simulated annealing"
            ],
            "author": [
                "Ansaf Salleb-Aouissi",
                "Christel Vrain",
                "Cyril Nortet",
                "Xiangrong Kong",
                "Daniel Cassard",
                "Brgm Fr"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/salleb-aouissi13a/salleb-aouissi13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "One-Shot-Learning Gesture Recognition using HOG-HOF Features",
            "abstract": [
                "The purpose of this paper is to describe one-shot-learning gesture recognition systems developed on the ChaLearn Gesture Dataset (ChaLearn). We use RGB and depth images and combine appearance (Histograms of Oriented Gradients) and motion descriptors (Histogram of Optical Flow) for parallel temporal segmentation and recognition. The Quadratic-Chi distance family is used to measure differences between histograms to capture cross-bin relationships. We also propose a new algorithm for trimming videos-to remove all the unimportant frames from videos. We present two methods that use a combination of HOG-HOF descriptors together with variants of a Dynamic Time Warping technique. Both methods outperform other published methods and help narrow the gap between human performance and algorithms on this task. The code is publicly available in the MLOSS repository."
            ],
            "keywords": [
                "ChaLearn",
                "histogram of oriented gradients",
                "histogram of optical flow",
                "dynamic time warping"
            ],
            "author": [
                "Jakub Konečný",
                "Michal Hagara",
                "Korešpondenčný Matematický Seminár",
                "Isabelle Guyon",
                "Vassilis Athitsos",
                "Sergio Escalera",
                "Jakub ©2014",
                "Michal Konečný"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/konecny14a/konecny14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Complete Dictionary Learning via 4 -Norm Maximization over the Orthogonal Group",
            "abstract": [
                "This paper considers the fundamental problem of learning a complete (orthogonal) dictionary from samples of sparsely generated signals. Most existing methods solve the dictionary (and sparse representations) based on heuristic algorithms, usually without theoretical guarantees for either optimality or complexity. The recent-minimization based methods do provide such guarantees but the associated algorithms recover the dictionary one column at a time. In this work, we propose a new formulation that maximizes the 4-norm over the orthogonal group, to learn the entire dictionary. We prove that under a random data model, with nearly minimum sample complexity, the global optima of the-norm are very close to signed permutations of the ground truth. Inspired by this observation, we give a conceptually simple and yet effective algorithm based on \"matching, stretching, and projection\" (MSP). The algorithm provably converges locally and cost per iteration is merely an SVD. In addition to strong theoretical guarantees, experiments show that the new algorithm is significantly more efficient and effective than existing methods, including KSVD and 1-based methods. Preliminary experimental results on mixed real imagery data clearly demonstrate advantages of so learned dictionary over classic PCA bases."
            ],
            "keywords": [
                "sparse dictionary learning",
                "4 -norm maximization",
                "orthogonal group",
                "measure concentration",
                "fixed point algorithm"
            ],
            "author": [
                "Yuexiang Zhai",
                "Zhenyu Liao",
                "John Wright",
                "Yi Ma"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-755/19-755.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Contextual Policy Search",
            "abstract": [
                "We consider the problem of learning skills that are versatilely applicable. One popular approach for learning such skills is contextual policy search in which the individual tasks are represented as context vectors. We are interested in settings in which the agent is able to actively select the tasks that it examines during the learning process. We argue that there is a better way than selecting each task equally often because some tasks might be easier to learn at the beginning and the knowledge that the agent can extract from these tasks can be transferred to similar but more difficult tasks. The methods that we propose for addressing the task-selection problem model the learning process as a nonstationary multi-armed bandit problem with custom intrinsic reward heuristics so that the estimated learning progress will be maximized. This approach does neither make any assumptions about the underlying contextual policy search algorithm nor about the policy representation. We present empirical results on an artificial benchmark problem and a ball throwing problem with a simulated Mitsubishi PA-10 robot arm which show that active context selection can improve the learning of skills considerably."
            ],
            "keywords": [
                "reinforcement learning",
                "policy search",
                "movement primitives",
                "active learning",
                "multi-task learning"
            ],
            "author": [
                "Alexander Fabisch",
                "Jan Hendrik Metzen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/fabisch14a/fabisch14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Message Passing",
            "abstract": [
                "Bayesian inference is now widely established as one of the principal foundations for machine learning. In practice, exact inference is rarely possible, and so a variety of approximation techniques have been developed, one of the most widely used being a deterministic framework called variational inference. In this paper we introduce Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to Bayesian Networks. Like belief propagation, VMP proceeds by sending messages between nodes in the network and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing additional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational message passing has been implemented in the form of a general purpose inference engine called VIBES ('Variational Inference for BayEsian networkS') which allows models to be specified graphically and then solved variationally without recourse to coding."
            ],
            "keywords": [
                "Bayesian networks",
                "variational inference",
                "message passing"
            ],
            "author": [
                "John Winn",
                "Christopher M Bishop"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/winn05a/winn05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structured Sparsity via Alternating Direction Methods",
            "abstract": [
                "We consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm that incorporates prior knowledge of the group structure of the features. Such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term. In this paper, we focus on two commonly adopted sparsity-inducing regularization terms, the overlapping Group Lasso penalty l 1 /l 2-norm and the l 1 /l ∞-norm. We propose a unified framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efficiently solved. As one of the core building-blocks of this framework, we develop new algorithms using a partial-linearization/splitting technique and prove that the accelerated versions of these algorithms require O(1 √ ε) iterations to obtain an ε-optimal solution. We compare the performance of these algorithms against that of the alternating direction augmented Lagrangian and FISTA methods on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms."
            ],
            "keywords": [
                "structured sparsity",
                "overlapping Group Lasso",
                "alternating direction methods",
                "variable splitting",
                "augmented Lagrangian"
            ],
            "author": [
                "Tony Qin",
                "Donald Goldfarb"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/qin12a/qin12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Loss Control with Rank-one Covariance Estimate for Short-term Portfolio Optimization",
            "abstract": [
                "In short-term portfolio optimization (SPO), some financial characteristics like the expected return and the true covariance might be dynamic. Then there are only a small window size w of observations that are sufficiently close to the current moment and reliable to make estimations. w is usually much smaller than the number of assets d, which leads to a typical undersampled problem. Worse still, the asset price relatives are not likely subject to any proper distributions. These facts violate the statistical assumptions of the traditional covariance estimates and invalidate their statistical efficiency and consistency in risk measurement. In this paper, we propose to reconsider the function of covariance estimates in the perspective of operators, and establish a rank-one covariance estimate in the principal rank-one tangent space at the observation matrix. Moreover, we propose a loss control scheme with this estimate, which effectively catches the instantaneous risk structure and avoids extreme losses. We conduct extensive experiments on 7 real-world benchmark daily or monthly data sets with stocks, funds and portfolios from diverse regional markets to show that the proposed method achieves state-of-the-art performance in comprehensive downside risk metrics and gains good investing incomes as well. It offers a novel perspective of rank-related approaches for undersampled estimations in SPO."
            ],
            "keywords": [
                "rank-one covariance estimate",
                "short-term portfolio optimization",
                "undersampled condition",
                "loss control",
                "downside risk"
            ],
            "author": [
                "Zhao-Rong Lai",
                "Xiaotian Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-959/19-959.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Practical Kernel-Based Reinforcement Learning",
            "abstract": [
                "Kernel-based reinforcement learning (KBRL) stands out among approximate reinforcement learning algorithms for its strong theoretical guarantees. By casting the learning problem as a local kernel approximation, KBRL provides a way of computing a decision policy which converges to a unique solution and is statistically consistent. Unfortunately, the model constructed by KBRL grows with the number of sample transitions, resulting in a computational cost that precludes its application to large-scale or on-line domains. In this paper we introduce an algorithm that turns KBRL into a practical reinforcement learning tool. Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a transition probability matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix, potentially much smaller than the original, which retains some fundamental properties of its precursor. KBSF exploits such an insight to compress the information contained in KBRL's model into an approximator of fixed size. This makes it possible to build an approximation considering both the difficulty of the problem and the associated computational cost. KBSF's computational complexity is linear in the number of sample transitions, which is the best one can do without discarding data. Moreover, the algorithm's simple mechanics allow for a fully incremental implementation that makes the amount of memory used independent of the number of sample transitions. The result is a kernel-based reinforcement learning algorithm that can be applied to large-scale problems in both off-line and on-line regimes. We derive upper bounds for the distance between the value functions computed by KBRL and KBSF using the same data. We also prove that it is possible to control the magnitude of the variables appearing in our bounds, which means that, given enough computational resources, we can make KBSF's value function as close as desired to the value function that would be computed by KBRL using the same set of sample transitions. The potential of our algorithm is demonstrated in an extensive empirical study in which KBSF is applied to difficult tasks based on real-world data. Not only does KBSF solve problems that had never been solved before, but it also significantly outperforms other state-of-the-art reinforcement learning algorithms on the tasks studied."
            ],
            "keywords": [
                "reinforcement learning",
                "dynamic programming",
                "Markov decision processes",
                "kernel-based approximation",
                "stochastic factorization"
            ],
            "author": [
                "André M S Barreto",
                "Doina Precup",
                "Joelle Pineau",
                "S Barreto"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/13-134/13-134.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "High-dimensional Covariance Estimation Based On Gaussian Graphical Models",
            "abstract": [
                "Undirected graphs are often used to describe high dimensional distributions. Under sparsity conditions, the graph can be estimated using ℓ 1-penalization methods. We propose and study the following method. We combine a multiple regression approach with ideas of thresholding and refitting: first we infer a sparse undirected graphical model structure via thresholding of each among many ℓ 1-norm penalized regression functions; we then estimate the covariance matrix and its inverse using the maximum likelihood estimator. We show that under suitable conditions, this approach yields consistent estimation in terms of graphical structure and fast convergence rates with respect to the operator and Frobenius norm for the covariance matrix and its inverse. We also derive an explicit bound for the Kullback Leibler divergence."
            ],
            "keywords": [
                "graphical model selection",
                "covariance estimation",
                "Lasso",
                "nodewise regression",
                "thresholding"
            ],
            "author": [
                "Shuheng Zhou",
                "Philipp Rütimann",
                "Min Xu",
                "Peter Bühlmann"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/zhou11a/zhou11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Information Rates of Nonparametric Gaussian Process Methods Aad van der Vaart",
            "abstract": [
                "We consider the quality of learning a response function by a nonparametric Bayesian approach using a Gaussian process (GP) prior on the response function. We upper bound the quadratic risk of the learning procedure, which in turn is an upper bound on the Kullback-Leibler information between the predictive and true data distribution. The upper bound is expressed in small ball probabilities and concentration measures of the GP prior. We illustrate the computation of the upper bound for the Matérn and squared exponential kernels. For these priors the risk, and hence the information criterion, tends to zero for all continuous response functions. However, the rate at which this happens depends on the combination of true response function and Gaussian prior, and is expressible in a certain concentration function. In particular, the results show that for good performance, the regularity of the GP prior should match the regularity of the unknown response function."
            ],
            "keywords": [
                "Bayesian learning",
                "Gaussian prior",
                "information rate",
                "risk",
                "Matérn kernel",
                "squared exponential kernel"
            ],
            "author": [
                "Aad @ Few",
                "J H V Zanten",
                "@ Tue"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/vandervaart11a/vandervaart11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Parameter-Free Classification Method for Large Scale Learning",
            "abstract": [
                "With the rapid growth of computer storage capacities, available data and demand for scoring models both follow an increasing trend, sharper than that of the processing power. However, the main limitation to a wide spread of data mining solutions is the non-increasing availability of skilled data analysts, which play a key role in data preparation and model selection. In this paper, we present a parameter-free scalable classification method, which is a step towards fully automatic data mining. The method is based on Bayes optimal univariate conditional density estimators, naive Bayes classification enhanced with a Bayesian variable selection scheme, and averaging of models using a logarithmic smoothing of the posterior distribution. We focus on the complexity of the algorithms and show how they can cope with data sets that are far larger than the available central memory. We finally report results on the Large Scale Learning challenge, where our method obtains state of the art performance within practicable computation time."
            ],
            "keywords": [
                "large scale learning",
                "naive Bayes",
                "Bayesianism",
                "model selection",
                "model averaging"
            ],
            "author": [
                "Marc Boullé"
            ],
            "ref": "http://www.jmlr.org/papers/volume10/boulle09a/boulle09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "State-by-state Minimax Adaptive Estimation for Nonparametric Hidden Markov Models",
            "abstract": [
                "In this paper, we introduce a new estimator for the emission densities of a nonparametric hidden Markov model. It is adaptive and minimax with respect to each state's regularityas opposed to globally minimax estimators, which adapt to the worst regularity among the emission densities. Our method is based on Goldenshluger and Lepski's methodology. It is computationally efficient and only requires a family of preliminary estimators, without any restriction on the type of estimators considered. We present two such estimators that allow to reach minimax rates up to a logarithmic term: a spectral estimator and a least squares estimator. We show how to calibrate it in practice and assess its performance on simulations and on real data."
            ],
            "keywords": [
                "hidden Markov model",
                "model selection",
                "nonparametric density estimation",
                "oracle inequality",
                "adaptive minimax estimation",
                "spectral method",
                "least squares method"
            ],
            "author": [
                "Luc Lehéricy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-345/17-345.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Layer-Wise Learning Strategy for Nonparametric Tensor Product Smoothing Spline Regression and Graphical Models",
            "abstract": [
                "Nonparametric estimation of multivariate functions is an important problem in statistical machine learning with many applications, ranging from nonparametric regression to nonparametric graphical models. Several authors have proposed to estimate multivariate functions under the smoothing spline analysis of variance (SSANOVA) framework, which assumes that the multivariate function can be decomposed into the summation of main effects, two-way interaction effects, and higher order interaction effects. However, existing methods are not scalable to the dimension of the random variables and the order of interactions. We propose a LAyer-wiSE leaRning strategy (LASER) to estimate multivariate functions under the SSANOVA framework. The main idea is to approximate the multivariate function sequentially starting from a model with only the main effects. Conditioned on the support of the estimated main effects, we estimate the two-way interaction effects only when the corresponding main effects are estimated to be non-zero. This process is continued until no more higher order interaction effects are identified. The proposed strategy provides a data-driven approach for estimating multivariate functions under the SSANOVA framework. Our proposal yields a sequence of estimators. To study the theoretical properties of the sequence of estimators, we establish the notion of post-selection persistency. Extensive numerical studies are performed to evaluate the performance of LASER."
            ],
            "keywords": [
                "Persistency",
                "nonparametric regression",
                "nonparametric graphical models",
                "sequential algorithm",
                "model selection"
            ],
            "author": [
                "Ming Kean",
                "Junwei Lu",
                "Tong Zhang",
                "Han Liu",
                "C 2019 Kean",
                "Ming Tan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-525/17-525.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Kernel Classifiers with Online and Active Learning",
            "abstract": [
                "Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efficient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We first present an online SVM algorithm based on this premise. LASVM yields competitive misclassification rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels."
            ],
            "keywords": [],
            "author": [
                "Antoine Bordes",
                "Jason Weston",
                "Léon Bottou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/bordes05a/bordes05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Very Fast Online Learning of Highly Non Linear Problems",
            "abstract": [
                "The experimental investigation on the efficient learning of highly non-linear problems by online training, using ordinary feed forward neural networks and stochastic gradient descent on the errors computed by back-propagation, gives evidence that the most crucial factors for efficient training are the hidden units' differentiation, the attenuation of the hidden units' interference and the selective attention on the parts of the problems where the approximation error remains high. In this report, we present global and local selective attention techniques and a new hybrid activation function that enables the hidden units to acquire individual receptive fields which may be global or local depending on the problem's local complexities. The presented techniques enable very efficient training on complex classification problems with embedded subproblems."
            ],
            "keywords": [
                "neural networks",
                "online training",
                "selective attention",
                "activation functions",
                "receptive fields"
            ],
            "author": [
                "Aggelos Chariatis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/chariatis07a/chariatis07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Introduction to the Special Issue on Inductive Logic Programming",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "James Cussens",
                "Alan M Frisch"
            ],
            "ref": "http://www.jmlr.org/papers/volume4/cussens03a/cussens03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Successor Features Combine Elements of Model-Free and Model-based Reinforcement Learning",
            "abstract": [
                "A key question in reinforcement learning is how an intelligent agent can generalize knowledge across different inputs. By generalizing across different inputs, information learned for one input can be immediately reused for improving predictions for another input. Reusing information allows an agent to compute an optimal decision-making strategy using less data. State representation is a key element of the generalization process, compressing a high-dimensional input space into a low-dimensional latent state space. This article analyzes properties of different latent state spaces, leading to new connections between modelbased and model-free reinforcement learning. Successor features, which predict frequencies of future observations, form a link between model-based and model-free learning: Learning to predict future expected reward outcomes, a key characteristic of model-based agents, is equivalent to learning successor features. Learning successor features is a form of temporal difference learning and is equivalent to learning to predict a single policy's utility, which is a characteristic of model-free agents. Drawing on the connection between model-based reinforcement learning and successor features, we demonstrate that representations that are predictive of future reward outcomes generalize across variations in both transitions and rewards. This result extends previous work on successor features, which is constrained to fixed transitions and assumes re-learning of the transferred state representation."
            ],
            "keywords": [
                "Successor Features",
                "Model-Based Reinforcement Learning",
                "State Representations",
                "State Abstractions"
            ],
            "author": [
                "Lucas Lehnert",
                "Michael L Littman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-060/19-060.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Comments on the \"Core Vector Machines: Fast SVM Training on Very Large Data Sets\"",
            "abstract": [
                "In a recently published paper in JMLR, Tsang et al. (2005) present an algorithm for SVM called Core Vector Machines (CVM) and illustrate its performances through comparisons with other SVM solvers. After reading the CVM paper we were surprised by some of the reported results. In order to clarify the matter, we decided to reproduce some of the experiments. It turns out that to some extent, our results contradict those reported. Reasons of these different behaviors are given through the analysis of the stopping criterion."
            ],
            "keywords": [
                "SVM",
                "CVM",
                "large scale",
                "KKT gap",
                "stopping condition",
                "stopping criteria"
            ],
            "author": [
                "Gaëlle Loosli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/loosli07a/loosli07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Evolution * 3",
            "abstract": [
                "Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics. We propose a temporal point process model, Coevolve, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate interleaved diffusion and network events, and generate. * Preliminary version of this work appeared in (Farajtabar et al., 2015b). ."
            ],
            "keywords": [
                "social networks",
                "information diffusion",
                "network structure",
                "co-evolutionary dynamics",
                "point processes",
                "Hawkes process",
                "survival analysis"
            ],
            "author": [
                "Mehrdad Farajtabar",
                "Yichen Wang",
                "Manuel Gomez-Rodriguez",
                "H Milton",
                "Hongyuan Zha",
                "Le Song",
                "Shuang Li",
                "Li Gomez-Rodriguez"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-132/16-132.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Finding the Most Interesting Patterns in a Database Quickly by Using Sequential Sampling",
            "abstract": [
                "Many discovery problems, e.g., subgroup or association rule discovery, can naturally be cast as nbest hypotheses problems where the goal is to find the n hypotheses from a given hypothesis space that score best according to a certain utility function. We present a sampling algorithm that solves this problem by issuing a small number of database queries while guaranteeing precise bounds on the confidence and quality of solutions. Known sampling approaches have treated single hypothesis selection problems, assuming that the utility is the average (over the examples) of some function-which is not the case for many frequently used utility functions. We show that our algorithm works for all utilities that can be estimated with bounded error. We provide these error bounds and resulting worst-case sample bounds for some of the most frequently used utilities, and prove that there is no sampling algorithm for a popular class of utility functions that cannot be estimated with bounded error. The algorithm is sequential in the sense that it starts to return (or discard) hypotheses that already seem to be particularly good (or bad) after a few examples. Thus, the algorithm is almost always faster than its worst-case bounds."
            ],
            "keywords": [
                "Sampling",
                "online learning",
                "incremental learning",
                "large databases"
            ],
            "author": [
                "Tobias Scheffer",
                "Stefan Wrobel",
                "Fhg Ais",
                "Schloß Birlinghoven",
                "Sankt Augustin",
                "Carla E Brodley",
                "Andrea Danyluk"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/scheffer02a/scheffer02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Constrained Dynamic Programming and Supervised Penalty Learning Algorithms for Peak Detection in Genomic Data",
            "abstract": [
                "Peak detection in genomic data involves segmenting counts of DNA sequence reads aligned to different locations of a chromosome. The goal is to detect peaks with higher counts, and filter out background noise with lower counts. Most existing algorithms for this problem are unsupervised heuristics tailored to patterns in specific data types. We propose a supervised framework for this problem, using optimal changepoint detection models with learned penalty functions. We propose the first dynamic programming algorithm that is guaranteed to compute the optimal solution to changepoint detection problems with constraints between adjacent segment mean parameters. Implementing this algorithm requires the choice of penalty parameter that determines the number of segments that are estimated. We show how the supervised learning ideas of Rigaill et al. (2013) can be used to choose this penalty. We compare the resulting implementation of our algorithm to several baselines in a benchmark of labeled ChIP-seq data sets with two different patterns (broad H3K36me3 data and sharp H3K4me3 data). Whereas baseline unsupervised methods only provide accurate peak detection for a single pattern, our supervised method achieves state-of-the-art accuracy in all data sets. The log-linear timings of our proposed dynamic programming algorithm make it scalable to the large genomic data sets that are now common. Our implementation is available in the PeakSegOptimal R package on CRAN."
            ],
            "keywords": [
                "Non-convex",
                "constrained",
                "optimization",
                "changepoint",
                "segmentation"
            ],
            "author": [
                "Toby Dylan Hocking",
                "Guillem Rigaill",
                "Paul Fearnhead",
                "Guillaume Bourque"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-843/18-843.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Sup-norm Perturbation of HOSVD and Low Rank Tensor Denoising",
            "abstract": [
                "The higher order singular value decomposition (HOSVD) of tensors is a generalization of matrix SVD. The perturbation analysis of HOSVD under random noise is more delicate than its matrix counterpart. Recently, polynomial time algorithms have been proposed where statistically optimal estimates of the singular subspaces and the low rank tensors are attainable in the Euclidean norm. In this article, we analyze the sup-norm perturbation bounds of HOSVD and introduce estimators of the singular subspaces with sharp deviation bounds in the sup-norm. We also investigate a low rank tensor denoising estimator and demonstrate its fast convergence rate with respect to the entry-wise errors. The supnorm perturbation bounds reveal unconventional phase transitions for statistical learning applications such as the exact clustering in high dimensional Gaussian mixture model and the exact support recovery in sub-tensor localizations. In addition, the bounds established for HOSVD also elaborate the one-sided sup-norm perturbation bounds for the singular subspaces of unbalanced (or fat) matrices."
            ],
            "keywords": [
                "HOSVD",
                "Entry-wise perturbation",
                "Gaussian noise",
                "High dimensional clustering"
            ],
            "author": [
                "Dong Xia",
                "Fan Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-397/17-397.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Survey of Preference-Based Reinforcement Learning Methods",
            "abstract": [
                "Reinforcement learning (RL) techniques optimize the accumulated long-term reward of a suitably chosen reward function. However, designing such a reward function often requires a lot of task-specific prior knowledge. The designer needs to consider different objectives that do not only influence the learned behavior but also the learning progress. To alleviate these issues, preference-based reinforcement learning algorithms (PbRL) have been proposed that can directly learn from an expert's preferences instead of a hand-designed numeric reward. PbRL has gained traction in recent years due to its ability to resolve the reward shaping problem, its ability to learn from non numeric rewards and the possibility to reduce the dependence on expert knowledge. We provide a unified framework for PbRL that describes the task formally and points out the different design principles that affect the evaluation task for the human as well as the computational complexity. The design principles include the type of feedback that is assumed, the representation that is learned to capture the preferences, the optimization problem that has to be solved as well as how the exploration/exploitation problem is tackled. Furthermore, we point out shortcomings of current algorithms, propose open research questions and briefly survey practical tasks that have been solved using PbRL."
            ],
            "keywords": [
                "Reinforcement Learning",
                "Preference Learning",
                "Qualitative Feedback",
                "Markov Decision Process",
                "Policy Search",
                "Temporal Difference Learning",
                "Preference-based Reinforcement Learning"
            ],
            "author": [
                "Christian Wirth",
                "Riad Akrour",
                "Gerhard Neumann",
                "Johannes Fürnkranz"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-634/16-634.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Restricted Eigenvalue Properties for Correlated Gaussian Designs",
            "abstract": [
                "Methods based on ℓ 1-relaxation, such as basis pursuit and the Lasso, are very popular for sparse regression in high dimensions. The conditions for success of these methods are now well-understood: (1) exact recovery in the noiseless setting is possible if and only if the design matrix X satisfies the restricted nullspace property, and (2) the squared ℓ 2-error of a Lasso estimate decays at the minimax optimal rate k log p n , where k is the sparsity of the p-dimensional regression problem with additive Gaussian noise, whenever the design satisfies a restricted eigenvalue condition. The key issue is thus to determine when the design matrix X satisfies these desirable properties. Thus far, there have been numerous results showing that the restricted isometry property, which implies both the restricted nullspace and eigenvalue conditions, is satisfied when all entries of X are independent and identically distributed (i.i.d.), or the rows are unitary. This paper proves directly that the restricted nullspace and eigenvalue conditions hold with high probability for quite general classes of Gaussian matrices for which the predictors may be highly dependent, and hence restricted isometry conditions can be violated with high probability. In this way, our results extend the attractive theoretical guarantees on ℓ 1-relaxations to a much broader class of problems than the case of completely independent or unitary designs."
            ],
            "keywords": [
                "Lasso",
                "basis pursuit",
                "random matrix theory",
                "Gaussian comparison inequality",
                "concentration of measure"
            ],
            "author": [
                "Garvesh Raskutti",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/raskutti10a/raskutti10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Concave Learners for Rankboost",
            "abstract": [
                "Rankboost has been shown to be an effective algorithm for combining ranks. However, its ability to generalize well and not overfit is directly related to the choice of weak learner, in the sense that regularization of the rank function is due to the regularization properties of its weak learners. We present a regularization property called consistency in preference and confidence that mathematically translates into monotonic concavity, and describe a new weak ranking learner (MWGR) that generates ranking functions with this property. In experiments combining ranks from multiple face recognition algorithms and an experiment combining text information retrieval systems, rank functions using MWGR proved superior to binary weak learners."
            ],
            "keywords": [
                "rankboost",
                "ranking",
                "convex/concave",
                "regularization"
            ],
            "author": [
                "Ofer Melnik"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/melnik07a/melnik07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A New Class of Time Dependent Latent Factor Models with Applications",
            "abstract": [
                "In many applications, observed data are influenced by some combination of latent causes. For example, suppose sensors are placed inside a building to record responses such as temperature, humidity, power consumption and noise levels. These random, observed responses are typically affected by many unobserved, latent factors (or features) within the building such as the number of individuals, the turning on and off of electrical devices, power surges, etc. These latent factors are usually present for a contiguous period of time before disappearing; further, multiple factors could be present at a time. This paper develops new probabilistic methodology and inference methods for random object generation influenced by latent features exhibiting temporal persistence. Every datum is associated with subsets of a potentially infinite number of hidden, persistent features that account for temporal dynamics in an observation. The ensuing class of dynamic models constructed by adapting the Indian Buffet Process-a probability measure on the space of random, unbounded binary matrices-finds use in a variety of applications arising in operations, signal processing, biomedicine, marketing, image analysis, etc. Illustrations using synthetic and real data are provided."
            ],
            "keywords": [
                "Bayesian Nonparametrics",
                "Latent Factor Models",
                "Time Dependence"
            ],
            "author": [
                "Sinead A Williamson",
                "Michael Minyi Zhang",
                "Paul Damien"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/16-639/16-639.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data",
            "abstract": [
                "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting."
            ],
            "keywords": [],
            "author": [
                "Kubota Rie",
                "Tong Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/ando05a/ando05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Integrated Common Sense Learning and Planning in POMDPs",
            "abstract": [
                "We formulate a new variant of the problem of planning in an unknown environment, for which we can provide algorithms with reasonable theoretical guarantees in spite of large state spaces and time horizons, partial observability, and complex dynamics. In this variant, an agent is given a collection of example traces produced by a reference policy, which may, for example, capture the agent's past behavior. The agent is (only) asked to find policies that are supported by regularities in the dynamics that are observable on these example traces. We describe an efficient algorithm that uses such \"common sense\" knowledge reflected in the example traces to construct decision tree policies for goal-oriented factored POMDPs. More precisely, our algorithm (provably) succeeds at finding a policy for a given input goal when (1) there is a CNF that is almost always observed satisfied on the traces of the POMDP, capturing a sufficient approximation of its dynamics and (2) for a decision tree policy of bounded complexity, there exist small-space resolution proofs that the goal is achieved on each branch using the aforementioned CNF capturing the \"common sense rules.\" Such a CNF always exists for noisy STRIPS domains, for example. Our results thus essentially establish that the possession of a suitable exploration policy for collecting the necessary examples is the fundamental obstacle to learning to act in such environments."
            ],
            "keywords": [
                "Partially Observed Markov Decision Process",
                "Decision Tree Policies",
                "PAC-Semantics",
                "Noisy STRIPS",
                "Non-monontonic Reasoning"
            ],
            "author": [
                "Brendan Juba"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/13-584/13-584.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multi-Assignment Clustering for Boolean Data",
            "abstract": [
                "We propose a probabilistic model for clustering Boolean data where an object can be simultaneously assigned to multiple clusters. By explicitly modeling the underlying generative process that combines the individual source emissions, highly structured data are expressed with substantially fewer clusters compared to single-assignment clustering. As a consequence, such a model provides robust parameter estimators even when the number of samples is low. We extend the model with different noise processes and demonstrate that maximum-likelihood estimation with multiple assignments consistently infers source parameters more accurately than single-assignment clustering. Our model is primarily motivated by the task of role mining for role-based access control, where users of a system are assigned one or more roles. In experiments with real-world access-control data, our model exhibits better generalization performance than state-of-the-art approaches."
            ],
            "keywords": [
                "clustering",
                "multi-assignments",
                "overlapping clusters",
                "Boolean data",
                "role mining",
                "latent feature models"
            ],
            "author": [
                "Mario Frank",
                "U C Berkeley",
                "Andreas P Streich",
                "Joachim M Buhmann",
                "Eth Zürich"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/frank12a/frank12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets",
            "abstract": [
                "Gaussian process regression is a flexible and powerful tool for machine learning, but the high computational complexity hinders its broader applications. In this paper, we propose a new approach for fast computation of Gaussian process regression with a focus on large spatial data sets. The approach decomposes the domain of a regression function into small subdomains and infers a local piece of the regression function for each subdomain. We explicitly address the mismatch problem of the local pieces on the boundaries of neighboring subdomains by imposing continuity constraints. The new approach has comparable or better computation complexity as other competing methods, but it is easier to be parallelized for faster computation. Moreover, the method can be adaptive to non-stationary features because of its local nature and, in particular, its use of different hyperparameters of the covariance function for different local regions. We illustrate application of the method and demonstrate its advantages over existing methods using two synthetic data sets and two real spatial data sets."
            ],
            "keywords": [
                "domain decomposition",
                "boundary value problem",
                "Gaussian process regression",
                "parallel computation",
                "spatial prediction"
            ],
            "author": [
                "Chiwoo Park",
                "Jianhua Z Huang",
                "Yu Ding"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/park11a/park11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "LIBLINEAR: A Library for Large Linear Classification",
            "abstract": [
                "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets."
            ],
            "keywords": [
                "large-scale linear classification",
                "logistic regression",
                "support vector machines",
                "open source",
                "machine learning"
            ],
            "author": [
                "Rong-En Fan",
                "Kai-Wei Chang",
                "Cho-Jui Hsieh",
                "Chih-Jen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/fan08a/fan08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust Near-Separable Nonnegative Matrix Factorization Using Linear Optimization",
            "abstract": [
                "Nonnegative matrix factorization (NMF) has been shown recently to be tractable under the separability assumption, under which all the columns of the input data matrix belong to the convex cone generated by only a few of these columns. Bittorf, Recht, Ré and Tropp ('Factoring nonnegative matrices with linear programs', NIPS 2012) proposed a linear programming (LP) model, referred to as Hottopixx, which is robust under any small perturbation of the input matrix. However, Hottopixx has two important drawbacks: (i) the input matrix has to be normalized, and (ii) the factorization rank has to be known in advance. In this paper, we generalize Hottopixx in order to resolve these two drawbacks, that is, we propose a new LP model which does not require normalization and detects the factorization rank automatically. Moreover, the new LP model is more flexible, significantly more tolerant to noise, and can easily be adapted to handle outliers and other noise models. Finally, we show on several synthetic data sets that it outperforms Hottopixx while competing favorably with two state-of-the-art methods."
            ],
            "keywords": [
                "nonnegative matrix factorization",
                "separability",
                "linear programming",
                "convex optimization",
                "robustness to noise",
                "pure-pixel assumption",
                "hyperspectral unmixing"
            ],
            "author": [
                "Nicolas Gillis",
                "Robert Luce",
                "MA Institut Für Mathematik"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/gillis14a/gillis14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Data-adaptive Non-parametric Kernels",
            "abstract": [
                "In this paper, we propose a data-adaptive non-parametric kernel learning framework in margin based kernel methods. In model formulation, given an initial kernel matrix, a data-adaptive matrix with two constraints is imposed in an entry-wise scheme. Learning this data-adaptive matrix in a formulation-free strategy enlarges the margin between classes and thus improves the model flexibility. The introduced two constraints are imposed either exactly (on small data sets) or approximately (on large data sets) in our model, which provides a controllable trade-off between model flexibility and complexity with theoretical demonstration. In algorithm optimization, the objective function of our learning framework is proven to be gradient-Lipschitz continuous. Thereby, kernel and classifier/regressor learning can be efficiently optimized in a unified framework via Nesterov's acceleration. For the scalability issue, we study a decomposition-based approach to our model in the large sample case. The effectiveness of this approximation is illustrated by both empirical studies and theoretical guarantees. Experimental results on various classification and regression benchmark data sets demonstrate that our non-parametric kernel learning framework achieves good performance when compared with other representative kernel learning based algorithms."
            ],
            "keywords": [
                "support vector machines",
                "non-parametric kernel learning",
                "gradient-Lipschitz continuous"
            ],
            "author": [
                "Fanghui Liu",
                "Xiaolin Huang",
                "Li Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-900/19-900.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Accelerating Stochastic Composition Optimization",
            "abstract": [
                "We consider the stochastic nested composition optimization problem where the objective is a composition of two expected-value functions. We propose a new stochastic first-order method, namely the accelerated stochastic compositional proximal gradient (ASC-PG) method. This algorithm updates the solution based on noisy gradient queries using a two-timescale iteration. The ASC-PG is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty. We show that the ASC-PG exhibits faster convergence than the best known algorithms, and that it achieves the optimal sample-error complexity in several important special cases. We demonstrate the application of ASC-PG to reinforcement learning and conduct numerical experiments."
            ],
            "keywords": [
                "Large-scale optimization",
                "stochastic gradient",
                "composition optimization",
                "sample complexity"
            ],
            "author": [
                "Mengdi Wang",
                "Ji Liu",
                "Ethan X Fang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-504/16-504.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Harmonic Mean Iteratively Reweighted Least Squares for Low-Rank Matrix Recovery",
            "abstract": [
                "We propose a new iteratively reweighted least squares (IRLS) algorithm for the recovery of a matrix X P C d1ˆd2 of rank r ! minpd 1 , d q from incomplete linear observations, solving a sequence of low complexity linear problems. The easily implementable algorithm, which we call harmonic mean iteratively reweighted least squares (HM-IRLS), optimizes a non-convex Schatten-p quasi-norm penalization to promote low-rankness and carries three major strengths, in particular for the matrix completion setting. First, we observe a remarkable global convergence behavior of the algorithm's iterates to the low-rank matrix for relevant, interesting cases, for which any other state-of-the-art optimization approach fails the recovery. Secondly, HM-IRLS exhibits an empirical recovery probability close to 1 even for a number of measurements very close to the theoretical lower bound rpd 1`d2´r q, i.e., already for significantly fewer linear observations than any other tractable approach in the literature. Thirdly, HM-IRLS exhibits a locally superlinear rate of convergence (of order 2´p) if the linear observations fulfill a suitable null space property. While for the first two properties we have so far only strong empirical evidence, we prove the third property as our main theoretical result."
            ],
            "keywords": [
                "Iteratively Reweighted Least Squares",
                "Low-Rank Matrix Recovery",
                "Matrix Completion",
                "Non-Convex Optimization"
            ],
            "author": [
                "Christian Kümmerle",
                "Juliane Sigl"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-244/17-244.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotic Consistency of α-Rényi-Approximate Posteriors",
            "abstract": [
                "We study the asymptotic consistency properties of α-Rényi approximate posteriors, a class of variational Bayesian methods that approximate an intractable Bayesian posterior with a member of a tractable family of distributions, the member chosen to minimize the α-Rényi divergence from the true posterior. Unique to our work is that we consider settings with α > 1, resulting in approximations that upperbound the log-likelihood, and consequently have wider spread than traditional variational approaches that minimize the Kullback-Liebler (KL) divergence from the posterior. Our primary result identifies sufficient conditions under which consistency holds, centering around the existence of a 'good' sequence of distributions in the approximating family that possesses, among other properties, the right rate of convergence to a limit distribution. We further characterize the good sequence by demonstrating that a sequence of distributions that converges too quickly cannot be a good sequence. We also extend our analysis to the setting where α equals one, corresponding to the minimizer of the reverse KL divergence, and to models with local latent variables. We also illustrate the existence of good sequence with a number of examples. Our results complement a growing body of work focused on the frequentist properties of variational Bayesian methods."
            ],
            "keywords": [
                "α-Rényi divergence",
                "Asymptotic consistency",
                "Bayesian computation",
                "Variational inference"
            ],
            "author": [
                "Prateek Jaiswal",
                "Vinayak Rao",
                "Harsha Honnappa"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-161/19-161.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators",
            "abstract": [
                "The False Discovery Rate (FDR) is a commonly used type I error rate in multiple testing problems. It is defined as the expected False Discovery Proportion (FDP), that is, the expected fraction of false positives among rejected hypotheses. When the hypotheses are independent, the Benjamini-Hochberg procedure achieves FDR control at any pre-specified level. By construction, FDR control offers no guarantee in terms of power, or type II error. A number of alternative procedures have been developed, including plug-in procedures that aim at gaining power by incorporating an estimate of the proportion of true null hypotheses. In this paper, we study the asymptotic behavior of a class of plug-in procedures based on kernel estimators of the density of the p-values, as the number m of tested hypotheses grows to infinity. In a setting where the hypotheses tested are independent, we prove that these procedures are asymptotically more powerful in two respects: (i) a tighter asymptotic FDR control for any target FDR level and (ii) a broader range of target levels yielding positive asymptotic power. We also show that this increased asymptotic power comes at the price of slower, non-parametric convergence rates for the FDP. These rates are of the form m −k/(2k+1) , where k is determined by the regularity of the density of the p-value distribution, or, equivalently, of the test statistics distribution. These results are applied to one-and two-sided tests statistics for Gaussian and Laplace location models, and for the Student model."
            ],
            "keywords": [
                "multiple testing",
                "false discovery rate",
                "Benjamini Hochberg's procedure",
                "power",
                "criticality",
                "plug-in procedures",
                "adaptive control",
                "test statistics distribution",
                "convergence rates",
                "kernel estimators"
            ],
            "author": [
                "Pierre Neuvial"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/neuvial13a/neuvial13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Rademacher and Gaussian Complexities: Risk Bounds and Structural Results",
            "abstract": [
                "We investigate the use of certain data-dependent estimates of the complexity of a function class, called Rademacher and Gaussian complexities. In a decision theoretic setting, we prove general risk bounds in terms of these complexities. We consider function classes that can be expressed as combinations of functions from basis classes and show how the Rademacher and Gaussian complexities of such a function class can be bounded in terms of the complexity of the basis classes. We give examples of the application of these techniques in finding data-dependent risk bounds for decision trees, neural networks and support vector machines."
            ],
            "keywords": [
                "Error Bounds",
                "Data-Dependent Complexity",
                "Rademacher Averages",
                "Maximum Discrepancy"
            ],
            "author": [
                "Peter L Bartlett",
                "Shahar Mendelson"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Inference and Optimal Design for the Sparse Linear Model",
            "abstract": [
                "The linear model with sparsity-favouring prior on the coefficients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identification from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in significant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identification application appears in Steinke et al. (2007)."
            ],
            "keywords": [
                "sparse linear model",
                "Laplace prior",
                "expectation propagation",
                "approximate inference",
                "optimal design",
                "Bayesian statistics",
                "gene network recovery",
                "image coding",
                "compressive sensing"
            ],
            "author": [
                "Matthias W Seeger"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/seeger08a/seeger08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "One-Class SVMs for Document Classification",
            "abstract": [
                "We implemented versions of the SVM appropriate for one-class classification in the context of information retrieval. The experiments were conducted on the standard Reuters data set. For the SVM implementation we used both a version of Schölkopf et al. and a somewhat different version of one-class SVM based on identifying \"outlier\" data as representative of the second-class. We report on experiments with different kernels for both of these implementations and with different representations of the data, including binary vectors, tf-idf representation and a modification called \"Hadamard\" representation. Then we compared it with one-class versions of the algorithms prototype (Rocchio), nearest neighbor, naive Bayes, and finally a natural one-class neural network classification method based on \"bottleneck\" compression generated filters. The SVM approach as represented by Schölkopf was superior to all the methods except the neural network one, where it was, although occasionally worse, essentially comparable. However, the SVM methods turned out to be quite sensitive to the choice of representation and kernel in ways which are not well understood; therefore, for the time being leaving the neural network approach as the most robust."
            ],
            "keywords": [
                "Support Vector Machine",
                "SVM",
                "Neural Network",
                "Compression Neural Network",
                "Text Retrieval",
                "Positive Information"
            ],
            "author": [
                "Larry M Manevitz",
                "Malik Yousef",
                "Nello Cristianini",
                "Bob Williamson"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/manevitz01a/manevitz01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Relational Dependency Networks",
            "abstract": [
                "Recent work on graphical models for relational data has demonstrated significant improvements in classification and inference when models represent the dependencies among instances. Despite its use in conventional statistical models, the assumption of instance independence is contradicted by most relational data sets. For example, in citation data there are dependencies among the topics of a paper's references, and in genomic data there are dependencies among the functions of interacting proteins. In this paper, we present relational dependency networks (RDNs), graphical models that are capable of expressing and reasoning with such dependencies in a relational setting. We discuss RDNs in the context of relational Bayes networks and relational Markov networks and outline the relative strengths of RDNs-namely, the ability to represent cyclic dependencies, simple methods for parameter estimation, and efficient structure learning techniques. The strengths of RDNs are due to the use of pseudolikelihood learning techniques, which estimate an efficient approximation of the full joint distribution. We present learned RDNs for a number of real-world data sets and evaluate the models in a prediction context, showing that RDNs identify and exploit cyclic relational dependencies to achieve significant performance gains over conventional conditional models. In addition, we use synthetic data to explore model performance under various relational data characteristics, showing that RDN learning and inference techniques are accurate over a wide range of conditions."
            ],
            "keywords": [
                "relational learning",
                "probabilistic relational models",
                "knowledge discovery",
                "graphical models",
                "dependency networks",
                "pseudolikelihood estimation"
            ],
            "author": [
                "Jennifer Neville",
                "David Jensen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/neville07a/neville07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric graphical model for counts",
            "abstract": [
                "Although multivariate count data are routinely collected in many application areas, there is surprisingly little work developing flexible models for characterizing their dependence structure. This is particularly true when interest focuses on inferring the conditional independence graph. In this article, we propose a new class of pairwise Markov random field-type models for the joint distribution of a multivariate count vector. By employing a novel type of transformation, we avoid restricting to non-negative dependence structures or inducing other restrictions through truncations. Taking a Bayesian approach to inference, we choose a Dirichlet process prior for the distribution of a random effect to induce great flexibility in the specification. An efficient Markov chain Monte Carlo (MCMC) algorithm is developed for posterior computation. We prove various theoretical properties, including posterior consistency, and show that our COunt Nonparametric Graphical Analysis (CONGA) approach has good performance relative to competitors in simulation studies. The methods are motivated by an application to neuron spike count data in mice."
            ],
            "keywords": [
                "Conditional independence",
                "Dirichlet process",
                "Graphical model",
                "Markov random field",
                "Multivariate count data"
            ],
            "author": [
                "Arkaprava Roy",
                "David B Dunson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-362/19-362.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast and Scalable Local Kernel Machines",
            "abstract": [
                "A computationally efficient approach to local learning with kernel methods is presented. The Fast Local Kernel Support Vector Machine (FaLK-SVM) trains a set of local SVMs on redundant neighbourhoods in the training set and an appropriate model for each query point is selected at testing time according to a proximity strategy. Supported by a recent result by Zakai and Ritov (2009) relating consistency and localizability, our approach achieves high classification accuracies by dividing the separation function in local optimisation problems that can be handled very efficiently from the computational viewpoint. The introduction of a fast local model selection further speeds-up the learning process. Learning and complexity bounds are derived for FaLK-SVM, and the empirical evaluation of the approach (with data sets up to 3 million points) showed that it is much faster and more accurate and scalable than state-of-the-art accurate and approximated SVM solvers at least for non high-dimensional data sets. More generally, we show that locality can be an important factor to sensibly speed-up learning approaches and kernel methods, differently from other recent techniques that tend to dismiss local information in order to improve scalability."
            ],
            "keywords": [
                "locality",
                "kernel methods",
                "local learning algorithms",
                "support vector machines",
                "instancebased learning"
            ],
            "author": [
                "Nicola Segata",
                "Enrico Blanzieri"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/segata10a/segata10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Random Spanning Trees and the Prediction of Weighted Graphs",
            "abstract": [
                "We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world data sets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice."
            ],
            "keywords": [
                "online learning",
                "learning on graphs",
                "graph prediction",
                "random spanning trees"
            ],
            "author": [
                "Nicolò Cesa-Bianchi",
                "Claudio Gentile",
                "Fabio Vitale",
                "Giovanni Zappella"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/cesa-bianchi13a/cesa-bianchi13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simultaneous Private Learning of Multiple Concepts *",
            "abstract": [
                "We investigate the direct-sum problem in the context of differentially private PAC learning: What is the sample complexity of solving k learning tasks simultaneously under differential privacy, and how does this cost compare to that of solving k learning tasks without privacy? In our setting, an individual example consists of a domain element x labeled by k unknown concepts (c 1 ,. .. , c k). The goal of a multi-learner is to output k hypotheses (h 1 ,. .. , h k) that generalize the input examples. Without concern for privacy, the sample complexity needed to simultaneously learn k concepts is essentially the same as needed for learning a single concept. Under differential privacy, the basic strategy of learning each hypothesis independently yields sample complexity that grows polynomially with k. For some concept classes, we give multi-learners that require fewer samples than the basic strategy. Unfortunately, however, we also give lower bounds showing that even for very simple concept classes, the sample cost of private multi-learning must grow polynomially in k."
            ],
            "keywords": [
                "Differential privacy",
                "PAC learning",
                "Agnostic learning",
                "Direct-sum"
            ],
            "author": [
                "Mark Bun",
                "Uri Stemmer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-549/18-549.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Active Learning to Recognize Multiple Types of Plankton",
            "abstract": [
                "This paper presents an active learning method which reduces the labeling effort of domain experts in multi-class classification problems. Active learning is applied in conjunction with support vector machines to recognize underwater zooplankton from higher-resolution, new generation SIPPER II images. Most previous work on active learning with support vector machines only deals with two class problems. In this paper, we propose an active learning approach \"breaking ties\" for multiclass support vector machines using the one-vs-one approach with a probability approximation. Experimental results indicate that our approach often requires significantly less labeled images to reach a given accuracy than the approach of labeling the least certain test example and random sampling. It can also be applied in batch mode resulting in an accuracy comparable to labeling one image at a time and retraining."
            ],
            "keywords": [
                "active learning",
                "support vector machine",
                "plankton recognition",
                "probabilistic output",
                "multi-class support vector machine"
            ],
            "author": [
                "Tong Luo",
                "Kurt Kramer",
                "Lawrence O Hall",
                "Scott Samson",
                "Andrew Remsen",
                "Thomas Hopkins"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/luo05a/luo05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparse and low-rank multivariate Hawkes processes",
            "abstract": [
                "We consider the problem of unveiling the implicit network structure of node interactions (such as user interactions in a social network), based only on high-frequency timestamps. Our inference is based on the minimization of the least-squares loss associated with a multivariate Hawkes model, penalized by 1 and trace norm of the interaction tensor. We provide a first theoretical analysis for this problem, that includes sparsity and low-rank inducing penalizations. This result involves a new data-driven concentration inequality for matrix martingales in continuous time with observable variance, which is a result of independent interest and a broad range of possible applications since it extends to matrix martingales former results restricted to the scalar case. A consequence of our analysis is the construction of sharply tuned 1 and trace-norm penalizations, that leads to a data-driven scaling of the variability of information available for each users. Numerical experiments illustrate the significant improvements achieved by the use of such data-driven penalizations."
            ],
            "keywords": [
                "Hawkes processes",
                "Sparsity",
                "Low-Rank",
                "Random matrices",
                "Data-driven concentration"
            ],
            "author": [
                "Emmanuel Bacry",
                "Martin Bompaire",
                "Jean-Francois Muzy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/15-114/15-114.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Links Between Multiplicity Automata, Observable Operator Models and Predictive State Representations -a Unified Learning Framework",
            "abstract": [
                "Stochastic multiplicity automata (SMA) are weighted finite automata that generalize probabilistic automata. They have been used in the context of probabilistic grammatical inference. Observable operator models (OOMs) are a generalization of hidden Markov models, which in turn are models for discrete-valued stochastic processes and are used ubiquitously in the context of speech recognition and bio-sequence modeling. Predictive state representations (PSRs) extend OOMs to stochastic input-output systems and are employed in the context of agent modeling and planning. We present SMA, OOMs, and PSRs under the common framework of sequential systems, which are an algebraic characterization of multiplicity automata, and examine the precise relationships between them. Furthermore, we establish a unified approach to learning such models from data. Many of the learning algorithms that have been proposed can be understood as variations of this basic learning scheme, and several turn out to be closely related to each other, or even equivalent."
            ],
            "keywords": [
                "multiplicity automata",
                "hidden Markov models",
                "observable operator models",
                "predictive state representations",
                "spectral learning algorithms"
            ],
            "author": [
                "Michael Thon",
                "Herbert Jaeger"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/thon15a/thon15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Reinforcement Learning in Finite MDPs: PAC Analysis",
            "abstract": [
                "We study the problem of learning near-optimal behavior in finite Markov Decision Processes (MDPs) with a polynomial number of samples. These \"PAC-MDP\" algorithms include the wellknown E 3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a unified theoretical framework. A more refined analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX."
            ],
            "keywords": [
                "reinforcement learning",
                "Markov decision processes",
                "PAC-MDP",
                "exploration",
                "sample complexity"
            ],
            "author": [
                "Alexander L Strehl",
                "Lihong Li",
                "Michael L Littman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/strehl09a/strehl09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ProtoAttend: Attention-Based Prototypical Learning",
            "abstract": [
                "We propose a novel inherently interpretable machine learning method that bases decisions on few relevant examples that we call prototypes. Our method, ProtoAttend, can be integrated into a wide range of neural network architectures including pre-trained models. It utilizes an attention mechanism that relates the encoded representations to samples in order to determine prototypes. Protoattend yields superior results in three high impact problems without sacrificing accuracy of the original model: (1) it enables high-quality interpretability that outputs samples most relevant to the decision-making (i.e. a samplebased interpretability method); (2) it achieves state of the art confidence estimation by quantifying the mismatch across prototype labels; and (3) it obtains state of the art in distribution mismatch detection. All these can be achieved with minimal additional test time and a practically viable training time computational cost."
            ],
            "keywords": [
                "Sample-based interpretability",
                "confidence",
                "attention",
                "explainable deep learning",
                "prototypical"
            ],
            "author": [
                "Sercanö Arık",
                "Tomas Pfister"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-042/20-042.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Particle Swarm Model Selection",
            "abstract": [
                "This paper proposes the application of particle swarm optimization (PSO) to the problem of full model selection, FMS, for classification tasks. FMS is defined as follows: given a pool of preprocessing methods, feature selection and learning algorithms, to select the combination of these that obtains the lowest classification error for a given data set; the task also includes the selection of hyperparameters for the considered methods. This problem generates a vast search space to be explored, well suited for stochastic optimization techniques. FMS can be applied to any classification domain as it does not require domain knowledge. Different model types and a variety of algorithms can be considered under this formulation. Furthermore, competitive yet simple models can be obtained with FMS. We adopt PSO for the search because of its proven performance in different problems and because of its simplicity, since neither expensive computations nor complicated operations are needed. Interestingly, the way the search is guided allows PSO to avoid overfitting to some extend. Experimental results on benchmark data sets give evidence that the proposed approach is very effective, despite its simplicity. Furthermore, results obtained in the framework of a model selection challenge show the competitiveness of the models selected with PSO, compared to models selected with other techniques that focus on a single algorithm and that use domain knowledge."
            ],
            "keywords": [
                "full model selection",
                "machine learning challenge",
                "particle swarm optimization",
                "experimentation",
                "cross validation"
            ],
            "author": [
                "Hugo Jair Escalante",
                "Manuel Montes",
                "Luis Enrique Sucar",
                "Isabelle Guyon",
                "Amir Saffari"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/escalante09a/escalante09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "More Efficient Estimation for Logistic Regression with Optimal Subsamples",
            "abstract": [
                "In this paper, we propose improved estimation method for logistic regression based on subsamples taken according the optimal subsampling probabilities developed in Wang et al. (2018). Both asymptotic results and numerical results show that the new estimator has a higher estimation efficiency. We also develop a new algorithm based on Poisson subsampling, which does not require to approximate the optimal subsampling probabilities all at once. This is computationally advantageous when available random-access memory is not enough to hold the full data. Interestingly, asymptotic distributions also show that Poisson subsampling produces a more efficient estimator if the sampling ratio, the ratio of the subsample size to the full data sample size, does not converge to zero. We also obtain the unconditional asymptotic distribution for the estimator based on Poisson subsampling. Pilot estimators are required to calculate subsampling probabilities and to correct biases in un-weighted estimators; interestingly, even if pilot estimators are inconsistent, the proposed method still produce consistent and asymptotically normal estimators."
            ],
            "keywords": [
                "Asymptotic Distribution",
                "Logistic Regression",
                "Massive Data",
                "Optimal Subsampling",
                "Poisson Sampling"
            ],
            "author": [
                "Haiying Wang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-596/18-596.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Supervised Learning via Euler's Elastica Models",
            "abstract": [
                "This paper investigates the Euler's elastica (EE) model for high-dimensional supervised learning problems in a function approximation framework. In 1744 Euler introduced the elastica energy for a 2D curve on modeling torsion-free thin elastic rods. Together with its degenerate form of total variation (TV), Euler's elastica has been successfully applied to low-dimensional data processing such as image denoising and image inpainting in the last two decades. Our motivation is to apply Euler's elastica to high-dimensional supervised learning problems. To this end, a supervised learning problem is modeled as an energy functional minimization under a new geometric regularization scheme, where the energy is composed of a squared loss and an elastica penalty. The elastica penalty aims at regularizing the approximated function by heavily penalizing large gradients and high curvature values on all level curves. We take a computational PDE approach to minimize the energy functional. By using variational principles, the energy minimization problem is transformed into an Euler-Lagrange PDE. However, this PDE is usually high-dimensional and can not be directly handled by common low-dimensional solvers. To circumvent this difficulty, we use radial basis functions (RBF) to approximate the target function, which reduces the optimization problem to finding the linear coefficients of these basis functions. Some theoretical properties of this new model, including the existence and uniqueness of solutions and universal consistency, are analyzed. Extensive experiments have demonstrated the effectiveness of the proposed model for binary classification, multi-class classification, and regression tasks."
            ],
            "keywords": [
                "supervised learning",
                "Euler's elastica",
                "total variation",
                "geometric regularization",
                "Euler-Lagrange PDE",
                "function approximation",
                "universal consistency"
            ],
            "author": [
                "Tong Lin",
                "Hanlin Xue",
                "Ling Wang",
                "Bo Huang",
                "Xue Hongbin Zha Lin",
                "Zha Huang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/lin15b/lin15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gaussian Processes for Ordinal Regression",
            "abstract": [
                "We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative filtering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach."
            ],
            "keywords": [
                "Gaussian processes",
                "ordinal regression",
                "approximate Bayesian inference",
                "collaborative filtering",
                "gene expression analysis",
                "feature selection"
            ],
            "author": [
                "Wei Chu",
                "Christopher K I Williams"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/chu05a/chu05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Asymptotic Normality of an Estimate of a Regression Functional",
            "abstract": [
                "An estimate of the second moment of the regression function is introduced. Its asymptotic normality is proved such that the asymptotic variance depends neither on the dimension of the observation vector, nor on the smoothness properties of the regression function. The asymptotic variance is given explicitly."
            ],
            "keywords": [
                "nonparametric estimation",
                "regression functional",
                "central limit theorem",
                "partitioning estimate"
            ],
            "author": [
                "László Györfi",
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/gyorfi15a/gyorfi15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Linear Algorithms for Online Multitask Classification",
            "abstract": [
                "We introduce new Perceptron-based algorithms for the online multitask binary classification problem. Under suitable regularity conditions, our algorithms are shown to improve on their baselines by a factor proportional to the number of tasks. We achieve these improvements using various types of regularization that bias our algorithms towards specific notions of task relatedness. More specifically, similarity among tasks is either measured in terms of the geometric closeness of the task reference vectors or as a function of the dimension of their spanned subspace. In addition to adapting to the online setting a mix of known techniques, such as the multitask kernels of Evgeniou et al., our analysis also introduces a matrix-based multitask extension of the p-norm Perceptron, which is used to implement spectral co-regularization. Experiments on real-world data sets complement and support our theoretical findings."
            ],
            "keywords": [
                "mistake bounds",
                "perceptron algorithm",
                "multitask learning",
                "spectral regularization"
            ],
            "author": [
                "Giovanni Cavallanti",
                "Nicolò Cesa-Bianchi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/cavallanti10a/cavallanti10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimating Causal Structure Using Conditional DAG Models",
            "abstract": [
                "This paper considers inference of causal structure in a class of graphical models called conditional DAGs. These are directed acyclic graph (DAG) models with two kinds of variables, primary and secondary. The secondary variables are used to aid in the estimation of the structure of causal relationships between the primary variables. We prove that, under certain assumptions, such causal structure is identifiable from the joint observational distribution of the primary and secondary variables. We give causal semantics for the model class, put forward a score-based approach for estimation and establish consistency results. Empirical results demonstrate gains compared with formulations that treat all variables on an equal footing, or that ignore secondary variables. The methodology is motivated by applications in biology that involve multiple data types and is illustrated here using simulated data and in an analysis of molecular data from the Cancer Genome Atlas."
            ],
            "keywords": [
                "Graphical Models",
                "Causal Inference",
                "Directed Acyclic Graphs",
                "Instrumental Variables",
                "Data Integration"
            ],
            "author": [
                "Chris J Oates",
                "Jim Q Smith",
                "Sach Mukherjee"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-479/14-479.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Assessing Approximate Inference for Binary Gaussian Process Classification",
            "abstract": [
                "Gaussian process priors can be used to define flexible, probabilistic classification models. Unfortunately exact Bayesian inference is analytically intractable and various approximation techniques have been proposed. In this work we review and compare Laplace's method and Expectation Propagation for approximate Bayesian inference in the binary Gaussian process classification model. We present a comprehensive comparison of the approximations, their predictive performance and marginal likelihood estimates to results obtained by MCMC sampling. We explain theoretically and corroborate empirically the advantages of Expectation Propagation compared to Laplace's method."
            ],
            "keywords": [
                "Gaussian process priors",
                "probabilistic classification",
                "Laplace's approximation",
                "expectation propagation",
                "marginal likelihood",
                "evidence",
                "MCMC"
            ],
            "author": [
                "Malte Kuss",
                "Carl Edward Rasmussen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/kuss05a/kuss05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparsity Regularized Estimation",
            "abstract": [
                "We analyze the convergence behaviour of a recently proposed algorithm for regularized estimation called Dual Augmented Lagrangian (DAL). Our analysis is based on a new interpretation of DAL as a proximal minimization algorithm. We theoretically show under some conditions that DAL converges super-linearly in a non-asymptotic and global sense. Due to a special modelling of sparse estimation problems in the context of machine learning, the assumptions we make are milder and more natural than those made in conventional analysis of augmented Lagrangian algorithms. In addition, the new interpretation enables us to generalize DAL to wide varieties of sparse estimation problems. We experimentally confirm our analysis in a large scale ℓ 1-regularized logistic regression problem and extensively compare the efficiency of DAL algorithm to previously proposed algorithms on both synthetic and benchmark data sets."
            ],
            "keywords": [
                "dual augmented Lagrangian",
                "proximal minimization",
                "global convergence",
                "sparse estimation",
                "convex optimization"
            ],
            "author": [
                "Ryota Tomioka",
                "Taiji Suzuki"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/tomioka11a/tomioka11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Kernel K-Means Clustering with Nyström Approximation: Relative-Error Bounds",
            "abstract": [
                "Kernel k-means clustering can correctly identify and extract a far more varied collection of cluster structures than the linear k-means clustering algorithm. However, kernel kmeans clustering is computationally expensive when the non-linear feature map is highdimensional and there are many input points. Kernel approximation, e.g., the Nyström method, has been applied in previous works to approximately solve kernel learning problems when both of the above conditions are present. This work analyzes the application of this paradigm to kernel k-means clustering, and shows that applying the linear k-means clustering algorithm to k (1 + o(1)) features constructed using a so-called rank-restricted Nyström approximation results in cluster assignments that satisfy a 1 + approximation ratio in terms of the kernel k-means cost function, relative to the guarantee provided by the same algorithm without the use of the Nyström method. As part of the analysis, this work establishes a novel 1 + relative-error trace norm guarantee for low-rank approximation using the rank-restricted Nyström approximation. Empirical evaluations on the 8.1 million instance MNIST8M dataset demonstrate the scalability and usefulness of kernel k-means clustering with Nyström approximation. This work argues that spectral clustering using Nyström approximation-a popular and computationally efficient, but theoretically unsound approach to non-linear clusteringshould be replaced with the efficient and theoretically sound combination of kernel k-means clustering with Nyström approximation. The superior performance of the latter approach is empirically verified."
            ],
            "keywords": [
                "kernel k-means clustering",
                "the Nyström method",
                "randomized linear algebra"
            ],
            "author": [
                "Shusen Wang",
                "Alex Gittens",
                "Michael W Mahoney"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-517/17-517.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Learning-Curve Sampling Method Applied to Model-Based Clustering",
            "abstract": [
                "We examine the learning-curve sampling method, an approach for applying machinelearning algorithms to large data sets. The approach is based on the observation that the computational cost of learning a model increases as a function of the sample size of the training data, whereas the accuracy of a model has diminishing improvements as a function of sample size. Thus, the learning-curve sampling method monitors the increasing costs and performance as larger and larger amounts of data are used for training, and terminates learning when future costs outweigh future benefits. In this paper, we formalize the learning-curve sampling method and its associated cost-benefit tradeoff in terms of decision theory. In addition, we describe the application of the learning-curve sampling method to the task of model-based clustering via the expectation-maximization (EM) algorithm. In experiments on three real data sets, we show that the learning-curve sampling method produces models that are nearly as accurate as those trained on complete data sets, but with dramatically reduced learning times. Finally, we describe an extension of the basic learning-curve approach for model-based clustering that results in an additional speedup. This extension is based on the observation that the shape of the learning curve for a given model and data set is roughly independent of the number of EM iterations used during training. Thus, we run EM for only a few iterations to decide how many cases to use for training, and then run EM to full convergence once the number of cases is selected."
            ],
            "keywords": [
                "Learning-curve sampling method",
                "clustering",
                "scalability",
                "decision theory",
                "sampling"
            ],
            "author": [
                "Christopher Meek",
                "Bo Thiesson",
                "David Heckerman"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/meek02a/meek02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Importance Sampling for Continuous Time Bayesian Networks",
            "abstract": [
                "A continuous time Bayesian network (CTBN) uses a structured representation to describe a dynamic system with a finite number of states which evolves in continuous time. Exact inference in a CTBN is often intractable as the state space of the dynamic system grows exponentially with the number of variables. In this paper, we first present an approximate inference algorithm based on importance sampling. We then extend it to continuous-time particle filtering and smoothing algorithms. These three algorithms can estimate the expectation of any function of a trajectory, conditioned on any evidence set constraining the values of subsets of the variables over subsets of the time line. We present experimental results on both synthetic networks and a network learned from a real data set on people's life history events. We show the accuracy as well as the time efficiency of our algorithms, and compare them to other approximate algorithms: expectation propagation and Gibbs sampling."
            ],
            "keywords": [
                "continuous time Bayesian networks",
                "importance sampling",
                "approximate inference",
                "filtering",
                "smoothing"
            ],
            "author": [
                "Yu Fan",
                "Jing Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/fan10a/fan10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Relative Margin and Data-Dependent Regularization",
            "abstract": [
                "Leading classification methods such as support vector machines (SVMs) and their counterparts achieve strong generalization performance by maximizing the margin of separation between data classes. While the maximum margin approach has achieved promising performance, this article identifies its sensitivity to affine transformations of the data and to directions with large data spread. Maximum margin solutions may be misled by the spread of data and preferentially separate classes along large spread directions. This article corrects these weaknesses by measuring margin not in the absolute sense but rather only relative to the spread of data in any projection direction. Maximum relative margin corresponds to a data-dependent regularization on the classification function while maximum absolute margin corresponds to an ℓ 2 norm constraint on the classification function. Interestingly, the proposed improvements only require simple extensions to existing maximum margin formulations and preserve the computational efficiency of SVMs. Through the maximization of relative margin, surprising performance gains are achieved on real-world problems such as digit, text classification and on several other benchmark data sets. In addition, risk bounds are derived for the new formulation based on Rademacher averages."
            ],
            "keywords": [
                "support vector machines",
                "kernel methods",
                "large margin",
                "Rademacher complexity"
            ],
            "author": [
                "Pannagadatta K Shivaswamy",
                "Tony Jebara"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/shivaswamy10a/shivaswamy10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices",
            "abstract": [
                "Given a large matrix A ∈ R n×d , we consider the problem of computing a sketch matrix B ∈ R ×d which is significantly smaller than but still well approximates A. We consider the problems in the streaming model, where the algorithm can only make one pass over the input with limited working space, and we are interested in minimizing the covariance error A T A − B T B 2. The popular Frequent Directions algorithm of Liberty (2013) and its variants achieve optimal space-error tradeoffs. However, whether the running time can be improved remains an unanswered question. In this paper, we almost settle the question by proving that the time complexity of this problem is equivalent to that of matrix multiplication up to lower order terms. Specifically, we provide new space-optimal algorithms with faster running times and also show that the running times of our algorithms can be improved if and only if the state-of-the-art running time of matrix multiplication can be improved significantly."
            ],
            "keywords": [
                "Matrix Sketching",
                "Frequent Directions",
                "Streaming Algorithms"
            ],
            "author": [
                "Zengfeng Huang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-875/18-875.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling Techniques",
            "abstract": [
                "In a series of recent works, we have generalised the consistency results in the stochastic block model literature to the case of uniform and non-uniform hypergraphs. The present paper continues the same line of study, where we focus on partitioning weighted uniform hypergraphs-a problem often encountered in computer vision. This work is motivated by two issues that arise when a hypergraph partitioning approach is used to tackle computer vision problems: (i) The uniform hypergraphs constructed for higher-order learning contain all edges, but most have negligible weights. Thus, the adjacency tensor is nearly sparse, and yet, not binary. (ii) A more serious concern is that standard partitioning algorithms need to compute all edge weights, which is computationally expensive for hypergraphs. This is usually resolved in practice by merging the clustering algorithm with a tensor sampling strategy-an approach that is yet to be analysed rigorously. We build on our earlier work on partitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati, ICML, 2015), and address the aforementioned issues by proposing provable and efficient partitioning algorithms. Our analysis justifies the empirical success of practical sampling techniques. We also complement our theoretical findings by elaborate empirical comparison of various hypergraph partitioning schemes."
            ],
            "keywords": [
                "Hypergraph partitioning",
                "planted model",
                "spectral method",
                "tensors",
                "sampling",
                "subspace clustering"
            ],
            "author": [
                "Debarghya Ghoshdastidar",
                "Ambedkar Dukkipati"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-100/16-100.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nearly Uniform Validation Improves Compression-Based Error Bounds",
            "abstract": [
                "This paper develops bounds on out-of-sample error rates for support vector machines (SVMs). The bounds are based on the numbers of support vectors in the SVMs rather than on VC dimension. The bounds developed here improve on support vector counting bounds derived using Littlestone and Warmuth's compression-based bounding technique."
            ],
            "keywords": [
                "compression",
                "error bound",
                "support vector machine",
                "nearly uniform"
            ],
            "author": [
                "Eric Bax"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/bax08b/bax08b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Clustering Algorithms for Chains",
            "abstract": [
                "We consider the problem of clustering a set of chains to k clusters. A chain is a totally ordered subset of a finite set of items. Chains are an intuitive way to express preferences over a set of alternatives, as well as a useful representation of ratings in situations where the item-specific scores are either difficult to obtain, too noisy due to measurement error, or simply not as relevant as the order that they induce over the items. First we adapt the classical k-means for chains by proposing a suitable distance function and a centroid structure. We also present two different approaches for mapping chains to a vector space. The first one is related to the planted partition model, while the second one has an intuitive geometrical interpretation. Finally we discuss a randomization test for assessing the significance of a clustering. To this end we present an MCMC algorithm for sampling random sets of chains that share certain properties with the original data. The methods are studied in a series of experiments using real and artificial data. Results indicate that the methods produce interesting clusterings, and for certain types of inputs improve upon previous work on clustering algorithms for orders."
            ],
            "keywords": [
                "Lloyd's algorithm",
                "orders",
                "preference statements",
                "planted partition model",
                "randomization testing"
            ],
            "author": [
                "Antti Ukkonen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/ukkonen11a/ukkonen11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Optimal Algorithms for Continuous Non-monotone Submodular and DR-Submodular Maximization",
            "abstract": [
                "In this paper we study the fundamental problems of maximizing a continuous nonmonotone submodular function over the hypercube, both with and without coordinate-wise concavity. This family of optimization problems has several applications in machine learning, economics, and communication systems. Our main result is the first 1-approximation algorithm for continuous submodular function maximization; this approximation factor of 1 is the best possible for algorithms that only query the objective function at polynomially many points. For the special case of DR-submodular maximization, i.e. when the submodular function is also coordinate-wise concave along all coordinates, we provide a different 1-approximation algorithm that runs in quasi-linear time. Both these results improve upon prior work (Bian et al., 2017a,b; Soma and Yoshida, 2017). Our first algorithm uses novel ideas such as reducing the guaranteed approximation problem to analyzing a zero-sum game for each coordinate, and incorporates the geometry of this zero-sum game to fix the value at this coordinate. Our second algorithm exploits coordinate-wise concavity to identify a monotone equilibrium condition sufficient for getting the required approximation guarantee, and hunts for the equilibrium point using binary search. We further run experiments to verify the performance of our proposed algorithms in related machine learning applications."
            ],
            "keywords": [
                "Continuous submodularity",
                "non-monotone submodular maximization",
                "approximation algorithms"
            ],
            "author": [
                "Rad Niazadeh",
                "Tim Roughgarden",
                "Joshua R Wang",
                "Prof Andreas Krause"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-527/18-527.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Online and Batch Learning Using Forward Backward Splitting",
            "abstract": [
                "We describe, analyze, and experiment with a framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we first perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the first phase. This view yields a simple yet effective algorithm that can be used for batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as ℓ 1. We derive concrete and very simple algorithms for minimization of loss functions with ℓ 1 , ℓ 2 , ℓ 2 2 , and ℓ ∞ regularization. We also show how to construct efficient algorithms for mixed-norm ℓ 1 /ℓ q regularization. We further extend the algorithms and give efficient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in a series of experiments with synthetic and natural data sets."
            ],
            "keywords": [
                "subgradient methods",
                "group sparsity",
                "online learning",
                "convex optimization"
            ],
            "author": [
                "John Duchi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/duchi09a/duchi09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Power Method for Sparse Principal Component Analysis",
            "abstract": [
                "In this paper we develop a new approach to sparse principal component analysis (sparse PCA). We propose two single-unit and two block optimization formulations of the sparse PCA problem, aimed at extracting a single sparse dominant principal component of a data matrix, or more components at once, respectively. While the initial formulations involve nonconvex functions, and are therefore computationally intractable, we rewrite them into the form of an optimization program involving maximization of a convex function on a compact set. The dimension of the search space is decreased enormously if the data matrix has many more columns (variables) than rows. We then propose and analyze a simple gradient method suited for the task. It appears that our algorithm has best convergence properties in the case when either the objective function or the feasible set are strongly convex, which is the case with our single-unit formulations and can be enforced in the block case. Finally, we demonstrate numerically on a set of random and gene expression test problems that our approach outperforms existing algorithms both in quality of the obtained solution and in computational speed."
            ],
            "keywords": [
                "sparse PCA",
                "power method",
                "gradient ascent",
                "strongly convex sets",
                "block algorithms"
            ],
            "author": [
                "Michel Journée",
                "Peter Richtárik",
                "Rodolphe Sepulchre"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/journee10a/journee10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimating the \"Wrong\" Graphical Model: Benefits in the Computation-Limited Setting",
            "abstract": [
                "Consider the problem of joint parameter estimation and prediction in a Markov random field: that is, the model parameters are estimated on the basis of an initial set of data, and then the fitted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for fitting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the \"wrong\" model even in the infinite data limit) is provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious benefit of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product."
            ],
            "keywords": [
                "graphical model",
                "Markov random field",
                "belief propagation",
                "variational method",
                "parameter estimation",
                "prediction error",
                "algorithmic stability"
            ],
            "author": [
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/wainwright06a/wainwright06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Optimization for Policy Search via Online-Offline Experimentation",
            "abstract": [
                "Online field experiments are the gold-standard way of evaluating changes to real-world interactive machine learning systems. Yet our ability to explore complex, multi-dimensional policy spaces-such as those found in recommendation and ranking problems-is often constrained by the limited number of experiments that can be run simultaneously. To alleviate these constraints, we augment online experiments with an offline simulator and apply multi-task Bayesian optimization to tune live machine learning systems. We describe practical issues that arise in these types of applications, including biases that arise from using a simulator and assumptions for the multi-task kernel. We measure empirical learning curves which show substantial gains from including data from biased offline experiments, and show how these learning curves are consistent with theoretical results for multi-task Gaussian process generalization. We find that improved kernel inference is a significant driver of multi-task generalization. Finally, we show several examples of Bayesian optimization efficiently tuning a live machine learning system by combining offline and online experiments."
            ],
            "keywords": [
                "Bayesian Optimization Special Issue Bayesian optimization",
                "multi-task Gaussian process",
                "policy search",
                "A/B testing",
                "multi-fidelity optimization"
            ],
            "author": [
                "Benjamin Letham"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-225/18-225.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency of Multiclass Empirical Risk Minimization Methods Based on Convex Loss",
            "abstract": [
                "The consistency of classification algorithm plays a central role in statistical learning theory. A consistent algorithm guarantees us that taking more samples essentially suffices to roughly reconstruct the unknown distribution. We consider the consistency of ERM scheme over classes of combinations of very simple rules (base classifiers) in multiclass classification. Our approach is, under some mild conditions, to establish a quantitative relationship between classification errors and convex risks. In comparison with the related previous work, the feature of our result is that the conditions are mainly expressed in terms of the differences between some values of the convex function."
            ],
            "keywords": [
                "multiclass classification",
                "classifier",
                "consistency",
                "empirical risk minimization",
                "constrained comparison method",
                "Tsybakov noise condition"
            ],
            "author": [
                "Di-Rong Chen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/chen06a/chen06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convex and Non-Convex Approaches for Statistical Inference with Class-Conditional Noisy Labels",
            "abstract": [
                "We study the problem of estimation and testing in logistic regression with class-conditional noise in the observed labels, which has an important implication in the Positive-Unlabeled (PU) learning setting. With the key observation that the label noise problem belongs to a special sub-class of generalized linear models (GLM), we discuss convex and non-convex approaches that address this problem. A non-convex approach based on the maximum likelihood estimation produces an estimator with several optimal properties, but a convex approach has an obvious advantage in optimization. We demonstrate that in the lowdimensional setting, both estimators are consistent and asymptotically normal, where the asymptotic variance of the non-convex estimator is smaller than the convex counterpart. We also quantify the efficiency gap which provides insight into when the two methods are comparable. In the high-dimensional setting, we show that both estimation procedures achieve 2-consistency at the minimax optimal s log p/n rates under mild conditions. Finally, we propose an inference procedure using a de-biasing approach. We validate our theoretical findings through simulations and a real-data example."
            ],
            "keywords": [
                "generalized linear model",
                "non-convexity",
                "class-conditional label noise",
                "PUlearning",
                "regularization"
            ],
            "author": [
                "Hyebin Song",
                "Ran Dai",
                "Garvesh Raskutti",
                "Rina Foygel Barber"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-833/19-833.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Classification Framework for Anomaly Detection",
            "abstract": [
                "One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of finding level sets for the data generating density. We interpret this learning problem as a binary classification problem and compare the corresponding classification risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classification risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justification for the well-known heuristic of artificially sampling \"labeled\" samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM."
            ],
            "keywords": [
                "unsupervised learning",
                "anomaly detection",
                "density levels",
                "classification",
                "SVMs"
            ],
            "author": [
                "Ingo Steinwart",
                "Clint Scovel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/steinwart05a/steinwart05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online Learning to Rank with Top-k Feedback",
            "abstract": [
                "We consider two settings of online learning to rank where feedback is restricted to top ranked items. The problem is cast as an online game between a learner and sequence of users, over T rounds. In both settings, the learners objective is to present ranked list of items to the users. The learner's performance is judged on the entire ranked list and true relevances of the items. However, the learner receives highly restricted feedback at end of each round, in form of relevances of only the top k ranked items, where k m. The first setting is non-contextual, where the list of items to be ranked is fixed. The second setting is contextual, where lists of items vary, in form of traditional query-document lists. No stochastic assumption is made on the generation process of relevances of items and contexts. We provide efficient ranking strategies for both the settings. The strategies achieve O(T 2/3) regret, where regret is based on popular ranking measures in first setting and ranking surrogates in second setting. We also provide impossibility results for certain ranking measures and a certain class of surrogates, when feedback is restricted to the top ranked item, i.e. k = 1. We empirically demonstrate the performance of our algorithms on simulated and real world data sets."
            ],
            "keywords": [
                "Learning to Rank",
                "Online Learning",
                "Partial Monitoring",
                "Online Bandits",
                "Learning Theory"
            ],
            "author": [
                "Sougata Chaudhuri"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-285/16-285.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Provably Efficient Learning with Typed Parametric Models",
            "abstract": [
                "To quickly achieve good performance, reinforcement-learning algorithms for acting in large continuous-valued domains must use a representation that is both sufficiently powerful to capture important domain characteristics, and yet simultaneously allows generalization, or sharing, among experiences. Our algorithm balances this tradeoff by using a stochastic, switching, parametric dynamics representation. We argue that this model characterizes a number of significant, real-world domains, such as robot navigation across varying terrain. We prove that this representational assumption allows our algorithm to be probably approximately correct with a sample complexity that scales polynomially with all problem-specific quantities including the state-space dimension. We also explicitly incorporate the error introduced by approximate planning in our sample complexity bounds, in contrast to prior Probably Approximately Correct (PAC) Markov Decision Processes (MDP) approaches, which typically assume the estimated MDP can be solved exactly. Our experimental results on constructing plans for driving to work using real car trajectory data, as well as a small robot experiment on navigating varying terrain, demonstrate that our dynamics representation enables us to capture real-world dynamics in a sufficient manner to produce good performance."
            ],
            "keywords": [],
            "author": [
                "Emma Brunskill",
                "Bethany R Leffler",
                "Lihong Li",
                "Michael L Littman",
                "Nicholas Roy",
                "Mit Edu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/brunskill09a/brunskill09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Coupled Clustering: A Method for Detecting Structural Correspondence",
            "abstract": [
                "This paper proposes a new paradigm and a computational framework for revealing equivalencies (analogies) between sub-structures of distinct composite systems that are initially represented by unstructured data sets. For this purpose, we introduce and investigate a variant of traditional data clustering, termed coupled clustering, which outputs a configuration of corresponding subsets of two such representative sets. We apply our method to synthetic as well as textual data. Its achievements in detecting topical correspondences between textual corpora are evaluated through comparison to performance of human experts."
            ],
            "keywords": [
                "Unsupervised learning",
                "Clustering",
                "Structure mapping",
                "Data mining in texts",
                "Natural language processing"
            ],
            "author": [
                "Zvika ©2002",
                "Ido Marx",
                "Joachim M Dagan",
                "Eli Buhmann",
                "Zvika Marx",
                "Joachim M Buhmann",
                "Eli Shamir",
                "Carla E Brodley",
                "Andrea Danyluk"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/marx02a/marx02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "AdaBoost is Consistent",
            "abstract": [
                "The risk, or probability of error, of the classifier produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after n 1−ε iterations-for sample size n and ε ∈ (0, 1)-the sequence of risks of the classifiers it produces approaches the Bayes risk."
            ],
            "keywords": [
                "boosting",
                "adaboost",
                "consistency"
            ],
            "author": [
                "Peter L Bartlett",
                "Mikhail Traskin",
                "Mtraskin @ Berkeley Stat"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/bartlett07b/bartlett07b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Nonparametric Parallelism Test",
            "abstract": [
                "Testing the hypothesis of parallelism is a fundamental statistical problem arising from many applied sciences. In this paper, we develop a nonparametric parallelism test for inferring whether the trends are parallel in treatment and control groups. In particular, the proposed nonparametric parallelism test is a Wald type test based on a smoothing spline ANOVA (SSANOVA) model which can characterize the complex patterns of the data. We derive that the asymptotic null distribution of the test statistic is a Chi-square distribution, unveiling a new version of Wilks phenomenon. Notably, we establish the minimax sharp lower bound of the distinguishable rate for the nonparametric parallelism test by using the information theory, and further prove that the proposed test is minimax optimal. Simulation studies are conducted to investigate the empirical performance of the proposed test. DNA methylation and neuroimaging studies are presented to illustrate potential applications of the test."
            ],
            "keywords": [
                "asymptotic distribution",
                "minimax optimality",
                "nonparametric inference",
                "parallelism test",
                "penalized least squares",
                "smoothing spline ANOVA",
                "Wald test"
            ],
            "author": [
                "Xin Xing",
                "Meimei Liu",
                "Ping Ma",
                "Wenxuan Zhong"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-800/19-800.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies",
            "abstract": [
                "We present methods able to predict the presence and strength of conditional and unconditional dependencies (correlations) between two variables Y and Z never jointly measured on the same samples, based on multiple data sets measuring a set of common variables. The algorithms are specializations of prior work on learning causal structures from overlapping variable sets. This problem has also been addressed in the field of statistical matching. The proposed methods are applied to a wide range of domains and are shown to accurately predict the presence of thousands of dependencies. Compared against prototypical statistical matching algorithms and within the scope of our experiments, the proposed algorithms make predictions that are better correlated with the sample estimates of the unknown parameters on test data ; this is particularly the case when the number of commonly measured variables is low. The enabling idea behind the methods is to induce one or all causal models that are simultaneously consistent with (fit) all available data sets and prior knowledge and reason with them. This allows constraints stemming from causal assumptions (e.g., Causal Markov Condition, Faithfulness) to propagate. Several methods have been developed based on this idea, for which we propose the unifying name Integrative Causal Analysis (INCA). A contrived example is presented demonstrating the theoretical potential to develop more general methods for co-analyzing heterogeneous data sets. The computational experiments with the novel methods provide evidence that causallyinspired assumptions such as Faithfulness often hold to a good degree of approximation in many real systems and could be exploited for statistical inference. Code, scripts, and data are available at www.mensxmachina.org."
            ],
            "keywords": [
                "integrative causal analysis",
                "causal discovery",
                "Bayesian networks",
                "maximal ancestral graphs",
                "structural equation models",
                "causality",
                "statistical matching",
                "data fusion"
            ],
            "author": [
                "Ioannis Tsamardinos",
                "Sofia Triantafillou",
                "Vincenzo Lagani"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/tsamardinos12a/tsamardinos12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers",
            "abstract": [
                "We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm. The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms."
            ],
            "keywords": [],
            "author": [
                "Erin L Allwein",
                "Robert E Schapire"
            ],
            "ref": "http://www.jmlr.org/papers/volume1/allwein00a/allwein00a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Computation of Gapped Substring Kernels on Large Alphabets",
            "abstract": [
                "We present a sparse dynamic programming algorithm that, given two strings s and t, a gap penalty λ, and an integer p, computes the value of the gap-weighted length-p subsequences kernel. The algorithm works in time O(p|M| log |t|), where M = {(i, j)|s i = t j } is the set of matches of characters in the two sequences. The algorithm is easily adapted to handle bounded length subsequences and different gap-penalty schemes, including penalizing by the total length of gaps and the number of gaps as well as incorporating character-specific match/gap penalties. The new algorithm is empirically evaluated against a full dynamic programming approach and a trie-based algorithm both on synthetic and newswire article data. Based on the experiments, the full dynamic programming approach is the fastest on short strings, and on long strings if the alphabet is small. On large alphabets, the new sparse dynamic programming algorithm is the most efficient. On medium-sized alphabets the trie-based approach is best if the maximum number of allowed gaps is strongly restricted."
            ],
            "keywords": [
                "kernel methods",
                "string kernels",
                "text categorization",
                "sparse dynamic programming"
            ],
            "author": [
                "Juho Rousu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/rousu05a/rousu05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines",
            "abstract": [
                "Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classification and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while fixing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efficient and stable than state of the art methods such as Pegasos and TRON."
            ],
            "keywords": [
                "linear support vector machines",
                "document classification",
                "coordinate descent"
            ],
            "author": [
                "Kai-Wei Chang",
                "Cho-Jui Hsieh",
                "Chih-Jen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/chang08a/chang08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion",
            "abstract": [
                "Constraint-based learning of Bayesian networks (BN) from limited data can lead to multiple testing problems when recovering dense areas of the skeleton and to conflicting results in the orientation of edges. In this paper, we present a new constraint-based algorithm, light mutual min (LMM) for improved accuracy of BN learning from small sample data. LMM improves the assessment of candidate edges by using a ranking criterion that considers conditional independence on neighboring variables at both sides of an edge simultaneously. The algorithm also employs an adaptive relaxation of constraints that, selectively, allows some nodes not to condition on some neighbors. This relaxation aims at reducing the incorrect rejection of true edges connecting high degree nodes due to multiple testing. LMM additionally incorporates a new criterion for ranking v-structures that is used to recover the completed partially directed acyclic graph (CPDAG) and to resolve conflicting v-structures, a common problem in small sample constraint-based learning. Using simulated data, each of these components of LMM is shown to significantly improve network inference compared to commonly applied methods when learning from limited data, including more accurate recovery of skeletons and CPDAGs compared to the PC, MaxMin, and MaxMin hill climbing algorithms. A proof of asymptotic correctness is also provided for LMM for recovering the correct skeleton and CPDAG."
            ],
            "keywords": [
                "Bayesian networks",
                "skeleton",
                "constraint-based learning",
                "mutual min"
            ],
            "author": [
                "Rami Mahdi",
                "Jason Mezey"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/mahdi13a/mahdi13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List",
            "abstract": [
                "We are interested in supervised ranking algorithms that perform especially well near the top of the ranked list, and are only required to perform sufficiently well on the rest of the list. In this work, we provide a general form of convex objective that gives high-scoring examples more importance. This \"push\" near the top of the list can be chosen arbitrarily large or small, based on the preference of the user. We choose ℓ p-norms to provide a specific type of push; if the user sets p larger, the objective concentrates harder on the top of the list. We derive a generalization bound based on the p-norm objective, working around the natural asymmetry of the problem. We then derive a boosting-style algorithm for the problem of ranking with a push at the top. The usefulness of the algorithm is illustrated through experiments on repository data. We prove that the minimizer of the algorithm's objective is unique in a specific sense. Furthermore, we illustrate how our objective is related to quality measurements for information retrieval."
            ],
            "keywords": [
                "ranking",
                "RankBoost",
                "generalization bounds",
                "ROC",
                "information retrieval"
            ],
            "author": [
                "Cynthia Rudin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/rudin09b/rudin09b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectrum Estimation from a Few Entries",
            "abstract": [
                "Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering spectral properties of the underlying matrix from a sampling of its entries. In this paper, we address the problem of directly recovering the spectrum, which is the set of singular values, and also in sample-efficient approaches for recovering a spectral sum function, which is an aggregate sum of a fixed function applied to each of the singular values. Our approach is to first estimate the Schatten k-norms of a matrix for a small set of values of k, and then apply Chebyshev approximation when estimating a spectral sum function or apply moment matching in Wasserstein distance when estimating the singular values directly. The main technical challenge is in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures called network motifs in a graph and provide guarantees that match its empirical performance. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods, below the matrix completion threshold, above which matrix completion algorithms recover the underlying low-rank matrix exactly."
            ],
            "keywords": [
                "spectrum estimation",
                "matrix completion",
                "counting subgraphs"
            ],
            "author": [
                "Ashish Khetan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-027/18-027.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamical Systems as Temporal Feature Spaces",
            "abstract": [
                "Parametrised state space models in the form of recurrent networks are often used in machine learning to learn from data streams exhibiting temporal dependencies. To break the black box nature of such models it is important to understand the dynamical features of the inputdriving time series that are formed in the state space. We propose a framework for rigorous analysis of such state representations in vanishing memory state space models such as echo state networks (ESN). In particular, we consider the state space a temporal feature space and the readout mapping from the state space a kernel machine operating in that feature space. We show that: (1) The usual ESN strategy of randomly generating input-to-state, as well as state coupling leads to shallow memory time series representations, corresponding to cross-correlation operator with fast exponentially decaying coefficients; (2) Imposing symmetry on dynamic coupling yields a constrained dynamic kernel matching the input time series with straightforward exponentially decaying motifs or exponentially decaying motifs of the highest frequency; (3) Simple ring (cycle) high-dimensional reservoir topology specified only through two free parameters can implement deep memory dynamic kernels with a rich variety of matching motifs. We quantify richness of feature representations imposed by dynamic kernels and demonstrate that for dynamic kernel associated with cycle reservoir topology, the kernel richness undergoes a phase transition close to the edge of stability."
            ],
            "keywords": [
                "Recurrent Neural Network",
                "Echo State Network",
                "Dynamical Systems",
                "Time Series",
                "Kernel Machines"
            ],
            "author": [
                "Peter Tino"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-589/19-589.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gaussian Processes with Linear Operator Inequality Constraints",
            "abstract": [
                "This paper presents an approach for constrained Gaussian Process (GP) regression where we assume that a set of linear transformations of the process are bounded. It is motivated by machine learning applications for high-consequence engineering systems, where this kind of information is often made available from phenomenological knowledge. We consider a GP f over functions on X ⊂ R n taking values in R, where the process Lf is still Gaussian when L is a linear operator. Our goal is to model f under the constraint that realizations of Lf are confined to a convex set of functions. In particular, we require that a ≤ Lf ≤ b, given two functions a and b where a < b pointwise. This formulation provides a consistent way of encoding multiple linear constraints, such as shape-constraints based on e.g. boundedness, monotonicity or convexity. We adopt the approach of using a sufficiently dense set of virtual observation locations where the constraint is required to hold, and derive the exact posterior for a conjugate likelihood. The results needed for stable numerical implementation are derived, together with an efficient sampling scheme for estimating the posterior process."
            ],
            "keywords": [
                "Gaussian processes",
                "Linear constraints",
                "Virtual observations",
                "Uncertainty Quantification",
                "Computer code emulation"
            ],
            "author": [
                "Christian Agrell"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-065/19-065.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "KeLP: a Kernel-based Learning Platform",
            "abstract": [
                "KeLP is a Java framework that enables fast and easy implementation of kernel functions over discrete data, such as strings, trees or graphs and their combination with standard vectorial kernels. Additionally, it provides several kernel-based algorithms, e.g., online and batch kernel machines for classification, regression and clustering, and a Java environment for easy implementation of new algorithms. KeLP is a versatile toolkit, very appealing both to experts and practitioners of machine learning and Java language programming, who can find extensive documentation, tutorials and examples of increasing complexity on the accompanying website. Interestingly, KeLP can be also used without any knowledge of Java programming through command line tools and JSON/XML interfaces enabling the declaration and instantiation of articulated learning models using simple templates. Finally, the extensive use of modularity and interfaces in KeLP enables developers to easily extend it with their own kernels and algorithms."
            ],
            "keywords": [
                "Kernel Machines",
                "Structured Data and Kernels",
                "Java Framework"
            ],
            "author": [
                "Simone Filice",
                "Giuseppe Castellucci",
                "Giovanni Da",
                "San Martino",
                "Alessandro Moschitti",
                "Danilo Croce",
                "Roberto Basili"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-087/16-087.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dynamics and Generalization Ability of LVQ Algorithms",
            "abstract": [
                "Learning vector quantization (LVQ) schemes constitute intuitive, powerful classification heuristics with numerous successful applications but, so far, limited theoretical background. We study LVQ rigorously within a simplifying model situation: two competing prototypes are trained from a sequence of examples drawn from a mixture of Gaussians. Concepts from statistical physics and the theory of on-line learning allow for an exact description of the training dynamics in highdimensional feature space. The analysis yields typical learning curves, convergence properties, and achievable generalization abilities. This is also possible for heuristic training schemes which do not relate to a cost function. We compare the performance of several algorithms, including Kohonen's LVQ1 and LVQ+/-, a limiting case of LVQ2.1. The former shows close to optimal performance, while LVQ+/-displays divergent behavior. We investigate how early stopping can overcome this difficulty. Furthermore, we study a crisp version of robust soft LVQ, which was recently derived from a statistical formulation. Surprisingly, it exhibits relatively poor generalization. Performance improves if a window for the selection of data is introduced; the resulting algorithm corresponds to cost function based LVQ2. The dependence of these results on the model parameters, for example, prior class probabilities, is investigated systematically, simulations confirm our analytical findings."
            ],
            "keywords": [
                "prototype based classification",
                "learning vector quantization",
                "Winner-Takes-All algorithms",
                "on-line learning",
                "competitive learning"
            ],
            "author": [
                "Michael Biehl",
                "Barbara Hammer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/biehl07a/biehl07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Mixability is Bayes Risk Curvature Relative to Log Loss Tim van Erven",
            "abstract": [
                "Mixability of a loss characterizes fast rates in the online learning setting of prediction with expert advice. The determination of the mixability constant for binary losses is straightforward but opaque. In the binary case we make this transparent and simpler by characterising mixability in terms of the second derivative of the Bayes risk of proper losses. We then extend this result to multiclass proper losses where there are few existing results. We show that mixability is governed by the maximum eigenvalue of the Hessian of the Bayes risk, relative to the Hessian of the Bayes risk for log loss. We conclude by comparing our result to other work that bounds prediction performance in terms of the geometry of the Bayes risk. Although all calculations are for proper losses, we also show how to carry the results across to improper losses."
            ],
            "keywords": [
                "mixability",
                "multiclass",
                "prediction with expert advice",
                "proper loss",
                "learning rates"
            ],
            "author": [
                "Tim @ Timvanerven",
                "Mark D Reid",
                "Robert C Williamson",
                "Mark D Erven",
                "Robert C Reid"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/vanerven12a/vanerven12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Fast Bayesian Inference of Sparse Networks with Automatic Sparsity Determination",
            "abstract": [
                "Structure learning of Gaussian graphical models typically involves careful tuning of penalty parameters, which balance the tradeoff between data fidelity and graph sparsity. Unfortunately, this tuning is often a \"black art\" requiring expert experience or brute-force search. It is therefore tempting to develop tuning-free algorithms that can determine the sparsity of the graph adaptively from the observed data in an automatic fashion. In this paper, we propose a novel approach, named BISN (Bayesian inference of Sparse Networks), for automatic Gaussian graphical model selection. Specifically, we regard the off-diagonal entries in the precision matrix as random variables and impose sparse-promoting horseshoe priors on them, resulting in automatic sparsity determination. With the help of stochastic gradients, an efficient variational Bayes algorithm is derived to learn the model. We further propose a decaying recursive stochastic gradient (DRSG) method to reduce the variance of the stochastic gradients and to accelerate the convergence. Our theoretical analysis shows that the time complexity of BISN scales only quadratically with the dimension, whereas the theoretical time complexity of the state-of-the-art methods for automatic graphical model selection is typically a third-order function of the dimension. Furthermore, numerical results show that BISN can achieve comparable or better performance than the state-of-the-art methods in terms of structure recovery, and yet its computational time is several orders of magnitude shorter, especially for large dimensions."
            ],
            "keywords": [
                "Gaussian Graphical Models",
                "Structure Learning",
                "Tuning Free",
                "Time Complexity",
                "Variational Bayes",
                "Variance Reduction",
                "Decaying Recursive Stochastic Gradient (DRSG)"
            ],
            "author": [
                "Hang Yu",
                "Songwei Wu",
                "Luyin Xin",
                "Justin Dauwels"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-514/18-514.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Nearest-Neighbor Error-Correcting Output Codes with Application to All-Pairs Multiclass Support Vector Machines",
            "abstract": [
                "A common way of constructing a multiclass classifier is by combining the outputs of several binary ones, according to an error-correcting output code (ECOC) scheme. The combination is typically done via a simple nearest-neighbor rule that finds the class that is closest in some sense to the outputs of the binary classifiers. For these nearest-neighbor ECOCs, we improve existing bounds on the error rate of the multiclass classifier given the average binary distance. The new bounds provide insight into the one-versus-rest and all-pairs matrices, which are compared through experiments with standard datasets. The results also show why elimination (also known as DAGSVM) and Hamming decoding often achieve the same accuracy."
            ],
            "keywords": [
                "Error-correcting output codes",
                "all-pairs ECOC matrix",
                "multiclass support vector machines"
            ],
            "author": [
                "Aldebaro Klautau",
                "Nikola Jevtić"
            ],
            "ref": "http://www.jmlr.org/papers/volume4/klautau03a/klautau03a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Sparse Representations by Non-Negative Matrix Factorization and Sequential Cone Programming",
            "abstract": [
                "We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing stateof-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P. O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems."
            ],
            "keywords": [
                "non-negative matrix factorization",
                "second-order cone programming",
                "sequential convex optimization",
                "reverse-convex programming",
                "sparsity"
            ],
            "author": [
                "Matthias Heiler",
                "Christoph Schnörr",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/heiler06a/heiler06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Kalai-Smorodinsky solution for many-objective Bayesian optimization",
            "abstract": [
                "An ongoing aim of research in multiobjective Bayesian optimization is to extend its applicability to a large number of objectives. While coping with a limited budget of evaluations, recovering the set of optimal compromise solutions generally requires numerous observations and is less interpretable since this set tends to grow larger with the number of objectives. We thus propose to focus on a specific solution originating from game theory, the Kalai-Smorodinsky solution, which possesses attractive properties. In particular, it ensures equal marginal gains over all objectives. We further make it insensitive to a monotonic transformation of the objectives by considering the objectives in the copula space. A novel tailored algorithm is proposed to search for the solution, in the form of a Bayesian optimization algorithm: sequential sampling decisions are made based on acquisition functions that derive from an instrumental Gaussian process prior. Our approach is tested on four problems with respectively four, six, eight, and nine objectives. The method is available in the R package GPGame available on CRAN at https://cran.r-project.org/ package=GPGame."
            ],
            "keywords": [
                "Bayesian Optimization Special Issue Gaussian process",
                "Game theory",
                "Stepwise uncertainty reduction"
            ],
            "author": [
                "M Binois",
                "V Picheny",
                "A Habbal"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-212/18-212.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Evolving GPU Machine Code",
            "abstract": [
                "Parallel Graphics Processing Unit (GPU) implementations of GP have appeared in the literature using three main methodologies: (i) compilation, which generates the individuals in GPU code and requires compilation; (ii) pseudo-assembly, which generates the individuals in an intermediary assembly code and also requires compilation; and (iii) interpretation, which interprets the codes. This paper proposes a new methodology that uses the concepts of quantum computing and directly handles the GPU machine code instructions. Our methodology utilizes a probabilistic representation of an individual to improve the global search capability. In addition, the evolution in machine code eliminates both the overhead of compiling the code and the cost of parsing the program during evaluation. We obtained up to 2.74 trillion GP operations per second for the 20-bit Boolean Multiplexer benchmark. We also compared our approach with the other three GPU-based acceleration methodologies implemented for quantum-inspired linear GP. Significant gains in performance were obtained."
            ],
            "keywords": [
                "genetic programming",
                "graphics processing units",
                "machine code"
            ],
            "author": [
                "Cleomar Pereira Da Silva",
                "Douglas Mota Dias",
                "Cristiana Bentes",
                "Marco Aurélio",
                "Cavalcanti Pacheco",
                "Leandro Fontoura Cupertino",
                "Dias, Bentes, Pacheco Cupertino P Silva",
                "M A C Pacheco",
                "L F Cupertino"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/dasilva15a/dasilva15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Unbiased Generative Semi-Supervised Learning",
            "abstract": [
                "Reliable semi-supervised learning, where a small amount of labelled data is complemented by a large body of unlabelled data, has been a long-standing goal of the machine learning community. However, while it seems intuitively obvious that unlabelled data can aid the learning process, in practise its performance has often been disappointing. We investigate this by examining generative maximum likelihood semi-supervised learning and derive novel upper and lower bounds on the degree of bias introduced by the unlabelled data. These bounds improve upon those provided in previous work, and are specifically applicable to the challenging case where the model is unable to exactly fit to the underlying distribution a situation which is common in practise, but for which fewer guarantees of semi-supervised performance have been found. Inspired by this new framework for analysing bounds, we propose a new, simple reweighing scheme which provides a provably unbiased estimator for arbitrary model/distribution pairs-an unusual property for a semi-supervised algorithm. This reweighing introduces no additional computational complexity and can be applied to very many models. Additionally, we provide specific conditions demonstrating the circumstance under which the unlabelled data will lower the estimator variance, thereby improving convergence."
            ],
            "keywords": [
                "Kullback-Leibler",
                "semi-supervised",
                "asymptotic bounds",
                "bias",
                "generative model"
            ],
            "author": [
                "Patrick Fox-Roberts",
                "Edward Rosten"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/foxroberts14a/foxroberts14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Alexey Chervonenkis's Bibliography: Introductory Comments",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/gammerman15b/gammerman15b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Near-Optimal Algorithm for Differentially-Private Principal Components *",
            "abstract": [
                "The principal components analysis (PCA) algorithm is a standard tool for identifying good lowdimensional approximations to high-dimensional data. Many data sets of interest contain private or sensitive information about individuals. Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs. Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs. In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output. We show that the sample complexity of the proposed method differs from the existing procedure in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling. We furthermore illustrate our results, showing that on real data there is a large performance gap between the existing method and our method."
            ],
            "keywords": [
                "differential privacy",
                "principal components analysis",
                "dimension reduction"
            ],
            "author": [
                "Kamalika Chaudhuri",
                "Anand D Sarwate",
                "Kaushik Sinha"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/chaudhuri13a/chaudhuri13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Batch Learning from Logged Bandit Feedback through Counterfactual Risk Minimization",
            "abstract": [
                "We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem (Bottou et al., 2013) through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. In analogy to the Structural Risk Minimization principle of Wapnik and Tscherwonenkis (1979), these constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method-called Policy Optimizer for Exponential Models (POEM)-for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. The effectiveness and efficiency of POEM is evaluated on several simulated multi-label classification problems, as well as on a real-world information retrieval problem. The empirical results show that the CRM objective implemented in POEM provides improved robustness and generalization performance compared to the state-of-the-art."
            ],
            "keywords": [
                "empirical risk minimization",
                "bandit feedback",
                "importance sampling",
                "propensity score matching",
                "structured prediction"
            ],
            "author": [
                "Adith Swaminathan",
                "Thorsten Joachims",
                "Alex Gammerman",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/swaminathan15a/swaminathan15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Policy Gradient and Actor-Critic Algorithms",
            "abstract": [
                "Policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate. Many conventional policy gradient methods use Monte-Carlo techniques to estimate this gradient. The policy is improved by adjusting the parameters in the direction of the gradient estimate. Since Monte-Carlo methods tend to have high variance, a large number of samples is required to attain accurate estimates, resulting in slow convergence. In this paper, we first propose a Bayesian framework for policy gradient, based on modeling the policy gradient as a Gaussian process. This reduces the number of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates, namely, the gradient covariance, are provided at little extra cost. Since the proposed Bayesian framework considers system trajectories as its basic observable unit, it does not require the dynamics within trajectories to be of any particular form, and thus, can be easily extended to partially observable problems. On the downside, it cannot take advantage of the Markov property when the system is Markovian. To address this issue, we proceed to supplement our Bayesian policy gradient framework with a new actor-critic learning model in which a Bayesian class of non-parametric critics, based on Gaussian process temporal difference learning, is used. Such critics model the action-value function as a Gaussian process, allowing Bayes' rule to be used in computing the posterior distribution over action-value functions, conditioned on the observed data. Appropriate choices of the policy parameterization and of the prior covariance (kernel) between action-values allow us to obtain closed-form expressions for the posterior distribution of the gradient of the expected return with respect to the policy parameters. We perform detailed experimental comparisons of the proposed Bayesian policy gradient and actor-critic algorithms with classic Monte-Carlo based policy gradient methods, as well as with each other, on a number of reinforcement learning problems."
            ],
            "keywords": [
                "reinforcement learning",
                "policy gradient methods",
                "actor-critic algorithms",
                "Bayesian inference",
                "Gaussian processes"
            ],
            "author": [
                "Mohammad Ghavamzadeh",
                "Yaakov Engel",
                "Michal Valko",
                "* Mohammad Ghavamzadeh"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/10-245/10-245.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Diffeomorphic Learning",
            "abstract": [
                "We introduce in this paper a learning paradigm in which training data is transformed by a diffeomorphic transformation before prediction. The learning algorithm minimizes a cost function evaluating the prediction error on the training set penalized by the distance between the diffeomorphism and the identity. The approach borrows ideas from shape analysis where diffeomorphisms are estimated for shape and image alignment, and brings them in a previously unexplored setting, estimating, in particular diffeomorphisms in much larger dimensions. After introducing the concept and describing a learning algorithm, we present diverse applications, mostly with synthetic examples, demonstrating the potential of the approach, as well as some insight on how it can be improved."
            ],
            "keywords": [
                "Diffeomorphisms",
                "Reproducing kernel Hilbert Spaces",
                "Classification"
            ],
            "author": [
                "Laurent Younes"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-415/18-415.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Transfer in Reinforcement Learning via Shared Features",
            "abstract": [
                "We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills."
            ],
            "keywords": [
                "reinforcement learning",
                "transfer",
                "shaping",
                "skills"
            ],
            "author": [
                "George Konidaris"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/konidaris12a/konidaris12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stagewise Lasso",
            "abstract": [
                "Many statistical machine learning algorithms minimize either an empirical loss function as in Ad-aBoost, or a penalized empirical loss as in Lasso or SVM. A single regularization tuning parameter controls the trade-off between fidelity to the data and generalizability, or equivalently between bias and variance. When this tuning parameter changes, a regularization \"path\" of solutions to the minimization problem is generated, and the whole path is needed to select a tuning parameter to optimize the prediction or interpretation performance. Algorithms such as homotopy-Lasso or LARS-Lasso and Forward Stagewise Fitting (FSF) (aka e-Boosting) are of great interest because of their resulted sparse models for interpretation in addition to prediction. In this paper, we propose the BLasso algorithm that ties the FSF (e-Boosting) algorithm with the Lasso method that minimizes the L 1 penalized L 2 loss. BLasso is derived as a coordinate descent method with a fixed stepsize applied to the general Lasso loss function (L 1 penalized convex loss). It consists of both a forward step and a backward step. The forward step is similar to e-Boosting or FSF, but the backward step is new and revises the FSF (or e-Boosting) path to approximate the Lasso path. In the cases of a finite number of base learners and a bounded Hessian of the loss function, the BLasso path is shown to converge to the Lasso path when the stepsize goes to zero. For cases with a larger number of base learners than the sample size and when the true model is sparse, our simulations indicate that the BLasso model estimates are sparser than those from FSF with comparable or slightly better prediction performance, and that the the discrete stepsize of BLasso and FSF has an additional regularization effect in terms of prediction and sparsity. Moreover, we introduce the Generalized BLasso algorithm to minimize a general convex loss penalized by a general convex function. Since the (Generalized) BLasso relies only on differences not derivatives, we conclude that it provides a class of simple and easy-to-implement algorithms for tracing the regularization or solution paths of penalized minimization problems."
            ],
            "keywords": [
                "backward step",
                "boosting",
                "convexity",
                "Lasso",
                "regularization path"
            ],
            "author": [
                "Peng Zhao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/zhao07a/zhao07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Algorithmic Stability and Meta-Learning",
            "abstract": [
                "A mechnism of transfer learning is analysed, where samples drawn from different learning tasks of an environment are used to improve the learners performance on a new task. We give a general method to prove generalisation error bounds for such meta-algorithms. The method can be applied to the bias learning model of J. Baxter and to derive novel generalisation bounds for metaalgorithms searching spaces of uniformly stable algorithms. We also present an application to regularized least squares regression."
            ],
            "keywords": [
                "algorithmic stability",
                "meta-learning",
                "learning to learn"
            ],
            "author": [
                "Andreas Maurer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/maurer05a/maurer05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Prediction and Clustering in Signed Networks: A Local to Global Perspective",
            "abstract": [
                "The study of social networks is a burgeoning research area. However, most existing work is on networks that simply encode whether relationships exist or not. In contrast, relationships in signed networks can be positive (\"like\", \"trust\") or negative (\"dislike\", \"distrust\"). The theory of social balance shows that signed networks tend to conform to some local patterns that, in turn, induce certain global characteristics. In this paper, we exploit both local as well as global aspects of social balance theory for two fundamental problems in the analysis of signed networks: sign prediction and clustering. Local patterns of social balance have been used in the past for sign prediction. We define more general measures of social imbalance (MOIs) based on-cycles in the network and give a simple sign prediction rule. Interestingly, by examining measures of social imbalance, we show that the classic Katz measure, which is used widely in unsigned link prediction, also has a balance theoretic interpretation when applied to signed networks. Motivated by the global structure of balanced networks, we propose an effective low rank modeling approach for both sign prediction and clustering. We provide theoretical performance guarantees for our low-rank matrix completion approach via convex relaxations, scale it up to large problem sizes using a matrix factorization based algorithm, and provide extensive experimental validation including comparisons with local approaches. Our experimental results indicate that, by adopting a more global viewpoint of social balance, we get significant performance and computational gains in prediction and clustering tasks on signed networks. Our work therefore highlights the usefulness of the global aspect of balance theory for the analysis of signed networks."
            ],
            "keywords": [
                "signed networks",
                "sign prediction",
                "balance theory",
                "low rank model",
                "matrix completion",
                "graph clustering"
            ],
            "author": [
                "Kai-Yang Chiang",
                "Cho-Jui Hsieh",
                "Nagarajan Natarajan",
                "Inderjit S Dhillon",
                "Ambuj Tewari",
                "Hsieh Chiang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/chiang14a/chiang14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Sparsity and Error Analysis of Empirical Feature-Based Regularization Schemes",
            "abstract": [
                "We consider a learning algorithm generated by a regularization scheme with a concave regularizer for the purpose of achieving sparsity and good learning rates in a least squares regression setting. The regularization is induced for linear combinations of empirical features, constructed in the literatures of kernel principal component analysis and kernel projection machines, based on kernels and samples. In addition to the separability of the involved optimization problem caused by the empirical features, we carry out sparsity and error analysis, giving bounds in the norm of the reproducing kernel Hilbert space, based on a priori conditions which do not require assumptions on sparsity in terms of any basis or system. In particular, we show that as the concave exponent q of the concave regularizer increases to 1, the learning ability of the algorithm improves. Some numerical simulations for both artificial and real MHC-peptide binding data involving the q regularizer and the SCAD penalty are presented to demonstrate the sparsity and error analysis."
            ],
            "keywords": [
                "Sparsity",
                "concave regularizer",
                "reproducing kernel Hilbert space",
                "regularization with empirical features",
                "q -penalty",
                "SCAD penalty"
            ],
            "author": [
                "Xin Guo",
                "Ding-Xuan Zhou"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/11-207/11-207.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "CVXPY: A Python-Embedded Modeling Language for Convex Optimization",
            "abstract": [
                "CVXPY is a domain-specific language for convex optimization embedded in Python. It allows the user to express convex optimization problems in a natural syntax that follows the math, rather than in the restrictive standard form required by solvers. CVXPY makes it easy to combine convex optimization with high-level features of Python such as parallelism and object-oriented design. CVXPY is available at http://www.cvxpy.org/ under the GPL license, along with documentation and examples."
            ],
            "keywords": [
                "convex optimization",
                "domain-specific languages",
                "Python",
                "conic programming",
                "convexity verification"
            ],
            "author": [
                "Steven Diamond",
                "Stephen Boyd"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-408/15-408.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Quadratic Decomposable Submodular Function Minimization: Theory and Practice",
            "abstract": [
                "We introduce a new convex optimization problem, termed quadratic decomposable submodular function minimization (QDSFM), which allows to model a number of learning tasks on graphs and hypergraphs. The problem exhibits close ties to decomposable submodular function minimization (DSFM) yet is much more challenging to solve. We approach the problem via a new dual strategy and formulate an objective that can be optimized through a number of double-loop algorithms. The outer-loop uses either random coordinate descent (RCD) or alternative projection (AP) methods, for both of which we prove linear convergence rates. The inner-loop computes projections onto cones generated by base polytopes of the submodular functions via the modified min-norm-point or Frank-Wolfe algorithms. We also describe two new applications of QDSFM: hypergraph-adapted PageRank and semi-supervised learning. The proposed hypergraph-based PageRank algorithm can be used for local hypergraph partitioning and comes with provable performance guarantees. For hypergraph-adapted semi-supervised learning, we provide numerical experiments demonstrating the efficiency of our QDSFM solvers and their significant improvements on prediction accuracy when compared to state-of-the-art methods."
            ],
            "keywords": [
                "Submodular functions",
                "Lovász extensions",
                "Random coordinate descent",
                "Frank-Wolfe method",
                "PageRank"
            ],
            "author": [
                "Pan Li",
                "Olgica Milenkovic"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-790/18-790.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exploration in Relational Domains for Model-based Reinforcement Learning",
            "abstract": [
                "A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E 3 and R-MAX algorithms. Efficient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efficiency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efficient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques."
            ],
            "keywords": [
                "reinforcement learning",
                "statistical relational learning",
                "exploration",
                "relational transition models",
                "robotics"
            ],
            "author": [
                "Tobias Lang",
                "Marc Toussaint",
                "Kristian Kersting"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/lang12a/lang12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Smoothing for Path Integral Control",
            "abstract": [
                "In Path Integral control problems a representation of an optimally controlled dynamical system can be formally computed and serve as a guidepost to learn a parametrized policy. The Path Integral Cross-Entropy (PICE) method tries to exploit this, but is hampered by poor sample efficiency. We propose a model-free algorithm called ASPIC (Adaptive Smoothing of Path Integral Control) that applies an inf-convolution to the cost function to speedup convergence of policy optimization. We identify PICE as the infinite smoothing limit of such technique and show that the sample efficiency problems that PICE suffers disappear for finite levels of smoothing. For zero smoothing, ASPIC becomes a greedy optimization of the cost, which is the standard approach in current reinforcement learning. ASPIC adapts the smoothness parameter to keep the variance of the gradient estimator at a predefined level, independently of the number of samples. We show analytically and empirically that intermediate levels of smoothing are optimal, which renders the new method superior to both PICE and direct cost optimization."
            ],
            "keywords": [
                "Path Integral Control",
                "Entropy-Regularization",
                "Cost Smoothing"
            ],
            "author": [
                "Dominik Thalmeier",
                "Hilbert J Kappen",
                "Simone Totaro"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-624/18-624.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Spectral Clustering, With Application To Speech Separation",
            "abstract": [
                "Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram."
            ],
            "keywords": [
                "spectral clustering",
                "blind source separation",
                "computational auditory scene analysis"
            ],
            "author": [
                "Francis R Bach",
                "Michael I Jordan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/bach06b/bach06b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Convergence of Maximum Variance Unfolding",
            "abstract": [
                "Maximum Variance Unfolding is one of the main methods for (nonlinear) dimensionality reduction. We study its large sample limit, providing specific rates of convergence under standard assumptions. We find that it is consistent when the underlying submanifold is isometric to a convex subset, and we provide some simple examples where it fails to be consistent."
            ],
            "keywords": [
                "maximum variance unfolding",
                "isometric embedding",
                "U-processes",
                "empirical processes",
                "proximity graphs"
            ],
            "author": [
                "Ery Arias-Castro",
                "Bruno Pelletier"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/arias-castro13a/arias-castro13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convolutional Neural Networks Analyzed via Convolutional Sparse Coding",
            "abstract": [
                "Convolutional neural networks (CNN) have led to many state-of-the-art results spanning through various fields. However, a clear and profound theoretical understanding of the forward pass, the core algorithm of CNN, is still lacking. In parallel, within the wide field of sparse approximation, Convolutional Sparse Coding (CSC) has gained increasing attention in recent years. A theoretical study of this model was recently conducted, establishing it as a reliable and stable alternative to the commonly practiced patch-based processing. Herein, we propose a novel multi-layer model, ML-CSC, in which signals are assumed to emerge from a cascade of CSC layers. This is shown to be tightly connected to CNN, so much so that the forward pass of the CNN is in fact the thresholding pursuit serving the ML-CSC model. This connection brings a fresh view to CNN, as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network, and their stable estimation, all guaranteed under simple local sparsity conditions. Lastly, identifying the weaknesses in the above pursuit scheme, we propose an alternative to the forward pass, which is connected to deconvolutional and recurrent networks, and also has better theoretical guarantees."
            ],
            "keywords": [
                "Deep Learning",
                "Convolutional Neural Networks",
                "Forward Pass",
                "Sparse Representation",
                "Convolutional Sparse Coding",
                "Thresholding Algorithm",
                "Basis Pursuit"
            ],
            "author": [
                "Vardan Papyan",
                "Yaniv Romano",
                "Michael Elad"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-505/16-505.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Comments on the Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion",
            "abstract": [
                "Loog (2007) provided a complete characterization of the family of solutions to a generalized Fisher criterion. We show that this characterization is essentially equivalent to the original characterization proposed in Ye (2005). The computational advantage of the original characterization over the new one is discussed, which justifies its practical use."
            ],
            "keywords": [
                "linear discriminant analysis",
                "dimension reduction",
                "linear transformation"
            ],
            "author": [
                "Jieping Ye"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/ye08a/ye08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Coherence Functions with Applications in Large-Margin Classification Methods",
            "abstract": [
                "Support vector machines (SVMs) naturally embody sparseness due to their use of hinge loss functions. However, SVMs can not directly estimate conditional class probabilities. In this paper we propose and study a family of coherence functions, which are convex and differentiable, as surrogates of the hinge function. The coherence function is derived by using the maximum-entropy principle and is characterized by a temperature parameter. It bridges the hinge function and the logit function in logistic regression. The limit of the coherence function at zero temperature corresponds to the hinge function, and the limit of the minimizer of its expected error is the minimizer of the expected error of the hinge loss. We refer to the use of the coherence function in large-margin classification as \"C-learning,\" and we present efficient coordinate descent algorithms for the training of regularized C-learning models."
            ],
            "keywords": [
                "large-margin classifiers",
                "hinge functions",
                "logistic functions",
                "coherence functions",
                "Clearning"
            ],
            "author": [
                "Zhihua Zhang",
                "Dehua Liu",
                "Michael I Jordan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/zhang12c/zhang12c.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An MDP-Based Recommender System *",
            "abstract": [
                "Typical recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential optimization problem and, consequently, that Markov decision processes (MDPs) provide a more appropriate model for recommender systems. MDPs introduce two benefits: they take into account the long-term effects of each recommendation and the expected value of each recommendation. To succeed in practice, an MDP-based recommender system must employ a strong initial model, must be solvable quickly, and should not consume too much memory. In this paper, we describe our particular MDP model, its initialization using a predictive model, the solution and update algorithm, and its actual performance on a commercial site. We also describe the particular predictive model we used which outperforms previous models. Our system is one of a small number of commercially deployed recommender systems. As far as we know, it is the first to report experimental analysis conducted on a real commercial site. These results validate the commercial value of recommender systems, and in particular, of our MDP-based approach."
            ],
            "keywords": [
                "recommender systems",
                "Markov decision processes",
                "learning",
                "commercial applications"
            ],
            "author": [
                "Guy Shani",
                "David Heckerman",
                "Ronen I Brafman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/shani05a/shani05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A View of Margin Losses as Regularizers of Probability Estimates",
            "abstract": [
                "Regularization is commonly used in classifier design, to assure good generalization. Classical regularization enforces a cost on classifier complexity, by constraining parameters. This is usually combined with a margin loss, which favors large-margin decision rules. A novel and unified view of this architecture is proposed, by showing that margin losses act as regularizers of posterior class probabilities, in a way that amplifies classical parameter regularization. The problem of controlling the regularization strength of a margin loss is considered, using a decomposition of the loss in terms of a link and a binding function. The link function is shown to be responsible for the regularization strength of the loss, while the binding function determines its outlier robustness. A large class of losses is then categorized into equivalence classes of identical regularization strength or outlier robustness. It is shown that losses in the same regularization class can be parameterized so as to have tunable regularization strength. This parameterization is finally used to derive boosting algorithms with loss regularization (BoostLR). Three classes of tunable regularization losses are considered in detail. Canonical losses can implement all regularization behaviors but have no flexibility in terms of outlier modeling. Shrinkage losses support equally parameterized link and binding functions, leading to boosting algorithms that implement the popular shrinkage procedure. This offers a new explanation for shrinkage as a special case of loss-based regularization. Finally, α-tunable losses enable the independent parameterization of link and binding functions, leading to boosting algorithms of great flexibility. This is illustrated by the derivation of an algorithm that generalizes both AdaBoost and LogitBoost, behaving as either one when that best suits the data to classify. Various experiments provide evidence of the benefits of probability regularization for both classification and estimation of posterior class probabilities."
            ],
            "keywords": [
                "classification",
                "margin losses",
                "regularization",
                "boosting",
                "probability elicitation",
                "generalization",
                "loss functions",
                "link functions",
                "binding functions",
                "shrinkage"
            ],
            "author": [
                "Hamed Masnadi-Shirazi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/masnadi15a/masnadi15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convex Calibration Dimension for Multiclass Loss Matrices",
            "abstract": [
                "We study consistency properties of surrogate loss functions for general multiclass learning problems, defined by a general multiclass loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be calibrated with respect to a loss matrix in this setting. We then introduce the notion of convex calibration dimension of a multiclass loss matrix, which measures the smallest 'size' of a prediction space in which it is possible to design a convex surrogate that is calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, we apply our framework to study various subset ranking losses, and use the convex calibration dimension as a tool to show both the existence and non-existence of various types of convex calibrated surrogates for these losses. Our results strengthen recent results of Duchi et al. (2010) and Calauzènes et al. (2012) on the non-existence of certain types of convex calibrated surrogates in subset ranking. We anticipate the convex calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems."
            ],
            "keywords": [
                "Statistical consistency",
                "multiclass loss",
                "loss matrix",
                "surrogate loss",
                "convex surrogates",
                "calibrated surrogates",
                "classification calibration",
                "subset ranking"
            ],
            "author": [
                "Harish G Ramaswamy",
                "Shivani Agarwal"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-316/14-316.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Maximum Margin Algorithms with Boolean Kernels",
            "abstract": [
                "Recent work has introduced Boolean kernels with which one can learn linear threshold functions over a feature space containing all conjunctions of length up to k (for any 1 ≤ k ≤ n) over the original n Boolean features in the input space. This motivates the question of whether maximum margin algorithms such as Support Vector Machines can learn Disjunctive Normal Form expressions in the Probably Approximately Correct (PAC) learning model by using this kernel. We study this question, as well as a variant in which structural risk minimization (SRM) is performed where the class hierarchy is taken over the length of conjunctions. We show that maximum margin algorithms using the Boolean kernels do not PAC learn t(n)term DNF for any t(n) = ω(1), even when used with such a SRM scheme. We also consider PAC learning under the uniform distribution and show that if the kernel uses conjunctions of length ω(√ n) then the maximum margin hypothesis will fail on the uniform distribution as well. Our results concretely illustrate that margin based algorithms may overfit when learning simple target functions with natural kernels."
            ],
            "keywords": [
                "computational learning theory",
                "kernel methods",
                "PAC learning",
                "Boolean functions"
            ],
            "author": [
                "Roni Khardon",
                "Rocco A Servedio"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/khardon05a/khardon05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hierarchical Average Reward Reinforcement Learning",
            "abstract": [
                "Hierarchical reinforcement learning (HRL) is a general framework for scaling reinforcement learning (RL) to problems with large state and action spaces by using the task (or action) structure to restrict the space of policies. Prior work in HRL including HAMs, options, MAXQ, and PHAMs has been limited to the discrete-time discounted reward semi-Markov decision process (SMDP)"
            ],
            "keywords": [],
            "author": [
                "Mohammad Ghavamzadeh",
                "Sridhar Mahadevan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/ghavamzadeh07a/ghavamzadeh07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Linear Non-Gaussian Causal Models in the Presence of Latent Variables",
            "abstract": [
                "We consider the problem of learning causal models from observational data generated by linear non-Gaussian acyclic causal models with latent variables. Without considering the effect of latent variables, the inferred causal relationships among the observed variables are often wrong. Under faithfulness assumption, we propose a method to check whether there exists a causal path between any two observed variables. From this information, we can obtain the causal order among the observed variables. The next question is whether the causal effects can be uniquely identified as well. We show that causal effects among observed variables cannot be identified uniquely under mere assumptions of faithfulness and non-Gaussianity of exogenous noises. However, we are able to propose an efficient method that identifies the set of all possible causal effects that are compatible with the observational data. We present additional structural conditions on the causal graph under which causal effects among observed variables can be determined uniquely. Furthermore, we provide necessary and sufficient graphical conditions for unique identification of the number of variables in the system. Experiments on synthetic data and real-world data show the effectiveness of our proposed algorithm for learning causal models."
            ],
            "keywords": [
                "Causal Discovery",
                "Structural Equation Models",
                "Non-Gaussianity",
                "Latent Variables",
                "Independent Component Analysis"
            ],
            "author": [
                "Saber Salehkaleybar",
                "Negar Kiyavash",
                "Kun Zhang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-260/19-260.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "SAMOA: Scalable Advanced Massive Online Analysis",
            "abstract": [
                "samoa (Scalable Advanced Massive Online Analysis) is a platform for mining big data streams. It provides a collection of distributed streaming algorithms for the most common data mining and machine learning tasks such as classification, clustering, and regression, as well as programming abstractions to develop new algorithms. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as Storm, S4, and Samza."
            ],
            "keywords": [
                "data streams",
                "distributed systems",
                "classification",
                "clustering",
                "regression",
                "toolbox",
                "machine learning"
            ],
            "author": [
                "Gianmarco De",
                "Francisci Morales",
                "Albert Bifet"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/morales15a/morales15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "An Error Bound Based on a Worst Likely Assignment",
            "abstract": [
                "This paper introduces a new PAC transductive error bound for classification. The method uses information from the training examples and inputs of working examples to develop a set of likely assignments to outputs of the working examples. A likely assignment with maximum error determines the bound. The method is very effective for small data sets."
            ],
            "keywords": [
                "error bound",
                "transduction",
                "nearest neighbor",
                "dynamic programming"
            ],
            "author": [
                "Eric Bax",
                "Augusto Callejas"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/bax08a/bax08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "EnsembleSVM: A Library for Ensemble Learning Using Support Vector Machines",
            "abstract": [
                "EnsembleSVM is a free software package containing efficient routines to perform ensemble learning with support vector machine (SVM) base models. It currently offers ensemble methods based on binary SVM models. Our implementation avoids duplicate storage and evaluation of support vectors which are shared between constituent models. Experimental results show that using ensemble approaches can drastically reduce training complexity while maintaining high predictive accuracy. The EnsembleSVM software package is freely available online at http://esat.kuleuven.be/stadius/ensemblesvm."
            ],
            "keywords": [
                "classification",
                "ensemble learning",
                "support vector machine",
                "bagging"
            ],
            "author": [
                "Marc Claesen",
                "K U Leuven",
                "Frank De Smet",
                "Johan A K Suykens",
                "Bart De"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/claesen14a/claesen14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Improving Markov Network Structure Learning Using Decision Trees",
            "abstract": [
                "Most existing algorithms for learning Markov network structure either are limited to learning interactions among few variables or are very slow, due to the large space of possible structures. In this paper, we propose three new methods for using decision trees to learn Markov network structures. The advantage of using decision trees is that they are very fast to learn and can represent complex interactions among many variables. The first method, DTSL, learns a decision tree to predict each variable and converts each tree into a set of conjunctive features that define the Markov network structure. The second, DT-BLM, builds on DTSL by using it to initialize a search-based Markov network learning algorithm recently proposed by Davis and Domingos (2010). The third, DT+L1, combines the features learned by DTSL with those learned by an L1-regularized logistic regression method (L1) proposed by Ravikumar et al. (2009). In an extensive empirical evaluation on 20 data sets, DTSL is comparable to L1 and significantly faster and more accurate than two other baselines. DT-BLM is slower than DTSL, but obtains slightly higher accuracy. DT+L1 combines the strengths of DTSL and L1 to perform significantly better than either of them with only a modest increase in training time."
            ],
            "keywords": [
                "Markov networks",
                "structure learning",
                "decision trees",
                "probabilistic methods"
            ],
            "author": [
                "Daniel Lowd",
                "Jesse Davis"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/lowd14a/lowd14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Gaussian Processes for Machine Learning (GPML) Toolbox",
            "abstract": [
                "The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are specified by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classification. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace's method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks."
            ],
            "keywords": [
                "Gaussian processes",
                "nonparametric Bayes",
                "probabilistic regression and classification"
            ],
            "author": [
                "Carl Edward Rasmussen",
                "Hannes Nickisch"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/rasmussen10a/rasmussen10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Unsupervised Basis Function Adaptation for Reinforcement Learning",
            "abstract": [
                "When using reinforcement learning (RL) algorithms it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on an agent's performance, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is currently interest among researchers in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures. One relatively unexplored method of adapting approximation architectures involves using feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. In this article we will: (a) informally discuss the potential advantages offered by such methods; (b) introduce a new algorithm based on such methods which adapts a state aggregation approximation architecture online and is designed for use in conjunction with SARSA; (c) provide theoretical results, in a policy evaluation setting, regarding this particular algorithm's complexity, convergence properties and potential to reduce VF error; and finally (d) test experimentally the extent to which this algorithm can improve performance given a number of different test problems. Taken together our results suggest that our algorithm (and potentially such methods more generally) can provide a versatile and computationally lightweight means of significantly boosting RL performance given suitable conditions which are commonly encountered in practice."
            ],
            "keywords": [
                "reinforcement learning",
                "unsupervised learning",
                "basis function adaptation",
                "state aggregation",
                "SARSA"
            ],
            "author": [
                "Edward Barker"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-392/18-392.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Variance-Reduced Cubic Regularization Methods",
            "abstract": [
                "We propose a stochastic variance-reduced cubic regularized Newton method (SVRC) for non-convex optimization. At the core of SVRC is a novel semi-stochastic gradient along with a semi-stochastic Hessian, which are specifically designed for cubic regularization method. For a nonconvex function with n component functions, we show that our algorithm is guaranteed to converge to an (, √)-approximate local minimum within O(n 4/5 / 3/2) 1 second-order oracle calls, which outperforms the state-of-the-art cubic regularization algorithms including subsampled cubic regularization. To further reduce the sample complexity of Hessian matrix computation in cubic regularization based methods, we also propose a sample efficient stochastic variance-reduced cubic regularization (Lite-SVRC) algorithm for finding the local minimum more efficiently. Lite-SVRC converges to an (, √)-approximate local minimum within O(n + n 2/3 / 3/2) Hessian sample complexity, which is faster than all existing cubic regularization based methods. Numerical experiments with different nonconvex optimization problems conducted on real datasets validate our theoretical results for both SVRC and Lite-SVRC."
            ],
            "keywords": [
                "Cubic Regularization",
                "Nonconvex Optimization",
                "Variance Reduction",
                "Hessian Sample Complexity",
                "Local Minimum"
            ],
            "author": [
                "Dongruo Zhou",
                "Pan Xu",
                "Quanquan Gu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/19-055/19-055.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ELFI: Engine for Likelihood-Free Inference",
            "abstract": [
                "Engine for Likelihood-Free Inference (ELFI) is a Python software library for performing likelihood-free inference (LFI). ELFI provides a convenient syntax for arranging components in LFI, such as priors, simulators, summaries or distances, to a network called ELFI graph. The components can be implemented in a wide variety of languages. The stand-alone ELFI graph can be used with any of the available inference methods without modifications. A central method implemented in ELFI is Bayesian Optimization for Likelihood-Free Inference (BOLFI), which has recently been shown to accelerate likelihood-free inference up to several orders of magnitude by surrogate-modelling the distance. ELFI also has an inbuilt support for output data storing for reuse and analysis, and supports parallelization of computation from multiple cores up to a cluster environment. ELFI is designed to be extensible and provides interfaces for widening its functionality. This makes the adding of new inference methods to ELFI straightforward and automatically compatible with the inbuilt features."
            ],
            "keywords": [
                "Likelihood-free inference",
                "approximate Bayesian computation",
                "Python",
                "BOLFI",
                "parallel computing"
            ],
            "author": [
                "Jarno Lintusaari",
                "Henri Vuollekoski",
                "Marko Järvenpää",
                "Pekka Marttinen",
                "Michael U Gutmann",
                "Aki Vehtari",
                "Jukka Corander",
                "Samuel Kaski",
                "Antti Kangasrääsiö",
                "Kusti Skytén"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-374/17-374.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Superior Guarantees for Sequential Prediction and Lossless Compression via Alphabet Decomposition",
            "abstract": [
                "We present worst case bounds for the learning rate of a known prediction method that is based on hierarchical applications of binary context tree weighting (CTW) predictors. A heuristic application of this approach that relies on Huffman's alphabet decomposition is known to achieve state-ofthe-art performance in prediction and lossless compression benchmarks. We show that our new bound for this heuristic is tighter than the best known performance guarantees for prediction and lossless compression algorithms in various settings. This result substantiates the efficiency of this hierarchical method and provides a compelling explanation for its practical success. In addition, we present the results of a few experiments that examine other possibilities for improving the multialphabet prediction performance of CTW-based algorithms."
            ],
            "keywords": [
                "sequential prediction",
                "the context tree weighting method",
                "variable order Markov models",
                "error bounds"
            ],
            "author": [
                "Ron Begleiter"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/begleiter06a/begleiter06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Complexity Analysis of the Primal Solutions for the Accelerated Randomized Dual Coordinate Ascent",
            "abstract": [
                "Dual first-order methods are essential techniques for large-scale constrained convex optimization. However, when recovering the primal solutions, we need T (−2) iterations to achieve an-optimal primal solution when we apply an algorithm to the non-strongly convex dual problem with T (−1) iterations to achieve an-optimal dual solution, where T (x) can be x or √ x. In this paper, we prove that the iteration complexity of the primal solutions and dual solutions have the same O 1 √ order of magnitude for the accelerated randomized dual coordinate ascent. When the dual function further satisfies the quadratic functional growth condition, by restarting the algorithm at any period, we establish the linear iteration complexity for both the primal solutions and dual solutions even if the condition number is unknown. When applied to the regularized empirical risk minimization problem, we prove the iteration complexity of O n log n + n in both primal space and dual space, where n is the number of samples. Our result takes out the log 1 factor compared with the methods based on smoothing/regularization or Catalyst reduction. As far as we know, this is the first time that the optimal O n iteration complexity in the primal space is established for the dual coordinate ascent based stochastic algorithms. We also establish the accelerated linear complexity for some problems with nonsmooth loss, e.g., the least absolute deviation and SVM."
            ],
            "keywords": [
                "accelerated randomized dual coordinate ascent",
                "restart at any period",
                "iteration complexity",
                "primal solutions",
                "dual first-order methods"
            ],
            "author": [
                "Huan Li",
                "Zhouchen Lin",
                "† Zhouchen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-425/18-425.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Accelerating t-SNE using Tree-Based Algorithms",
            "abstract": [
                "The paper investigates the acceleration of t-SNE-an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots-using two treebased algorithms. In particular, the paper develops variants of the Barnes-Hut algorithm and of the dual-tree algorithm that approximate the gradient used for learning t-SNE embeddings in O(N log N). Our experiments show that the resulting algorithms substantially accelerate t-SNE, and that they make it possible to learn embeddings of data sets with millions of objects. Somewhat counterintuitively, the Barnes-Hut variant of t-SNE appears to outperform the dual-tree variant."
            ],
            "keywords": [
                "embedding",
                "multidimensional scaling",
                "t-SNE",
                "space-partitioning trees",
                "Barnes-Hut algorithm",
                "dual-tree algorithm"
            ],
            "author": [
                "Laurens Van Der Maaten",
                "Aaron Courville",
                "Rob Fergus",
                "Christopher Manning"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/vandermaaten14a/vandermaaten14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Kernel Analysis of Deep Networks",
            "abstract": [
                "When training deep networks it is common knowledge that an efficient and well generalizing representation of the problem is formed. In this paper we aim to elucidate what makes the emerging representation successful. We analyze the layer-wise evolution of the representation in a deep network by building a sequence of deeper and deeper kernels that subsume the mapping performed by more and more layers of the deep network and measuring how these increasingly complex kernels fit the learning problem. We observe that deep networks create increasingly better representations of the learning problem and that the structure of the deep network controls how fast the representation of the task is formed layer after layer."
            ],
            "keywords": [
                "deep networks",
                "kernel principal component analysis",
                "representations"
            ],
            "author": [
                "Grégoire Montavon",
                "Mikio L Braun",
                "Klaus-Robert Müller"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/montavon11a/montavon11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Multimodal Gesture Recognition via Multiple Hypotheses Rescoring",
            "abstract": [
                "We present a new framework for multimodal gesture recognition that is based on a multiple hypotheses rescoring fusion scheme. We specifically deal with a demanding Kinect-based multimodal data set, introduced in a recent gesture recognition challenge (ChaLearn 2013), where multiple subjects freely perform multimodal gestures. We employ multiple modalities, that is, visual cues, such as skeleton data, color and depth images, as well as audio, and we extract feature descriptors of the hands' movement, handshape, and audio spectral properties. Using a common hidden Markov model framework we build single-stream gesture models based on which we can generate multiple single stream-based hypotheses for an unknown gesture sequence. By multimodally rescoring these hypotheses via constrained decoding and a weighted combination scheme, we end up with a multimodally-selected best hypothesis. This is further refined by means of parallel fusion of the monomodal gesture models applied at a segmental level. In this setup, accurate gesture modeling is proven to be critical and is facilitated by an activity detection system that is also presented. The overall approach achieves 93.3% gesture recognition accuracy in the ChaLearn Kinect-based multimodal data set, significantly outperforming all recently published approaches on the same challenging multimodal gesture recognition task, providing a relative error rate reduction of at least 47.6%."
            ],
            "keywords": [
                "multimodal gesture recognition",
                "HMMs",
                "speech recognition",
                "multimodal fusion",
                "activity detection"
            ],
            "author": [
                "Vassilis Pitsikalis",
                "Athanasios Katsamanis",
                "Stavros Theodorakis",
                "Petros Maragos",
                "Isabelle Guyon",
                "Vassilis Athitsos",
                "Sergio Escalera"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/pitsikalis15a/pitsikalis15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A New Algorithm and Theory for Penalized Regression-based Clustering",
            "abstract": [
                "Clustering is unsupervised and exploratory in nature. Yet, it can be performed through penalized regression with grouping pursuit, as demonstrated in Pan et al. (2013). In this paper, we develop a more efficient algorithm for scalable computation and a new theory of clustering consistency for the method. This algorithm, called DC-ADMM, combines difference of convex (DC) programming with the alternating direction method of multipliers (ADMM). This algorithm is shown to be more computationally efficient than the quadratic penalty based algorithm of Pan et al. (2013) because of the former's closed-form updating formulas. Numerically, we compare the DC-ADMM algorithm with the quadratic penalty algorithm to demonstrate its utility and scalability. Theoretically, we establish a finitesample mis-clustering error bound for penalized regression based clustering with the L 0 constrained regularization in a general setting. On this ground, we provide conditions for clustering consistency of the penalized clustering method. As an end product, we put R package prclust implementing PRclust with various loss and grouping penalty functions available on GitHub and CRAN."
            ],
            "keywords": [
                "Alternating direction method of multipliers (ADMM)",
                "Difference of convex (DC) programming",
                "Clustering consistency",
                "Truncated L 1 -penalty (TLP)"
            ],
            "author": [
                "Chong Wu",
                "Sunghoon Kwon",
                "Xiaotong Shen",
                "Wei Pan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-553/15-553.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Communication-efficient Sparse Regression",
            "abstract": [
                "We devise a communication-efficient approach to distributed sparse regression in the highdimensional setting. The key idea is to average \"debiased\" or \"desparsified\" lasso estimators. We show the approach converges at the same rate as the lasso as long as the dataset is not split across too many machines, and consistently estimates the support under weaker conditions than the lasso. On the computational side, we propose a new parallel and computationally-efficient algorithm to compute the approximate inverse covariance required in the debiasing approach, when the dataset is split across samples. We further extend the approach to generalized linear models."
            ],
            "keywords": [
                "Distributed Sparse Regression",
                "Averaging",
                "Debiasing",
                "lasso",
                "high-dimensional statistics"
            ],
            "author": [
                "Jason D Lee",
                "Qiang Liu",
                "Jonathan E Taylor"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-002/16-002.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Rate of Convergence of the Bagged Nearest Neighbor Estimate",
            "abstract": [
                "Bagging is a simple way to combine estimates in order to improve their performance. This method, suggested by Breiman in 1996, proceeds by resampling from the original data set, constructing a predictor from each subsample, and decide by combining. By bagging an n-sample, the crude nearest neighbor regression estimate is turned into a consistent weighted nearest neighbor regression estimate, which is amenable to statistical analysis. Letting the resampling size k n grows appropriately with n, it is shown that this estimate may achieve optimal rate of convergence, independently from the fact that resampling is done with or without replacement. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, adaptation results by data-splitting are presented."
            ],
            "keywords": [
                "bagging",
                "resampling",
                "nearest neighbor estimate",
                "rates of convergence"
            ],
            "author": [
                "Gérard Biau"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/biau10a/biau10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers",
            "abstract": [
                "There is a large literature explaining why AdaBoost is a successful classifier. The literature on AdaBoost focuses on classifier margins and boosting's interpretation as the optimization of an exponential likelihood function. These existing explanations, however, have been pointed out to be incomplete. A random forest is another popular ensemble method for which there is substantially less explanation in the literature. We introduce a novel perspective on AdaBoost and random forests that proposes that the two algorithms work for similar reasons. While both classifiers achieve similar predictive accuracy, random forests cannot be conceived as a direct optimization procedure. Rather, random forests is a selfaveraging, interpolating algorithm which creates what we denote as a \"spiked-smooth\" classifier, and we view AdaBoost in the same light. We conjecture that both AdaBoost and random forests succeed because of this mechanism. We provide a number of examples to support this explanation. In the process, we question the conventional wisdom that suggests that boosting algorithms for classification require regularization or early stopping and should be limited to low complexity classes of learners, such as decision stumps. We conclude that boosting should be used like random forests: with large decision trees, without regularization or early stopping."
            ],
            "keywords": [
                "AdaBoost",
                "random forests",
                "tree-ensembles",
                "overfitting",
                "classification"
            ],
            "author": [
                "Abraham J Wyner",
                "Matthew Olson",
                "Justin Bleich",
                "David Mease"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-240/15-240.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Inferring Application Protocol Behaviors in Encrypted Network Traffic",
            "abstract": [
                "Several fundamental security mechanisms for restricting access to network resources rely on the ability of a reference monitor to inspect the contents of traffic as it traverses the network. However, with the increasing popularity of cryptographic protocols, the traditional means of inspecting packet contents to enforce security policies is no longer a viable approach as message contents are concealed by encryption. In this paper, we investigate the extent to which common application protocols can be identified using only the features that remain intact after encryption-namely packet size, timing, and direction. We first present what we believe to be the first exploratory look at protocol identification in encrypted tunnels which carry traffic from many TCP connections simultaneously, using only post-encryption observable features. We then explore the problem of protocol identification in individual encrypted TCP connections, using much less data than in other recent approaches. The results of our evaluation show that our classifiers achieve accuracy greater than 90% for several protocols in aggregate traffic, and, for most protocols, greater than 80% when making fine-grained classifications on single connections. Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%."
            ],
            "keywords": [
                "traffic classification",
                "hidden Markov models",
                "network security"
            ],
            "author": [
                "Charles V Wright",
                "Fabian Monrose",
                "Gerald M Masson",
                "Masson @ Jhu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/wright06a/wright06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Theory of Randomized Kaczmarz Algorithm",
            "abstract": [
                "A relaxed randomized Kaczmarz algorithm is investigated in a least squares regression setting by a learning theory approach. When the sampling values are accurate and the regression function (conditional means) is linear, such an algorithm has been well studied in the community of non-uniform sampling. In this paper, we are mainly interested in the different case of either noisy random measurements or a nonlinear regression function. In this case, we show that relaxation is needed. A necessary and sufficient condition on the sequence of relaxation parameters or step sizes for the convergence of the algorithm in expectation is presented. Moreover, polynomial rates of convergence, both in expectation and in probability, are provided explicitly. As a result, the almost sure convergence of the algorithm is proved by applying the Borel-Cantelli Lemma."
            ],
            "keywords": [
                "learning theory",
                "relaxed randomized Kaczmarz algorithm",
                "online learning",
                "space of homogeneous linear functions",
                "almost sure convergence"
            ],
            "author": [
                "Junhong Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/lin15a/lin15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scaling up Data Augmentation MCMC via Calibration",
            "abstract": [
                "There has been considerable interest in making Bayesian inference more scalable. In big data settings, most of the focus has been on reducing the computing time per iteration rather than reducing the number of iterations needed in Markov chain Monte Carlo (MCMC). This article considers data augmentation MCMC (DA-MCMC), a widely used technique. DA-MCMC samples tend to become highly autocorrelated in large samples, due to a mis-calibration problem in which conditional posterior distributions given augmented data are too concentrated. This makes it necessary to collect very long MCMC paths to obtain acceptably low MC error. To combat this inefficiency, we propose a family of calibrated data augmentation algorithms, which appropriately adjust the variance of conditional posterior distributions. A Metropolis-Hastings step is used to eliminate bias in the stationary distribution of the resulting sampler. Compared to existing alternatives, this approach can dramatically reduce MC error by reducing autocorrelation and increasing the effective number of DA-MCMC samples per unit of computing time. The approach is simple and applicable to a broad variety of existing data augmentation algorithms. We focus on three popular generalized linear models: probit, logistic and Poisson log-linear. Dramatic gains in computational efficiency are shown in applications."
            ],
            "keywords": [
                "Bayesian Probit",
                "Biased subsampling",
                "Big n",
                "Data augmentation",
                "Log-linear model",
                "Logistic regression",
                "Maximal correlation",
                "Polya-Gamma"
            ],
            "author": [
                "Leo L Duan",
                "James E Johndrow",
                "David B Dunson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-573/17-573.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graph-Based Hierarchical Conceptual Clustering",
            "abstract": [
                "Hierarchical conceptual clustering has proven to be a useful, although under-explored, data mining technique. A graph-based representation of structural information combined with a substructure discovery technique has been shown to be successful in knowledge discovery. The SUBDUE substructure discovery system provides one such combination of approaches. This work presents SUBDUE and the development of its clustering functionalities. Several examples are used to illustrate the validity of the approach both in structured and unstructured domains, as well as to compare SUBDUE to the Cobweb clustering algorithm. We also develop a new metric for comparing structurally-defined clusterings. Results show that SUBDUE successfully discovers hierarchical clusterings in both structured and unstructured data."
            ],
            "keywords": [
                "Clustering",
                "Cluster Analysis",
                "Concept Formation",
                "Structural Data",
                "Graph Match"
            ],
            "author": [
                "Istvan Jonyer",
                "Diane J Cook",
                "Lawrence B Holder"
            ],
            "ref": "http://www.jmlr.org/papers/volume2/jonyer01a/jonyer01a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalized Nonbacktracking Bounds on the Influence in Independent Cascade Models",
            "abstract": [
                "This paper develops deterministic upper and lower bounds on the influence measure in a network, more precisely, the expected number of nodes that a seed set can influence in the independent cascade model. In particular, our bounds exploit r-nonbacktracking walks and Fortuin-Kasteleyn-Ginibre (FKG) type inequalities, and are computed by message passing algorithms. Further, we provide parameterized versions of the bounds that control the trade-off between efficiency and accuracy. Finally, the tightness of the bounds is illustrated on various network models."
            ],
            "keywords": [
                "Influence Estimation",
                "Nonbacktracking Walk",
                "Message Passing",
                "Social Networks",
                "Independent Cascade Model"
            ],
            "author": [
                "Emmanuel Abbe",
                "Sanjeev Kulkarni",
                "Eun Jee Lee"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-112/18-112.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Security Analysis of Online Centroid Anomaly Detection",
            "abstract": [
                "Security issues are crucial in a number of machine learning applications, especially in scenarios dealing with human activity rather than natural phenomena (e.g., information ranking, spam detection, malware detection, etc.). In such cases, learning algorithms may have to cope with manipulated data aimed at hampering decision making. Although some previous work addressed the issue of handling malicious data in the context of supervised learning, very little is known about the behavior of anomaly detection methods in such scenarios. In this contribution, 1 we analyze the performance of a particular method-online centroid anomaly detection-in the presence of adversarial noise. Our analysis addresses the following security-related issues: formalization of learning and attack processes, derivation of an optimal attack, and analysis of attack efficiency and limitations. We derive bounds on the effectiveness of a poisoning attack against centroid anomaly detection under different conditions: attacker's full or limited control over the traffic and bounded false positive rate. Our bounds show that whereas a poisoning attack can be effectively staged in the unconstrained case, it can be made arbitrarily difficult (a strict upper bound on the attacker's gain) if external constraints are properly used. Our experimental evaluation, carried out on real traces of HTTP and exploit traffic, confirms the tightness of our theoretical bounds and the practicality of our protection mechanisms."
            ],
            "keywords": [
                "anomaly detection",
                "adversarial",
                "security analysis",
                "support vector data description",
                "computer security",
                "network intrusion detection"
            ],
            "author": [
                "Marius Kloft",
                "Pavel Laskov",
                "* Mk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/kloft12b/kloft12b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Exact 1-Norm Support Vector Machines via Unconstrained Convex Differentiable Minimization",
            "abstract": [
                "Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional finite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classification problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values."
            ],
            "keywords": [],
            "author": [
                "Olvi L Mangasarian",
                "Kristin P Bennett",
                "Emilio Parrado-Hernández"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/mangasarian06a/mangasarian06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "apricot: Submodular selection for data summarization in Python",
            "abstract": [
                "We present apricot, an open source Python package for selecting representative subsets from large data sets using submodular optimization. The package implements several efficient greedy selection algorithms that offer strong theoretical guarantees on the quality of the selected set. Additionally, several submodular set functions are implemented, including facility location, which is broadly applicable but requires memory quadratic in the number of examples in the data set, and a feature-based function that is less broadly applicable but can scale to millions of examples. Apricot is extremely efficient, using both algorithmic speedups such as the lazy greedy algorithm and memoization as well as code optimization using numba. We demonstrate the use of subset selection by training machine learning models to comparable accuracy using either the full data set or a representative subset thereof. This paper presents an explanation of submodular selection, an overview of the features in apricot, and applications to two data sets."
            ],
            "keywords": [
                "submodular selection",
                "submodularity",
                "big data",
                "machine learning",
                "subset selection"
            ],
            "author": [
                "Jacob Schreiber",
                "Paul G Allen",
                "Jeffrey Bilmes",
                "William Stafford Noble"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-467/19-467.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spectral Methods Meet EM: A Provably Optimal Algorithm for Crowdsourcing",
            "abstract": [
                "Crowdsourcing is a popular paradigm for effectively collecting labels at low cost. The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods."
            ],
            "keywords": [
                "crowdsourcing",
                "spectral methods",
                "EM",
                "Dawid-Skene model",
                "non-convex optimization",
                "minimax rate"
            ],
            "author": [
                "Yuchen Zhang",
                "Xi Chen",
                "Dengyong Zhou",
                "Michael I Jordan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-511/14-511.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Closer Look at Adaptive Regret",
            "abstract": [
                "For the prediction with expert advice setting, we consider methods to construct algorithms that have low adaptive regret. The adaptive regret of an algorithm on a time interval [t 1 , t 2 ] is the loss of the algorithm minus the loss of the best expert over that interval. Adaptive regret measures how well the algorithm approximates the best expert locally, and so is different from, although closely related to, both the classical regret, measured over an initial time interval [1, t], and the tracking regret, where the algorithm is compared to a good sequence of experts over [1, t]. We investigate two existing intuitive methods for deriving algorithms with low adaptive regret, one based on specialist experts and the other based on restarts. Quite surprisingly, we show that both methods lead to the same algorithm, namely Fixed Share, which is known for its tracking regret. We provide a thorough analysis of the adaptive regret of Fixed Share. We obtain the exact worst-case adaptive regret for Fixed Share, from which the classical tracking bounds follow. We prove that Fixed Share is optimal for adaptive regret: the worst-case adaptive regret of any algorithm is at least that of an instance of Fixed Share."
            ],
            "keywords": [
                "online learning",
                "adaptive regret",
                "Fixed Share",
                "specialist experts"
            ],
            "author": [
                "Dmitry Adamskiy",
                "Wouter M Koolen",
                "Alexey Chernov",
                "Vladimir Vovk"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/13-533/13-533.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Spam Filtering Using Statistical Data Compression Models",
            "abstract": [
                "Spam filtering poses a special problem in text categorization, of which the defining characteristic is that filters face an active adversary, which constantly attempts to evade filtering. Since spam evolves continuously and most practical applications are based on online user feedback, the task calls for fast, incremental and robust learning algorithms. In this paper, we investigate a novel approach to spam filtering based on adaptive statistical data compression models. The nature of these models allows them to be employed as probabilistic text classifiers based on character-level or binary sequences. By modeling messages as sequences, tokenization and other error-prone preprocessing steps are omitted altogether, resulting in a method that is very robust. The models are also fast to construct and incrementally updateable. We evaluate the filtering performance of two different compression algorithms; dynamic Markov compression and prediction by partial matching. The results of our empirical evaluation indicate that compression models outperform currently established spam filters, as well as a number of methods proposed in previous studies."
            ],
            "keywords": [
                "text categorization",
                "spam filtering",
                "Markov models",
                "dynamic Markov compression",
                "prediction by partial matching"
            ],
            "author": [
                "Andrej Bratko",
                "David R Cheriton",
                "Bogdan Filipič",
                "Andrej ©2006",
                "Gordon V Bratko",
                "Bogdan Cormack",
                "Thomas R Filipič",
                "Blaž Lynam"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/bratko06a/bratko06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "From Predictive Methods to Missing Data Imputation: An Optimization Approach",
            "abstract": [
                "Missing data is a common problem in real-world settings and for this reason has attracted significant attention in the statistical literature. We propose a flexible framework based on formal optimization to impute missing data with mixed continuous and categorical variables. This framework can readily incorporate various predictive models including Knearest neighbors, support vector machines, and decision tree based methods, and can be adapted for multiple imputation. We derive fast first-order methods that obtain high quality solutions in seconds following a general imputation algorithm opt.impute presented in this paper. We demonstrate that our proposed method improves out-of-sample accuracy in large-scale computational experiments across a sample of 84 data sets taken from the UCI Machine Learning Repository. In all scenarios of missing at random mechanisms and various missing percentages, opt.impute produces the best overall imputation in most data sets benchmarked against five other methods: mean impute, K-nearest neighbors, iterative knn, Bayesian PCA, and predictive-mean matching, with an average reduction in mean absolute error of 8.3% against the best cross-validated benchmark method. Moreover, opt.impute leads to improved out-of-sample performance of learning algorithms trained using the imputed data, demonstrated by computational experiments on 10 downstream tasks. For models trained using opt.impute single imputations with 50% data missing, the average out-of-sample R 2 is 0.339 in the regression tasks and the average out-of-sample accuracy is 86.1% in the classification tasks, compared to 0.315 and 84.4% for the best cross-validated benchmark method. In the multiple imputation setting, downstream models trained using opt.impute obtain a statistically significant improvement over models trained using multivariate imputation by chained equations (mice) in 8/10 missing data scenarios considered."
            ],
            "keywords": [
                "missing data imputation",
                "K-NN",
                "SVM",
                "optimal decision trees"
            ],
            "author": [
                "Dimitris Bertsimas",
                "Colin Pawlowski",
                "Ying Daisy Zhuo"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-073/17-073.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Direct Estimation of High Dimensional Stationary Vector Autoregressions",
            "abstract": [
                "The vector autoregressive (VAR) model is a powerful tool in learning complex time series and has been exploited in many fields. The VAR model poses some unique challenges to researchers: On one hand, the dimensionality, introduced by incorporating multiple numbers of time series and adding the order of the vector autoregression, is usually much higher than the time series length; On the other hand, the temporal dependence structure naturally present in the VAR model gives rise to extra difficulties in data analysis. The regular way in cracking the VAR model is via \"least squares\" and usually involves adding different penalty terms (e.g., ridge or lasso penalty) in handling high dimensionality. In this manuscript, we propose an alternative way in estimating the VAR model. The main idea is, via exploiting the temporal dependence structure, formulating the estimating problem to a linear program. There is instant advantage of the proposed approach over the lassotype estimators: The estimation equation can be decomposed to multiple sub-equations and accordingly can be solved efficiently using parallel computing. Besides that, we also bring new theoretical insights into the VAR model analysis. So far the theoretical results developed in high dimensions (e.g., Song and Bickel, 2011 and Kock and Callot, 2015) are based on stringent assumptions that are not transparent. Our results, on the other hand, show that the spectral norms of the transition matrices play an important role in estimation accuracy and build estimation and prediction consistency accordingly. Moreover, we provide some experiments on both synthetic and real-world equity data. We show that there are empirical advantages of our method over the lasso-type estimators in parameter estimation and forecasting."
            ],
            "keywords": [
                "transition matrix",
                "multivariate time series",
                "vector autoregressive model",
                "double asymptotic framework",
                "linear program"
            ],
            "author": [
                "Fang Han",
                "Huanran Lu",
                "Han Liu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/han15a/han15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Risk Bounds for Reservoir Computing",
            "abstract": [
                "We analyze the practices of reservoir computing in the framework of statistical learning theory. In particular, we derive finite sample upper bounds for the generalization error committed by specific families of reservoir computing systems when processing discrete-time inputs under various hypotheses on their dependence structure. Non-asymptotic bounds are explicitly written down in terms of the multivariate Rademacher complexities of the reservoir systems and the weak dependence structure of the signals that are being handled. This allows, in particular, to determine the minimal number of observations needed in order to guarantee a prescribed estimation accuracy with high probability for a given reservoir family. At the same time, the asymptotic behavior of the devised bounds guarantees the consistency of the empirical risk minimization procedure for various hypothesis classes of reservoir functionals."
            ],
            "keywords": [
                "Reservoir computing",
                "RC",
                "echo state networks",
                "ESN",
                "state affine systems",
                "SAS",
                "random reservoirs",
                "Rademacher complexity",
                "weak dependence",
                "empirical risk minimization",
                "PAC bounds",
                "risk bounds"
            ],
            "author": [
                "Lukas Gonon"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-902/19-902.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "When Is There a Representer Theorem? Vector Versus Matrix Regularizers",
            "abstract": [
                "We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L 2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufficient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions."
            ],
            "keywords": [
                "kernel methods",
                "matrix learning",
                "minimal norm interpolation",
                "multi-task learning",
                "regularization"
            ],
            "author": [
                "Andreas Argyriou",
                "Charles A Micchelli"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/argyriou09a/argyriou09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Expectation Truncation and the Benefits of Preselection In Training Generative Models",
            "abstract": [
                "We show how a preselection of hidden variables can be used to efficiently train generative models with binary hidden variables. The approach is based on Expectation Maximization (EM) and uses an efficiently computable approximation to the sufficient statistics of a given model. The computational cost to compute the sufficient statistics is strongly reduced by selecting, for each data point, the relevant hidden causes. The approximation is applicable to a wide range of generative models and provides an interpretation of the benefits of preselection in terms of a variational EM approximation. To empirically show that the method maximizes the data likelihood, it is applied to different types of generative models including: a version of non-negative matrix factorization (NMF), a model for non-linear component extraction (MCA), and a linear generative model similar to sparse coding. The derived algorithms are applied to both artificial and realistic data, and are compared to other models in the literature. We find that the training scheme can reduce computational costs by orders of magnitude and allows for a reliable extraction of hidden causes."
            ],
            "keywords": [
                "maximum likelihood",
                "deterministic approximations",
                "variational EM",
                "generative models",
                "component extraction",
                "multiple-cause models"
            ],
            "author": [
                "Jörg Lücke",
                "Julian Eggert"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/lucke10a/lucke10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Composite Binary Losses",
            "abstract": [
                "We study losses for binary classification and class probability estimation and extend the understanding of them from margin losses to general composite losses which are the composition of a proper loss with a link function. We characterise when margin losses can be proper composite losses, explicitly show how to determine a symmetric loss in full from half of one of its partial losses, introduce an intrinsic parametrisation of composite binary losses and give a complete characterisation of the relationship between proper losses and \"classification calibrated\" losses. We also consider the question of the \"best\" surrogate binary loss. We introduce a precise notion of \"best\" and show there exist situations where two convex surrogate losses are incommensurable. We provide a complete explicit characterisation of the convexity of composite binary losses in terms of the link function and the weight function associated with the proper loss which make up the composite loss. This characterisation suggests new ways of \"surrogate tuning\" as well as providing an explicit characterisation of when Bregman divergences on the unit interval are convex in their second argument. Finally, in an appendix we present some new algorithm-independent results on the relationship between properness, convexity and robustness to misclassification noise for binary losses and show that all convex proper losses are non-robust to misclassification noise."
            ],
            "keywords": [
                "surrogate loss",
                "convexity",
                "probability estimation",
                "classification",
                "Fisher consistency",
                "classification-calibrated",
                "regret bound",
                "proper scoring rule",
                "Bregman divergence",
                "robustness",
                "misclassification noise"
            ],
            "author": [
                "Mark D Reid",
                "Robert C Williamson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/reid10a/reid10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Approximate Bayesian Inference for Outlier Detection under Informative Sampling",
            "abstract": [
                "Government surveys of business establishments receive a large volume of submissions where a small subset contain errors. Analysts need a fast-computing algorithm to flag this subset due to a short time window between collection and reporting. We offer a computationallyscalable optimization method based on non-parametric mixtures of hierarchical Dirichlet processes that allows discovery of multiple industry-indexed local partitions linked to a set of global cluster centers. Outliers are nominated as those clusters containing few observations. We extend an existing approach with a new \"merge\" step that reduces sensitivity to hyperparameter settings. Survey data are typically acquired under an informative sampling design where the probability of inclusion depends on the surveyed response such that the distribution for the observed sample is different from the population. We extend the derivation of a penalized objective function to use a pseudo-posterior that incorporates sampling weights that \"undo\" the informative design. We provide a simulation study to demonstrate that our approach produces unbiased estimation for the outlying cluster under informative sampling. The method is applied for outlier nomination for the Current Employment Statistics survey conducted by the Bureau of Labor Statistics."
            ],
            "keywords": [
                "survey sampling",
                "hierarchical dirichlet process",
                "clustering",
                "bayesian hierarchical models",
                "optimization 2"
            ],
            "author": [
                "Terrance D Savitsky"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-088/15-088.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Latent Simplex Position Model: High Dimensional Multi-view Clustering with Uncertainty Quantification",
            "abstract": [
                "High dimensional data often contain multiple facets, and several clustering patterns can co-exist under different variable subspaces, also known as the views. While multi-view clustering algorithms were proposed, the uncertainty quantification remains difficult-a particular challenge is in the high complexity of estimating the cluster assignment probability under each view, and sharing information among views. In this article, we propose an approximate Bayes approach-treating the similarity matrices generated over the views as rough first-stage estimates for the co-assignment probabilities; in its Kullback-Leibler neighborhood, we obtain a refined low-rank matrix, formed by the pairwise product of simplex coordinates. Interestingly, each simplex coordinate directly encodes the cluster assignment uncertainty. For multi-view clustering, we let each view draw a parameterization from a few candidates, leading to dimension reduction. With high model flexibility, the estimation can be efficiently carried out as a continuous optimization problem, hence enjoys gradient-based computation. The theory establishes the connection of this model to a random partition distribution under multiple views. Compared to single-view clustering approaches, substantially more interpretable results are obtained when clustering brains from a human traumatic brain injury study, using high-dimensional gene expression data."
            ],
            "keywords": [
                "Co-regularized Clustering",
                "Consensus",
                "PAC-Bayes",
                "Random Cluster Graph",
                "Variable Selection"
            ],
            "author": [
                "Leo L Duan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-239/19-239.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonuniformity of P-values Can Occur Early in Diverging Dimensions",
            "abstract": [
                "Evaluating the joint significance of covariates is of fundamental importance in a wide range of applications. To this end, p-values are frequently employed and produced by algorithms that are powered by classical large-sample asymptotic theory. It is well known that the conventional p-values in Gaussian linear model are valid even when the dimensionality is a non-vanishing fraction of the sample size, but can break down when the design matrix becomes singular in higher dimensions or when the error distribution deviates from Gaussianity. A natural question is when the conventional p-values in generalized linear models become invalid in diverging dimensions. We establish that such a breakdown can occur early in nonlinear models. Our theoretical characterizations are confirmed by simulation studies."
            ],
            "keywords": [
                "Nonuniformity",
                "p-value",
                "breakdown point",
                "generalized linear model",
                "high dimensionality",
                "joint significance testing"
            ],
            "author": [
                "Yingying Fan",
                "Emre Demirkaya",
                "Jinchi Lv"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-314/18-314.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Natural Evolution Strategies",
            "abstract": [
                "This paper presents Natural Evolution Strategies (NES), a recent family of black-box optimization algorithms that use the natural gradient to update a parameterized search distribution in the direction of higher expected fitness. We introduce a collection of techniques that address issues of convergence, robustness, sample complexity, computational complexity and sensitivity to hyperparameters. This paper explores a number of implementations of the NES family, such as general-purpose multi-variate normal distributions and separable distributions tailored towards search in high dimensional spaces. Experimental results show best published performance on various standard benchmarks, as well as competitive performance on others."
            ],
            "keywords": [
                "natural gradient",
                "stochastic search",
                "evolution strategies",
                "black-box optimization",
                "sampling"
            ],
            "author": [
                "Daan Wierstra",
                "Tom Schaul",
                "Yi Sun",
                "Jan Peters",
                "Jürgen Schmidhuber"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/wierstra14a/wierstra14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Factor Graphs in Polynomial Time and Sample Complexity",
            "abstract": [
                "We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion. 1"
            ],
            "keywords": [
                "probabilistic graphical models",
                "parameter and structure learning",
                "factor graphs",
                "Markov networks",
                "Bayesian networks"
            ],
            "author": [
                "Pieter Abbeel",
                "Daphne Koller",
                "Andrew Y Ng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/abbeel06a/abbeel06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning with Decision Lists of Data-Dependent Features",
            "abstract": [
                "We present a learning algorithm for decision lists which allows features that are constructed from the data and allows a trade-off between accuracy and complexity. We provide bounds on the generalization error of this learning algorithm in terms of the number of errors and the size of the classifier it finds on the training data. We also compare its performance on some natural data sets with the set covering machine and the support vector machine. Furthermore, we show that the proposed bounds on the generalization error provide effective guides for model selection."
            ],
            "keywords": [
                "decision list machines",
                "set covering machines",
                "sparsity",
                "data-dependent features",
                "sample compression",
                "model selection",
                "learning theory"
            ],
            "author": [
                "Mario Marchand",
                "Manfred K Warmuth"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/marchand05a/marchand05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stability of Randomized Learning Algorithms",
            "abstract": [
                "We extend existing theory on stability, namely how much changes in the training data influence the estimated models, and generalization performance of deterministic learning algorithms to the case of randomized algorithms. We give formal definitions of stability for randomized algorithms and prove non-asymptotic bounds on the difference between the empirical and expected error as well as the leave-one-out and expected error of such algorithms that depend on their random stability. The setup we develop for this purpose can be also used for generally studying randomized learning algorithms. We then use these general results to study the effects of bagging on the stability of a learning method and to prove non-asymptotic bounds on the predictive performance of bagging which have not been possible to prove with the existing theory of stability for deterministic learning algorithms."
            ],
            "keywords": [
                "stability",
                "randomized learning algorithms",
                "sensitivity analysis",
                "bagging",
                "bootstrap methods",
                "generalization error",
                "leave-one-out error"
            ],
            "author": [
                "Andre Elisseeff"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/elisseeff05a/elisseeff05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence",
            "abstract": [
                "Data in the form of pairwise comparisons arises in many domains, including preference elicitation, sporting competitions, and peer grading among others. We consider parametric ordinal models for such pairwise comparison data involving a latent vector w * ∈ R d that represents the \"qualities\" of the d items being compared; this class of models includes the two most widely used parametric models-the Bradley-Terry-Luce (BTL) and the Thurstone models. Working within a standard minimax framework, we provide tight upper and lower bounds on the optimal error in estimating the quality score vector w * under this class of models. The bounds depend on the topology of the comparison graph induced by the subset of pairs being compared, via the spectrum of the Laplacian of the comparison graph. Thus, in settings where the subset of pairs may be chosen, our results provide principled guidelines for making this choice. Finally, we compare these error rates to those under cardinal measurement models and show that the error rates in the ordinal and cardinal settings have identical scalings apart from constant pre-factors."
            ],
            "keywords": [
                "Pairwise comparisons",
                "graph",
                "topology",
                "ranking",
                "crowdsourcing"
            ],
            "author": [
                "Nihar B Shah",
                "Sivaraman Balakrishnan",
                "Joseph Bradley",
                "Martin J Wainwright"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-189/15-189.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Distributionally Ambiguous Optimization for Batch Bayesian Optimization",
            "abstract": [
                "We propose a novel, theoretically-grounded, acquisition function for Batch Bayesian Optimization informed by insights from distributionally ambiguous optimization. Our acquisition function is a lower bound on the well-known Expected Improvement function, which requires evaluation of a Gaussian expectation over a multivariate piecewise affine function. Our bound is computed instead by evaluating the best-case expectation over all probability distributions consistent with the same mean and variance as the original Gaussian distribution. Unlike alternative approaches, including Expected Improvement, our proposed acquisition function avoids multi-dimensional integrations entirely, and can be computed exactly-even on large batch sizes-as the solution of a tractable convex optimization problem. Our suggested acquisition function can also be optimized efficiently, since first and second derivative information can be calculated inexpensively as by-products of the acquisition function calculation itself. We derive various novel theorems that ground our work theoretically and we demonstrate superior performance via simple motivating examples, benchmark functions and real-world problems."
            ],
            "keywords": [
                "Bayesian Optimization Special Issue Bayesian Optimization",
                "Convex Optimization",
                "Distributionally Robust Optimization",
                "Batch Optimization",
                "Black-Box Optimization"
            ],
            "author": [
                "Nikitas Rontsis",
                "Michael A Osborne",
                "Paul J Goulart"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-211/18-211.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Simultaneous Clustering and Estimation of Heterogeneous Graphical Models",
            "abstract": [
                "We consider joint estimation of multiple graphical models arising from heterogeneous and high-dimensional observations. Unlike most previous approaches which assume that the cluster structure is given in advance, an appealing feature of our method is to learn cluster structure while estimating heterogeneous graphical models. This is achieved via a high dimensional version of Expectation Conditional Maximization (ECM) algorithm (Meng and Rubin, 1993). A joint graphical lasso penalty is imposed on the conditional maximization step to extract both homogeneity and heterogeneity components across all clusters. Our algorithm is computationally efficient due to fast sparse learning routines and can be implemented without unsupervised learning knowledge. The superior performance of our method is demonstrated by extensive experiments and its application to a Glioblastoma cancer dataset reveals some new insights in understanding the Glioblastoma cancer. In theory, a non-asymptotic error bound is established for the output directly from our high dimensional ECM algorithm, and it consists of two quantities: statistical error (statistical accuracy) and optimization error (computational complexity). Such a result gives a theoretical guideline in terminating our ECM iterations."
            ],
            "keywords": [
                "Clustering",
                "finite-sample analysis",
                "graphical models",
                "high-dimensional statistics",
                "non-convex optimization"
            ],
            "author": [
                "Botao Hao",
                "Will Wei Sun",
                "Yufeng Liu",
                "Guang Cheng"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-019/17-019.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Existence and Uniqueness of Proper Scoring Rules",
            "abstract": [
                "To discuss the existence and uniqueness of proper scoring rules one needs to extend the associated entropy functions as sublinear functions to the conic hull of the prediction set. In some natural function spaces, such as the Lebesgue L p-spaces over R d , the positive cones have empty interior. Entropy functions defined on such cones have directional derivatives only, which typically exist on large subspaces and behave similarly to gradients. Certain entropies may be further extended continuously to open cones in normed spaces containing signed densities. The extended entropies are Gâteaux differentiable except on a negligible set and have everywhere continuous subgradients due to the supporting hyperplane theorem. We introduce the necessary framework from analysis and algebra that allows us to give an affirmative answer to the titular question of the paper. As a result of this, we give a formal sense in which entropy functions have uniquely associated proper scoring rules. We illustrate our framework by studying the derivatives and subgradients of the following three prototypical entropies: Shannon entropy, Hyvärinen entropy, and quadratic entropy."
            ],
            "keywords": [
                "proper scoring rules",
                "entropy",
                "characterisation",
                "existence",
                "uniqueness",
                "quasiinterior",
                "directional derivative",
                "Gâteaux derivative",
                "subgradient",
                "sublinear",
                "convex analysis"
            ],
            "author": [
                "Evgeni Y Ovcharov"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/ovcharov15a/ovcharov15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Online matrix factorization for Markovian data and applications to Network Dictionary Learning",
            "abstract": [
                "Online Matrix Factorization (OMF) is a fundamental tool for dictionary learning problems, giving an approximate representation of complex data sets in terms of a reduced number of extracted features. Convergence guarantees for most of the OMF algorithms in the literature assume independence between data matrices, and the case of dependent data streams remains largely unexplored. In this paper, we show that a non-convex generalization of the well-known OMF algorithm for i.i.d. stream of data in (Mairal et al., 2010) converges almost surely to the set of critical points of the expected loss function, even when the data matrices are functions of some underlying Markov chain satisfying a mild mixing condition. This allows one to extract features more efficiently from dependent data streams, as there is no need to subsample the data sequence to approximately satisfy the independence assumption. As the main application, by combining online non-negative matrix factorization and a recent MCMC algorithm for sampling motifs from networks, we propose a novel framework of Network Dictionary Learning, which extracts \"network dictionary patches\" from a given network in an online manner that encodes main features of the network. We demonstrate this technique and its application to network denoising problems on real-world network data."
            ],
            "keywords": [
                "Online matrix factorization",
                "convergence analysis",
                "Markovian data",
                "dictionary learning",
                "non-negative matrix factorization",
                "networks"
            ],
            "author": [
                "Hanbaek Lyu",
                "Deanna Needell",
                "Laura Balzano"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/20-444/20-444.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Convergence of Distributed Approximate Newton Methods: Globalization, Sharper Bounds and Beyond",
            "abstract": [
                "The DANE algorithm is an approximate Newton method popularly used for communicationefficient distributed machine learning. Reasons for the interest in DANE include scalability and efficiency. Convergence of DANE, however, can be tricky; its appealing convergence rate is only rigorous for quadratic objective function, and for more general convex functions the known results are no stronger than those of the classic first-order methods. To remedy these drawbacks, we propose in this article some new alternatives of DANE which are more suitable for analysis. We first introduce a simple variant of DANE equipped with backtracking line search, for which global asymptotic convergence and sharper local nonasymptotic convergence guarantees can be proved for both quadratic and non-quadratic strongly convex functions. Then we propose a heavy-ball method to accelerate the convergence of DANE, showing that the near-tight local rate of convergence can be established for strongly convex functions, and with proper modification of the algorithm about the same result applies globally to linear prediction models. Numerical evidence is provided to confirm the theoretical and practical advantages of our methods."
            ],
            "keywords": [
                "Communication-efficient distributed learning",
                "Approximate Newton method",
                "Global convergence",
                "Heavy-Ball acceleration"
            ],
            "author": [
                "Xiao-Tong Yuan",
                "Ping Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-764/19-764.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Cluster Elastic Net for Multivariate Regression",
            "abstract": [
                "We propose a method for simultaneously estimating regression coefficients and clustering response variables in a multivariate regression model, to increase prediction accuracy and give insights into the relationship between response variables. The estimates of the regression coefficients and clusters are found by using a penalized likelihood estimator, which includes a cluster fusion penalty, to shrink the difference in fitted values from responses in the same cluster, and an L 1 penalty for simultaneous variable selection and estimation. We propose a two-step algorithm, that iterates between k-means clustering and solving the penalized likelihood function assuming the clusters are known, which has desirable parallel computational properties obtained by using the cluster fusion penalty. If the response variable clusters are known a priori then the algorithm reduces to just solving the penalized likelihood problem. Theoretical results are presented for the penalized least squares case, including asymptotic results allowing for p n. We extend our method to the setting where the responses are binomial variables. We propose a coordinate descent algorithm for the normal likelihood and a proximal gradient descent algorithm for the binomial likelihood, which can easily be extended to other generalized linear model (GLM) settings. Simulations and data examples from business operations and genomics are presented to show the merits of both the least squares and binomial methods."
            ],
            "keywords": [
                "Multivariate Regression",
                "Clustering",
                "Fusion Penalty"
            ],
            "author": [
                "Bradley S Price",
                "Ben Sherwood"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-445/17-445.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large-scale Linear Support Vector Regression",
            "abstract": [
                "Support vector regression (SVR) and support vector classification (SVC) are popular learning techniques, but their use with kernels is often time consuming. Recently, linear SVC without kernels has been shown to give competitive accuracy for some applications, but enjoys much faster training/testing. However, few studies have focused on linear SVR. In this paper, we extend state-of-theart training methods for linear SVC to linear SVR. We show that the extension is straightforward for some methods, but is not trivial for some others. Our experiments demonstrate that for some problems, the proposed linear-SVR training methods can very efficiently produce models that are as good as kernel SVR."
            ],
            "keywords": [
                "support vector regression",
                "Newton methods",
                "coordinate descent methods"
            ],
            "author": [
                "Chia-Hua Ho",
                "Chih-Jen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/ho12a/ho12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Error-Correcting Ouput Codes Library",
            "abstract": [
                "In this paper, we present an open source Error-Correcting Output Codes (ECOC) library. The ECOC framework is a powerful tool to deal with multi-class categorization problems. This library contains both state-of-the-art coding (one-versus-one, one-versus-all, dense random, sparse random, DECOC, forest-ECOC, and ECOC-ONE) and decoding designs (hamming, euclidean, inverse hamming, laplacian, β-density, attenuated, loss-based, probabilistic kernel-based, and lossweighted) with the parameters defined by the authors, as well as the option to include your own coding, decoding, and base classifier."
            ],
            "keywords": [
                "error-correcting output codes",
                "multi-class classification",
                "coding",
                "decoding",
                "open source",
                "matlab",
                "octave"
            ],
            "author": [
                "Sergio Escalera"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/escalera10a/escalera10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Conditional Random Field with High-order Dependencies for Sequence Labeling and Segmentation",
            "abstract": [
                "Dependencies among neighboring labels in a sequence are important sources of information for sequence labeling and segmentation. However, only first-order dependencies, which are dependencies between adjacent labels or segments, are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account. In this paper, we give efficient inference algorithms to handle high-order dependencies between labels or segments in conditional random fields, under the assumption that the number of distinct label patterns used in the features is small. This leads to efficient learning algorithms for these conditional random fields. We show experimentally that exploiting high-order dependencies can lead to substantial performance improvements for some problems, and we discuss conditions under which high-order features can be effective."
            ],
            "keywords": [
                "conditional random field",
                "semi-Markov conditional random field",
                "high-order feature",
                "sequence labeling",
                "segmentation",
                "label sparsity"
            ],
            "author": [
                "Viet Nguyen",
                "Nan Ye",
                "Hai Leong Chieu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/cuong14a/cuong14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Asymptotic Model Selection for Naive Bayesian Networks",
            "abstract": [
                "We develop a closed form asymptotic formula to compute the marginal likelihood of data given a naive Bayesian network model with two hidden states and binary features. This formula deviates from the standard BIC score. Our work provides a concrete example that the BIC score is generally incorrect for statistical models that belong to stratified exponential families. This claim stands in contrast to linear and curved exponential families, where the BIC score has been proven to provide a correct asymptotic approximation for the marginal likelihood."
            ],
            "keywords": [
                "Bayesian networks",
                "asymptotic model selection",
                "Bayesian information criterion (BIC)"
            ],
            "author": [
                "Dmitry Rusakov",
                "Dan Geiger"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume6/rusakov05a/rusakov05a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Divide-and-Conquer for Debiased l 1 -norm Support Vector Machine in Ultra-high Dimensions",
            "abstract": [
                "1-norm support vector machine (SVM) generally has competitive performance compared to standard 2-norm support vector machine in classification problems, with the advantage of automatically selecting relevant features. We propose a divide-and-conquer approach in the large sample size and high-dimensional setting by splitting the data set across multiple machines, and then averaging the debiased estimators. Extension of existing theoretical studies to SVM is challenging in estimation of the inverse Hessian matrix that requires approximating the Dirac delta function via smoothing. We show that under appropriate conditions the aggregated estimator can obtain the same convergence rate as the central estimator utilizing all observations."
            ],
            "keywords": [
                "classification",
                "debiased estimator",
                "distributed estimator",
                "divide and conquer",
                "sparsity"
            ],
            "author": [
                "Heng Lian"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-343/17-343.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Adaptive Randomized Dimension Reduction on Massive Data",
            "abstract": [
                "The scalability of statistical estimators is of increasing importance in modern applications. One approach to implementing scalable algorithms is to compress data into a low dimensional latent space using dimension reduction methods. In this paper, we develop an approach for dimension reduction that exploits the assumption of low rank structure in high dimensional data to gain both computational and statistical advantages. We adapt recent randomized low-rank approximation algorithms to provide an efficient solution to principal component analysis (PCA), and we use this efficient solver to improve estimation in large-scale linear mixed models (LMM) for association mapping in statistical genomics. A key observation in this paper is that randomization serves a dual role, improving both computational and statistical performance by implicitly regularizing the covariance matrix estimate of the random effect in an LMM. These statistical and computational advantages are highlighted in our experiments on simulated data and large-scale genomic studies."
            ],
            "keywords": [
                "dimension reduction",
                "generalized eigendecompositon",
                "low-rank",
                "genomics",
                "linear mixed models",
                "supervised",
                "random projections",
                "randomized algorithms",
                "Krylov subspace methods"
            ],
            "author": [
                "Gregory Darnell",
                "Sayan Mukherjee",
                "Barbara E Engelhardt"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-143/15-143.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hilbert Space Embeddings and Metrics on Probability Measures",
            "abstract": [
                "nature of the topology induced by γ k , we relate γ k to other popular metrics on probability measures, and present conditions on the kernel k under which γ k metrizes the weak topology."
            ],
            "keywords": [
                "probability metrics",
                "homogeneity tests",
                "independence tests",
                "kernel methods",
                "universal kernels",
                "characteristic kernels",
                "Hilbertian metric",
                "weak topology"
            ],
            "author": [
                "BHARATHSV@UCSD Bharath K Sriperumbudur",
                "Arthur Gretton",
                "Gert R G Lanckriet",
                "Arthur Sriperumbudur",
                "Kenji Gretton",
                "Bernhard Fukumizu",
                "Gert R G Schölkopf",
                "GRETTON, FUKUMIZU, SCHÖLKOPF AND LANCKRIET Sriperumbudur"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Pycobra: A Python Toolbox for Ensemble Learning and Visualisation",
            "abstract": [
                "We introduce pycobra, a Python library devoted to ensemble learning (regression and classification) and visualisation. Its main assets are the implementation of several ensemble learning algorithms, a flexible and generic interface to compare and blend any existing machine learning algorithm available in Python libraries (as long as a predict method is given), and visualisation tools such as Voronoi tessellations."
            ],
            "keywords": [
                "ensemble methods",
                "machine learning",
                "Voronoi tesselation",
                "Python",
                "open source software"
            ],
            "author": [
                "Benjamin Guedj"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-228/17-228.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dirichlet Process Mixtures of Generalized Linear Models",
            "abstract": [
                "We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM), a new class of methods for nonparametric regression. Given a data set of input-response pairs, the DP-GLM produces a global model of the joint distribution through a mixture of local generalized linear models. DP-GLMs allow both continuous and categorical inputs, and can model the same class of responses that can be modeled with a generalized linear model. We study the properties of the DP-GLM, and show why it provides better predictions and density estimates than existing Dirichlet process mixture regression models. We give conditions for weak consistency of the joint distribution and pointwise consistency of the regression estimate."
            ],
            "keywords": [
                "Bayesian nonparametrics",
                "generalized linear models",
                "posterior consistency"
            ],
            "author": [
                "Lauren A Hannah",
                "David M Blei",
                "Warren B Powell"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/hannah11a/hannah11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Dependent Multi-output Gaussian Process Dynamical Systems",
            "abstract": [
                "This paper presents a dependent multi-output Gaussian process (GP) for modeling complex dynamical systems. The outputs are dependent in this model, which is largely different from previous GP dynamical systems. We adopt convolved multi-output GPs to model the outputs, which are provided with a flexible multi-output covariance function. We adapt the variational inference method with inducing points for learning the model. Conjugate gradient based optimization is used to solve parameters involved by maximizing the variational lower bound of the marginal likelihood. The proposed model has superiority on modeling dynamical systems under the more reasonable assumption and the fully Bayesian learning framework. Further, it can be flexibly extended to handle regression problems. We evaluate the model on both synthetic and real-world data including motion capture data, traffic flow data and robot inverse dynamics data. Various evaluation methods are taken on the experiments to demonstrate the effectiveness of our model, and encouraging results are observed."
            ],
            "keywords": [
                "Gaussian process",
                "variational inference",
                "dynamical system",
                "multi-output modeling"
            ],
            "author": [
                "Jing Zhao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-423/14-423.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Empirical Risk Minimization in the Non-interactive Local Model of Differential Privacy",
            "abstract": [
                "In this paper, we study the Empirical Risk Minimization (ERM) problem in the noninteractive Local Differential Privacy (LDP) model. Previous research on this problem (Smith et al., 2017) indicates that the sample complexity, to achieve error α, needs to be exponentially depending on the dimensionality p for general loss functions. In this paper, we make two attempts to resolve this issue by investigating conditions on the loss functions that allow us to remove such a limit. In our first attempt, we show that if the loss function is (∞, T)-smooth, by using the Bernstein polynomial approximation we can avoid the exponential dependency in the term of α. We then propose player-efficient algorithms with 1-bit communication complexity and O(1) computation cost for each player. The error bound of these algorithms is asymptotically the same as the original one. With some additional assumptions, we also give an algorithm which is more efficient for the server. In our second attempt, we show that for any 1-Lipschitz generalized linear convex loss function, there is an (, δ)-LDP algorithm whose sample complexity for achieving error α is only linear in the dimensionality p. Our results use a polynomial of inner product approximation technique. Finally, motivated by the idea of using polynomial approximation and based on different types of polynomial approximations, we propose (efficient) non-interactive locally differentially private algorithms for learning the set of k-way marginal queries and the set of smooth queries."
            ],
            "keywords": [
                "Differential Privacy",
                "Empirical Risk Minimization",
                "Local Differential Privacy",
                "Round Complexity",
                "Convex Learning"
            ],
            "author": [
                "Di Wang",
                "Marco Gaboardi",
                "Adam Smith",
                "Jinhui Xu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-253/19-253.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Covariances, Robustness, and Variational Bayes",
            "abstract": [
                "Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior inference technique that is increasingly popular due to its fast runtimes on large-scale data sets. However, even when MFVB provides accurate posterior means for certain parameters, it often mis-estimates variances and covariances. Furthermore, prior robustness measures have remained undeveloped for MFVB. By deriving a simple formula for the effect of infinitesimal model perturbations on MFVB posterior means, we provide both improved covariance estimates and local robustness measures for MFVB, thus greatly expanding the practical usefulness of MFVB posterior approximations. The estimates for MFVB posterior covariances rely on a result from the classical Bayesian robustness literature that relates derivatives of posterior expectations to posterior covariances and includes the Laplace approximation as a special case. Our key condition is that the MFVB approximation provides good estimates of a select subset of posterior means-an assumption that has been shown to hold in many practical settings. In our experiments, we demonstrate that our methods are simple, general, and fast, providing accurate posterior uncertainty estimates and robustness measures with runtimes that can be an order of magnitude faster than MCMC."
            ],
            "keywords": [
                "Variational Bayes",
                "Bayesian robustness",
                "Mean field approximation",
                "Linear response theory",
                "Laplace approximation",
                "Automatic differentiation"
            ],
            "author": [
                "Ryan Giordano",
                "Tamara Broderick",
                "Michael I Jordan",
                "Mohammad Emtiyaz Khan"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-670/17-670.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations",
            "abstract": [
                "The success of deep convolutional architectures is often attributed in part to their ability to learn multiscale and invariant representations of natural signals. However, a precise study of these properties and how they affect learning guarantees is still missing. In this paper, we consider deep convolutional representations of signals; we study their invariance to translations and to more general groups of transformations, their stability to the action of diffeomorphisms, and their ability to preserve signal information. This analysis is carried by introducing a multilayer kernel based on convolutional kernel networks and by studying the geometry induced by the kernel mapping. We then characterize the corresponding reproducing kernel Hilbert space (RKHS), showing that it contains a large class of convolutional neural networks with homogeneous activation functions. This analysis allows us to separate data representation from learning, and to provide a canonical measure of model complexity, the RKHS norm, which controls both stability and generalization of any learned model. In addition to models in the constructed RKHS, our stability analysis also applies to convolutional networks with generic activations such as rectified linear units, and we discuss its relationship with recent generalization bounds based on spectral norms."
            ],
            "keywords": [
                "invariant representations",
                "deep learning",
                "stability",
                "kernel methods"
            ],
            "author": [
                "Alberto Bietti"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-190/18-190.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convergence Rate of a Simulated Annealing Algorithm with Noisy Observations",
            "abstract": [
                "In this paper we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem. More precisely, we address the problem of finding a global minimizer of a function with noisy evaluations. We provide a rate of convergence and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1. This work is completed with a set of numerical experimentations and assesses the practical performance both on benchmark test cases and on real world examples."
            ],
            "keywords": [
                "Simulated Annealing",
                "Stochastic Optimization",
                "Markov Process",
                "Convergence Rate",
                "Aircraft Trajectory Optimization"
            ],
            "author": [
                "Clément Bouttier",
                "Ioana Gavra"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/16-588/16-588.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly",
            "abstract": [
                "Bayesian Optimisation (BO) refers to a suite of techniques for global optimisation of expensive black box functions, which use introspective Bayesian models of the function to efficiently search for the optimum. While BO has been applied successfully in many applications, modern optimisation tasks usher in new challenges where conventional methods fail spectacularly. In this work, we present Dragonfly, an open source Python library for scalable and robust BO. Dragonfly incorporates multiple recently developed methods that allow BO to be applied in challenging real world settings; these include better methods for handling higher dimensional domains, methods for handling multi-fidelity evaluations when cheap approximations of an expensive function are available, methods for optimising over structured combinatorial spaces, such as the space of neural network architectures, and methods for handling parallel evaluations. Additionally, we develop new methodological improvements in BO for selecting the Bayesian model, selecting the acquisition function, and optimising over complex domains with different variable types and additional constraints. We compare Dragonfly to a suite of other packages and algorithms for global optimisation and demonstrate that when the above methods are integrated, they enable significant improvements in the performance of BO. The Dragonfly library is available at dragonfly.github.io."
            ],
            "keywords": [],
            "author": [
                "Kirthevasan Kandasamy",
                "Willie Neiswanger",
                "Christopher R Collins",
                "Jeff Schneider",
                "Eric P Xing",
                "Karun Raju Vysyraju",
                "Biswajit Paria"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-223/18-223.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Nonparametric Guidance of Autoencoder Representations using Label Information",
            "abstract": [
                "While unsupervised learning has long been useful for density modeling, exploratory data analysis and visualization, it has become increasingly important for discovering features that will later be used for discriminative tasks. Discriminative algorithms often work best with highly-informative features; remarkably, such features can often be learned without the labels. One particularly effective way to perform such unsupervised learning has been to use autoencoder neural networks, which find latent representations that are constrained but nevertheless informative for reconstruction. However, pure unsupervised learning with autoencoders can find representations that may or may not be useful for the ultimate discriminative task. It is a continuing challenge to guide the training of an autoencoder so that it finds features which will be useful for predicting labels. Similarly, we often have a priori information regarding what statistical variation will be irrelevant to the ultimate discriminative task, and we would like to be able to use this for guidance as well. Although a typical strategy would be to include a parametric discriminative model as part of the autoencoder training, here we propose a nonparametric approach that uses a Gaussian process to guide the representation. By using a nonparametric model, we can ensure that a useful discriminative function exists for a given set of features, without explicitly instantiating it. We demonstrate the superiority of this guidance mechanism on four data sets, including a real-world application to rehabilitation research. We also show how our proposed approach can learn to explicitly ignore statistically significant covariate information that is label-irrelevant, by evaluating on the small NORB image recognition problem in which pose and lighting labels are available."
            ],
            "keywords": [
                "autoencoder",
                "gaussian process",
                "gaussian process latent variable model",
                "representation learning",
                "unsupervised learning"
            ],
            "author": [
                "Jasper Snoek",
                "Ryan P Adams",
                "Hugo Larochelle"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/snoek12a/snoek12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "ADMMBO: Bayesian Optimization with Unknown Constraints using ADMM",
            "abstract": [
                "There exist many problems in science and engineering that involve optimization of an unknown or partially unknown objective function. Recently, Bayesian Optimization (BO) has emerged as a powerful tool for solving optimization problems whose objective functions are only available as a black box and are expensive to evaluate. Many practical problems, however, involve optimization of an unknown objective function subject to unknown constraints. This is an important yet challenging problem for which, unlike optimizing an unknown function, existing methods face several limitations. In this paper, we present a novel constrained Bayesian optimization framework to optimize an unknown objective function subject to unknown constraints. We introduce an equivalent optimization by augmenting the objective function with constraints, introducing auxiliary variables for each constraint, and forcing the new variables to be equal to the main variable. Building on the Alternating Direction Method of Multipliers (ADMM) algorithm, we propose ADMM-Bayesian Optimization (ADMMBO) to solve the problem in an iterative fashion. Our framework leads to multiple unconstrained subproblems with unknown objective functions, which we then solve via BO. Our method resolves several challenges of state-of-the-art techniques: it can start from infeasible points, is insensitive to initialization, can efficiently handle 'decoupled problems' and has a concrete stopping criterion. Extensive experiments on a number of challenging BO benchmark problems show that our proposed approach outperforms the state-of-the-art methods in terms of the speed of obtaining a feasible solution and convergence to the global optimum as well as minimizing the number of total evaluations of unknown objective and constraints functions."
            ],
            "keywords": [
                "Bayesian Optimization Special Issue Ariafar",
                "Coll-Font",
                "Brooks",
                "Dy Bayesian Optimization",
                "Gaussian Processes",
                "ADMM",
                "Expected Improvement"
            ],
            "author": [
                "Setareh Ariafar",
                "Dana Brooks",
                "Jennifer Dy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-227/18-227.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "fastFM: A Library for Factorization Machines",
            "abstract": [
                "Factorization Machines (FM) are currently only used in a narrow range of applications and are not yet part of the standard machine learning toolbox, despite their great success in collaborative filtering and click-through rate prediction. However, Factorization Machines are a general model to deal with sparse and high dimensional features. Our Factorization Machine implementation (fastFM) provides easy access to many solvers and supports regression, classification and ranking tasks. Such an implementation simplifies the use of FM for a wide range of applications. Therefore, our implementation has the potential to improve understanding of the FM model and drive new development."
            ],
            "keywords": [
                "Python",
                "MCMC",
                "matrix factorization",
                "context-aware recommendation"
            ],
            "author": [
                "Immanuel Bayer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-355/15-355.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Handling Missing Values when Applying Classification Models",
            "abstract": [
                "Much work has studied the effect of different treatments of missing values on model induction, but little work has analyzed treatments for the common case of missing values at prediction time. This paper first compares several different methods-predictive value imputation, the distributionbased imputation used by C4.5, and using reduced models-for applying classification trees to instances with missing values (and also shows evidence that the results generalize to bagged trees and to logistic regression). The results show that for the two most popular treatments, each is preferable under different conditions. Strikingly the reduced-models approach, seldom mentioned or used, consistently outperforms the other two methods, sometimes by a large margin. The lack of attention to reduced modeling may be due in part to its (perceived) expense in terms of computation or storage. Therefore, we then introduce and evaluate alternative, hybrid approaches that allow users to balance between more accurate but computationally expensive reduced modeling and the other, less accurate but less computationally expensive treatments. The results show that the hybrid methods can scale gracefully to the amount of investment in computation/storage, and that they outperform imputation even for small investments."
            ],
            "keywords": [
                "missing data",
                "classification",
                "classification trees",
                "decision trees",
                "imputation"
            ],
            "author": [
                "Maytal Saar-Tsechansky",
                "Foster Provost"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/saar-tsechansky07a/saar-tsechansky07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On the Estimation of the Gradient Lines of a Density and the Consistency of the Mean-Shift Algorithm",
            "abstract": [
                "We consider the problem of estimating the gradient lines of a density, which can be used to cluster points sampled from that density, for example via the mean-shift algorithm of Fukunaga and Hostetler (1975). We prove general convergence bounds that we then specialize to kernel density estimation."
            ],
            "keywords": [
                "mean-shift",
                "gradient lines",
                "density estimation",
                "nonparametric clustering"
            ],
            "author": [
                "Ery Arias-Castro",
                "David Mason",
                "Bruno Pelletier"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/ariascastro16a/ariascastro16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Recovering PCA and Sparse PCA via Hybrid-( 1 , 2 ) Sparse Sampling of Data Elements",
            "abstract": [
                "This paper addresses how well we can recover a data matrix when only given a few of its elements. We present a randomized algorithm that element-wise sparsifies the data, retaining only a few of its entries. Our new algorithm independently samples the data using probabilities that depend on both squares (2 sampling) and absolute values (1 sampling) of the entries. We prove that this hybrid algorithm (i) achieves a near-PCA reconstruction of the data, and (ii) recovers sparse principal components of the data, from a sketch formed by a sublinear sample size. Hybrid-(1 , 2) inherits the 2-ability to sample the important elements, as well as the regularization properties of 1 sampling, and maintains strictly better quality than either 1 or 2 on their own. Extensive experimental results on synthetic, image, text, biological, and financial data show that not only are we able to recover PCA and sparse PCA from incomplete data, but we can speed up such computations significantly using our sparse sketch ."
            ],
            "keywords": [
                "element-wise sampling",
                "sparse representation",
                "pca",
                "sparse pca",
                "hybrid-( 1",
                "2 )"
            ],
            "author": [
                "Abhisek Kundu",
                "Malik Magdon-Ismail"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-258/16-258.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Bayesian Entropy Estimation for Countable Discrete Distributions",
            "abstract": [
                "We consider the problem of estimating Shannon's entropy H from discrete data, in cases where the number of possible symbols is unknown or even countably infinite. The Pitman-Yor process, a generalization of Dirichlet process, provides a tractable prior distribution over the space of countably infinite discrete distributions, and has found major applications in Bayesian non-parametric statistics and machine learning. Here we show that it provides a natural family of priors for Bayesian entropy estimation, due to the fact that moments of the induced posterior distribution over H can be computed analytically. We derive formulas for the posterior mean (Bayes' least squares estimate) and variance under Dirichlet and Pitman-Yor process priors. Moreover, we show that a fixed Dirichlet or Pitman-Yor process prior implies a narrow prior distribution over H, meaning the prior strongly determines the entropy estimate in the under-sampled regime. We derive a family of continuous measures for mixing Pitman-Yor processes to produce an approximately flat prior over H. We show that the resulting \"Pitman-Yor Mixture\" (PYM) entropy estimator is consistent for a large class of distributions. Finally, we explore the theoretical properties of the resulting estimator, and show that it performs well both in simulation and in application to real data."
            ],
            "keywords": [
                "entropy",
                "information theory",
                "Bayesian estimation",
                "Bayesian nonparametrics",
                "Dirichlet process",
                "Pitman-Yor process",
                "neural coding"
            ],
            "author": [
                "Evan Archer",
                "Il Memming Park",
                "Jonathan W Pillow"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/archer14a/archer14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robust PCA by Manifold Optimization",
            "abstract": [
                "Robust PCA is a widely used statistical procedure to recover an underlying low-rank matrix with grossly corrupted observations. This work considers the problem of robust PCA as a nonconvex optimization problem on the manifold of low-rank matrices and proposes two algorithms based on manifold optimization. It is shown that, with a properly designed initialization, the proposed algorithms are guaranteed to converge to the underlying lowrank matrix linearly. Compared with a previous work based on the factorization of low-rank matrices Yi et al. (2016), the proposed algorithms reduce the dependence on the condition number of the underlying low-rank matrix theoretically. Simulations and real data examples confirm the competitive performance of our method."
            ],
            "keywords": [
                "principal component analysis",
                "low-rank modeling",
                "manifold of low-rank matrices"
            ],
            "author": [
                "Teng Zhang",
                "Yi Yang"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-473/17-473.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "MinReg: A Scalable Algorithm for Learning Parsimonious Regulatory Networks in Yeast and Mammals",
            "abstract": [
                "In recent years, there has been a growing interest in applying Bayesian networks and their extensions to reconstruct regulatory networks from gene expression data. Since the gene expression domain involves a large number of variables and a limited number of samples, it poses both computational and statistical challenges to Bayesian network learning algorithms. Here we define a constrained family of Bayesian network structures suitable for this domain and devise an efficient search algorithm that utilizes these structural constraints to find high scoring networks from data. Interestingly, under reasonable assumptions on the underlying probability distribution, we can provide performance guarantees on our algorithm. Evaluation on real data from yeast and mouse, demonstrates that our method cannot only reconstruct a high quality model of the yeast regulatory network, but is also the first method to scale to the complexity of mammalian networks and successfully reconstructs a reasonable model over thousands of variables."
            ],
            "keywords": [
                "Bayesian networks",
                "structure learning",
                "gene networks",
                "gene expression",
                "approximation algorithms"
            ],
            "author": [
                "Dana Pe'er",
                "Amos Tanay"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/peer06a/peer06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "New Results for Random Walk Learning",
            "abstract": [
                "In a very strong positive result for passive learning algorithms, Bshouty et al. showed that DNF expressions are efficiently learnable in the uniform random walk model. It is natural to ask whether the more expressive class of thresholds of parities (TOP) can also be learned efficiently in this model, since both DNF and TOP are efficiently uniform-learnable from queries. However, the time bounds of the algorithms of Bshouty et al. are exponential for TOP. We present a new approach to weak parity learning that leads to quasi-efficient uniform random walk learnability of TOP. We also introduce a more general random walk model and give two positive results in this new model: DNF is efficiently learnable and juntas are efficiently agnostically learnable."
            ],
            "keywords": [
                "computational learning theory",
                "Fourier analysis of Boolean functions",
                "random walks",
                "DNF learning",
                "TOP learning"
            ],
            "author": [
                "Jeffrey C Jackson",
                "Karl Wimmer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/jackson14a/jackson14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Ancestral Gumbel-Top-k Sampling for Sampling Without Replacement",
            "abstract": [
                "We develop ancestral Gumbel-Top-k sampling: a generic and efficient method for sampling without replacement from discrete-valued Bayesian networks, which includes multivariate discrete distributions, Markov chains and sequence models. The method uses an extension of the Gumbel-Max trick to sample without replacement by finding the top k of perturbed log-probabilities among all possible configurations of a Bayesian network. Despite the exponentially large domain, the algorithm has a complexity linear in the number of variables and sample size k. Our algorithm allows to set the number of parallel processors m, to trade off the number of iterations versus the total cost (iterations times m) of running the algorithm. For m = 1 the algorithm has minimum total cost, whereas for m = k the number of iterations is minimized, and the resulting algorithm is known as Stochastic Beam Search. 1 We provide extensions of the algorithm and discuss a number of related algorithms. We analyze the properties of Gumbel-Top-k sampling and compare against alternatives on randomly generated Bayesian networks with different levels of connectivity. In the context of (deep) sequence models, we show its use as a method to generate diverse but high-quality translations and statistical estimates of translation quality and entropy."
            ],
            "keywords": [
                "sampling without replacement",
                "ancestral sampling",
                "bayesian networks",
                "stochastic beam search",
                "gumbel-max trick"
            ],
            "author": [
                "Wouter Kool",
                "Max Welling"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-985/19-985.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Expected Policy Gradients for Reinforcement Learning",
            "abstract": [
                "We propose expected policy gradients (EPG), which unify stochastic policy gradients (SPG) and deterministic policy gradients (DPG) for reinforcement learning. Inspired by expected sarsa, EPG integrates (or sums) across actions when estimating the gradient, instead of relying only on the action in the sampled trajectory. For continuous action spaces, we first derive a practical result for Gaussian policies and quadratic critics and then extend it to a universal analytical method, covering a broad class of actors and critics, including Gaussian, exponential families, and policies with bounded support. For Gaussian policies, we introduce an exploration method that uses covariance proportional to e H , where H is the scaled Hessian of the critic with respect to the actions. For discrete action spaces, we derive a variant of EPG based on softmax policies. We also establish a new general policy gradient theorem, of which the stochastic and deterministic policy gradient theorems are special cases. Furthermore, we prove that EPG reduces the variance of the gradient estimates without requiring deterministic policies and with little computational overhead. Finally, we provide an extensive experimental evaluation of EPG and show that it outperforms existing approaches on multiple challenging control domains."
            ],
            "keywords": [
                "policy gradients",
                "exploration",
                "bounded actions",
                "reinforcement learning",
                "Markov decision process (MDP)"
            ],
            "author": [
                "Kamil Ciosek",
                "Shimon Whiteson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-012/18-012.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes",
            "abstract": [
                "The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximised over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximising an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from i.i.d. observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the non-linear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain or partially missing inputs. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data."
            ],
            "keywords": [
                "Gaussian processes",
                "variational inference",
                "latent variable models",
                "dynamical systems",
                "uncertain inputs"
            ],
            "author": [
                "Andreas C Damianou",
                "Michalis K Titsias",
                "Neil D Lawrence"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/damianou16a/damianou16a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Some Discriminant-Based PAC Algorithms",
            "abstract": [
                "A classical approach in multi-class pattern classification is the following. Estimate the probability distributions that generated the observations for each label class, and then label new instances by applying the Bayes classifier to the estimated distributions. That approach provides more useful information than just a class label; it also provides estimates of the conditional distribution of class labels, in situations where there is class overlap. We would like to know whether it is harder to build accurate classifiers via this approach, than by techniques that may process all data with distinct labels together. In this paper we make that question precise by considering it in the context of PAC learnability. We propose two restrictions on the PAC learning framework that are intended to correspond with the above approach, and consider their relationship with standard PAC learning. Our main restriction of interest leads to some interesting algorithms that show that the restriction is not stronger (more restrictive) than various other well-known restrictions on PAC learning. An alternative slightly milder restriction turns out to be almost equivalent to unrestricted PAC learning."
            ],
            "keywords": [
                "computational learning theory",
                "computational complexity",
                "pattern classification"
            ],
            "author": [
                "Paul W Goldberg"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/goldberg06a/goldberg06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Robustness and Regularization of Support Vector Machines",
            "abstract": [
                "We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classification that explicitly build in protection to noise, and at the same time control overfitting. On the analysis front, the equivalence of robustness and regularization provides a robust optimization interpretation for the success of regularized SVMs. We use this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well."
            ],
            "keywords": [
                "robustness",
                "regularization",
                "generalization",
                "kernel",
                "support vector machine"
            ],
            "author": [
                "Huan Xu",
                "Constantine Caramanis",
                "Shie Mannor",
                "@ Mcgill"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/xu09b/xu09b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Beyond Trees: Classification with Sparse Pairwise Dependencies",
            "abstract": [
                "Several classification methods assume that the underlying distributions follow tree-structured graphical models. Indeed, trees capture statistical dependencies between pairs of variables, which may be crucial to attaining low classification errors. In this setting, the optimal classifier is linear in the log-transformed univariate and bivariate densities that correspond to the tree edges. In practice, observed data may not be well approximated by trees. Yet, motivated by the importance of pairwise dependencies for accurate classification, here we propose to approximate the optimal decision boundary by a sparse linear combination of the univariate and bivariate log-transformed densities. Our proposed approach is semi-parametric in nature: we non-parametrically estimate the univariate and bivariate densities, remove pairs of variables that are nearly independent using the Hilbert-Schmidt independence criterion, and finally construct a linear SVM using the retained log-transformed densities. We demonstrate on synthetic and real data sets, that our classifier, named SLB (sparse log-bivariate density), is competitive with other popular classification methods."
            ],
            "keywords": [
                "binary classification",
                "graphical model",
                "Bayesian network",
                "sparsity",
                "semi-parametric"
            ],
            "author": [
                "Yaniv Tenzer",
                "Mary Frances Dorn",
                "Boaz Nadler",
                "Clifford Spiegelman",
                "Amit Moscovich"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/18-348/18-348.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Separating Models of Learning from Correlated and Uncorrelated Data",
            "abstract": [
                "We consider a natural framework of learning from correlated data, in which successive examples used for learning are generated according to a random walk over the space of possible examples. A recent paper by Bshouty et al. (2003) shows that the class of polynomial-size DNF formulas is efficiently learnable in this random walk model; this result suggests that the Random Walk model is more powerful than comparable standard models of learning from independent examples, in which similarly efficient DNF learning algorithms are not known. We give strong evidence that the Random Walk model is indeed more powerful than the standard model, by showing that if any cryptographic one-way function exists (a universally held belief in cryptography), then there is a class of functions that can be learned efficiently in the Random Walk setting but not in the standard setting where all examples are independent."
            ],
            "keywords": [
                "random walks",
                "uniform distribution learning",
                "cryptographic hardness",
                "correlated data",
                "PAC learning"
            ],
            "author": [
                "Ariel Elbaz",
                "Rocco A Servedio",
                "Andrew Wan",
                "Peter Auer",
                "Ron Meir"
            ],
            "ref": "http://www.jmlr.org/papers/volume8/elbaz07a/elbaz07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Double Updating Online Learning",
            "abstract": [
                "In most kernel based online learning algorithms, when an incoming instance is misclassified, it will be added into the pool of support vectors and assigned with a weight, which often remains unchanged during the rest of the learning process. This is clearly insufficient since when a new support vector is added, we generally expect the weights of the other existing support vectors to be updated in order to reflect the influence of the added support vector. In this paper, we propose a new online learning method, termed Double Updating Online Learning, or DUOL for short, that explicitly addresses this problem. Instead of only assigning a fixed weight to the misclassified example received at the current trial, the proposed online learning algorithm also tries to update the weight for one of the existing support vectors. We show that the mistake bound can be improved by the proposed online learning method. We conduct an extensive set of empirical evaluations for both binary and multi-class online learning tasks. The experimental results show that the proposed technique is considerably more effective than the state-of-the-art online learning algorithms. The source code is available to public at http://www.cais.ntu.edu.sg/˜chhoi/DUOL/."
            ],
            "keywords": [
                "online learning",
                "kernel method",
                "support vector machines",
                "maximum margin learning",
                "classification"
            ],
            "author": [
                "Peilin Zhao",
                "Steven C H Hoi"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/zhao11a/zhao11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Classification Module for Genetic Programming Algorithms in JCLEC",
            "abstract": [
                "JCLEC-Classification is a usable and extensible open source library for genetic programming classification algorithms. It houses implementations of rule-based methods for classification based on genetic programming, supporting multiple model representations and providing to users the tools to implement any classifier easily. The software is written in Java and it is available from http://jclec.sourceforge.net/classification under the GPL license."
            ],
            "keywords": [
                "classification",
                "evolutionary algorithms",
                "genetic programming",
                "JCLEC"
            ],
            "author": [
                "Alberto Cano",
                "José María Luna",
                "Amelia Zafra",
                "Sebastián Ventura"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume16/cano15a/cano15a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning",
            "abstract": [
                "We show a Talagrand-type concentration inequality for Multi-Task Learning (MTL), with which we establish sharp excess risk bounds for MTL in terms of the Local Rademacher Complexity (LRC). We also give a new bound on the LRC for any norm regularized hypothesis classes, which applies not only to MTL, but also to the standard Single-Task Learning (STL) setting. By combining both results, one can easily derive fast-rate bounds on the excess risk for many prominent MTL methods, including-as we demonstrate-Schatten norm, group norm, and graph regularized MTL. The derived bounds reflect a relationship akin to a conservation law of asymptotic convergence rates. When compared to the rates obtained via a traditional, global Rademacher analysis, this very relationship allows for trading off slower rates with respect to the number of tasks for faster rates with respect to the number of available samples per task."
            ],
            "keywords": [
                "Excess Risk Bounds",
                "Local Rademacher Complexity",
                "Multi-task Learning"
            ],
            "author": [
                "Niloofar Yousefi",
                "Yunwen Lei",
                "Marius Kloft",
                "Mansooreh Mollaghasemi",
                "Georgios C Anagnostopoulos"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/17-144/17-144.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Approximating the Permanent with Fractional Belief Propagation",
            "abstract": [
                "We discuss schemes for exact and approximate computations of permanents, and compare them with each other. Specifically, we analyze the belief propagation (BP) approach and its fractional belief propagation (FBP) generalization for computing the permanent of a non-negative matrix. Known bounds and Conjectures are verified in experiments, and some new theoretical relations, bounds and Conjectures are proposed. The fractional free energy (FFE) function is parameterized by a scalar parameter γ ∈ [−1; 1], where γ = −1 corresponds to the BP limit and γ = 1 corresponds to the exclusion principle (but ignoring perfect matching constraints) mean-field (MF) limit. FFE shows monotonicity and continuity with respect to γ. For every non-negative matrix, we define its special value γ * ∈ [−1; 0] to be the γ for which the minimum of the γ-parameterized FFE function is equal to the permanent of the matrix, where the lower and upper bounds of the γ-interval corresponds to respective bounds for the permanent. Our experimental analysis suggests that the distribution of γ * varies for different ensembles but γ * always lies within the [−1; −1/2] interval. Moreover, for all ensembles considered, the behavior of γ * is highly distinctive, offering an empirical practical guidance for estimating permanents of non-negative matrices via the FFE approach."
            ],
            "keywords": [
                "permanent",
                "graphical models",
                "belief propagation",
                "exact and approximate algorithms",
                "learning flows"
            ],
            "author": [
                "Michael Chertkov",
                "Adam B Yedidia"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/chertkov13a/chertkov13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Dimension Estimation Using Random Connection Models",
            "abstract": [
                "Information about intrinsic dimension is crucial to perform dimensionality reduction, compress information, design efficient algorithms, and do statistical adaptation. In this paper we propose an estimator for the intrinsic dimension of a data set. The estimator is based on binary neighbourhood information about the observations in the form of two adjacency matrices, and does not require any explicit distance information. The underlying graph is modelled according to a subset of a specific random connection model, sometimes referred to as the Poisson blob model. Computationally the estimator scales like n log n, and we specify its asymptotic distribution and rate of convergence. A simulation study on both real and simulated data shows that our approach compares favourably with some competing methods from the literature, including approaches that rely on distance information."
            ],
            "keywords": [
                "adaptation",
                "dimensionality reduction",
                "intrinsic dimension",
                "random connection model",
                "random graph"
            ],
            "author": [
                "Paulo Serra",
                "Michel Mandjes"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-232/16-232.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Scalable Approximations for Generalized Linear Problems",
            "abstract": [
                "In stochastic optimization, the population risk is generally approximated by the empirical risk which is in turn minimized by an iterative algorithm. However, in the large-scale setting, empirical risk minimization may be computationally restrictive. In this paper, we design an efficient algorithm to approximate the population risk minimizer in generalized linear problems such as binary classification with surrogate losses and generalized linear regression models. We focus on large-scale problems where the iterative minimization of the empirical risk is computationally intractable, i.e., the number of observations n is much larger than the dimension of the parameter p (n p 1). We show that under random sub-Gaussian design, the true minimizer of the population risk is approximately proportional to the corresponding ordinary least squares (OLS) estimator. Using this relation, we design an algorithm that achieves the same accuracy as the empirical risk minimizer through iterations that attain up to a quadratic convergence rate, and that are computationally cheaper than any batch optimization algorithm by at least a factor of O(p). We provide theoretical guarantees for our algorithm, and analyze the convergence behavior in terms of data dimensions. Finally, we demonstrate the performance of our algorithm on well-known classification and regression problems, through extensive numerical studies on large-scale datasets, and show that it achieves the highest performance compared to several other widely used optimization algorithms."
            ],
            "keywords": [
                "Generalized Linear Problems",
                "Stochastic optimization",
                "Subsampling",
                "Dimension reduction in optimization"
            ],
            "author": [
                "Murat A Erdogdu",
                "Mohsen Bayati",
                "Lee H Dicker"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-279/17-279.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Theory for Distribution Regression",
            "abstract": [
                "We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax op"
            ],
            "keywords": [
                "Two-Stage Sampled Distribution Regression",
                "Kernel Ridge Regression",
                "Mean Embedding",
                "Multi-Instance Learning",
                "Minimax Optimality"
            ],
            "author": [
                "Zoltán Szabó",
                "Barnabás Póczos",
                "Arthur Gretton"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-510/14-510.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Generalization from Observed to Unobserved Features by Clustering",
            "abstract": [
                "We argue that when objects are characterized by many attributes, clustering them on the basis of a random subset of these attributes can capture information on the unobserved attributes as well. Moreover, we show that under mild technical conditions, clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set. We prove finite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting. We use our framework to analyze generalization to unobserved features of two well-known clustering algorithms: k-means and the maximum likelihood multinomial mixture model. The scheme is demonstrated for collaborative filtering of users with movie ratings as attributes and document clustering with words as attributes."
            ],
            "keywords": [
                "clustering",
                "unobserved features",
                "learning theory",
                "generalization in clustering",
                "information bottleneck"
            ],
            "author": [
                "Eyal Krupka",
                "Naftali Tishby"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/krupka08a/krupka08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Community Detection and Stochastic Block Models: Recent Developments",
            "abstract": [
                "The stochastic block model (SBM) is a random graph model with planted clusters. It is widely employed as a canonical model to study clustering and community detection, and provides generally a fertile ground to study the statistical and computational tradeoffs that arise in network and data sciences. This note surveys the recent developments that establish the fundamental limits for community detection in the SBM, both with respect to information-theoretic and computational thresholds, and for various recovery requirements such as exact, partial and weak recovery (a.k.a., detection). The main results discussed are the phase transitions for exact recovery at the Chernoff-Hellinger threshold, the phase transition for weak recovery at the Kesten-Stigum threshold, the optimal distortion-SNR tradeoff for partial recovery, the learning of the SBM parameters and the gap between information-theoretic and computational thresholds. The note also covers some of the algorithms developed in the quest of achieving the limits, in particular two-round algorithms via graph-splitting, semi-definite programming, linearized belief propagation, classical and nonbacktracking spectral methods. A few open problems are also discussed."
            ],
            "keywords": [
                "Community detection",
                "clustering",
                "stochastic block models",
                "random graphs",
                "unsupervised learning",
                "spectral algorithms",
                "computational gaps",
                "network data analysis"
            ],
            "author": [
                "Emmanuel Abbe"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-480/16-480.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient Inference for Nonparametric Hawkes Processes Using Auxiliary Latent Variables",
            "abstract": [
                "The expressive ability of classic Hawkes processes is limited due to the parametric assumption on the baseline intensity and triggering kernel. Therefore, it is desirable to perform inference in a data-driven, nonparametric approach. Many recent works have proposed nonparametric Hawkes process models based on Gaussian processes (GP). However, the likelihood is non-conjugate to the prior resulting in a complicated and time-consuming inference procedure. To address the problem, we present the sigmoid Gaussian Hawkes process model in this paper: the baseline intensity and triggering kernel are both modeled as the sigmoid transformation of random trajectories drawn from a GP. By introducing auxiliary latent random variables (branching structure, Pólya-Gamma random variables and latent marked Poisson processes), the likelihood is converted to two decoupled components with a Gaussian form which allows for an efficient conjugate analytical inference. Using the augmented likelihood, we derive an efficient Gibbs sampling algorithm to sample from the posterior; an efficient expectation-maximization (EM) algorithm to obtain the maximum a posteriori (MAP) estimate and furthermore an efficient mean-field variational inference algorithm to approximate the posterior. To further accelerate the inference, a sparse GP approximation is introduced to reduce complexity. We demonstrate the performance of our three algorithms on both simulated and real data. The experiments show that our proposed inference algorithms can recover well the underlying prompting characteristics efficiently."
            ],
            "keywords": [
                "Hawkes process",
                "Gaussian process",
                "Pólya-Gamma distribution",
                "conjugacy"
            ],
            "author": [
                "Feng Zhou",
                "Zhidong Li",
                "Yang Wang",
                "Arcot Sowmya",
                "Fang Chen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-930/19-930.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bayesian Framework for Learning Rule Sets for Interpretable Classification",
            "abstract": [
                "We present a machine learning algorithm for building classifiers that are comprised of a small number of short rules. These are restricted disjunctive normal form models. An example of a classifier of this form is as follows: If X satisfies (condition A AND condition B) OR (condition C) OR • • • , then Y = 1. Models of this form have the advantage of being interpretable to human experts since they produce a set of rules that concisely describe a specific class. We present two probabilistic models with prior parameters that the user can set to encourage the model to have a desired size and shape, to conform with a domain-specific definition of interpretability. We provide a scalable MAP inference approach and develop theoretical bounds to reduce computation by iteratively pruning the search space. We apply our method (Bayesian Rule Sets-BRS) to characterize and predict user behavior with respect to in-vehicle context-aware personalized recommender systems. Our method has a major advantage over classical associative classification methods and decision trees in that it does not greedily grow the model."
            ],
            "keywords": [
                "disjunctive normal form",
                "statistical learning",
                "data mining",
                "association rules",
                "interpretable classifier",
                "Bayesian modeling"
            ],
            "author": [
                "Tong Wang",
                "Cynthia Rudin",
                "Yimin Liu",
                "Edward Jones",
                "Perry Macneille"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-003/16-003.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Convex and Network Flow Optimization for Structured Sparsity",
            "abstract": [
                "We consider a class of learning problems regularized by a structured sparsity-inducing norm defined as the sum of ℓ 2-or ℓ ∞-norms over groups of variables. Whereas much effort has been put in developing fast optimization techniques when the groups are disjoint or embedded in a hierarchy, we address here the case of general overlapping groups. To this end, we present two different strategies: On the one hand, we show that the proximal operator associated with a sum of ℓ ∞norms can be computed exactly in polynomial time by solving a quadratic min-cost flow problem, allowing the use of accelerated proximal gradient methods. On the other hand, we use proximal splitting techniques, and address an equivalent formulation with non-overlapping groups, but in higher dimension and with additional constraints. We propose efficient and scalable algorithms exploiting these two strategies, which are significantly faster than alternative approaches. We illustrate these methods with several problems such as CUR matrix factorization, multi-task learning of tree-structured dictionaries, background subtraction in video sequences, image denoising with wavelets, and topographic dictionary learning of natural image patches."
            ],
            "keywords": [
                "convex optimization",
                "proximal methods",
                "sparse coding",
                "structured sparsity",
                "matrix factorization",
                "network flow optimization",
                "alternating direction method of multipliers"
            ],
            "author": [
                "Julien Mairal",
                "Stat Berkeley Edu",
                "Rodolphe Jenatton",
                "Guillaume Obozinski",
                "Francis Bach",
                "Julien ©2011",
                "Rodolphe Mairal",
                "Guillaume Jenatton",
                "Francis Obozinski"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/mairal11a/mairal11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence",
            "abstract": [
                "The problems of dimension reduction and inference of statistical dependence are addressed by the modeling framework of learning gradients. The models we propose hold for Euclidean spaces as well as the manifold setting. The central quantity in this approach is an estimate of the gradient of the regression or classification function. Two quadratic forms are constructed from gradient estimates: the gradient outer product and gradient based diffusion maps. The first quantity can be used for supervised dimension reduction on manifolds as well as inference of a graphical model encoding dependencies that are predictive of a response variable. The second quantity can be used for nonlinear projections that incorporate both the geometric structure of the manifold as well as variation of the response variable on the manifold. We relate the gradient outer product to standard statistical quantities such as covariances and provide a simple and precise comparison of a variety of supervised dimensionality reduction methods. We provide rates of convergence for both inference of informative directions as well as inference of a graphical model of variable dependencies."
            ],
            "keywords": [
                "gradient estimates",
                "manifold learning",
                "graphical models",
                "inverse regression",
                "dimension reduction",
                "gradient diffusion maps"
            ],
            "author": [
                "Qiang Wu",
                "Justin Guinney",
                "Sage Bionetworks",
                "Mauro Maggioni",
                "Sayan Mukherjee"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/wu10a/wu10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Large Scale Online Learning of Image Similarity Through Ranking",
            "abstract": [
                "Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or finding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, the approaches that exist today for learning such semantic similarity do not scale to large data sets. This is both because typically their CPU and storage requirements grow quadratically with the sample size, and because many methods impose complex positivity constraints on the space of learned similarity functions. The current paper presents OASIS, an Online Algorithm for Scalable Image Similarity learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efficient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a data set with thousands of images, it achieves better results than existing state-of-the-art methods, while being an order of magnitude faster. For large, web scale, data sets, OASIS can be trained on more than two million images from 150K text queries within 3 days on a single CPU. On this large scale data set, human evaluations showed that 35% of the ten nearest neighbors of a given test image, as found by OASIS, were semantically relevant to that image. This suggests that query independent similarity could be accurately learned even for large scale data sets that could not be handled before."
            ],
            "keywords": [
                "large scale",
                "metric learning",
                "image similarity",
                "online learning"
            ],
            "author": [
                "Gal Chechik",
                "Varun Sharma",
                "Uri Shalit",
                "Samy Bengio",
                "* Varun Sharma"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/chechik10a/chechik10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Consistency of the Group Lasso and Multiple Kernel Learning",
            "abstract": [
                "We consider the least-square regression problem with regularization by a block 1-norm, that is, a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1-norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic group selection consistency of the group Lasso. We derive necessary and sufficient conditions for the consistency of group Lasso under practical assumptions, such as model misspecification. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this infinite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisfied."
            ],
            "keywords": [
                "sparsity",
                "regularization",
                "consistency",
                "convex optimization",
                "covariance operators"
            ],
            "author": [
                "Francis R Bach"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/bach08b/bach08b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Statistical Comparisons of Classifiers over Multiple Data Sets",
            "abstract": [
                "While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams."
            ],
            "keywords": [
                "comparative studies",
                "statistical methods",
                "Wilcoxon signed ranks test",
                "Friedman test",
                "multiple comparisons tests"
            ],
            "author": [
                "Janez Demšar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/demsar06a/demsar06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Subgroup Analysis via Recursive Partitioning",
            "abstract": [
                "Subgroup analysis is an integral part of comparative analysis where assessing the treatment effect on a response is of central interest. Its goal is to determine the heterogeneity of the treatment effect across subpopulations. In this paper, we adapt the idea of recursive partitioning and introduce an interaction tree (IT) procedure to conduct subgroup analysis. The IT procedure automatically facilitates a number of objectively defined subgroups, in some of which the treatment effect is found prominent while in others the treatment has a negligible or even negative effect. The standard CART (Breiman et al., 1984) methodology is inherited to construct the tree structure. Also, in order to extract factors that contribute to the heterogeneity of the treatment effect, variable importance measure is made available via random forests of the interaction trees. Both simulated experiments and analysis of census wage data are presented for illustration."
            ],
            "keywords": [
                "CART",
                "interaction",
                "subgroup analysis",
                "random forests"
            ],
            "author": [
                "Xiaogang Su",
                "Chih-Ling Tsai",
                "Hansheng Wang",
                "David M Nickerson",
                "Bogong Li"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume10/su09a/su09a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Training SVMs Without Offset",
            "abstract": [
                "We develop, analyze, and test a training algorithm for support vector machine classifiers without offset. Key features of this algorithm are a new, statistically motivated stopping criterion, new warm start options, and a set of inexpensive working set selection strategies that significantly reduce the number of iterations. For these working set strategies, we establish convergence rates that, not surprisingly, coincide with the best known rates for SVMs with offset. We further conduct various experiments that investigate both the run time behavior and the performed iterations of the new training algorithm. It turns out, that the new algorithm needs significantly less iterations and also runs substantially faster than standard training algorithms for SVMs with offset."
            ],
            "keywords": [],
            "author": [
                "Ingo Steinwart",
                "Clint Scovel"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/steinwart11a/steinwart11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Special Issue on the Eighteenth International Conference on Machine Learning (ICML2001)",
            "abstract": NaN,
            "keywords": [],
            "author": [
                "Carla E Brodley",
                "Andrea P Danyluk"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/brodley02a/brodley02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Quadratic Variance Function (QVF) DAG Models via OverDispersion Scoring (ODS)",
            "abstract": [
                "Learning DAG or Bayesian network models is an important problem in multi-variate causal inference. However, a number of challenges arises in learning large-scale DAG models including model identifiability and computational complexity since the space of directed graphs is huge. In this paper, we address these issues in a number of steps for a broad class of DAG models where the noise or variance is signal-dependent. Firstly we introduce a new class of identifiable DAG models, where each node has a distribution where the variance is a quadratic function of the mean (QVF DAG models). Our QVF DAG models include many interesting classes of distributions such as Poisson, Binomial, Geometric, Exponential, Gamma and many other distributions in which the noise variance depends on the mean. We prove that this class of QVF DAG models is identifiable, and introduce a new algorithm, the OverDispersion Scoring (ODS) algorithm, for learning large-scale QVF DAG models. Our algorithm is based on firstly learning the moralized or undirected graphical model representation of the DAG to reduce the DAG search-space, and then exploiting the quadratic variance property to learn the ordering. We show through theoretical results and simulations that our algorithm is statistically consistent in the high-dimensional p > n setting provided that the degree of the moralized graph is bounded and performs well compared to state-of-the-art DAG-learning algorithms. We also demonstrate through a real data example involving multi-variate count data, that our ODS algorithm is wellsuited to estimating DAG models for count data in comparison to other methods used for discrete data."
            ],
            "keywords": [
                "Bayesian Networks",
                "Directed Acyclic Graph",
                "Identifiability",
                "Multi-variate Count Distribution",
                "Overdispersion"
            ],
            "author": [
                "Gunwoong Park"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/17-243/17-243.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tests of Mutual or Serial Independence of Random Vectors with Applications",
            "abstract": [
                "The problem of testing mutual independence between many random vectors is addressed. The closely related problem of testing serial independence of a multivariate stationary sequence is also considered. The Möbius transformation of characteristic functions is used to characterize independence. A generalization to p vectors of distance covariance and Hilbert-Schmidt independence criterion (HSIC) tests with the translation invariant kernel of a stable probability distribution is proposed. Both test statistics can be expressed in a simple form as a sum over all elements of a componentwise product of p doubly-centered matrices. It is shown that an HSIC statistic with sufficiently small scale parameters is equivalent to a distance covariance statistic. Consistency and weak convergence of both types of statistics are established. Approximation of p-values is made by randomization tests without recomputing interpoint distances for each randomized sample. The dependogram is adapted to the proposed tests for the graphical identification of sources of dependencies. Empirical rejection rates obtained through extensive simulations confirm both the applicability of the testing procedures in small samples and the high level of competitiveness in terms of power. Applications to meteorological and financial data provide some interesting interpretations of dependencies revealed by dependograms."
            ],
            "keywords": [
                "Distance covariance",
                "Hilbert-Schmidt independence criterion",
                "Möbius transformation",
                "mutual independence",
                "serial independence"
            ],
            "author": [
                "Martin Bilodeau"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-184/16-184.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?",
            "abstract": [
                "We evaluate 179 classifiers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classifiers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearestneighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classifiers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve significant conclusions about the classifier behavior, not dependent on the data set collection. The classifiers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1% of the maximum accuracy overcoming 90% in the 84.3% of the data sets. However, the difference is not statistically significant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3% of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classifiers (3 out of 5 bests classifiers are RF), followed by SVM (4 classifiers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively)."
            ],
            "keywords": [
                "classification",
                "UCI data base",
                "random forest",
                "support vector machine",
                "neural networks",
                "decision trees",
                "ensembles",
                "rule-based classifiers",
                "discriminant analysis",
                "Bayesian classifiers",
                "generalized linear models",
                "partial least squares and principal component regression",
                "multiple adaptive regression splines",
                "nearest-neighbors",
                "logistic and multinomial regression"
            ],
            "author": [
                "Manuel Fernández-Delgado",
                "Eva Cernadas",
                "Dinani Amorim"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/delgado14a/delgado14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Covariate Shift Adaptation by Importance Weighted Cross Validation",
            "abstract": [
                "A common assumption in supervised learning is that the input points in the training set follow the same probability distribution as the input points that will be given in the future test phase. However, this assumption is not satisfied, for example, when the outside of the training region is extrapolated. The situation where the training input points and test input points follow different distributions while the conditional distribution of output values given input points is unchanged is called the covariate shift. Under the covariate shift, standard model selection techniques such as cross validation do not work as desired since its unbiasedness is no longer maintained. In this paper, we propose a new method called importance weighted cross validation (IWCV), for which we prove its unbiasedness even under the covariate shift. The IWCV procedure is the only one that can be applied for unbiased classification under covariate shift, whereas alternatives to IWCV exist for regression. The usefulness of our proposed method is illustrated by simulations, and furthermore demonstrated in the brain-computer interface, where strong non-stationarity effects can be seen between training and test sessions."
            ],
            "keywords": [
                "covariate shift",
                "cross validation",
                "importance sampling",
                "extrapolation",
                "brain-computer interface"
            ],
            "author": [
                "Masashi Sugiyama",
                "Matthias Krauledat",
                "Klaus- Robert Müller"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume8/sugiyama07a/sugiyama07a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Identifiability of Additive Noise Models Using Conditional Variances",
            "abstract": [
                "This paper considers a new identifiability condition for additive noise models (ANMs) in which each variable is determined by an arbitrary Borel measurable function of its parents plus an independent error. It has been shown that ANMs are fully recoverable under some identifiability conditions, such as when all error variances are equal. However, this identifiable condition could be restrictive, and hence, this paper focuses on a relaxed identifiability condition that involves not only error variances, but also the influence of parents. This new class of identifiable ANMs does not put any constraints on the form of dependencies, or distributions of errors, and allows different error variances. It further provides a statistically consistent and computationally feasible structure learning algorithm for the identifiable ANMs based on the new identifiability condition. The proposed algorithm assumes that all relevant variables are observed, while it does not assume faithfulness or a sparse graph. Demonstrated through extensive simulated and real multivariate data is that the proposed algorithm successfully recovers directed acyclic graphs."
            ],
            "keywords": [
                "Bayesian Network",
                "Causal Inference",
                "Directed Acyclic Graph",
                "Identifiability",
                "Structural Equation Modeling",
                "Structure Learning"
            ],
            "author": [
                "Gunwoong Park"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-664/19-664.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives",
            "abstract": [
                "In cognitive science and neuroscience, there have been two leading models describing how humans perceive and classify facial expressions of emotion-the continuous and the categorical model. The continuous model defines each facial expression of emotion as a feature vector in a face space. This model explains, for example, how expressions of emotion can be seen at different intensities. In contrast, the categorical model consists of C classifiers, each tuned to a specific emotion category. This model explains, among other findings, why the images in a morphing sequence between a happy and a surprise face are perceived as either happy or surprise but not something in between. While the continuous model has a more difficult time justifying this latter finding, the categorical model is not as good when it comes to explaining how expressions are recognized at different intensities or modes. Most importantly, both models have problems explaining how one can recognize combinations of emotion categories such as happily surprised versus angrily surprised versus surprise. To resolve these issues, in the past several years, we have worked on a revised model that justifies the results reported in the cognitive science and neuroscience literature. This model consists of C distinct continuous spaces. Multiple (compound) emotion categories can be recognized by linearly combining these C face spaces. The dimensions of these spaces are shown to be mostly configural. According to this model, the major task for the classification of facial expressions of emotion is precise, detailed detection of facial landmarks rather than recognition. We provide an overview of the literature justifying the model, show how the resulting model can be employed to build algorithms for the recognition of facial expression of emotion, and propose research directions in machine learning and computer vision researchers to keep pushing the state of the art in these areas. We also discuss how the model can aid in studies of human perception, social interactions and disorders."
            ],
            "keywords": [
                "vision",
                "face perception",
                "emotions",
                "computational modeling",
                "categorical perception",
                "face detection"
            ],
            "author": [
                "Aleix Martinez",
                "Isabelle Guyon",
                "Vassilis Athitsos"
            ],
            "ref": "http://www.jmlr.org/papers/volume13/martinez12a/martinez12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "NIMFA : A Python Library for Nonnegative Matrix Factorization",
            "abstract": [
                "NIMFA is an open-source Python library that provides a unified interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA's component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks."
            ],
            "keywords": [
                "nonnegative matrix factorization",
                "initialization methods",
                "quality measures",
                "scripting",
                "Python"
            ],
            "author": [],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/zitnik12a/zitnik12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Tree Decomposition for Large-Scale SVM Problems",
            "abstract": [
                "To handle problems created by large data sets, we propose a method that uses a decision tree to decompose a given data space and train SVMs on the decomposed regions. Although there are other means of decomposing a data space, we show that the decision tree has several merits for large-scale SVM training. First, it can classify some data points by its own means, thereby reducing the cost of SVM training for the remaining data points. Second, it is efficient in determining the parameter values that maximize the validation accuracy, which helps maintain good test accuracy. Third, the tree decomposition method can derive a generalization error bound for the classifier. For data sets whose size can be handled by current non-linear, or kernel-based, SVM training techniques, the proposed method can speed up the training by a factor of thousands, and still achieve comparable test accuracy."
            ],
            "keywords": [
                "binary tree",
                "generalization errorïbound",
                "margin-based theory",
                "pattern classification",
                "tree decomposition",
                "support vector machine",
                "VC theory"
            ],
            "author": [
                "Fu Chang",
                "Chien-Yang Guo",
                "Xiao-Rong Lin",
                "Chi-Jen Lu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/chang10b/chang10b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Bayesian Approximation Method for Online Ranking",
            "abstract": [
                "This paper describes a Bayesian approximation method to obtain online ranking algorithms for games with multiple teams and multiple players. Recently for Internet games large online ranking systems are much needed. We consider game models in which a k-team game is treated as several two-team games. By approximating the expectation of teams' (or players') performances, we derive simple analytic update rules. These update rules, without numerical integrations, are very easy to interpret and implement. Experiments on game data show that the accuracy of our approach is competitive with state of the art systems such as TrueSkill, but the running time as well as the code is much shorter."
            ],
            "keywords": [
                "Bayesian inference",
                "rating system",
                "Bradley-Terry model",
                "Thurstone-Mosteller model",
                "Plackett-Luce model"
            ],
            "author": [
                "Ruby C Weng",
                "Chih-Jen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume12/weng11a/weng11a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Boosting with Polynomially Bounded Distributions",
            "abstract": [
                "We construct a framework which allows an algorithm to turn the distributions produced by some boosting algorithms into polynomially smooth distributions (w.r.t. the PAC oracle's distribution), with minimal performance loss. Further, we explore the case of Freund and Schapire's AdaBoost algorithm, bounding its distributions to polynomially smooth. The main advantage of AdaBoost over other boosting techniques is that it is adaptive, i.e., it is able to take advantage of weak hypotheses that are more accurate than it was assumed a priori. We show that the feature of adaptiveness is preserved and improved by our technique. Our scheme allows the execution of AdaBoost in the on-line boosting mode (i.e., to perform boosting \"by filtering\"). Executed this way (and possessing the quality of smoothness), now AdaBoost may be efficiently applied to a wider range of learning problems than before. In particular, we demonstrate AdaBoost's application to the task of DNF learning using membership queries. This application results in an algorithm that chooses the number of boosting iterations adaptively, and, consequently, adaptively chooses the size of the produced final hypothesis. This answers affirmatively a question posed by Jackson."
            ],
            "keywords": [],
            "author": [
                "Nader H Bshouty",
                "Dmitry Gavinsky"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/bshouty02b/bshouty02b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Structure Learning in Bayesian Networks of a Moderate Size by Efficient Sampling Ru He",
            "abstract": [
                "We study the Bayesian model averaging approach to learning Bayesian network structures (DAGs) from data. We develop new algorithms including the first algorithm that is able to efficiently sample DAGs of a moderate size (with up to about 25 variables) according to the exact structure posterior. The DAG samples can then be used to construct estimators for the posterior of any feature. We theoretically prove good properties of our estimators and empirically show that our estimators considerably outperform the estimators from the previous state-of-the-art methods."
            ],
            "keywords": [
                "Bayesian model averaging",
                "Bayesian networks",
                "DAG sampling",
                "dynamic programming",
                "order sampling",
                "structure learning"
            ],
            "author": [
                "Hrheru @ Gmail",
                "Jin Tian",
                "Huaiqing Wu"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/14-497/14-497.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Minimax Manifold Estimation",
            "abstract": [
                "We find the minimax rate of convergence in Hausdorff distance for estimating a manifold M of dimension d embedded in R D given a noisy sample from the manifold. Under certain conditions, we show that the optimal rate of convergence is n −2/(2+d). Thus, the minimax rate depends only on the dimension of the manifold, not on the dimension of the space in which M is embedded."
            ],
            "keywords": [],
            "author": [
                "Christopher R Genovese",
                "Marco Perone-Pacifico",
                "Isabella Verdinelli",
                "Larry Wasserman"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/genovese12a/genovese12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Rotation Test to Verify Latent Structure",
            "abstract": [
                "In multivariate regression models we have the opportunity to look for hidden structure unrelated to the observed predictors. However, when one fits a model involving such latent variables it is important to be able to tell if the structure is real, or just an artifact of correlation in the regression errors. We develop a new statistical test based on random rotations for verifying the existence of latent variables. The rotations are carefully constructed to rotate orthogonally to the column space of the regression model. We find that only non-Gaussian latent variables are detectable, a finding that parallels a well known phenomenon in independent components analysis. We base our test on a measure of non-Gaussianity in the histogram of the principal eigenvector components instead of on the eigenvalue. The method finds and verifies some latent dichotomies in the microarray data from the AGEMAP consortium."
            ],
            "keywords": [
                "independent components analysis",
                "Kronecker covariance",
                "latent variables",
                "projection pursuit",
                "transposable data"
            ],
            "author": [
                "Patrick O Perry",
                "Art B Owen",
                "Owen @ Stanford Stat"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/perry10a/perry10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "GURLS: A Least Squares Library for Supervised Learning",
            "abstract": [
                "We present GURLS, a least squares, modular, easy-to-extend software library for efficient supervised learning. GURLS is targeted to machine learning practitioners, as well as non-specialists. It offers a number state-of-the-art training strategies for medium and large-scale learning, and routines for efficient model selection. The library is particularly well suited for multi-output problems (multi-category/multi-label). GURLS is currently available in two independent implementations: Matlab and C++. It takes advantage of the favorable properties of regularized least squares algorithm to exploit advanced tools in linear algebra. Routines to handle computations with very large matrices by means of memory-mapped storage and distributed task execution are available."
            ],
            "keywords": [
                "regularized least squares",
                "big data",
                "linear algebra"
            ],
            "author": [
                "Andrea Tacchetti",
                "Pavan K Mallapragada",
                "Matteo Santoro",
                "Lorenzo Rosasco",
                "C 2013"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/tacchetti13a/tacchetti13a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "POMDPs.jl: A Framework for Sequential Decision Making under Uncertainty",
            "abstract": [
                "POMDPs.jl is an open-source framework for solving Markov decision processes (MDPs) and partially observable MDPs (POMDPs). POMDPs.jl allows users to specify sequential decision making problems with minimal effort without sacrificing the expressive nature of POMDPs, making this framework viable for both educational and research purposes. It is written in the Julia language to allow flexible prototyping and large-scale computation that leverages the high-performance nature of the language. The associated JuliaPOMDP community also provides a number of state-of-the-art MDP and POMDP solvers and a rich library of support tools to help with implementing new solvers and evaluating the solution results. The most recent version of POMDPs.jl, the related packages, and documentation can be found at https://github.com/JuliaPOMDP/POMDPs.jl."
            ],
            "keywords": [
                "POMDP",
                "MDP",
                "sequential decision making",
                "Julia",
                "open-source"
            ],
            "author": [
                "Maxim Egorov",
                "Zachary N Sunberg",
                "Edward Balaban",
                "Tim A Wheeler",
                "Jayesh K Gupta",
                "Mykel J Kochenderfer"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-300/16-300.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Rate of Convergence of AdaBoost",
            "abstract": [
                "The AdaBoost algorithm was designed to combine many \"weak\" hypotheses that perform slightly better than random guessing into a \"strong\" hypothesis that has very low error. We study the rate at which AdaBoost iteratively converges to the minimum of the \"exponential loss.\" Unlike previous work, our proofs do not require a weak-learning assumption, nor do they require that minimizers of the exponential loss are finite. Our first result shows that the exponential loss of AdaBoost's computed parameter vector will be at most ε more than that of any parameter vector of ℓ 1-norm bounded by B in a number of rounds that is at most a polynomial in B and 1/ε. We also provide lower bounds showing that a polynomial dependence is necessary. Our second result is that within C/ε iterations, AdaBoost achieves a value of the exponential loss that is at most ε more than the best possible value, where C depends on the data set. We show that this dependence of the rate on ε is optimal up to constant factors, that is, at least Ω(1/ε) rounds are necessary to achieve within ε of the optimal exponential loss."
            ],
            "keywords": [
                "AdaBoost",
                "optimization",
                "coordinate descent",
                "convergence rate"
            ],
            "author": [
                "Indraneel Mukherjee",
                "Cynthia Rudin",
                "Robert E Schapire"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume14/mukherjee13b/mukherjee13b.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Identifiability and Consistent Estimation of Nonparametric Translation Hidden Markov Models with General State Spacé",
            "abstract": [
                "This paper considers hidden Markov models where the observations are given as the sum of a latent state which lies in a general state space and some independent noise with unknown distribution. It is shown that these fully nonparametric translation models are identifiable with respect to both the distribution of the latent variables and the distribution of the noise, under mostly a light tail assumption on the latent variables. Two nonparametric estimation methods are proposed and we prove that the corresponding estimators are consistent for the weak convergence topology. These results are illustrated with numerical experiments."
            ],
            "keywords": [
                "Nonparametric estimation",
                "latent variable models",
                "deconvolution"
            ],
            "author": [
                "Elisabeth Gassiat",
                "Sylvain Le Corff",
                "Télécom Samovar"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-802/19-802.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Efficient augmentation and relaxation learning for individualized treatment rules using observational data",
            "abstract": [
                "Individualized treatment rules aim to identify if, when, which, and to whom treatment should be applied. A globally aging population, rising healthcare costs, and increased access to patient-level data have created an urgent need for high-quality estimators of individualized treatment rules that can be applied to observational data. A recent and promising line of research for estimating individualized treatment rules recasts the problem of estimating an optimal treatment rule as a weighted classification problem. We consider a class of estimators for optimal treatment rules that are analogous to convex large-margin classifiers. The proposed class applies to observational data and is doubly-robust in the sense that correct specification of either a propensity or outcome model leads to consistent estimation of the optimal individualized treatment rule. Using techniques from semiparametric efficiency theory, we derive rates of convergence for the proposed estimators and use these rates to characterize the bias-variance trade-off for estimating individualized treatment rules with classification-based methods. Simulation experiments informed by these results demonstrate that it is possible to construct new estimators within the proposed framework that significantly outperform existing ones. We illustrate the proposed methods using data from a labor training program and a study of inflammatory bowel syndrome."
            ],
            "keywords": [
                "Individualized treatment rules",
                "convex surrogate",
                "double-robustness",
                "classification",
                "personalized medicine"
            ],
            "author": [
                "Ying-Qi Zhao",
                "Eric B Laber",
                "Yang Ning",
                "Sumona Saha",
                "Laber Zhao"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-191/18-191.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Student-t Mixture as a Natural Image Patch Prior with Application to Image Compression Aäron van den Oord",
            "abstract": [
                "Recent results have shown that Gaussian mixture models (GMMs) are remarkably good at density modeling of natural image patches, especially given their simplicity. In terms of log likelihood on real-valued data they are comparable with the best performing techniques published, easily outperforming more advanced ones, such as deep belief networks. They can be applied to various image processing tasks, such as image denoising, deblurring and inpainting, where they improve on other generic prior methods, such as sparse coding and field of experts. Based on this we propose the use of another, even richer mixture model based image prior: the Student-t mixture model (STM). We demonstrate that it convincingly surpasses GMMs in terms of log likelihood, achieving performance competitive with the state of the art in image patch modeling. We apply both the GMM and STM to the task of lossy and lossless image compression, and propose efficient coding schemes that can easily be extended to other unsupervised machine learning models. Finally, we show that the suggested techniques outperform JPEG, with results comparable to or better than JPEG 2000."
            ],
            "keywords": [
                "image compression",
                "mixture models",
                "GMM",
                "density modeling",
                "unsupervised learning"
            ],
            "author": [
                "Benjamin Schrauwen"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume15/vandenoord14a/vandenoord14a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Set Covering Machine",
            "abstract": [
                "We extend the classical algorithms of Valiant and Haussler for learning compact conjunctions and disjunctions of Boolean attributes to allow features that are constructed from the data and to allow a trade-off between accuracy and complexity. The result is a generalpurpose learning machine, suitable for practical learning tasks, that we call the set covering machine. We present a version of the set covering machine that uses data-dependent balls for its set of features and compare its performance with the support vector machine. By extending a technique pioneered by Littlestone and Warmuth, we bound its generalization error as a function of the amount of data compression it achieves during training. In experiments with real-world learning tasks, the bound is shown to be extremely tight and to provide an effective guide for model selection."
            ],
            "keywords": [
                "Set Covering Machines",
                "Computational Learning Theory",
                "Support Vector Machines",
                "Data-dependent Features",
                "Feature Selection",
                "Sample Compression",
                "Model Selection"
            ],
            "author": [
                "Mario Marchand",
                "John Shawe-Taylor",
                "Carla E Brodley",
                "Andrea Danyluk"
            ],
            "ref": "http://www.jmlr.org/papers/volume3/marchand02a/marchand02a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns",
            "abstract": [
                "In structured classification problems, there is a direct conflict between expressive models and efficient inference: while graphical models such as Markov random fields or factor graphs can represent arbitrary dependences among instance labels, the cost of inference via belief propagation in these models grows rapidly as the graph structure becomes more complicated. One important source of complexity in belief propagation is the need to marginalize large factors to compute messages. This operation takes time exponential in the number of variables in the factor, and can limit the expressiveness of the models we can use. In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efficient algorithms for computing messages from these potentials during belief propagation. We believe these new potentials provide a good balance between expressive power and efficient inference in practical structured classification problems. We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. We use these potentials to classify images of protein subcellular location patterns in groups of cells. Classifying subcellular location patterns can help us answer many important questions in computational biology, including questions about how various treatments affect the synthesis and behavior of proteins and networks of proteins within a cell. Our new representation and algorithm lead to substantial improvements in both inference speed and classification accuracy."
            ],
            "keywords": [
                "factor graphs",
                "approximate inference algorithms",
                "structured classification",
                "protein subcellular location patterns",
                "location proteomics"
            ],
            "author": [
                "Shann-Ching Chen",
                "Geoffrey J Gordon",
                "Robert F Murphy"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume9/chen08a/chen08a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "New Insights and Perspectives on the Natural Gradient Method",
            "abstract": [
                "Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of 2nd-order optimization method, with the Fisher information matrix acting as a substitute for the Hessian. In many important cases, the Fisher information matrix is shown to be equivalent to the Generalized Gauss-Newton matrix, which both approximates the Hessian, but also has certain properties that favor its use over the Hessian. This perspective turns out to have significant implications for the design of a practical and robust natural gradient optimizer, as it motivates the use of techniques like trust regions and Tikhonov regularization. Additionally, we make a series of contributions to the understanding of natural gradient and 2nd-order methods, including: a thorough analysis of the convergence speed of stochastic natural gradient descent (and more general stochastic 2nd-order methods) as applied to convex quadratics, a critical examination of the oft-used \"empirical\" approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by natural gradient methods (which we show also holds for certain other curvature matrices, but notably not the Hessian)."
            ],
            "keywords": [
                "natural gradient methods",
                "2nd-order optimization",
                "neural networks",
                "convergence rate",
                "parameterization invariance"
            ],
            "author": [
                "James Martens"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/17-678/17-678.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "On Asymptotic and Finite-Time Optimality of Bayesian Predictors",
            "abstract": [
                "The problem is that of sequential probability forecasting for finite-valued time series. The data is generated by an unknown probability distribution over the space of all one-way infinite sequences. Two settings are considered: the realizable and the non-realizable one. Assume first that the probability measure generating the sequence belongs to a given set C (realizable case), but the latter is completely arbitrary (uncountably infinite, without any structure given). It is shown that the minimax asymptotic average loss-which may be positive-is always attainable, and it is attained by a Bayesian predictor whose prior is discrete and concentrated on C. Moreover, the finite-time loss of the Bayesian predictor is also optimal up to an additive log n term (where n is the time step). This upper bound is complemented by a lower bound that goes to infinity but may do so arbitrarily slow. Passing to the non-realizable setting, let the probability measure generating the data be arbitrary, and consider the given set C as a set of experts to compete with. The goal is to minimize the regret with respect to the experts. It is shown that in this setting it is possible that all Bayesian strategies are strictly suboptimal even asymptotically. In other words, a sublinear regret may be attainable but the regret of every Bayesian predictor is linear. A very general recommendation for choosing a model can be made based on these results: it is better to take a model large enough to make sure it includes the process that generates the data, even if it entails positive asymptotic average loss, for otherwise any combination of predictors in the model class may be useless."
            ],
            "keywords": [
                "sequence prediction",
                "Bayesian prediction",
                "complete-class theorems",
                "minimax theorems"
            ],
            "author": [
                "Daniil Ryabko"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/18-512/18-512.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "A Study of the Classification of Low-Dimensional Data with Supervised Manifold Learning",
            "abstract": [
                "Supervised manifold learning methods learn data representations by preserving the geometric structure of data while enhancing the separation between data samples from different classes. In this work, we propose a theoretical study of supervised manifold learning for classification. We consider nonlinear dimensionality reduction algorithms that yield linearly separable embeddings of training data and present generalization bounds for this type of algorithms. A necessary condition for satisfactory generalization performance is that the embedding allow the construction of a sufficiently regular interpolation function in relation with the separation margin of the embedding. We show that for supervised embeddings satisfying this condition, the classification error decays at an exponential rate with the number of training samples. Finally, we examine the separability of supervised nonlinear embeddings that aim to preserve the low-dimensional geometric structure of data based on graph representations. The proposed analysis is supported by experiments on several real data sets."
            ],
            "keywords": [
                "Manifold learning",
                "dimensionality reduction",
                "classification",
                "out-of-sample extensions",
                "RBF interpolation"
            ],
            "author": [
                "Elif Vural",
                "Christine Guillemot"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/15-373/15-373.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Posterior sampling strategies based on discretized stochastic differential equations for machine learning applications",
            "abstract": [
                "With the advent of GPU-assisted hardware and maturing high-efficiency software platforms such as TensorFlow and PyTorch, Bayesian posterior sampling for neural networks becomes plausible. In this article we discuss Bayesian parametrization in machine learning based on Markov Chain Monte Carlo methods, specifically discretized stochastic differential equations such as Langevin dynamics and extended system methods in which an ensemble of walkers is employed to enhance sampling. We provide a glimpse of the potential of the sampling-intensive approach by studying (and visualizing) the loss landscape of a neural network applied to the MNIST data set. Moreover, we investigate how the sampling efficiency itself can be significantly enhanced through an ensemble quasi-Newton preconditioning method. This article accompanies the release of a new TensorFlow software package, the Thermodynamic Analytics ToolkIt, which is used in the computational experiments."
            ],
            "keywords": [
                "Bayesian posterior sampling",
                "Markov Chain Monte Carlo",
                "Neural network training",
                "software platforms for machine learning",
                "ensemble sampling strategies"
            ],
            "author": [
                "Frederik Heber",
                "Benedict Leimkuhler"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume21/19-339/19-339.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "JSAT: Java Statistical Analysis Tool, a Library for Machine Learning",
            "abstract": [
                "Java Statistical Analysis Tool (JSAT) is a Machine Learning library written in pure Java. It works to fill a void in the Java ecosystem for a general purpose library that is relatively high performance and flexible, which is not adequately fulfilled by Weka (Hall et al., 2009) and Java-ML (Abeel et al."
            ],
            "keywords": [
                "java",
                "machine learning",
                "open source",
                "java library",
                "machine learning software"
            ],
            "author": [
                "Edward Raff"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume18/16-131/16-131.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning to Detect and Classify Malicious Executables in the Wild *",
            "abstract": [
                "We describe the use of machine learning and data mining to detect and classify malicious executables as they appear in the wild. We gathered 1, 971 benign and 1, 651 malicious executables and encoded each as a training example using n-grams of byte codes as features. Such processing resulted in more than 255 million distinct n-grams. After selecting the most relevant n-grams for prediction, we evaluated a variety of inductive methods, including naive Bayes, decision trees, support vector machines, and boosting. Ultimately, boosted decision trees outperformed other methods with an area under the ROC curve of 0.996. Results suggest that our methodology will scale to larger collections of executables. We also evaluated how well the methods classified executables based on the function of their payload, such as opening a backdoor and mass-mailing. Areas under the ROC curve for detecting payload function were in the neighborhood of 0.9, which were smaller than those for the detection task. However, we attribute this drop in performance to fewer training examples and to the challenge of obtaining properly labeled examples, rather than to a failing of the methodology or to some inherent difficulty of the classification task. Finally, we applied detectors to 291 malicious executables discovered after we gathered our original collection, and boosted decision trees achieved a true-positive rate of 0.98 for a desired false-positive rate of 0.05. This result is particularly important, for it suggests that our methodology could be used as the basis for an operational system for detecting previously undiscovered malicious executables."
            ],
            "keywords": [
                "data mining",
                "concept learning",
                "computer security",
                "invasive software"
            ],
            "author": [
                "J Zico Kolter",
                "Marcus A Maloof"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume7/kolter06a/kolter06a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Hierarchical Relative Entropy Policy Search",
            "abstract": [
                "Many reinforcement learning (RL) tasks, especially in robotics, consist of multiple sub-tasks that are strongly structured. Such task structures can be exploited by incorporating hierarchical policies that consist of gating networks and sub-policies. However, this concept has only been partially explored for real world settings and complete methods, derived from first principles, are needed. Real world settings are challenging due to large and continuous state-action spaces that are prohibitive for exhaustive sampling methods. We define the problem of learning sub-policies in continuous state action spaces as finding a hierarchical policy that is composed of a high-level gating policy to select the low-level sub-policies for execution by the agent. In order to efficiently share experience with all sub-policies, also called inter-policy learning, we treat these sub-policies as latent variables which allows for distribution of the update information between the sub-policies. We present three different variants of our algorithm, designed to be suitable for a wide variety of real world robot learning tasks and evaluate our algorithms in two real robot learning scenarios as well as several simulations and comparisons."
            ],
            "keywords": [
                "Reinforcement Learning",
                "Policy Search",
                "Hierarchical Learning",
                "Robot Learning",
                "Motor Skill Learning",
                "Robust Learning",
                "Structured Learning",
                "Temporal Correlation",
                "HiREPS",
                "REPS"
            ],
            "author": [
                "Christian Daniel",
                "Gerhard Neumann",
                "Oliver Kroemer",
                "Jan Peters"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume17/15-188/15-188.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Symbolic Representations of Hybrid Dynamical Systems",
            "abstract": [
                "A hybrid dynamical system is a mathematical model suitable for describing an extensive spectrum of multi-modal, time-series behaviors, ranging from bouncing balls to air traffic controllers. This paper describes multi-modal symbolic regression (MMSR): a learning algorithm to construct non-linear symbolic representations of discrete dynamical systems with continuous mappings from unlabeled, time-series data. MMSR consists of two subalgorithms-clustered symbolic regression, a method to simultaneously identify distinct behaviors while formulating their mathematical expressions, and transition modeling, an algorithm to infer symbolic inequalities that describe binary classification boundaries. These subalgorithms are combined to infer hybrid dynamical systems as a collection of apt, mathematical expressions. MMSR is evaluated on a collection of four synthetic data sets and outperforms other multi-modal machine learning approaches in both accuracy and interpretability, even in the presence of noise. Furthermore, the versatility of MMSR is demonstrated by identifying and inferring classical expressions of transistor modes from recorded measurements."
            ],
            "keywords": [
                "hybrid dynamical systems",
                "evolutionary computation",
                "symbolic piecewise functions",
                "symbolic binary classification"
            ],
            "author": [
                "Daniel L Ly",
                "Hod Lipson"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume13/ly12a/ly12a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Parallelizing Spectrally Regularized Kernel Algorithms",
            "abstract": [
                "We consider a distributed learning approach in supervised learning for a large class of spectral regularization methods in an reproducing kernel Hilbert space (RKHS) framework. The data set of size n is partitioned into m = O(n α), α < 1 2 , disjoint subsamples. On each subsample, some spectral regularization method (belonging to a large class, including in particular Kernel Ridge Regression, L 2-boosting and spectral cutoff) is applied. The regression function f is then estimated via simple averaging, leading to a substantial reduction in computation time. We show that minimax optimal rates of convergence are preserved if m grows sufficiently slowly (corresponding to an upper bound for α) as n → ∞, depending on the smoothness assumptions on f and the intrinsic dimensionality. In spirit, the analysis relies on a classical bias/stochastic error analysis."
            ],
            "keywords": [
                "Distributed Learning",
                "Spectral Regularization",
                "Minimax Optimality"
            ],
            "author": [
                "Nicole Mücke",
                "Gilles Blanchard"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume19/16-569/16-569.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Learning Non-Stationary Dynamic Bayesian Networks",
            "abstract": [
                "Learning dynamic Bayesian network structures provides a principled mechanism for identifying conditional dependencies in time-series data. An important assumption of traditional DBN structure learning is that the data are generated by a stationary process, an assumption that is not true in many important settings. In this paper, we introduce a new class of graphical model called a nonstationary dynamic Bayesian network, in which the conditional dependence structure of the underlying data-generation process is permitted to change over time. Non-stationary dynamic Bayesian networks represent a new framework for studying problems in which the structure of a network is evolving over time. Some examples of evolving networks are transcriptional regulatory networks during an organism's development, neural pathways during learning, and traffic patterns during the day. We define the non-stationary DBN model, present an MCMC sampling algorithm for learning the structure of the model from time-series data under different assumptions, and demonstrate the effectiveness of the algorithm on both simulated and biological data."
            ],
            "keywords": [
                "Bayesian networks",
                "graphical models",
                "model selection",
                "structure learning",
                "Monte Carlo methods"
            ],
            "author": [
                "Joshua W Robinson",
                "Alexander J Hartemink"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume11/robinson10a/robinson10a.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "The Common-directions Method for Regularized Empirical Risk Minimization",
            "abstract": [
                "State-of-the-art first-and second-order optimization methods are able to achieve either fast global linear convergence rates or quadratic convergence, but not both of them. In this work, we propose an interpolation between first-and second-order methods for regularized empirical risk minimization that exploits the problem structure to efficiently combine multiple update directions. Our method attains both optimal global linear convergence rate for first-order methods, and local quadratic convergence. Experimental results show that our method outperforms state-of-the-art first-and second-order optimization methods in terms of the number of data accesses, while is competitive in training time. 1. The requirement ρ ≥ σ comes from the conditions (3) and (4)."
            ],
            "keywords": [],
            "author": [
                "Po-Wei Wang",
                "Chih-Jen Lin"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/16-309/16-309.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        },
        {
            "title": "Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations",
            "abstract": [
                "We develop the mathematical foundations of the stochastic modified equations (SME) framework for analyzing the dynamics of stochastic gradient algorithms, where the latter is approximated by a class of stochastic differential equations with small noise parameters. We prove that this approximation can be understood mathematically as an weak approximation, which leads to a number of precise and useful results on the approximations of stochastic gradient descent (SGD), momentum SGD and stochastic Nesterov's accelerated gradient method in the general setting of stochastic objectives. We also demonstrate through explicit calculations that this continuous-time approach can uncover important analytical insights into the stochastic gradient algorithms under consideration that may not be easy to obtain in a purely discrete-time setting."
            ],
            "keywords": [
                "stochastic gradient algorithms",
                "modified equations",
                "stochastic differential equations",
                "momentum",
                "Nesterov's accelerated gradient"
            ],
            "author": [
                "Qianxiao Li",
                "Cheng Tai"
            ],
            "ref": "https://jmlr.csail.mit.edu//papers/volume20/17-526/17-526.pdf",
            "datasource": "Journal of Machine Learning Research",
            "datasource_url": "https://jmlr.csail.mit.edu"
        }
    ]
}