{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FP3_S8ULQHnl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "import gensim.parsing.preprocessing\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJjmwdz1qi7r"
   },
   "outputs": [],
   "source": [
    "input_path = '../../data/data_2021-02-01 22:27:13.862993.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtnHVf1in888",
    "outputId": "7a711e75-892f-41e5-bbc7-b810a1b81c44"
   },
   "outputs": [],
   "source": [
    "with open(input_path, encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# Keywords from every paper  \n",
    "keywords_df = pd.json_normalize(data['papers'])['keywords']\n",
    "\n",
    "print(\"No of papers: {}\".format(keywords_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjLR3ckPhmfS"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "STOPWORDS = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZyshRqDr6cf"
   },
   "outputs": [],
   "source": [
    "def process(flat_words, lemmatize=True, min_word_len=2):\n",
    "  flat_words = [gensim.parsing.preprocessing.strip_non_alphanum(word) for word in flat_words]\n",
    "  doc = spacy.tokens.Doc(nlp.vocab, words=flat_words)\n",
    "\n",
    "  processed = []\n",
    "\n",
    "  if lemmatize:\n",
    "    processed = [token.lemma_ for token in doc]\n",
    "    processed = [token.lower() for token in processed]\n",
    "  else:\n",
    "    processed = [token.lower_ for token in doc]\n",
    "\n",
    "  processed = [token for token in processed if token not in STOPWORDS]\n",
    "  processed = [token for token in processed if len(token) >= min_word_len]\n",
    "\n",
    "  return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkOFqDCdQFON"
   },
   "outputs": [],
   "source": [
    "def process_corpus(keywords_df, lemmatize=True, min_word_len=2):\n",
    "  processed_corpus = []\n",
    "  flat_words = []\n",
    "\n",
    "  # n: no of research papers\n",
    "  # [[kw11, kw12, ...] ...[kwn1, kwn2, kwn3, ...]]\n",
    "  keywords = keywords_df.tolist()\n",
    "  # j: no of document in which the keyword is present\n",
    "  # kwji -> [wji1, wij2, ...] ~ list of all words associated with keyword i of paper j\n",
    "  # [kwj1, kwj2, ...] -> [kwj11, kwj12, wkwj13, kwj21, kwj22, ...] ~ list of all words associated with the keywords of paper j\n",
    "  for l in keywords:\n",
    "    tmp = []\n",
    "    for keyword in l:\n",
    "      words = keyword.split(\" \")\n",
    "      for word in words:\n",
    "        tmp.append(word)\n",
    "    flat_words.append(tmp)\n",
    "\n",
    "  for l in flat_words:\n",
    "    processed_corpus.append(process(l, True, 2))\n",
    "\n",
    "  return processed_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXoWUpqQTRey",
    "outputId": "08437c26-6dc9-47f8-d5d2-d3cd6f908bd5"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "processed_corpus = process_corpus(keywords_df)\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(words) for words in processed_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ihcPTv8XgKv"
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# Train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iI7n_2znX-YI"
   },
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=len(dictionary.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxQbH2vLYQ37"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_similarities(processed_corpus, threshold=0.9):\n",
    "  similarities = defaultdict(list)\n",
    "\n",
    "  for query_doc_index in range(len(processed_corpus)):\n",
    "    query_doc =  processed_corpus[query_doc_index]\n",
    "    query_bow = dictionary.doc2bow(query_doc)\n",
    "    sims = index[tfidf[query_bow]]\n",
    "    for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "      if score >= threshold and document_number != query_doc_index:\n",
    "        similarities[query_doc_index].append((document_number, score))\n",
    "\n",
    "  return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kcB_-zGrKnk"
   },
   "outputs": [],
   "source": [
    "# Compute similarites between every two papers\n",
    "sims = compute_similarities(processed_corpus, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import floor\n",
    "from tqdm import tqdm\n",
    "\n",
    "avg_kws_thr = [0] * 11\n",
    "\n",
    "last_mean = [0] *  11\n",
    "\n",
    "for item in tqdm(sims.items()):\n",
    "  curr_paper_kws = processed_corpus[item[0]]\n",
    "  for paper in item[1]:\n",
    "    if paper[0] > item[0]: \n",
    "      thr = (int) (floor(paper[1] * 10))\n",
    "      paper_kws = processed_corpus[paper[0]]\n",
    "      inter = list((Counter(curr_paper_kws) & Counter(paper_kws)).elements())\n",
    "      last_mean[thr] += 1\n",
    "      # Calculate rolling mean\n",
    "      avg_kws_thr[thr] = avg_kws_thr[thr] + (len(inter) - avg_kws_thr[thr]) / last_mean[thr]\n",
    "\n",
    "for thr in range(11):\n",
    "  print(\"a 0.{}% similarity between papers coresponds to {} common words\".format(thr, avg_kws_thr[thr]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Keywords-BOW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1612914873333"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}