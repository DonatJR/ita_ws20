{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FP3_S8ULQHnl"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-36fdf6980d43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "import gensim.parsing.preprocessing\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qJjmwdz1qi7r"
   },
   "outputs": [],
   "source": [
    "input_path = '../../data/data_2021-02-01 22:27:13.862993.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtnHVf1in888",
    "outputId": "7a711e75-892f-41e5-bbc7-b810a1b81c44"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'json_normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f566d9cd2d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Keywords from every paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkeywords_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'papers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keywords'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No of papers: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mPanel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"module 'pandas' has no attribute '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'json_normalize'"
     ]
    }
   ],
   "source": [
    "with open(input_path, encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# Keywords from every paper  \n",
    "keywords_df = pd.json_normalize(data['papers'])['keywords']\n",
    "\n",
    "print(\"No of papers: {}\".format(keywords_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KjLR3ckPhmfS"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-51483ea6a952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSTOPWORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "STOPWORDS = nlp.Defaults.stop_words\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WZyshRqDr6cf"
   },
   "outputs": [],
   "source": [
    "def process(flat_words, lemmatize=True, min_word_len=2):\n",
    "  flat_words = [gensim.parsing.preprocessing.strip_non_alphanum(word) for word in flat_words]\n",
    "  doc = spacy.tokens.Doc(nlp.vocab, words=flat_words)\n",
    "\n",
    "  processed = []\n",
    "\n",
    "  if lemmatize:\n",
    "    processed = [token.lemma_ for token in doc]\n",
    "    processed = [token.lower() for token in processed]\n",
    "  else:\n",
    "    processed = [token.lower_ for token in doc]\n",
    "\n",
    "  processed = [token for token in processed if token not in STOPWORDS]\n",
    "  processed = [token for token in processed if len(token) >= min_word_len]\n",
    "\n",
    "  return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WkOFqDCdQFON"
   },
   "outputs": [],
   "source": [
    "def process_corpus(keywords_df, lemmatize=True, min_word_len=2):\n",
    "  processed_corpus = []\n",
    "  flat_words = []\n",
    "\n",
    "  # n: no of research papers\n",
    "  # [[kw11, kw12, ...] ...[kwn1, kwn2, kwn3, ...]]\n",
    "  keywords = keywords_df.tolist()\n",
    "  # j: no of document in which the keyword is present\n",
    "  # kwji -> [wji1, wij2, ...] ~ list of all words associated with keyword i of paper j\n",
    "  # [kwj1, kwj2, ...] -> [kwj11, kwj12, wkwj13, kwj21, kwj22, ...] ~ list of all words associated with the keywords of paper j\n",
    "  for l in keywords:\n",
    "    tmp = []\n",
    "    for keyword in l:\n",
    "      words = keyword.split(\" \")\n",
    "      for word in words:\n",
    "        tmp.append(word)\n",
    "    flat_words.append(tmp)\n",
    "\n",
    "  for l in flat_words:\n",
    "    processed_corpus.append(process(l, True, 2))\n",
    "\n",
    "  return processed_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXoWUpqQTRey",
    "outputId": "08437c26-6dc9-47f8-d5d2-d3cd6f908bd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2230\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "processed_corpus = process_corpus(keywords_df)\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(words) for words in processed_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "0ihcPTv8XgKv"
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# Train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "iI7n_2znX-YI"
   },
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=len(dictionary.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "hxQbH2vLYQ37"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_similarities(processed_corpus, threshold=0.9):\n",
    "  similarities = defaultdict(list)\n",
    "\n",
    "  for query_doc_index in range(len(processed_corpus)):\n",
    "    query_doc =  processed_corpus[query_doc_index]\n",
    "    query_bow = dictionary.doc2bow(query_doc)\n",
    "    sims = index[tfidf[query_bow]]\n",
    "    for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "      if score >= threshold and document_number != query_doc_index:\n",
    "        similarities[query_doc_index].append((document_number, score))\n",
    "\n",
    "  return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "6kcB_-zGrKnk"
   },
   "outputs": [],
   "source": [
    "sims = compute_similarities(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUKkOrAqiArW"
   },
   "outputs": [],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuGVQtxVicsl"
   },
   "source": [
    "Brainstorming with Chen from 05.February.2021: \n",
    "\n",
    "Paper j: kwj1 -> kwi1,......,n \n",
    "0 - 1 - save only min \n",
    "\n",
    "keywords processing \n",
    "remove learning? -> outliers outliers outliers!\n",
    "\n",
    "1. Keine andere Distanc (word2vec nicht ausprobieren) [Chris langweilt sich -> word2vec] aber wir bleiben bei tfidf\n",
    "2. Wie viele Cluster bekommen wir mit diesem BOW/TFIDF Ansatz? (threshold (?)) -> \n",
    "3. Clustern auf die Similarities -> Neue Daten mit einem speziellen Label \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Keywords-BOW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
