\subsection{Evaluation}
\label{subsec:eval}
It is non-trivial to characterize clustering metrics for evaluation. Since we will not use anything out of the ordinary, we refer to \cite{aggarwal2015data}. In general we can distinguish between \textit{internal} and \textit{external} measures. This directly corresponds to degrees of supervision. 

\paragraph{Unsupervised Clustering.}
Data is divided into clusters without knowing ground truth clusters, such that data inside share max. similarity w.r.t. a specific data attribute. Classification requires knowing the label set $ \mathcal{L} $, which we do not know in this case. Example algorithms for unsupervised clustering are: \textit{K-Means}, \textit{Mean shift}, \textit{Expectation-Maximization} and many more. Efficient implementations exist in \textit{sklearn} and similar Python libraries.

Because no ground truth exists, internal measures are highly dependent on the application. Typical internal measures depend on the number of clusters, intra- and inter-cluster distance distributions. Examples are the \textit{Davies-Bouldin index}, \textit{Dunn index} or the \textit{Silhouette coefficient}. However, we usually do not know much about the underlying data, which makes it hard to compare algorithms this way as internal measures only indicate if one algorithms is better than another in some situations. In the end, we might however not know if an algorithms produces more valid results than another.

\paragraph{Supervised Clustering.}
Supervision for clustering research papers is non-trivial. There are several possible solutions for computing a supervisory signal. One of the general ideas of this project was to exploit the standardized submission process in academic journals/conferences. Authors need to submit a list of key words that best describe their work. On top, all documents follow structural guidelines for their specific journal/conference. This gives us the chance to use the standardized format to simplify processing. Furthermore, we can exploit the labels of platforms hosting our data. Articles are usually already grouped according to their topics. We propose three supervision modes that we are planning to try out: 
\begin{enumerate}
	\item Create hard labels, by using the preexisting group assignments of hosting websites
	\item Create labels from provided key words upon submission. We could create a label set across the data and perform classification on our training set. This would allow to a) assign likely key words to papers b) cluster papers according to assigned key words. A down-stream task that benefits from our improvements would be key word generation.
	\item Generate hard assignments based on key words. By assigning cluster labels to key words, we could compute a hard cluster from a combination of key words for a given paper
\end{enumerate}

Supervised clustering allows validating an algorithm based on a dataset split and the ground truth labels. Typical evaluation metrics include: \textit{Precision}, \textit{Recall}, \textit{F1-score}, \textit{Jaccard index} or a \textit{confusion matrix}. The supervision would be needed to achieve our bonus goal with a learning algorithm. We will compare our algorithms with baselines from the related works w.r.t to the mentioned metrics.