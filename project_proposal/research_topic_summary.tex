\section{Research Topic Summary}
There exist various approaches to simplify getting an overview of a research field without extensive research and reading countless papers. Browsing through digital libraries and search engines can yield papers matching search strings, but to get an overview of an entire research area this is not good enough.

In \cite{Rapid_understanding_of_scientific_paper_collections} a tool is presented that uses the citation network to divide papers into clusters and identify trends, gaps and outliers.
Another way to gain insights is to find key statements of a paper. As finding these statements is computationally expensive, the authors use `Multi-Document Summarization' which is only applied to abstracts and citation contexts. The dataset used is ACL Anthology Network (AAN) \cite{aan} and contains the network of citations, as well as the full text of each article, its metadata, summary, references and citation sentences.

These techniques make it possible to gain insight into an entire research field more quickly, but for this project we will concentrate only on the clustering.
Attempts to cluster papers have been made for many years. In 1973, for example, it had already been tried to cluster journals by comparing reference patterns and looking at mutual references \cite{Clustering_of_scientific_journals}.

In \cite{Document_clustering_of_scientific_texts_using_citation_contexts} the context of the citations is used in addition to the citations itself to cluster.
Citations are identified and text around it is extracted to then use link-based clustering approaches, term-based clustering approaches and hierarchical document clustering, as well as a combination of all three, on this data.
For comparison, this technique is also applied to the entire document and contrasted against the approach of using only citation context.

In \cite{Clustering_scientific_documents_with_topic_modeling} abstracts and titles of documents from Web of Science \cite{web_of_science} are used.
They perform two types of pre-processing, the first treats each word as a token, and stopwords are deleted. The second method uses term-clumping to find noun phrases with significant commonality.
Then, several topic modeling algorithms are used: The Latent Dirichlet allocation (LDA), Correlated Topic Models (CTM), Hierarchical Latent Dirichlet Allocation (Hierarchical LDA) and Hierarchical Dirichlet Process (HDP).

Abstracts are also used in \cite{An_Approach_to_Clustering_Abstracts} to perform clustering. They use tokenization, remove stopwords and then apply a stemming algorithm.
Keywords are then grouped and weighted and the closeness of two documents is calculated using cosine similarity measure. 
Additionally, clustering methods are applied to the whole abstract. Three algorithms from three different approaches are used: the k-medoid method from the example-based approach, the nearest neighbor method from the hierarchy-based approach and the MajorClust method from the density-based approach. The data source consists of 48 human classified abstracts from \cite{cicling}.

There are multiple approaches we could take in our own project. The final goal is of course to achieve a better clustering than previous attempts.
In regards to \cite{An_Approach_to_Clustering_Abstracts}, \cite{Document_clustering_of_scientific_texts_using_citation_contexts} and \cite{Clustering_scientific_documents_with_topic_modeling} we can leverage abstracts and citation contexts for clustering in favor of only either one of them.
There is also an opportunity in taking different clustering techniques (e.g. k-means, hierarchical clustering or one of the ones mentioned above) and comparing their respective performance with each other.
In addition we could also use a neural network architecture to either automate the process of generating word vectors from the texts or to automate the clustering from word vectors that are obtained in a more traditional fashion, but this is more of a bonus goal given the time constraints.
