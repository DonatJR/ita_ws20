\subsection{Evaluation}
\subcomment{Written by Christian Homeyer}
Evaluation of a clustering algorithm is fundamentally different depending on whether ground truth labels are available or not. We therefore perform a supervised and unsupervised evaluation. An unlabeled data set puts us in the position of having to find the ground truth on our own. It is shown in subsection \ref{sec:create_gt}, how this task is a non-trivial problem itself. 

\subsubsection{Metrics}
There exist several clustering metrics for evaluation. Since we will not use anything out of the ordinary, we refer to \cite{aggarwal2015data}. In general we can distinguish between \textit{internal} and \textit{external} measures. This directly corresponds to degrees of supervision.

\paragraph{Unsupervised Clustering.}
Data is divided into clusters without knowing ground truth clusters, such that data inside share max. similarity w.r.t. a specific data attribute. Classification requires knowing the label set $ \mathcal{L} $, which we do not know in this case. Because no ground truth exists, internal measures are highly dependent on the application. Typical internal measures depend on the number of clusters, intra- and inter-cluster distance distributions. Examples are the \textit{Davies-Bouldin score}, \textit{Calinsky-Harabasz score} or the \textit{Silhouette score}. However, we usually do not know much about the underlying data, which makes it hard to compare algorithms this way as internal measures only indicate if one algorithm is better than another in some situations. In the end, we might however not know if an algorithm produces more valid results than another.

\paragraph{Supervised Clustering.}
Supervised clustering allows validating an algorithm based on the ground truth labels $ \mathcal{L} $. Typical evaluation metrics include: \textit{Precision}, \textit{Recall}, \textit{F1-score}, \textit{Jaccard index} or a \textit{confusion matrix}. 

\subsubsection{Creating a ground truth}
\label{sec:create_gt}
The general idea of our project is to exploit the paper submission process for evaluating our clustering algorithms. Every paper submission is accompanied by a set of keywords $ \mathcal{K} = \{ K_{1},\; K_{2},\; \dots ,\; K_{N} \} $, that best describe the content of the paper. We make the following assumption: 
\begin{center}
	\textit{An abstract can contain non-relevant information, but the keywords are supposed to be distilled truth.}
\end{center}
They describe the paper contents in the least number of words. In the light of these facts, we decided to use the keywords to create an integer label ground truth. 
The term keyword refers to multiple words describing a concept (e.g., "behavioral data in strategic settings"). Each paper has multiple of such sets.

\paragraph{Problem.} Given a set of papers $ \mathcal{P} = \{ P_{1},\; P_{2},\; \dots,\; P_{N} \} $, each assigned a set of keywords $ K_{i} = \{ k_{i1},\; \dots ,\; k_{im} \} $, find an integer labelling $ \mathcal{L} = \{ L_{1},\; \dots,\; L_{N} \} \in \mathbb{N}^{N} $ that divides $ \mathcal{P} $ into $ k $ separate clusters. The number of clusters $ k $ is not known a priori. Papers do not have keyword sets with the same cardinality.

We identify several problems: 
\begin{enumerate}
	\item Multiple keywords correspond to the same information. Again, this is the general problem with text data, that we have to lemmatize words or group them to a common semantic meaning.
	\item In practice, our initial assumption may not be correct, because keywords may not accurately describe a paper. These can be highly subjective and some authors assign their papers keywords with an agenda.
	\item We have to make an assumption about the importance of individual keywords. We assume all keywords to be equally informative, while in reality some keywords are much more important for distinguishing papers.
\end{enumerate}

Because the total number of clusters is unknown it is unclear what measure to take for creating the ground truth. Essentially this problem is similar to our general problem, only that we are working on the keywords instead of the text body. We address 1. by applying the same preprocessing techniques on the keywords as on the text body, i.e. lemmatization, stop word removal, etc., resulting in a set of word tokens for each paper.

In order to create an integer label we need to be able to compute a distance between elements of $ \mathcal{P} $. Our first question is therefore, how to measure distance/similarity between sets of keywords/tokens. 

\paragraph{Naive.}
We first naively computed a superset of all keywords $ \mathcal{K}_{super} = \bigcup_{i=1}^{N} \mathcal{K}_{i} $, that only contains distinct elements. From this superset we create binary feature vectors $ F_{i} \in \{0,\; 1 \}^{|\mathcal{K}_{super}|} $ for each paper, indicating if a word from the superset is assigned or not. We then compute the Hamming distance between papers for clustering. We found this to not work very well, because the resulting feature vectors are very sparse. This will also result in many small distances and clusters, since most papers only have few keywords in common. 

\paragraph{Better.}
We split the keywords into separate tokens to account for different forms of the same concept and hierarchical categories. A paper containing the keyword "Bayesian learning," should have a certain similarity degree to one containing the keyword "machine learning". We preprocessed the resulting tokens and created feature vectors using the term frequency-inverse document frequency (TFIDF). We've then computed the cosine similarity between the papers and manually checked random samples with different similarity scores. Scientific papers with a cosine similarity score of above .7 shared an average of six words. Six was not a negligible number as most research papers had on average four keywords. We decided to cluster the research papers with DBSCAN, using the features to compute the cosine distance matrix and obtain the labels. We chose DBSCAN to cluster the keywords as it doesn't require the expected number of clusters as input. We call the resulting labels ground truth. We will use them to check whether the abstract clusters matched the keyword clusters. 

\newpage
\subsection{Issues}
\subcomment{Written by Daniela Fichiu, Christian Homeyer}
The clustering results were sensitive to the hyperparameters of the algorithm. We could sadly not find a good configuration that results in reasonable clusters. We found out that the majority of our dataset (60 \%) will be labeled as outliers based on the keywords alone. Our supervised metrics therefore do not seem to be meaningful. For example, we only achieve an adjusted mutual-information score of $ 0.06 $ between the clusters of our abstracts and the ground truth. 

This showed how unbalanced our collected data is, because we have only few similar papers while the majority only shared a single common keyword. It turned out, that most papers were only related by sharing general words such as e.g. "learning" or "algorithm". We therefore decided to only use the unsupervised metrics for evaluating clustering algorithms in the later section. Future work seriously needs to take dataset balance into account when collecting papers. We made the mistake of collecting only broader "machine learning" papers without paying attention to more specific subfields as clusters. Our ground truth of the keywords therefore did not align with the abstract clusters. Another potential explanation would be that keywords and abstract do not have as much in common as we assumed.