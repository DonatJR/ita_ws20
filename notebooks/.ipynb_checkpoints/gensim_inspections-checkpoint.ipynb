{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gIEBEtMQ-Ej5",
    "outputId": "f16bcb81-88a1-4d65-9776-b543fc6613b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
      "Requirement already satisfied: funcy in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.15)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.17.0)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.36.1)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.1.5)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.6.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (50.3.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (20.3.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DS-hwUieQpl4"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXhg3ZbCArKj"
   },
   "source": [
    "# Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "P9XtoG6pAxYJ"
   },
   "outputs": [],
   "source": [
    "num_topics = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNCUjKjS-gVF"
   },
   "source": [
    "# Data loading and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jM8x-Zi-6g6"
   },
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../src/data/manual_datasource.json') as f:\n",
    "  data = json.load(f)\n",
    "data_df = pd.json_normalize(data['papers'])\n",
    "corpus = data_df[\"abstract\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgxJMF2y-_L_"
   },
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cxy2fcGoxxaF",
    "outputId": "1bbf2491-7b41-4a3f-f6f3-3cefa4417fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['data', 'driven', 'analysis', 'has', 'been', 'increasingly', 'used', 'in', 'various', 'decision', 'making', 'processes', 'with', 'more', 'sources', 'including', 'reviews', 'news', 'and', 'pictures', 'can', 'now', 'be', 'used', 'for', 'data', 'analysis', 'the', 'authenticity', 'of', 'data', 'sources', 'is', 'in', 'doubt', 'while', 'previous', 'literature', 'attempted', 'to', 'detect', 'fake', 'data', 'piece', 'by', 'piece', 'in', 'the', 'current', 'work', 'we', 'try', 'to', 'capture', 'the', 'fake', 'data', 'sender', 'strategic', 'behavior', 'to', 'detect', 'the', 'fake', 'data', 'source', 'specifically', 'we', 'model', 'the', 'tension', 'between', 'data', 'receiver', 'who', 'makes', 'data', 'driven', 'decisions', 'and', 'fake', 'data', 'sender', 'who', 'benefits', 'from', 'misleading', 'the', 'receiver', 'we', 'propose', 'potentially', 'infinite', 'horizon', 'continuous', 'time', 'game', 'theoretic', 'model', 'with', 'asymmetric', 'information', 'to', 'capture', 'the', 'fact', 'that', 'the', 'receiver', 'does', 'not', 'initially', 'know', 'the', 'existence', 'of', 'fake', 'data', 'and', 'learns', 'about', 'it', 'during', 'the', 'course', 'of', 'the', 'game', 'we', 'use', 'point', 'processes', 'to', 'model', 'the', 'data', 'traffic', 'where', 'each', 'piece', 'of', 'data', 'can', 'occur', 'at', 'any', 'discrete', 'moment', 'in', 'continuous', 'time', 'flow', 'we', 'fully', 'solve', 'the', 'model', 'and', 'employ', 'numerical', 'examples', 'to', 'illustrate', 'the', 'players', 'strategies', 'and', 'payoffs', 'for', 'insights', 'specifically', 'our', 'results', 'show', 'that', 'maintaining', 'some', 'suspicion', 'about', 'the', 'data', 'sources', 'and', 'understanding', 'that', 'the', 'sender', 'can', 'be', 'strategic', 'are', 'very', 'helpful', 'to', 'the', 'data', 'receiver', 'in', 'addition', 'based', 'on', 'our', 'model', 'we', 'propose', 'methodology', 'of', 'detecting', 'fake', 'data', 'that', 'is', 'complementary', 'to', 'the', 'previous', 'studies', 'on', 'this', 'topic', 'which', 'suggested', 'various', 'approaches', 'on', 'analyzing', 'the', 'data', 'piece', 'by', 'piece', 'we', 'show', 'that', 'after', 'analyzing', 'each', 'piece', 'of', 'data', 'understanding', 'source', 'by', 'looking', 'at', 'the', 'its', 'whole', 'history', 'of', 'pushing', 'data', 'can', 'be', 'helpful'], ['sliced', 'inverse', 'regression', 'is', 'an', 'effective', 'paradigm', 'that', 'achieves', 'the', 'goal', 'of', 'dimension', 'reduction', 'through', 'replacing', 'high', 'dimensional', 'covariates', 'with', 'small', 'number', 'of', 'linear', 'combinations', 'it', 'does', 'not', 'impose', 'parametric', 'assumptions', 'on', 'the', 'dependence', 'structure', 'more', 'importantly', 'such', 'reduction', 'of', 'dimension', 'is', 'sufficient', 'in', 'that', 'it', 'does', 'not', 'cause', 'loss', 'of', 'information', 'in', 'this', 'paper', 'we', 'adapt', 'the', 'stationary', 'sliced', 'inverse', 'regression', 'to', 'cope', 'with', 'the', 'rapidly', 'changing', 'environments', 'we', 'propose', 'to', 'implement', 'sliced', 'inverse', 'regression', 'in', 'an', 'online', 'fashion', 'this', 'online', 'learner', 'consists', 'of', 'two', 'steps', 'in', 'the', 'first', 'step', 'we', 'construct', 'an', 'online', 'estimate', 'for', 'the', 'kernel', 'matrix', 'in', 'the', 'second', 'step', 'we', 'propose', 'two', 'online', 'algorithms', 'one', 'is', 'motivated', 'by', 'the', 'perturbation', 'method', 'and', 'the', 'other', 'is', 'originated', 'from', 'the', 'gradient', 'descent', 'optimization', 'to', 'perform', 'online', 'singular', 'value', 'decomposition', 'the', 'theoretical', 'properties', 'of', 'this', 'online', 'learner', 'are', 'established', 'we', 'demonstrate', 'the', 'numerical', 'performance', 'of', 'this', 'online', 'learner', 'through', 'simulations', 'and', 'real', 'world', 'applications', 'all', 'numerical', 'studies', 'confirm', 'that', 'this', 'online', 'learner', 'performs', 'as', 'well', 'as', 'the', 'batch', 'learner'], ['common', 'divide', 'and', 'conquer', 'approach', 'for', 'bayesian', 'computation', 'with', 'big', 'data', 'is', 'to', 'partition', 'the', 'data', 'perform', 'local', 'inference', 'for', 'each', 'piece', 'separately', 'and', 'combine', 'the', 'results', 'to', 'obtain', 'global', 'posterior', 'approximation', 'while', 'being', 'conceptually', 'and', 'computationally', 'appealing', 'this', 'method', 'involves', 'the', 'problematic', 'need', 'to', 'also', 'split', 'the', 'prior', 'for', 'the', 'local', 'inferences', 'these', 'weakened', 'priors', 'may', 'not', 'provide', 'enough', 'regularization', 'for', 'each', 'separate', 'computation', 'thus', 'eliminating', 'one', 'of', 'the', 'key', 'advantages', 'of', 'bayesian', 'methods', 'to', 'resolve', 'this', 'dilemma', 'while', 'still', 'retaining', 'the', 'of', 'the', 'underlying', 'local', 'inference', 'method', 'we', 'apply', 'the', 'idea', 'of', 'expectation', 'propagation', 'ep', 'as', 'framework', 'for', 'distributed', 'bayesian', 'inference', 'the', 'central', 'idea', 'is', 'to', 'iteratively', 'update', 'approximations', 'to', 'the', 'local', 'likelihoods', 'given', 'the', 'state', 'of', 'the', 'other', 'approximations', 'and', 'the', 'prior', 'the', 'present', 'paper', 'has', 'two', 'roles', 'we', 'review', 'the', 'steps', 'that', 'are', 'needed', 'to', 'keep', 'ep', 'algorithms', 'numerically', 'stable', 'and', 'we', 'suggest', 'general', 'approach', 'inspired', 'by', 'ep', 'for', 'approaching', 'data', 'partitioning', 'problems', 'in', 'way', 'that', 'achieves', 'the', 'computational', 'benefits', 'of', 'parallelism', 'while', 'allowing', 'each', 'local', 'update', 'to', 'make', 'use', 'of', 'relevant', 'information', 'from', 'the', 'other', 'sites', 'in', 'addition', 'we', 'demonstrate', 'how', 'the', 'method', 'can', 'be', 'applied', 'in', 'hierarchical', 'context', 'to', 'make', 'use', 'of', 'partitioning', 'of', 'both', 'data', 'and', 'parameters', 'the', 'paper', 'describes', 'general', 'algorithmic', 'framework', 'rather', 'than', 'specific', 'algorithm', 'and', 'presents', 'an', 'example', 'implementation', 'for', 'it'], ['this', 'paper', 'develops', 'deterministic', 'upper', 'and', 'lower', 'bounds', 'on', 'the', 'influence', 'measure', 'in', 'network', 'more', 'precisely', 'the', 'expected', 'number', 'of', 'nodes', 'that', 'seed', 'set', 'can', 'influence', 'in', 'the', 'independent', 'cascade', 'model', 'in', 'particular', 'our', 'bounds', 'exploit', 'nonbacktracking', 'walks', 'and', 'fortuin', 'kasteleyn', 'ginibre', 'fkg', 'type', 'inequalities', 'and', 'are', 'computed', 'by', 'message', 'passing', 'algorithms', 'further', 'we', 'provide', 'parameterized', 'versions', 'of', 'the', 'bounds', 'that', 'control', 'the', 'trade', 'off', 'between', 'efficiency', 'and', 'accuracy', 'finally', 'the', 'tightness', 'of', 'the', 'bounds', 'is', 'illustrated', 'on', 'various', 'network', 'models'], ['creating', 'cross', 'language', 'article', 'links', 'among', 'different', 'online', 'encyclopedias', 'is', 'now', 'an', 'important', 'task', 'in', 'the', 'unification', 'of', 'multilingual', 'knowledge', 'bases', 'in', 'this', 'paper', 'we', 'propose', 'cross', 'language', 'article', 'linking', 'method', 'using', 'mixed', 'language', 'topic', 'model', 'and', 'hypernym', 'translation', 'features', 'based', 'on', 'an', 'svm', 'model', 'to', 'link', 'english', 'wikipedia', 'and', 'chinese', 'baidu', 'baike', 'the', 'most', 'widely', 'used', 'wiki', 'like', 'encyclopedia', 'in', 'china', 'to', 'evaluate', 'our', 'approach', 'we', 'compile', 'data', 'set', 'from', 'the', 'top', 'baidu', 'baike', 'articles', 'and', 'their', 'corresponding', 'english', 'wiki', 'articles', 'the', 'evaluation', 'results', 'show', 'that', 'our', 'approach', 'achieves', 'in', 'mrr', 'and', 'in', 'recall', 'our', 'method', 'does', 'not', 'heavily', 'depend', 'on', 'linguistic', 'characteristics', 'and', 'can', 'be', 'easily', 'extended', 'to', 'generate', 'crosslanguage', 'article', 'links', 'among', 'different', 'online', 'encyclopedias', 'in', 'other', 'languages'], ['the', 'topic', 'of', 'document', 'can', 'prove', 'to', 'be', 'useful', 'information', 'for', 'word', 'sense', 'disambiguation', 'wsd', 'since', 'certain', 'meanings', 'tend', 'to', 'be', 'associated', 'with', 'particular', 'topics', 'this', 'paper', 'presents', 'an', 'lda', 'based', 'approach', 'for', 'wsd', 'which', 'is', 'trained', 'using', 'any', 'available', 'wsd', 'system', 'to', 'establish', 'sense', 'per', 'latent', 'dirichlet', 'allocation', 'based', 'topic', 'the', 'technique', 'is', 'tested', 'using', 'three', 'unsupervised', 'and', 'one', 'supervised', 'wsd', 'algorithms', 'within', 'the', 'sport', 'and', 'finance', 'domains', 'giving', 'performance', 'increase', 'each', 'time', 'suggesting', 'that', 'the', 'technique', 'may', 'be', 'useful', 'to', 'improve', 'the', 'performance', 'of', 'any', 'available', 'wsd', 'system'], ['we', 'determine', 'the', 'subjectivity', 'of', 'word', 'senses', 'to', 'avoid', 'costly', 'annotation', 'we', 'evaluate', 'how', 'useful', 'existing', 'resources', 'established', 'in', 'opinion', 'mining', 'are', 'for', 'this', 'task', 'we', 'show', 'that', 'results', 'achieved', 'with', 'existing', 'resources', 'that', 'are', 'not', 'tailored', 'towards', 'word', 'sense', 'subjectivity', 'classification', 'can', 'rival', 'results', 'achieved', 'with', 'supervision', 'on', 'manually', 'annotated', 'training', 'set', 'however', 'results', 'with', 'different', 'resources', 'vary', 'substantially', 'and', 'are', 'dependent', 'on', 'the', 'different', 'definitions', 'of', 'subjectivity', 'used', 'in', 'the', 'establishment', 'of', 'the', 'resources'], ['this', 'paper', 'describes', 'novel', 'approach', 'to', 'generate', 'potential', 'foreign', 'accented', 'phonetic', 'transcriptions', 'using', 'phonological', 'rewrite', 'rules', 'for', 'each', 'pair', 'of', 'native', 'language', 'and', 'target', 'language', 'set', 'of', 'postlexical', 'rules', 'is', 'designed', 'to', 'transform', 'canonical', 'phonetic', 'dictionaries', 'of', 'into', 'adapted', 'dictionaries', 'for', 'native', 'speakers', 'some', 'general', 'considerations', 'on', 'the', 'design', 'of', 'such', 'rule', 'based', 'system', 'are', 'presented'], ['to', 'improve', 'coverage', 'of', 'example', 'bases', 'two', 'methods', 'are', 'introduced', 'into', 'the', 'best', 'match', 'algorithm', 'the', 'first', 'is', 'for', 'acquiring', 'conjunctive', 'relationships', 'from', 'corpora', 'as', 'measures', 'of', 'word', 'similarity', 'that', 'can', 'be', 'used', 'in', 'addition', 'to', 'thesauruses', 'the', 'second', 'used', 'when', 'word', 'does', 'not', 'appear', 'in', 'an', 'example', 'base', 'or', 'thesaurus', 'is', 'for', 'inferring', 'links', 'to', 'words', 'in', 'the', 'example', 'base', 'by', 'comparing', 'the', 'usage', 'of', 'the', 'word', 'in', 'the', 'text', 'and', 'that', 'of', 'words', 'in', 'the', 'example', 'base']]\n"
     ]
    }
   ],
   "source": [
    "### Preprocess the Dataset ### \n",
    "tokenized =[] \n",
    "for sentence in corpus: \n",
    "  # the simple_preprocess function returns a list of each sentence \n",
    "  tokenized.append(simple_preprocess(sentence ,min_len=2, max_len=15)) # the minimum length of a token and  maximum length of a token.\n",
    "print(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jwKfl1zf_m3t"
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokenized) \n",
    "BoW_corpus = [dictionary.doc2bow(text) for text in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QIeFsoBl_-XH"
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(BoW_corpus)\n",
    "corpus_tfidf = tfidf[BoW_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfDe2RHnh6P0"
   },
   "source": [
    "# LSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-R_Xu-sipJ6"
   },
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RmvsgAuRh5gF",
    "outputId": "3b9deceb-b09b-4214-9846-3f7373231590"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -0.3814593962031656), (1, -0.13724577365918944), (2, 0.5491547433319053)]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tfidf = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)# train model\n",
    "lsi_tfidf[corpus_tfidf[1]]  # apply model to  document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLEOxC6nirlW"
   },
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZHtXeqMinNp",
    "outputId": "56042158-6060-413e-ce2d-c5a02a0852f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.16633675021485836), (1, 0.36636148585096373), (2, -0.6094608397254359)]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_bow = models.LsiModel(BoW_corpus, id2word=dictionary, num_topics=num_topics)\n",
    "lsi_bow[corpus_tfidf[1]]  # apply model to  document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "840g3KiPHH_a"
   },
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qc03jENeAhwC"
   },
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CaZcsssOHLmA",
    "outputId": "09079639-4ec4-429e-9295-5dfdb9c4d564"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1077: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  score += np.sum(cnt * logsumexp(Elogthetad + Elogbeta[:, int(id)]) for id, cnt in doc)\n"
     ]
    }
   ],
   "source": [
    "# LDA model training \n",
    "lda_model = models.ldamodel.LdaModel(corpus=corpus_tfidf,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ0Mcm1VAlfS"
   },
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77q-Q49ACoXk"
   },
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZkGA8NSgUFx",
    "outputId": "500d3506-f167-4b3e-cf89-d1df793220dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data-driven analysis', 'fake data', 'game theory', 'point process']\n",
      "['Dimension reduction', 'online learning', 'perturbation', 'singular value decomposition', 'sliced inverse regression', 'gradient descent']\n",
      "['Bayesian computation', 'data partitioning', 'expectation propagation', 'hierarchical models', 'statistical computing']\n",
      "['Influence Estimation', 'Nonbacktracking Walk', 'Message Passing', 'Social Networks', 'Independent Cascade Model']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for keyword in data_df[\"keywords\"]:\n",
    "  print(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MU7RbvD2CtKC"
   },
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSsGfh5KgfQm",
    "outputId": "cdaf3943-efda-4f90-ed7d-19ca670a8c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'about': 0,\n",
      " 'accented': 454,\n",
      " 'accuracy': 303,\n",
      " 'achieved': 429,\n",
      " 'achieves': 130,\n",
      " 'acquiring': 478,\n",
      " 'adapt': 131,\n",
      " 'adapted': 455,\n",
      " 'addition': 1,\n",
      " 'advantages': 209,\n",
      " 'after': 2,\n",
      " 'algorithm': 210,\n",
      " 'algorithmic': 211,\n",
      " 'algorithms': 132,\n",
      " 'all': 133,\n",
      " 'allocation': 394,\n",
      " 'allowing': 212,\n",
      " 'also': 213,\n",
      " 'among': 343,\n",
      " 'an': 134,\n",
      " 'analysis': 3,\n",
      " 'analyzing': 4,\n",
      " 'and': 5,\n",
      " 'annotated': 430,\n",
      " 'annotation': 431,\n",
      " 'any': 6,\n",
      " 'appealing': 214,\n",
      " 'appear': 479,\n",
      " 'applications': 135,\n",
      " 'applied': 215,\n",
      " 'apply': 216,\n",
      " 'approach': 217,\n",
      " 'approaches': 7,\n",
      " 'approaching': 218,\n",
      " 'approximation': 219,\n",
      " 'approximations': 220,\n",
      " 'are': 8,\n",
      " 'article': 344,\n",
      " 'articles': 345,\n",
      " 'as': 136,\n",
      " 'associated': 395,\n",
      " 'assumptions': 137,\n",
      " 'asymmetric': 9,\n",
      " 'at': 10,\n",
      " 'attempted': 11,\n",
      " 'authenticity': 12,\n",
      " 'available': 396,\n",
      " 'avoid': 432,\n",
      " 'baidu': 346,\n",
      " 'baike': 347,\n",
      " 'base': 480,\n",
      " 'based': 13,\n",
      " 'bases': 348,\n",
      " 'batch': 138,\n",
      " 'bayesian': 221,\n",
      " 'be': 14,\n",
      " 'been': 15,\n",
      " 'behavior': 16,\n",
      " 'being': 222,\n",
      " 'benefits': 17,\n",
      " 'best': 481,\n",
      " 'between': 18,\n",
      " 'big': 223,\n",
      " 'both': 224,\n",
      " 'bounds': 304,\n",
      " 'by': 19,\n",
      " 'can': 20,\n",
      " 'canonical': 456,\n",
      " 'capture': 21,\n",
      " 'cascade': 305,\n",
      " 'cause': 139,\n",
      " 'central': 225,\n",
      " 'certain': 397,\n",
      " 'changing': 140,\n",
      " 'characteristics': 349,\n",
      " 'china': 350,\n",
      " 'chinese': 351,\n",
      " 'classification': 433,\n",
      " 'combinations': 141,\n",
      " 'combine': 226,\n",
      " 'common': 227,\n",
      " 'comparing': 482,\n",
      " 'compile': 352,\n",
      " 'complementary': 22,\n",
      " 'computation': 228,\n",
      " 'computational': 229,\n",
      " 'computationally': 230,\n",
      " 'computed': 306,\n",
      " 'conceptually': 231,\n",
      " 'confirm': 142,\n",
      " 'conjunctive': 483,\n",
      " 'conquer': 232,\n",
      " 'considerations': 457,\n",
      " 'consists': 143,\n",
      " 'construct': 144,\n",
      " 'context': 233,\n",
      " 'continuous': 23,\n",
      " 'control': 307,\n",
      " 'cope': 145,\n",
      " 'corpora': 484,\n",
      " 'corresponding': 353,\n",
      " 'costly': 434,\n",
      " 'course': 24,\n",
      " 'covariates': 146,\n",
      " 'coverage': 485,\n",
      " 'creating': 354,\n",
      " 'cross': 355,\n",
      " 'crosslanguage': 356,\n",
      " 'current': 25,\n",
      " 'data': 26,\n",
      " 'decision': 27,\n",
      " 'decisions': 28,\n",
      " 'decomposition': 147,\n",
      " 'definitions': 435,\n",
      " 'demonstrate': 148,\n",
      " 'depend': 357,\n",
      " 'dependence': 149,\n",
      " 'dependent': 436,\n",
      " 'descent': 150,\n",
      " 'describes': 234,\n",
      " 'design': 458,\n",
      " 'designed': 459,\n",
      " 'detect': 29,\n",
      " 'detecting': 30,\n",
      " 'determine': 437,\n",
      " 'deterministic': 308,\n",
      " 'develops': 309,\n",
      " 'dictionaries': 460,\n",
      " 'different': 358,\n",
      " 'dilemma': 235,\n",
      " 'dimension': 151,\n",
      " 'dimensional': 152,\n",
      " 'dirichlet': 398,\n",
      " 'disambiguation': 399,\n",
      " 'discrete': 31,\n",
      " 'distributed': 236,\n",
      " 'divide': 237,\n",
      " 'document': 400,\n",
      " 'does': 32,\n",
      " 'domains': 401,\n",
      " 'doubt': 33,\n",
      " 'driven': 34,\n",
      " 'during': 35,\n",
      " 'each': 36,\n",
      " 'easily': 359,\n",
      " 'effective': 153,\n",
      " 'efficiency': 310,\n",
      " 'eliminating': 238,\n",
      " 'employ': 37,\n",
      " 'encyclopedia': 360,\n",
      " 'encyclopedias': 361,\n",
      " 'english': 362,\n",
      " 'enough': 239,\n",
      " 'environments': 154,\n",
      " 'ep': 240,\n",
      " 'establish': 402,\n",
      " 'established': 155,\n",
      " 'establishment': 438,\n",
      " 'estimate': 156,\n",
      " 'evaluate': 363,\n",
      " 'evaluation': 364,\n",
      " 'example': 241,\n",
      " 'examples': 38,\n",
      " 'existence': 39,\n",
      " 'existing': 439,\n",
      " 'expectation': 242,\n",
      " 'expected': 311,\n",
      " 'exploit': 312,\n",
      " 'extended': 365,\n",
      " 'fact': 40,\n",
      " 'fake': 41,\n",
      " 'fashion': 157,\n",
      " 'features': 366,\n",
      " 'finally': 313,\n",
      " 'finance': 403,\n",
      " 'first': 158,\n",
      " 'fkg': 314,\n",
      " 'flow': 42,\n",
      " 'for': 43,\n",
      " 'foreign': 461,\n",
      " 'fortuin': 315,\n",
      " 'framework': 243,\n",
      " 'from': 44,\n",
      " 'fully': 45,\n",
      " 'further': 316,\n",
      " 'game': 46,\n",
      " 'general': 244,\n",
      " 'generate': 367,\n",
      " 'ginibre': 317,\n",
      " 'given': 245,\n",
      " 'giving': 404,\n",
      " 'global': 246,\n",
      " 'goal': 159,\n",
      " 'gradient': 160,\n",
      " 'has': 47,\n",
      " 'heavily': 368,\n",
      " 'helpful': 48,\n",
      " 'hierarchical': 247,\n",
      " 'high': 161,\n",
      " 'history': 49,\n",
      " 'horizon': 50,\n",
      " 'how': 248,\n",
      " 'however': 440,\n",
      " 'hypernym': 369,\n",
      " 'idea': 249,\n",
      " 'illustrate': 51,\n",
      " 'illustrated': 318,\n",
      " 'implement': 162,\n",
      " 'implementation': 250,\n",
      " 'important': 370,\n",
      " 'importantly': 163,\n",
      " 'impose': 164,\n",
      " 'improve': 405,\n",
      " 'in': 52,\n",
      " 'including': 53,\n",
      " 'increase': 406,\n",
      " 'increasingly': 54,\n",
      " 'independent': 319,\n",
      " 'inequalities': 320,\n",
      " 'inference': 251,\n",
      " 'inferences': 252,\n",
      " 'inferring': 486,\n",
      " 'infinite': 55,\n",
      " 'influence': 321,\n",
      " 'information': 56,\n",
      " 'initially': 57,\n",
      " 'insights': 58,\n",
      " 'inspired': 253,\n",
      " 'into': 462,\n",
      " 'introduced': 487,\n",
      " 'inverse': 165,\n",
      " 'involves': 254,\n",
      " 'is': 59,\n",
      " 'it': 60,\n",
      " 'iteratively': 255,\n",
      " 'its': 61,\n",
      " 'kasteleyn': 322,\n",
      " 'keep': 256,\n",
      " 'kernel': 166,\n",
      " 'key': 257,\n",
      " 'know': 62,\n",
      " 'knowledge': 371,\n",
      " 'language': 372,\n",
      " 'languages': 373,\n",
      " 'latent': 407,\n",
      " 'lda': 408,\n",
      " 'learner': 167,\n",
      " 'learns': 63,\n",
      " 'like': 374,\n",
      " 'likelihoods': 258,\n",
      " 'linear': 168,\n",
      " 'linguistic': 375,\n",
      " 'link': 376,\n",
      " 'linking': 377,\n",
      " 'links': 378,\n",
      " 'literature': 64,\n",
      " 'local': 259,\n",
      " 'looking': 65,\n",
      " 'loss': 169,\n",
      " 'lower': 323,\n",
      " 'maintaining': 66,\n",
      " 'make': 260,\n",
      " 'makes': 67,\n",
      " 'making': 68,\n",
      " 'manually': 441,\n",
      " 'match': 488,\n",
      " 'matrix': 170,\n",
      " 'may': 261,\n",
      " 'meanings': 409,\n",
      " 'measure': 324,\n",
      " 'measures': 489,\n",
      " 'message': 325,\n",
      " 'method': 171,\n",
      " 'methodology': 69,\n",
      " 'methods': 262,\n",
      " 'mining': 442,\n",
      " 'misleading': 70,\n",
      " 'mixed': 379,\n",
      " 'model': 71,\n",
      " 'models': 326,\n",
      " 'moment': 72,\n",
      " 'more': 73,\n",
      " 'most': 380,\n",
      " 'motivated': 172,\n",
      " 'mrr': 381,\n",
      " 'multilingual': 382,\n",
      " 'native': 463,\n",
      " 'need': 263,\n",
      " 'needed': 264,\n",
      " 'network': 327,\n",
      " 'news': 74,\n",
      " 'nodes': 328,\n",
      " 'nonbacktracking': 329,\n",
      " 'not': 75,\n",
      " 'novel': 464,\n",
      " 'now': 76,\n",
      " 'number': 173,\n",
      " 'numerical': 77,\n",
      " 'numerically': 265,\n",
      " 'obtain': 266,\n",
      " 'occur': 78,\n",
      " 'of': 79,\n",
      " 'off': 330,\n",
      " 'on': 80,\n",
      " 'one': 174,\n",
      " 'online': 175,\n",
      " 'opinion': 443,\n",
      " 'optimization': 176,\n",
      " 'or': 490,\n",
      " 'originated': 177,\n",
      " 'other': 178,\n",
      " 'our': 81,\n",
      " 'pair': 465,\n",
      " 'paper': 179,\n",
      " 'paradigm': 180,\n",
      " 'parallelism': 267,\n",
      " 'parameterized': 331,\n",
      " 'parameters': 268,\n",
      " 'parametric': 181,\n",
      " 'particular': 332,\n",
      " 'partition': 269,\n",
      " 'partitioning': 270,\n",
      " 'passing': 333,\n",
      " 'payoffs': 82,\n",
      " 'per': 410,\n",
      " 'perform': 182,\n",
      " 'performance': 183,\n",
      " 'performs': 184,\n",
      " 'perturbation': 185,\n",
      " 'phonetic': 466,\n",
      " 'phonological': 467,\n",
      " 'pictures': 83,\n",
      " 'piece': 84,\n",
      " 'players': 85,\n",
      " 'point': 86,\n",
      " 'posterior': 271,\n",
      " 'postlexical': 468,\n",
      " 'potential': 469,\n",
      " 'potentially': 87,\n",
      " 'precisely': 334,\n",
      " 'present': 272,\n",
      " 'presented': 470,\n",
      " 'presents': 273,\n",
      " 'previous': 88,\n",
      " 'prior': 274,\n",
      " 'priors': 275,\n",
      " 'problematic': 276,\n",
      " 'problems': 277,\n",
      " 'processes': 89,\n",
      " 'propagation': 278,\n",
      " 'properties': 186,\n",
      " 'propose': 90,\n",
      " 'prove': 411,\n",
      " 'provide': 279,\n",
      " 'pushing': 91,\n",
      " 'rapidly': 187,\n",
      " 'rather': 280,\n",
      " 'real': 188,\n",
      " 'recall': 383,\n",
      " 'receiver': 92,\n",
      " 'reduction': 189,\n",
      " 'regression': 190,\n",
      " 'regularization': 281,\n",
      " 'relationships': 491,\n",
      " 'relevant': 282,\n",
      " 'replacing': 191,\n",
      " 'resolve': 283,\n",
      " 'resources': 444,\n",
      " 'results': 93,\n",
      " 'retaining': 284,\n",
      " 'review': 285,\n",
      " 'reviews': 94,\n",
      " 'rewrite': 471,\n",
      " 'rival': 445,\n",
      " 'roles': 286,\n",
      " 'rule': 472,\n",
      " 'rules': 473,\n",
      " 'second': 192,\n",
      " 'seed': 335,\n",
      " 'sender': 95,\n",
      " 'sense': 412,\n",
      " 'senses': 446,\n",
      " 'separate': 287,\n",
      " 'separately': 288,\n",
      " 'set': 336,\n",
      " 'show': 96,\n",
      " 'similarity': 492,\n",
      " 'simulations': 193,\n",
      " 'since': 413,\n",
      " 'singular': 194,\n",
      " 'sites': 289,\n",
      " 'sliced': 195,\n",
      " 'small': 196,\n",
      " 'solve': 97,\n",
      " 'some': 98,\n",
      " 'source': 99,\n",
      " 'sources': 100,\n",
      " 'speakers': 474,\n",
      " 'specific': 290,\n",
      " 'specifically': 101,\n",
      " 'split': 291,\n",
      " 'sport': 414,\n",
      " 'stable': 292,\n",
      " 'state': 293,\n",
      " 'stationary': 197,\n",
      " 'step': 198,\n",
      " 'steps': 199,\n",
      " 'still': 294,\n",
      " 'strategic': 102,\n",
      " 'strategies': 103,\n",
      " 'structure': 200,\n",
      " 'studies': 104,\n",
      " 'subjectivity': 447,\n",
      " 'substantially': 448,\n",
      " 'such': 201,\n",
      " 'sufficient': 202,\n",
      " 'suggest': 295,\n",
      " 'suggested': 105,\n",
      " 'suggesting': 415,\n",
      " 'supervised': 416,\n",
      " 'supervision': 449,\n",
      " 'suspicion': 106,\n",
      " 'svm': 384,\n",
      " 'system': 417,\n",
      " 'tailored': 450,\n",
      " 'target': 475,\n",
      " 'task': 385,\n",
      " 'technique': 418,\n",
      " 'tend': 419,\n",
      " 'tension': 107,\n",
      " 'tested': 420,\n",
      " 'text': 493,\n",
      " 'than': 296,\n",
      " 'that': 108,\n",
      " 'the': 109,\n",
      " 'their': 386,\n",
      " 'theoretic': 110,\n",
      " 'theoretical': 203,\n",
      " 'thesaurus': 494,\n",
      " 'thesauruses': 495,\n",
      " 'these': 297,\n",
      " 'this': 111,\n",
      " 'three': 421,\n",
      " 'through': 204,\n",
      " 'thus': 298,\n",
      " 'tightness': 337,\n",
      " 'time': 112,\n",
      " 'to': 113,\n",
      " 'top': 387,\n",
      " 'topic': 114,\n",
      " 'topics': 422,\n",
      " 'towards': 451,\n",
      " 'trade': 338,\n",
      " 'traffic': 115,\n",
      " 'trained': 423,\n",
      " 'training': 452,\n",
      " 'transcriptions': 476,\n",
      " 'transform': 477,\n",
      " 'translation': 388,\n",
      " 'try': 116,\n",
      " 'two': 205,\n",
      " 'type': 339,\n",
      " 'underlying': 299,\n",
      " 'understanding': 117,\n",
      " 'unification': 389,\n",
      " 'unsupervised': 424,\n",
      " 'update': 300,\n",
      " 'upper': 340,\n",
      " 'usage': 496,\n",
      " 'use': 118,\n",
      " 'used': 119,\n",
      " 'useful': 425,\n",
      " 'using': 390,\n",
      " 'value': 206,\n",
      " 'various': 120,\n",
      " 'vary': 453,\n",
      " 'versions': 341,\n",
      " 'very': 121,\n",
      " 'walks': 342,\n",
      " 'way': 301,\n",
      " 'we': 122,\n",
      " 'weakened': 302,\n",
      " 'well': 207,\n",
      " 'when': 497,\n",
      " 'where': 123,\n",
      " 'which': 124,\n",
      " 'while': 125,\n",
      " 'who': 126,\n",
      " 'whole': 127,\n",
      " 'widely': 391,\n",
      " 'wiki': 392,\n",
      " 'wikipedia': 393,\n",
      " 'with': 128,\n",
      " 'within': 426,\n",
      " 'word': 427,\n",
      " 'words': 498,\n",
      " 'work': 129,\n",
      " 'world': 208,\n",
      " 'wsd': 428}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dictionary.token2id)#token -> tokenId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3L5clqRgvGa",
    "outputId": "692ca74f-7bf7-4afd-92fb-618b1ff1df06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1,\n",
      " 1: 3,\n",
      " 2: 1,\n",
      " 3: 1,\n",
      " 4: 1,\n",
      " 5: 9,\n",
      " 6: 2,\n",
      " 7: 1,\n",
      " 8: 7,\n",
      " 9: 1,\n",
      " 10: 1,\n",
      " 11: 1,\n",
      " 12: 1,\n",
      " 13: 4,\n",
      " 14: 5,\n",
      " 15: 1,\n",
      " 16: 1,\n",
      " 17: 2,\n",
      " 18: 2,\n",
      " 19: 5,\n",
      " 20: 7,\n",
      " 21: 1,\n",
      " 22: 1,\n",
      " 23: 1,\n",
      " 24: 1,\n",
      " 25: 1,\n",
      " 26: 3,\n",
      " 27: 1,\n",
      " 28: 1,\n",
      " 29: 1,\n",
      " 30: 1,\n",
      " 31: 1,\n",
      " 32: 4,\n",
      " 33: 1,\n",
      " 34: 1,\n",
      " 35: 1,\n",
      " 36: 4,\n",
      " 37: 1,\n",
      " 38: 1,\n",
      " 39: 1,\n",
      " 40: 1,\n",
      " 41: 1,\n",
      " 42: 1,\n",
      " 43: 7,\n",
      " 44: 5,\n",
      " 45: 1,\n",
      " 46: 1,\n",
      " 47: 2,\n",
      " 48: 1,\n",
      " 49: 1,\n",
      " 50: 1,\n",
      " 51: 1,\n",
      " 52: 7,\n",
      " 53: 1,\n",
      " 54: 1,\n",
      " 55: 1,\n",
      " 56: 4,\n",
      " 57: 1,\n",
      " 58: 1,\n",
      " 59: 8,\n",
      " 60: 3,\n",
      " 61: 1,\n",
      " 62: 1,\n",
      " 63: 1,\n",
      " 64: 1,\n",
      " 65: 1,\n",
      " 66: 1,\n",
      " 67: 1,\n",
      " 68: 1,\n",
      " 69: 1,\n",
      " 70: 1,\n",
      " 71: 3,\n",
      " 72: 1,\n",
      " 73: 3,\n",
      " 74: 1,\n",
      " 75: 6,\n",
      " 76: 2,\n",
      " 77: 2,\n",
      " 78: 1,\n",
      " 79: 9,\n",
      " 80: 6,\n",
      " 81: 3,\n",
      " 82: 1,\n",
      " 83: 1,\n",
      " 84: 2,\n",
      " 85: 1,\n",
      " 86: 1,\n",
      " 87: 1,\n",
      " 88: 1,\n",
      " 89: 1,\n",
      " 90: 3,\n",
      " 91: 1,\n",
      " 92: 1,\n",
      " 93: 4,\n",
      " 94: 1,\n",
      " 95: 1,\n",
      " 96: 3,\n",
      " 97: 1,\n",
      " 98: 2,\n",
      " 99: 1,\n",
      " 100: 1,\n",
      " 101: 1,\n",
      " 102: 1,\n",
      " 103: 1,\n",
      " 104: 2,\n",
      " 105: 1,\n",
      " 106: 1,\n",
      " 107: 1,\n",
      " 108: 8,\n",
      " 109: 9,\n",
      " 110: 1,\n",
      " 111: 8,\n",
      " 112: 2,\n",
      " 113: 8,\n",
      " 114: 3,\n",
      " 115: 1,\n",
      " 116: 1,\n",
      " 117: 1,\n",
      " 118: 2,\n",
      " 119: 4,\n",
      " 120: 2,\n",
      " 121: 1,\n",
      " 122: 6,\n",
      " 123: 1,\n",
      " 124: 2,\n",
      " 125: 2,\n",
      " 126: 1,\n",
      " 127: 1,\n",
      " 128: 5,\n",
      " 129: 1,\n",
      " 130: 3,\n",
      " 131: 1,\n",
      " 132: 4,\n",
      " 133: 1,\n",
      " 134: 5,\n",
      " 135: 1,\n",
      " 136: 3,\n",
      " 137: 1,\n",
      " 138: 1,\n",
      " 139: 1,\n",
      " 140: 1,\n",
      " 141: 1,\n",
      " 142: 1,\n",
      " 143: 1,\n",
      " 144: 1,\n",
      " 145: 1,\n",
      " 146: 1,\n",
      " 147: 1,\n",
      " 148: 2,\n",
      " 149: 1,\n",
      " 150: 1,\n",
      " 151: 1,\n",
      " 152: 1,\n",
      " 153: 1,\n",
      " 154: 1,\n",
      " 155: 2,\n",
      " 156: 1,\n",
      " 157: 1,\n",
      " 158: 2,\n",
      " 159: 1,\n",
      " 160: 1,\n",
      " 161: 1,\n",
      " 162: 1,\n",
      " 163: 1,\n",
      " 164: 1,\n",
      " 165: 1,\n",
      " 166: 1,\n",
      " 167: 1,\n",
      " 168: 1,\n",
      " 169: 1,\n",
      " 170: 1,\n",
      " 171: 3,\n",
      " 172: 1,\n",
      " 173: 2,\n",
      " 174: 3,\n",
      " 175: 2,\n",
      " 176: 1,\n",
      " 177: 1,\n",
      " 178: 3,\n",
      " 179: 6,\n",
      " 180: 1,\n",
      " 181: 1,\n",
      " 182: 2,\n",
      " 183: 2,\n",
      " 184: 1,\n",
      " 185: 1,\n",
      " 186: 1,\n",
      " 187: 1,\n",
      " 188: 1,\n",
      " 189: 1,\n",
      " 190: 1,\n",
      " 191: 1,\n",
      " 192: 2,\n",
      " 193: 1,\n",
      " 194: 1,\n",
      " 195: 1,\n",
      " 196: 1,\n",
      " 197: 1,\n",
      " 198: 1,\n",
      " 199: 2,\n",
      " 200: 1,\n",
      " 201: 2,\n",
      " 202: 1,\n",
      " 203: 1,\n",
      " 204: 1,\n",
      " 205: 3,\n",
      " 206: 1,\n",
      " 207: 1,\n",
      " 208: 1,\n",
      " 209: 1,\n",
      " 210: 2,\n",
      " 211: 1,\n",
      " 212: 1,\n",
      " 213: 1,\n",
      " 214: 1,\n",
      " 215: 1,\n",
      " 216: 1,\n",
      " 217: 4,\n",
      " 218: 1,\n",
      " 219: 1,\n",
      " 220: 1,\n",
      " 221: 1,\n",
      " 222: 1,\n",
      " 223: 1,\n",
      " 224: 1,\n",
      " 225: 1,\n",
      " 226: 1,\n",
      " 227: 1,\n",
      " 228: 1,\n",
      " 229: 1,\n",
      " 230: 1,\n",
      " 231: 1,\n",
      " 232: 1,\n",
      " 233: 1,\n",
      " 234: 2,\n",
      " 235: 1,\n",
      " 236: 1,\n",
      " 237: 1,\n",
      " 238: 1,\n",
      " 239: 1,\n",
      " 240: 1,\n",
      " 241: 2,\n",
      " 242: 1,\n",
      " 243: 1,\n",
      " 244: 2,\n",
      " 245: 1,\n",
      " 246: 1,\n",
      " 247: 1,\n",
      " 248: 2,\n",
      " 249: 1,\n",
      " 250: 1,\n",
      " 251: 1,\n",
      " 252: 1,\n",
      " 253: 1,\n",
      " 254: 1,\n",
      " 255: 1,\n",
      " 256: 1,\n",
      " 257: 1,\n",
      " 258: 1,\n",
      " 259: 1,\n",
      " 260: 1,\n",
      " 261: 2,\n",
      " 262: 2,\n",
      " 263: 1,\n",
      " 264: 1,\n",
      " 265: 1,\n",
      " 266: 1,\n",
      " 267: 1,\n",
      " 268: 1,\n",
      " 269: 1,\n",
      " 270: 1,\n",
      " 271: 1,\n",
      " 272: 1,\n",
      " 273: 2,\n",
      " 274: 1,\n",
      " 275: 1,\n",
      " 276: 1,\n",
      " 277: 1,\n",
      " 278: 1,\n",
      " 279: 2,\n",
      " 280: 1,\n",
      " 281: 1,\n",
      " 282: 1,\n",
      " 283: 1,\n",
      " 284: 1,\n",
      " 285: 1,\n",
      " 286: 1,\n",
      " 287: 1,\n",
      " 288: 1,\n",
      " 289: 1,\n",
      " 290: 1,\n",
      " 291: 1,\n",
      " 292: 1,\n",
      " 293: 1,\n",
      " 294: 1,\n",
      " 295: 1,\n",
      " 296: 1,\n",
      " 297: 1,\n",
      " 298: 1,\n",
      " 299: 1,\n",
      " 300: 1,\n",
      " 301: 1,\n",
      " 302: 1,\n",
      " 303: 1,\n",
      " 304: 1,\n",
      " 305: 1,\n",
      " 306: 1,\n",
      " 307: 1,\n",
      " 308: 1,\n",
      " 309: 1,\n",
      " 310: 1,\n",
      " 311: 1,\n",
      " 312: 1,\n",
      " 313: 1,\n",
      " 314: 1,\n",
      " 315: 1,\n",
      " 316: 1,\n",
      " 317: 1,\n",
      " 318: 1,\n",
      " 319: 1,\n",
      " 320: 1,\n",
      " 321: 1,\n",
      " 322: 1,\n",
      " 323: 1,\n",
      " 324: 1,\n",
      " 325: 1,\n",
      " 326: 1,\n",
      " 327: 1,\n",
      " 328: 1,\n",
      " 329: 1,\n",
      " 330: 1,\n",
      " 331: 1,\n",
      " 332: 2,\n",
      " 333: 1,\n",
      " 334: 1,\n",
      " 335: 1,\n",
      " 336: 4,\n",
      " 337: 1,\n",
      " 338: 1,\n",
      " 339: 1,\n",
      " 340: 1,\n",
      " 341: 1,\n",
      " 342: 1,\n",
      " 343: 1,\n",
      " 344: 1,\n",
      " 345: 1,\n",
      " 346: 1,\n",
      " 347: 1,\n",
      " 348: 2,\n",
      " 349: 1,\n",
      " 350: 1,\n",
      " 351: 1,\n",
      " 352: 1,\n",
      " 353: 1,\n",
      " 354: 1,\n",
      " 355: 1,\n",
      " 356: 1,\n",
      " 357: 1,\n",
      " 358: 2,\n",
      " 359: 1,\n",
      " 360: 1,\n",
      " 361: 1,\n",
      " 362: 1,\n",
      " 363: 2,\n",
      " 364: 1,\n",
      " 365: 1,\n",
      " 366: 1,\n",
      " 367: 2,\n",
      " 368: 1,\n",
      " 369: 1,\n",
      " 370: 1,\n",
      " 371: 1,\n",
      " 372: 2,\n",
      " 373: 1,\n",
      " 374: 1,\n",
      " 375: 1,\n",
      " 376: 1,\n",
      " 377: 1,\n",
      " 378: 2,\n",
      " 379: 1,\n",
      " 380: 1,\n",
      " 381: 1,\n",
      " 382: 1,\n",
      " 383: 1,\n",
      " 384: 1,\n",
      " 385: 2,\n",
      " 386: 1,\n",
      " 387: 1,\n",
      " 388: 1,\n",
      " 389: 1,\n",
      " 390: 3,\n",
      " 391: 1,\n",
      " 392: 1,\n",
      " 393: 1,\n",
      " 394: 1,\n",
      " 395: 1,\n",
      " 396: 1,\n",
      " 397: 1,\n",
      " 398: 1,\n",
      " 399: 1,\n",
      " 400: 1,\n",
      " 401: 1,\n",
      " 402: 1,\n",
      " 403: 1,\n",
      " 404: 1,\n",
      " 405: 2,\n",
      " 406: 1,\n",
      " 407: 1,\n",
      " 408: 1,\n",
      " 409: 1,\n",
      " 410: 1,\n",
      " 411: 1,\n",
      " 412: 2,\n",
      " 413: 1,\n",
      " 414: 1,\n",
      " 415: 1,\n",
      " 416: 1,\n",
      " 417: 2,\n",
      " 418: 1,\n",
      " 419: 1,\n",
      " 420: 1,\n",
      " 421: 1,\n",
      " 422: 1,\n",
      " 423: 1,\n",
      " 424: 1,\n",
      " 425: 2,\n",
      " 426: 1,\n",
      " 427: 3,\n",
      " 428: 1,\n",
      " 429: 1,\n",
      " 430: 1,\n",
      " 431: 1,\n",
      " 432: 1,\n",
      " 433: 1,\n",
      " 434: 1,\n",
      " 435: 1,\n",
      " 436: 1,\n",
      " 437: 1,\n",
      " 438: 1,\n",
      " 439: 1,\n",
      " 440: 1,\n",
      " 441: 1,\n",
      " 442: 1,\n",
      " 443: 1,\n",
      " 444: 1,\n",
      " 445: 1,\n",
      " 446: 1,\n",
      " 447: 1,\n",
      " 448: 1,\n",
      " 449: 1,\n",
      " 450: 1,\n",
      " 451: 1,\n",
      " 452: 1,\n",
      " 453: 1,\n",
      " 454: 1,\n",
      " 455: 1,\n",
      " 456: 1,\n",
      " 457: 1,\n",
      " 458: 1,\n",
      " 459: 1,\n",
      " 460: 1,\n",
      " 461: 1,\n",
      " 462: 2,\n",
      " 463: 1,\n",
      " 464: 1,\n",
      " 465: 1,\n",
      " 466: 1,\n",
      " 467: 1,\n",
      " 468: 1,\n",
      " 469: 1,\n",
      " 470: 1,\n",
      " 471: 1,\n",
      " 472: 1,\n",
      " 473: 1,\n",
      " 474: 1,\n",
      " 475: 1,\n",
      " 476: 1,\n",
      " 477: 1,\n",
      " 478: 1,\n",
      " 479: 1,\n",
      " 480: 1,\n",
      " 481: 1,\n",
      " 482: 1,\n",
      " 483: 1,\n",
      " 484: 1,\n",
      " 485: 1,\n",
      " 486: 1,\n",
      " 487: 1,\n",
      " 488: 1,\n",
      " 489: 1,\n",
      " 490: 1,\n",
      " 491: 1,\n",
      " 492: 1,\n",
      " 493: 1,\n",
      " 494: 1,\n",
      " 495: 1,\n",
      " 496: 1,\n",
      " 497: 1,\n",
      " 498: 1}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dictionary.dfs) # token_id -> how many documents contain this token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHq0GUSGgvCs",
    "outputId": "12a93c52-cf59-46b0-b74a-5a05cbb052a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2),\n",
      "  (1, 1),\n",
      "  (2, 1),\n",
      "  (3, 2),\n",
      "  (4, 2),\n",
      "  (5, 6),\n",
      "  (6, 1),\n",
      "  (7, 1),\n",
      "  (8, 1),\n",
      "  (9, 1),\n",
      "  (10, 2),\n",
      "  (11, 1),\n",
      "  (12, 1),\n",
      "  (13, 1),\n",
      "  (14, 3),\n",
      "  (15, 1),\n",
      "  (16, 1),\n",
      "  (17, 1),\n",
      "  (18, 1),\n",
      "  (19, 3),\n",
      "  (20, 4),\n",
      "  (21, 2),\n",
      "  (22, 1),\n",
      "  (23, 2),\n",
      "  (24, 1),\n",
      "  (25, 1),\n",
      "  (26, 18),\n",
      "  (27, 1),\n",
      "  (28, 1),\n",
      "  (29, 2),\n",
      "  (30, 1),\n",
      "  (31, 1),\n",
      "  (32, 1),\n",
      "  (33, 1),\n",
      "  (34, 2),\n",
      "  (35, 1),\n",
      "  (36, 2),\n",
      "  (37, 1),\n",
      "  (38, 1),\n",
      "  (39, 1),\n",
      "  (40, 1),\n",
      "  (41, 6),\n",
      "  (42, 1),\n",
      "  (43, 2),\n",
      "  (44, 1),\n",
      "  (45, 1),\n",
      "  (46, 2),\n",
      "  (47, 1),\n",
      "  (48, 2),\n",
      "  (49, 1),\n",
      "  (50, 1),\n",
      "  (51, 1),\n",
      "  (52, 5),\n",
      "  (53, 1),\n",
      "  (54, 1),\n",
      "  (55, 1),\n",
      "  (56, 1),\n",
      "  (57, 1),\n",
      "  (58, 1),\n",
      "  (59, 2),\n",
      "  (60, 1),\n",
      "  (61, 1),\n",
      "  (62, 1),\n",
      "  (63, 1),\n",
      "  (64, 1),\n",
      "  (65, 1),\n",
      "  (66, 1),\n",
      "  (67, 1),\n",
      "  (68, 1),\n",
      "  (69, 1),\n",
      "  (70, 1),\n",
      "  (71, 5),\n",
      "  (72, 1),\n",
      "  (73, 1),\n",
      "  (74, 1),\n",
      "  (75, 1),\n",
      "  (76, 1),\n",
      "  (77, 1),\n",
      "  (78, 1),\n",
      "  (79, 7),\n",
      "  (80, 3),\n",
      "  (81, 2),\n",
      "  (82, 1),\n",
      "  (83, 1),\n",
      "  (84, 6),\n",
      "  (85, 1),\n",
      "  (86, 1),\n",
      "  (87, 1),\n",
      "  (88, 2),\n",
      "  (89, 2),\n",
      "  (90, 2),\n",
      "  (91, 1),\n",
      "  (92, 4),\n",
      "  (93, 1),\n",
      "  (94, 1),\n",
      "  (95, 3),\n",
      "  (96, 2),\n",
      "  (97, 1),\n",
      "  (98, 1),\n",
      "  (99, 2),\n",
      "  (100, 3),\n",
      "  (101, 2),\n",
      "  (102, 2),\n",
      "  (103, 1),\n",
      "  (104, 1),\n",
      "  (105, 1),\n",
      "  (106, 1),\n",
      "  (107, 1),\n",
      "  (108, 5),\n",
      "  (109, 20),\n",
      "  (110, 1),\n",
      "  (111, 1),\n",
      "  (112, 2),\n",
      "  (113, 8),\n",
      "  (114, 1),\n",
      "  (115, 1),\n",
      "  (116, 1),\n",
      "  (117, 2),\n",
      "  (118, 1),\n",
      "  (119, 2),\n",
      "  (120, 2),\n",
      "  (121, 1),\n",
      "  (122, 7),\n",
      "  (123, 1),\n",
      "  (124, 1),\n",
      "  (125, 1),\n",
      "  (126, 2),\n",
      "  (127, 1),\n",
      "  (128, 2),\n",
      "  (129, 1)],\n",
      " [(5, 2),\n",
      "  (8, 1),\n",
      "  (19, 1),\n",
      "  (32, 2),\n",
      "  (43, 1),\n",
      "  (44, 1),\n",
      "  (52, 5),\n",
      "  (56, 1),\n",
      "  (59, 4),\n",
      "  (60, 2),\n",
      "  (73, 1),\n",
      "  (75, 2),\n",
      "  (77, 2),\n",
      "  (79, 7),\n",
      "  (80, 1),\n",
      "  (90, 2),\n",
      "  (104, 1),\n",
      "  (108, 3),\n",
      "  (109, 13),\n",
      "  (111, 5),\n",
      "  (113, 3),\n",
      "  (122, 5),\n",
      "  (128, 2),\n",
      "  (130, 1),\n",
      "  (131, 1),\n",
      "  (132, 1),\n",
      "  (133, 1),\n",
      "  (134, 3),\n",
      "  (135, 1),\n",
      "  (136, 2),\n",
      "  (137, 1),\n",
      "  (138, 1),\n",
      "  (139, 1),\n",
      "  (140, 1),\n",
      "  (141, 1),\n",
      "  (142, 1),\n",
      "  (143, 1),\n",
      "  (144, 1),\n",
      "  (145, 1),\n",
      "  (146, 1),\n",
      "  (147, 1),\n",
      "  (148, 1),\n",
      "  (149, 1),\n",
      "  (150, 1),\n",
      "  (151, 2),\n",
      "  (152, 1),\n",
      "  (153, 1),\n",
      "  (154, 1),\n",
      "  (155, 1),\n",
      "  (156, 1),\n",
      "  (157, 1),\n",
      "  (158, 1),\n",
      "  (159, 1),\n",
      "  (160, 1),\n",
      "  (161, 1),\n",
      "  (162, 1),\n",
      "  (163, 1),\n",
      "  (164, 1),\n",
      "  (165, 3),\n",
      "  (166, 1),\n",
      "  (167, 5),\n",
      "  (168, 1),\n",
      "  (169, 1),\n",
      "  (170, 1),\n",
      "  (171, 1),\n",
      "  (172, 1),\n",
      "  (173, 1),\n",
      "  (174, 1),\n",
      "  (175, 8),\n",
      "  (176, 1),\n",
      "  (177, 1),\n",
      "  (178, 1),\n",
      "  (179, 1),\n",
      "  (180, 1),\n",
      "  (181, 1),\n",
      "  (182, 1),\n",
      "  (183, 1),\n",
      "  (184, 1),\n",
      "  (185, 1),\n",
      "  (186, 1),\n",
      "  (187, 1),\n",
      "  (188, 1),\n",
      "  (189, 2),\n",
      "  (190, 3),\n",
      "  (191, 1),\n",
      "  (192, 1),\n",
      "  (193, 1),\n",
      "  (194, 1),\n",
      "  (195, 3),\n",
      "  (196, 1),\n",
      "  (197, 1),\n",
      "  (198, 2),\n",
      "  (199, 1),\n",
      "  (200, 1),\n",
      "  (201, 1),\n",
      "  (202, 1),\n",
      "  (203, 1),\n",
      "  (204, 2),\n",
      "  (205, 2),\n",
      "  (206, 1),\n",
      "  (207, 1),\n",
      "  (208, 1)],\n",
      " [(1, 1),\n",
      "  (5, 7),\n",
      "  (8, 1),\n",
      "  (14, 1),\n",
      "  (17, 1),\n",
      "  (19, 1),\n",
      "  (20, 1),\n",
      "  (26, 4),\n",
      "  (36, 3),\n",
      "  (43, 7),\n",
      "  (44, 1),\n",
      "  (47, 1),\n",
      "  (52, 3),\n",
      "  (56, 1),\n",
      "  (59, 2),\n",
      "  (60, 1),\n",
      "  (75, 1),\n",
      "  (79, 9),\n",
      "  (84, 1),\n",
      "  (93, 1),\n",
      "  (108, 2),\n",
      "  (109, 20),\n",
      "  (111, 2),\n",
      "  (113, 9),\n",
      "  (118, 2),\n",
      "  (122, 4),\n",
      "  (125, 3),\n",
      "  (128, 1),\n",
      "  (130, 1),\n",
      "  (132, 1),\n",
      "  (134, 1),\n",
      "  (136, 1),\n",
      "  (148, 1),\n",
      "  (171, 3),\n",
      "  (174, 1),\n",
      "  (178, 2),\n",
      "  (179, 2),\n",
      "  (182, 1),\n",
      "  (199, 1),\n",
      "  (205, 1),\n",
      "  (209, 1),\n",
      "  (210, 1),\n",
      "  (211, 1),\n",
      "  (212, 1),\n",
      "  (213, 1),\n",
      "  (214, 1),\n",
      "  (215, 1),\n",
      "  (216, 1),\n",
      "  (217, 2),\n",
      "  (218, 1),\n",
      "  (219, 1),\n",
      "  (220, 2),\n",
      "  (221, 3),\n",
      "  (222, 1),\n",
      "  (223, 1),\n",
      "  (224, 1),\n",
      "  (225, 1),\n",
      "  (226, 1),\n",
      "  (227, 1),\n",
      "  (228, 2),\n",
      "  (229, 1),\n",
      "  (230, 1),\n",
      "  (231, 1),\n",
      "  (232, 1),\n",
      "  (233, 1),\n",
      "  (234, 1),\n",
      "  (235, 1),\n",
      "  (236, 1),\n",
      "  (237, 1),\n",
      "  (238, 1),\n",
      "  (239, 1),\n",
      "  (240, 3),\n",
      "  (241, 1),\n",
      "  (242, 1),\n",
      "  (243, 2),\n",
      "  (244, 2),\n",
      "  (245, 1),\n",
      "  (246, 1),\n",
      "  (247, 1),\n",
      "  (248, 1),\n",
      "  (249, 2),\n",
      "  (250, 1),\n",
      "  (251, 3),\n",
      "  (252, 1),\n",
      "  (253, 1),\n",
      "  (254, 1),\n",
      "  (255, 1),\n",
      "  (256, 1),\n",
      "  (257, 1),\n",
      "  (258, 1),\n",
      "  (259, 5),\n",
      "  (260, 2),\n",
      "  (261, 1),\n",
      "  (262, 1),\n",
      "  (263, 1),\n",
      "  (264, 1),\n",
      "  (265, 1),\n",
      "  (266, 1),\n",
      "  (267, 1),\n",
      "  (268, 1),\n",
      "  (269, 1),\n",
      "  (270, 2),\n",
      "  (271, 1),\n",
      "  (272, 1),\n",
      "  (273, 1),\n",
      "  (274, 2),\n",
      "  (275, 1),\n",
      "  (276, 1),\n",
      "  (277, 1),\n",
      "  (278, 1),\n",
      "  (279, 1),\n",
      "  (280, 1),\n",
      "  (281, 1),\n",
      "  (282, 1),\n",
      "  (283, 1),\n",
      "  (284, 1),\n",
      "  (285, 1),\n",
      "  (286, 1),\n",
      "  (287, 1),\n",
      "  (288, 1),\n",
      "  (289, 1),\n",
      "  (290, 1),\n",
      "  (291, 1),\n",
      "  (292, 1),\n",
      "  (293, 1),\n",
      "  (294, 1),\n",
      "  (295, 1),\n",
      "  (296, 1),\n",
      "  (297, 1),\n",
      "  (298, 1),\n",
      "  (299, 1),\n",
      "  (300, 2),\n",
      "  (301, 1),\n",
      "  (302, 1)],\n",
      " [(5, 4),\n",
      "  (8, 1),\n",
      "  (18, 1),\n",
      "  (19, 1),\n",
      "  (20, 1),\n",
      "  (52, 3),\n",
      "  (59, 1),\n",
      "  (71, 1),\n",
      "  (73, 1),\n",
      "  (79, 3),\n",
      "  (80, 2),\n",
      "  (81, 1),\n",
      "  (108, 2),\n",
      "  (109, 7),\n",
      "  (111, 1),\n",
      "  (120, 1),\n",
      "  (122, 1),\n",
      "  (132, 1),\n",
      "  (173, 1),\n",
      "  (179, 1),\n",
      "  (279, 1),\n",
      "  (303, 1),\n",
      "  (304, 4),\n",
      "  (305, 1),\n",
      "  (306, 1),\n",
      "  (307, 1),\n",
      "  (308, 1),\n",
      "  (309, 1),\n",
      "  (310, 1),\n",
      "  (311, 1),\n",
      "  (312, 1),\n",
      "  (313, 1),\n",
      "  (314, 1),\n",
      "  (315, 1),\n",
      "  (316, 1),\n",
      "  (317, 1),\n",
      "  (318, 1),\n",
      "  (319, 1),\n",
      "  (320, 1),\n",
      "  (321, 2),\n",
      "  (322, 1),\n",
      "  (323, 1),\n",
      "  (324, 1),\n",
      "  (325, 1),\n",
      "  (326, 1),\n",
      "  (327, 2),\n",
      "  (328, 1),\n",
      "  (329, 1),\n",
      "  (330, 1),\n",
      "  (331, 1),\n",
      "  (332, 1),\n",
      "  (333, 1),\n",
      "  (334, 1),\n",
      "  (335, 1),\n",
      "  (336, 1),\n",
      "  (337, 1),\n",
      "  (338, 1),\n",
      "  (339, 1),\n",
      "  (340, 1),\n",
      "  (341, 1),\n",
      "  (342, 1)],\n",
      " [(5, 5),\n",
      "  (13, 1),\n",
      "  (14, 1),\n",
      "  (20, 1),\n",
      "  (26, 1),\n",
      "  (32, 1),\n",
      "  (44, 1),\n",
      "  (52, 6),\n",
      "  (59, 1),\n",
      "  (71, 2),\n",
      "  (75, 1),\n",
      "  (76, 1),\n",
      "  (79, 1),\n",
      "  (80, 2),\n",
      "  (81, 3),\n",
      "  (90, 1),\n",
      "  (93, 1),\n",
      "  (96, 1),\n",
      "  (108, 1),\n",
      "  (109, 4),\n",
      "  (111, 1),\n",
      "  (113, 3),\n",
      "  (114, 1),\n",
      "  (119, 1),\n",
      "  (122, 2),\n",
      "  (130, 1),\n",
      "  (134, 2),\n",
      "  (171, 2),\n",
      "  (175, 2),\n",
      "  (178, 1),\n",
      "  (179, 1),\n",
      "  (217, 2),\n",
      "  (336, 1),\n",
      "  (343, 2),\n",
      "  (344, 3),\n",
      "  (345, 2),\n",
      "  (346, 2),\n",
      "  (347, 2),\n",
      "  (348, 1),\n",
      "  (349, 1),\n",
      "  (350, 1),\n",
      "  (351, 1),\n",
      "  (352, 1),\n",
      "  (353, 1),\n",
      "  (354, 1),\n",
      "  (355, 2),\n",
      "  (356, 1),\n",
      "  (357, 1),\n",
      "  (358, 2),\n",
      "  (359, 1),\n",
      "  (360, 1),\n",
      "  (361, 2),\n",
      "  (362, 2),\n",
      "  (363, 1),\n",
      "  (364, 1),\n",
      "  (365, 1),\n",
      "  (366, 1),\n",
      "  (367, 1),\n",
      "  (368, 1),\n",
      "  (369, 1),\n",
      "  (370, 1),\n",
      "  (371, 1),\n",
      "  (372, 3),\n",
      "  (373, 1),\n",
      "  (374, 1),\n",
      "  (375, 1),\n",
      "  (376, 1),\n",
      "  (377, 1),\n",
      "  (378, 2),\n",
      "  (379, 1),\n",
      "  (380, 1),\n",
      "  (381, 1),\n",
      "  (382, 1),\n",
      "  (383, 1),\n",
      "  (384, 1),\n",
      "  (385, 1),\n",
      "  (386, 1),\n",
      "  (387, 1),\n",
      "  (388, 1),\n",
      "  (389, 1),\n",
      "  (390, 1),\n",
      "  (391, 1),\n",
      "  (392, 2),\n",
      "  (393, 1)],\n",
      " [(5, 2),\n",
      "  (6, 2),\n",
      "  (13, 2),\n",
      "  (14, 3),\n",
      "  (20, 1),\n",
      "  (36, 1),\n",
      "  (43, 2),\n",
      "  (56, 1),\n",
      "  (59, 2),\n",
      "  (79, 2),\n",
      "  (108, 1),\n",
      "  (109, 5),\n",
      "  (111, 1),\n",
      "  (112, 1),\n",
      "  (113, 4),\n",
      "  (114, 2),\n",
      "  (124, 1),\n",
      "  (128, 1),\n",
      "  (132, 1),\n",
      "  (134, 1),\n",
      "  (174, 1),\n",
      "  (179, 1),\n",
      "  (183, 2),\n",
      "  (217, 1),\n",
      "  (261, 1),\n",
      "  (273, 1),\n",
      "  (332, 1),\n",
      "  (390, 2),\n",
      "  (394, 1),\n",
      "  (395, 1),\n",
      "  (396, 2),\n",
      "  (397, 1),\n",
      "  (398, 1),\n",
      "  (399, 1),\n",
      "  (400, 1),\n",
      "  (401, 1),\n",
      "  (402, 1),\n",
      "  (403, 1),\n",
      "  (404, 1),\n",
      "  (405, 1),\n",
      "  (406, 1),\n",
      "  (407, 1),\n",
      "  (408, 1),\n",
      "  (409, 1),\n",
      "  (410, 1),\n",
      "  (411, 1),\n",
      "  (412, 2),\n",
      "  (413, 1),\n",
      "  (414, 1),\n",
      "  (415, 1),\n",
      "  (416, 1),\n",
      "  (417, 2),\n",
      "  (418, 2),\n",
      "  (419, 1),\n",
      "  (420, 1),\n",
      "  (421, 1),\n",
      "  (422, 1),\n",
      "  (423, 1),\n",
      "  (424, 1),\n",
      "  (425, 2),\n",
      "  (426, 1),\n",
      "  (427, 1),\n",
      "  (428, 5)],\n",
      " [(5, 1),\n",
      "  (8, 3),\n",
      "  (20, 1),\n",
      "  (43, 1),\n",
      "  (52, 2),\n",
      "  (75, 1),\n",
      "  (79, 3),\n",
      "  (80, 2),\n",
      "  (93, 3),\n",
      "  (96, 1),\n",
      "  (108, 2),\n",
      "  (109, 4),\n",
      "  (111, 1),\n",
      "  (113, 1),\n",
      "  (119, 1),\n",
      "  (122, 3),\n",
      "  (128, 3),\n",
      "  (155, 1),\n",
      "  (248, 1),\n",
      "  (336, 1),\n",
      "  (358, 2),\n",
      "  (363, 1),\n",
      "  (385, 1),\n",
      "  (412, 1),\n",
      "  (425, 1),\n",
      "  (427, 2),\n",
      "  (429, 2),\n",
      "  (430, 1),\n",
      "  (431, 1),\n",
      "  (432, 1),\n",
      "  (433, 1),\n",
      "  (434, 1),\n",
      "  (435, 1),\n",
      "  (436, 1),\n",
      "  (437, 1),\n",
      "  (438, 1),\n",
      "  (439, 2),\n",
      "  (440, 1),\n",
      "  (441, 1),\n",
      "  (442, 1),\n",
      "  (443, 1),\n",
      "  (444, 4),\n",
      "  (445, 1),\n",
      "  (446, 1),\n",
      "  (447, 3),\n",
      "  (448, 1),\n",
      "  (449, 1),\n",
      "  (450, 1),\n",
      "  (451, 1),\n",
      "  (452, 1),\n",
      "  (453, 1)],\n",
      " [(5, 1),\n",
      "  (8, 1),\n",
      "  (13, 1),\n",
      "  (36, 1),\n",
      "  (43, 2),\n",
      "  (59, 1),\n",
      "  (79, 4),\n",
      "  (80, 1),\n",
      "  (98, 1),\n",
      "  (109, 1),\n",
      "  (111, 1),\n",
      "  (113, 2),\n",
      "  (179, 1),\n",
      "  (201, 1),\n",
      "  (217, 1),\n",
      "  (234, 1),\n",
      "  (244, 1),\n",
      "  (336, 1),\n",
      "  (367, 1),\n",
      "  (372, 2),\n",
      "  (390, 1),\n",
      "  (417, 1),\n",
      "  (454, 1),\n",
      "  (455, 1),\n",
      "  (456, 1),\n",
      "  (457, 1),\n",
      "  (458, 1),\n",
      "  (459, 1),\n",
      "  (460, 2),\n",
      "  (461, 1),\n",
      "  (462, 1),\n",
      "  (463, 2),\n",
      "  (464, 1),\n",
      "  (465, 1),\n",
      "  (466, 2),\n",
      "  (467, 1),\n",
      "  (468, 1),\n",
      "  (469, 1),\n",
      "  (470, 1),\n",
      "  (471, 1),\n",
      "  (472, 1),\n",
      "  (473, 2),\n",
      "  (474, 1),\n",
      "  (475, 1),\n",
      "  (476, 1),\n",
      "  (477, 1)],\n",
      " [(1, 1),\n",
      "  (5, 1),\n",
      "  (8, 1),\n",
      "  (14, 1),\n",
      "  (19, 1),\n",
      "  (20, 1),\n",
      "  (32, 1),\n",
      "  (43, 2),\n",
      "  (44, 1),\n",
      "  (52, 5),\n",
      "  (59, 2),\n",
      "  (75, 1),\n",
      "  (79, 4),\n",
      "  (108, 2),\n",
      "  (109, 8),\n",
      "  (113, 3),\n",
      "  (119, 2),\n",
      "  (134, 1),\n",
      "  (136, 1),\n",
      "  (158, 1),\n",
      "  (192, 1),\n",
      "  (205, 1),\n",
      "  (210, 1),\n",
      "  (241, 4),\n",
      "  (262, 1),\n",
      "  (348, 1),\n",
      "  (378, 1),\n",
      "  (405, 1),\n",
      "  (427, 3),\n",
      "  (462, 1),\n",
      "  (478, 1),\n",
      "  (479, 1),\n",
      "  (480, 3),\n",
      "  (481, 1),\n",
      "  (482, 1),\n",
      "  (483, 1),\n",
      "  (484, 1),\n",
      "  (485, 1),\n",
      "  (486, 1),\n",
      "  (487, 1),\n",
      "  (488, 1),\n",
      "  (489, 1),\n",
      "  (490, 1),\n",
      "  (491, 1),\n",
      "  (492, 1),\n",
      "  (493, 1),\n",
      "  (494, 1),\n",
      "  (495, 1),\n",
      "  (496, 1),\n",
      "  (497, 1),\n",
      "  (498, 2)]]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(BoW_corpus)# list of (token_id, token_count) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J71uZSxFhS_u"
   },
   "source": [
    "TODO: You can furthur filter and clean your data by using functions such as filter_extremes (remove all tokens that are less frequent or more frequent than a number), filter_n_most_frequent(filter out the remove_n most frequent tokens), merge_with (to merge multiple dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFoTL2eLhnEC",
    "outputId": "8b4cc321-8c25-4f1e-ac13-ce82cf037a21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.11125130923708117), (1, 0.02781282730927029), (2, 0.05562565461854058), (3, 0.11125130923708117), (4, 0.11125130923708117), (6, 0.03807771433817718), (7, 0.05562565461854058), (8, 0.0063623581004894824), (9, 0.05562565461854058), (10, 0.11125130923708117), (11, 0.05562565461854058), (12, 0.05562565461854058), (13, 0.02052977405781377), (14, 0.04464179722247752), (15, 0.05562565461854058), (16, 0.05562565461854058), (17, 0.03807771433817718), (18, 0.03807771433817718), (19, 0.04464179722247752), (20, 0.02544943240195793), (21, 0.11125130923708117), (22, 0.05562565461854058), (23, 0.11125130923708117), (24, 0.05562565461854058), (25, 0.05562565461854058), (26, 0.5006308915668652), (27, 0.05562565461854058), (28, 0.05562565461854058), (29, 0.11125130923708117), (30, 0.05562565461854058), (31, 0.05562565461854058), (32, 0.02052977405781377), (33, 0.05562565461854058), (34, 0.11125130923708117), (35, 0.05562565461854058), (36, 0.04105954811562754), (37, 0.05562565461854058), (38, 0.05562565461854058), (39, 0.05562565461854058), (40, 0.05562565461854058), (41, 0.3337539277112435), (42, 0.05562565461854058), (43, 0.012724716200978965), (44, 0.014880599074159171), (45, 0.05562565461854058), (46, 0.11125130923708117), (47, 0.03807771433817718), (48, 0.11125130923708117), (49, 0.05562565461854058), (50, 0.05562565461854058), (51, 0.05562565461854058), (52, 0.031811790502447417), (53, 0.05562565461854058), (54, 0.05562565461854058), (55, 0.05562565461854058), (56, 0.02052977405781377), (57, 0.05562565461854058), (58, 0.05562565461854058), (59, 0.005963667554900727), (60, 0.02781282730927029), (61, 0.05562565461854058), (62, 0.05562565461854058), (63, 0.05562565461854058), (64, 0.05562565461854058), (65, 0.05562565461854058), (66, 0.05562565461854058), (67, 0.05562565461854058), (68, 0.05562565461854058), (69, 0.05562565461854058), (70, 0.05562565461854058), (71, 0.13906413654635147), (72, 0.05562565461854058), (73, 0.02781282730927029), (74, 0.05562565461854058), (75, 0.010264887028906884), (76, 0.03807771433817718), (77, 0.03807771433817718), (78, 0.05562565461854058), (80, 0.030794661086720656), (81, 0.05562565461854058), (82, 0.05562565461854058), (83, 0.05562565461854058), (84, 0.22846628602906308), (85, 0.05562565461854058), (86, 0.05562565461854058), (87, 0.05562565461854058), (88, 0.11125130923708117), (89, 0.11125130923708117), (90, 0.05562565461854058), (91, 0.05562565461854058), (92, 0.22250261847416233), (93, 0.02052977405781377), (94, 0.05562565461854058), (95, 0.16687696385562176), (96, 0.05562565461854058), (97, 0.05562565461854058), (98, 0.03807771433817718), (99, 0.11125130923708117), (100, 0.16687696385562176), (101, 0.11125130923708117), (102, 0.11125130923708117), (103, 0.05562565461854058), (104, 0.03807771433817718), (105, 0.05562565461854058), (106, 0.05562565461854058), (107, 0.05562565461854058), (108, 0.014909168887251816), (110, 0.05562565461854058), (111, 0.0029818337774503633), (112, 0.07615542867635436), (113, 0.023854670219602907), (114, 0.02781282730927029), (115, 0.05562565461854058), (116, 0.05562565461854058), (117, 0.11125130923708117), (118, 0.03807771433817718), (119, 0.04105954811562754), (120, 0.07615542867635436), (121, 0.05562565461854058), (122, 0.0718542092023482), (123, 0.05562565461854058), (124, 0.03807771433817718), (125, 0.03807771433817718), (126, 0.11125130923708117), (127, 0.05562565461854058), (128, 0.029761198148318343), (129, 0.05562565461854058)]\n",
      "[(8, 0.00889707469654732), (19, 0.020808920120667432), (32, 0.057417369601235134), (43, 0.00889707469654732), (44, 0.020808920120667432), (52, 0.044485373482736595), (56, 0.028708684800617567), (59, 0.01667909754945903), (60, 0.07778650562712319), (73, 0.038893252813561596), (75, 0.028708684800617567), (77, 0.10649519042774076), (80, 0.014354342400308783), (90, 0.07778650562712319), (104, 0.05324759521387038), (108, 0.012509323162094273), (111, 0.02084887193682379), (113, 0.012509323162094273), (122, 0.07177171200154392), (128, 0.041617840241334865), (130, 0.038893252813561596), (131, 0.07778650562712319), (132, 0.028708684800617567), (133, 0.07778650562712319), (134, 0.0624267603620023), (135, 0.07778650562712319), (136, 0.07778650562712319), (137, 0.07778650562712319), (138, 0.07778650562712319), (139, 0.07778650562712319), (140, 0.07778650562712319), (141, 0.07778650562712319), (142, 0.07778650562712319), (143, 0.07778650562712319), (144, 0.07778650562712319), (145, 0.07778650562712319), (146, 0.07778650562712319), (147, 0.07778650562712319), (148, 0.05324759521387038), (149, 0.07778650562712319), (150, 0.07778650562712319), (151, 0.15557301125424638), (152, 0.07778650562712319), (153, 0.07778650562712319), (154, 0.07778650562712319), (155, 0.05324759521387038), (156, 0.07778650562712319), (157, 0.07778650562712319), (158, 0.05324759521387038), (159, 0.07778650562712319), (160, 0.07778650562712319), (161, 0.07778650562712319), (162, 0.07778650562712319), (163, 0.07778650562712319), (164, 0.07778650562712319), (165, 0.23335951688136958), (166, 0.07778650562712319), (167, 0.38893252813561596), (168, 0.07778650562712319), (169, 0.07778650562712319), (170, 0.07778650562712319), (171, 0.038893252813561596), (172, 0.07778650562712319), (173, 0.05324759521387038), (174, 0.038893252813561596), (175, 0.42598076171096305), (176, 0.07778650562712319), (177, 0.07778650562712319), (178, 0.038893252813561596), (179, 0.014354342400308783), (180, 0.07778650562712319), (181, 0.07778650562712319), (182, 0.05324759521387038), (183, 0.05324759521387038), (184, 0.07778650562712319), (185, 0.07778650562712319), (186, 0.07778650562712319), (187, 0.07778650562712319), (188, 0.07778650562712319), (189, 0.15557301125424638), (190, 0.23335951688136958), (191, 0.07778650562712319), (192, 0.05324759521387038), (193, 0.07778650562712319), (194, 0.07778650562712319), (195, 0.23335951688136958), (196, 0.07778650562712319), (197, 0.07778650562712319), (198, 0.15557301125424638), (199, 0.05324759521387038), (200, 0.07778650562712319), (201, 0.05324759521387038), (202, 0.07778650562712319), (203, 0.07778650562712319), (204, 0.15557301125424638), (205, 0.07778650562712319), (206, 0.07778650562712319), (207, 0.07778650562712319), (208, 0.07778650562712319)]\n",
      "[(1, 0.03690255402819175), (8, 0.008441689905857204), (14, 0.01974384355821991), (17, 0.050522188737219094), (19, 0.01974384355821991), (20, 0.008441689905857204), (26, 0.147610216112767), (36, 0.08171780825416403), (43, 0.05909182934100043), (44, 0.01974384355821991), (47, 0.050522188737219094), (52, 0.025325069717571612), (56, 0.02723926941805467), (59, 0.007912700197780512), (60, 0.03690255402819175), (75, 0.013619634709027335), (84, 0.050522188737219094), (93, 0.02723926941805467), (108, 0.007912700197780512), (111, 0.007912700197780512), (113, 0.035607150890012304), (118, 0.10104437747443819), (122, 0.05447853883610934), (125, 0.1515665662116573), (128, 0.01974384355821991), (130, 0.03690255402819175), (132, 0.02723926941805467), (134, 0.01974384355821991), (136, 0.03690255402819175), (148, 0.050522188737219094), (171, 0.11070766208457528), (174, 0.03690255402819175), (178, 0.0738051080563835), (179, 0.02723926941805467), (182, 0.050522188737219094), (199, 0.050522188737219094), (205, 0.03690255402819175), (209, 0.0738051080563835), (210, 0.050522188737219094), (211, 0.0738051080563835), (212, 0.0738051080563835), (213, 0.0738051080563835), (214, 0.0738051080563835), (215, 0.0738051080563835), (216, 0.0738051080563835), (217, 0.05447853883610934), (218, 0.0738051080563835), (219, 0.0738051080563835), (220, 0.147610216112767), (221, 0.22141532416915055), (222, 0.0738051080563835), (223, 0.0738051080563835), (224, 0.0738051080563835), (225, 0.0738051080563835), (226, 0.0738051080563835), (227, 0.0738051080563835), (228, 0.147610216112767), (229, 0.0738051080563835), (230, 0.0738051080563835), (231, 0.0738051080563835), (232, 0.0738051080563835), (233, 0.0738051080563835), (234, 0.050522188737219094), (235, 0.0738051080563835), (236, 0.0738051080563835), (237, 0.0738051080563835), (238, 0.0738051080563835), (239, 0.0738051080563835), (240, 0.22141532416915055), (241, 0.050522188737219094), (242, 0.0738051080563835), (243, 0.147610216112767), (244, 0.10104437747443819), (245, 0.0738051080563835), (246, 0.0738051080563835), (247, 0.0738051080563835), (248, 0.050522188737219094), (249, 0.147610216112767), (250, 0.0738051080563835), (251, 0.22141532416915055), (252, 0.0738051080563835), (253, 0.0738051080563835), (254, 0.0738051080563835), (255, 0.0738051080563835), (256, 0.0738051080563835), (257, 0.0738051080563835), (258, 0.0738051080563835), (259, 0.36902554028191753), (260, 0.147610216112767), (261, 0.050522188737219094), (262, 0.050522188737219094), (263, 0.0738051080563835), (264, 0.0738051080563835), (265, 0.0738051080563835), (266, 0.0738051080563835), (267, 0.0738051080563835), (268, 0.0738051080563835), (269, 0.0738051080563835), (270, 0.147610216112767), (271, 0.0738051080563835), (272, 0.0738051080563835), (273, 0.050522188737219094), (274, 0.147610216112767), (275, 0.0738051080563835), (276, 0.0738051080563835), (277, 0.0738051080563835), (278, 0.0738051080563835), (279, 0.050522188737219094), (280, 0.0738051080563835), (281, 0.0738051080563835), (282, 0.0738051080563835), (283, 0.0738051080563835), (284, 0.0738051080563835), (285, 0.0738051080563835), (286, 0.0738051080563835), (287, 0.0738051080563835), (288, 0.0738051080563835), (289, 0.0738051080563835), (290, 0.0738051080563835), (291, 0.0738051080563835), (292, 0.0738051080563835), (293, 0.0738051080563835), (294, 0.0738051080563835), (295, 0.0738051080563835), (296, 0.0738051080563835), (297, 0.0738051080563835), (298, 0.0738051080563835), (299, 0.0738051080563835), (300, 0.147610216112767), (301, 0.0738051080563835), (302, 0.0738051080563835)]\n",
      "[(8, 0.014432941927024545), (18, 0.08637889145448448), (19, 0.033756481305280715), (20, 0.014432941927024545), (52, 0.043298825781073635), (59, 0.006764258324703593), (71, 0.06309310400968746), (73, 0.06309310400968746), (80, 0.046571574889594035), (81, 0.06309310400968746), (108, 0.013528516649407187), (111, 0.006764258324703593), (120, 0.08637889145448448), (122, 0.023285787444797017), (132, 0.046571574889594035), (173, 0.08637889145448448), (179, 0.023285787444797017), (279, 0.08637889145448448), (303, 0.1261862080193749), (304, 0.5047448320774996), (305, 0.1261862080193749), (306, 0.1261862080193749), (307, 0.1261862080193749), (308, 0.1261862080193749), (309, 0.1261862080193749), (310, 0.1261862080193749), (311, 0.1261862080193749), (312, 0.1261862080193749), (313, 0.1261862080193749), (314, 0.1261862080193749), (315, 0.1261862080193749), (316, 0.1261862080193749), (317, 0.1261862080193749), (318, 0.1261862080193749), (319, 0.1261862080193749), (320, 0.1261862080193749), (321, 0.2523724160387498), (322, 0.1261862080193749), (323, 0.1261862080193749), (324, 0.1261862080193749), (325, 0.1261862080193749), (326, 0.1261862080193749), (327, 0.2523724160387498), (328, 0.1261862080193749), (329, 0.1261862080193749), (330, 0.1261862080193749), (331, 0.1261862080193749), (332, 0.08637889145448448), (333, 0.1261862080193749), (334, 0.1261862080193749), (335, 0.1261862080193749), (336, 0.046571574889594035), (337, 0.1261862080193749), (338, 0.1261862080193749), (339, 0.1261862080193749), (340, 0.1261862080193749), (341, 0.1261862080193749), (342, 0.1261862080193749)]\n",
      "[(13, 0.03772753138749916), (14, 0.027346051985479854), (20, 0.011692094820856888), (26, 0.051111586144622696), (32, 0.03772753138749916), (44, 0.027346051985479854), (52, 0.07015256892514132), (59, 0.005479710936626045), (71, 0.10222317228924539), (75, 0.01886376569374958), (76, 0.06997535183837228), (80, 0.03772753138749916), (81, 0.1533347584338681), (90, 0.051111586144622696), (93, 0.03772753138749916), (96, 0.051111586144622696), (108, 0.005479710936626045), (111, 0.005479710936626045), (113, 0.016439132809878132), (114, 0.051111586144622696), (119, 0.03772753138749916), (122, 0.03772753138749916), (130, 0.051111586144622696), (134, 0.05469210397095971), (171, 0.10222317228924539), (175, 0.13995070367674456), (178, 0.051111586144622696), (179, 0.01886376569374958), (217, 0.07545506277499832), (336, 0.03772753138749916), (343, 0.20444634457849079), (344, 0.3066695168677362), (345, 0.20444634457849079), (346, 0.20444634457849079), (347, 0.20444634457849079), (348, 0.06997535183837228), (349, 0.10222317228924539), (350, 0.10222317228924539), (351, 0.10222317228924539), (352, 0.10222317228924539), (353, 0.10222317228924539), (354, 0.10222317228924539), (355, 0.20444634457849079), (356, 0.10222317228924539), (357, 0.10222317228924539), (358, 0.13995070367674456), (359, 0.10222317228924539), (360, 0.10222317228924539), (361, 0.20444634457849079), (362, 0.20444634457849079), (363, 0.06997535183837228), (364, 0.10222317228924539), (365, 0.10222317228924539), (366, 0.10222317228924539), (367, 0.06997535183837228), (368, 0.10222317228924539), (369, 0.10222317228924539), (370, 0.10222317228924539), (371, 0.10222317228924539), (372, 0.20992605551511684), (373, 0.10222317228924539), (374, 0.10222317228924539), (375, 0.10222317228924539), (376, 0.10222317228924539), (377, 0.10222317228924539), (378, 0.13995070367674456), (379, 0.10222317228924539), (380, 0.10222317228924539), (381, 0.10222317228924539), (382, 0.10222317228924539), (383, 0.10222317228924539), (384, 0.10222317228924539), (385, 0.06997535183837228), (386, 0.10222317228924539), (387, 0.10222317228924539), (388, 0.10222317228924539), (389, 0.10222317228924539), (390, 0.051111586144622696), (391, 0.10222317228924539), (392, 0.20444634457849079), (393, 0.10222317228924539)]\n",
      "[(6, 0.156301550705905), (13, 0.0842706968275387), (14, 0.09162291189804939), (20, 0.013058116204413803), (36, 0.04213534841376935), (43, 0.026116232408827607), (56, 0.04213534841376935), (59, 0.012239842949172387), (108, 0.006119921474586194), (111, 0.006119921474586194), (112, 0.0781507753529525), (113, 0.024479685898344775), (114, 0.11416620229213567), (124, 0.0781507753529525), (128, 0.03054097063268313), (132, 0.04213534841376935), (134, 0.03054097063268313), (174, 0.057083101146067836), (179, 0.021067674206884674), (183, 0.156301550705905), (217, 0.04213534841376935), (261, 0.0781507753529525), (273, 0.0781507753529525), (332, 0.0781507753529525), (390, 0.11416620229213567), (394, 0.11416620229213567), (395, 0.11416620229213567), (396, 0.22833240458427134), (397, 0.11416620229213567), (398, 0.11416620229213567), (399, 0.11416620229213567), (400, 0.11416620229213567), (401, 0.11416620229213567), (402, 0.11416620229213567), (403, 0.11416620229213567), (404, 0.11416620229213567), (405, 0.0781507753529525), (406, 0.11416620229213567), (407, 0.11416620229213567), (408, 0.11416620229213567), (409, 0.11416620229213567), (410, 0.11416620229213567), (411, 0.11416620229213567), (412, 0.156301550705905), (413, 0.11416620229213567), (414, 0.11416620229213567), (415, 0.11416620229213567), (416, 0.11416620229213567), (417, 0.156301550705905), (418, 0.22833240458427134), (419, 0.11416620229213567), (420, 0.11416620229213567), (421, 0.11416620229213567), (422, 0.11416620229213567), (423, 0.11416620229213567), (424, 0.11416620229213567), (425, 0.156301550705905), (426, 0.11416620229213567), (427, 0.057083101146067836), (428, 0.5708310114606783)]\n",
      "[(8, 0.04331051003209035), (20, 0.01443683667736345), (43, 0.01443683667736345), (52, 0.0288736733547269), (75, 0.02329207114835523), (80, 0.04658414229671046), (93, 0.13975242689013137), (96, 0.06311012977542994), (108, 0.013532167339271496), (111, 0.006766083669635748), (113, 0.006766083669635748), (119, 0.04658414229671046), (122, 0.06987621344506569), (128, 0.10129677162234899), (155, 0.08640220092378517), (248, 0.08640220092378517), (336, 0.04658414229671046), (358, 0.17280440184757034), (363, 0.08640220092378517), (385, 0.08640220092378517), (412, 0.08640220092378517), (425, 0.08640220092378517), (427, 0.1262202595508599), (429, 0.2524405191017198), (430, 0.1262202595508599), (431, 0.1262202595508599), (432, 0.1262202595508599), (433, 0.1262202595508599), (434, 0.1262202595508599), (435, 0.1262202595508599), (436, 0.1262202595508599), (437, 0.1262202595508599), (438, 0.1262202595508599), (439, 0.2524405191017198), (440, 0.1262202595508599), (441, 0.1262202595508599), (442, 0.1262202595508599), (443, 0.1262202595508599), (444, 0.5048810382034395), (445, 0.1262202595508599), (446, 0.1262202595508599), (447, 0.37866077865257963), (448, 0.1262202595508599), (449, 0.1262202595508599), (450, 0.1262202595508599), (451, 0.1262202595508599), (452, 0.1262202595508599), (453, 0.1262202595508599)]\n",
      "[(8, 0.017841100010953136), (13, 0.057568867766111385), (36, 0.057568867766111385), (43, 0.03568220002190627), (59, 0.008361553027868278), (80, 0.028784433883055693), (98, 0.1067761825043545), (111, 0.008361553027868278), (113, 0.016723106055736556), (179, 0.028784433883055693), (201, 0.1067761825043545), (217, 0.057568867766111385), (234, 0.1067761825043545), (244, 0.1067761825043545), (336, 0.057568867766111385), (367, 0.1067761825043545), (372, 0.213552365008709), (390, 0.0779917486212988), (417, 0.1067761825043545), (454, 0.1559834972425976), (455, 0.1559834972425976), (456, 0.1559834972425976), (457, 0.1559834972425976), (458, 0.1559834972425976), (459, 0.1559834972425976), (460, 0.3119669944851952), (461, 0.1559834972425976), (462, 0.1067761825043545), (463, 0.3119669944851952), (464, 0.1559834972425976), (465, 0.1559834972425976), (466, 0.3119669944851952), (467, 0.1559834972425976), (468, 0.1559834972425976), (469, 0.1559834972425976), (470, 0.1559834972425976), (471, 0.1559834972425976), (472, 0.1559834972425976), (473, 0.3119669944851952), (474, 0.1559834972425976), (475, 0.1559834972425976), (476, 0.1559834972425976), (477, 0.1559834972425976)]\n",
      "[(1, 0.07239393659413132), (8, 0.016560565518719206), (14, 0.0387326730173277), (19, 0.0387326730173277), (20, 0.016560565518719206), (32, 0.05343689603745666), (43, 0.03312113103743841), (44, 0.0387326730173277), (52, 0.08280282759359602), (59, 0.01552281492410735), (75, 0.02671844801872833), (108, 0.01552281492410735), (113, 0.023284222386161027), (119, 0.10687379207491332), (134, 0.0387326730173277), (136, 0.07239393659413132), (158, 0.09911238461285966), (192, 0.09911238461285966), (205, 0.07239393659413132), (210, 0.09911238461285966), (241, 0.39644953845143865), (262, 0.09911238461285966), (348, 0.09911238461285966), (378, 0.09911238461285966), (405, 0.09911238461285966), (427, 0.217181809782394), (462, 0.09911238461285966), (478, 0.14478787318826264), (479, 0.14478787318826264), (480, 0.434363619564788), (481, 0.14478787318826264), (482, 0.14478787318826264), (483, 0.14478787318826264), (484, 0.14478787318826264), (485, 0.14478787318826264), (486, 0.14478787318826264), (487, 0.14478787318826264), (488, 0.14478787318826264), (489, 0.14478787318826264), (490, 0.14478787318826264), (491, 0.14478787318826264), (492, 0.14478787318826264), (493, 0.14478787318826264), (494, 0.14478787318826264), (495, 0.14478787318826264), (496, 0.14478787318826264), (497, 0.14478787318826264), (498, 0.2895757463765253)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGqv-N-1CKh1"
   },
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfCHt_HqiTq2",
    "outputId": "74c53758-70d0-4bf8-d628-59f78e607a57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '-0.268*\"data\" + -0.187*\"online\" + -0.138*\"local\" + -0.135*\"language\" + -0.129*\"article\" + -0.129*\"wsd\" + -0.127*\"fake\" + -0.120*\"example\" + -0.119*\"resources\" + -0.117*\"learner\"'),\n",
       " (1,\n",
       "  '0.325*\"data\" + -0.193*\"resources\" + 0.181*\"local\" + 0.176*\"fake\" + -0.161*\"wsd\" + 0.145*\"piece\" + -0.145*\"subjectivity\" + 0.117*\"receiver\" + -0.115*\"language\" + 0.108*\"inference\"'),\n",
       " (2,\n",
       "  '-0.276*\"wsd\" + 0.257*\"online\" + 0.212*\"learner\" + -0.173*\"dictionaries\" + -0.173*\"native\" + -0.173*\"phonetic\" + -0.173*\"rules\" + -0.135*\"system\" + 0.127*\"inverse\" + 0.127*\"sliced\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tfidf.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1JGVwtH8i1uf",
    "outputId": "922e60f9-1274-44ef-c5c5-d78f463877a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.668*\"the\" + 0.288*\"of\" + 0.284*\"data\" + 0.263*\"to\" + 0.213*\"and\" + 0.190*\"in\" + 0.189*\"we\" + 0.135*\"that\" + 0.128*\"for\" + 0.098*\"is\"'),\n",
       " (1,\n",
       "  '-0.550*\"data\" + -0.209*\"fake\" + 0.199*\"online\" + -0.191*\"piece\" + 0.175*\"of\" + -0.161*\"model\" + 0.143*\"this\" + -0.139*\"receiver\" + 0.134*\"the\" + 0.120*\"for\"'),\n",
       " (2,\n",
       "  '-0.350*\"online\" + 0.245*\"for\" + -0.230*\"in\" + 0.206*\"local\" + -0.202*\"learner\" + 0.172*\"to\" + -0.141*\"we\" + -0.140*\"this\" + 0.124*\"ep\" + 0.124*\"inference\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_bow.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q49HyKGYDTJV",
    "outputId": "a70304a9-b341-4b56-e459-041bc5643a93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"online\" + 0.004*\"learner\" + 0.003*\"regression\" + 0.003*\"sliced\" + 0.003*\"inverse\" + 0.003*\"step\" + 0.003*\"dimension\" + 0.003*\"through\" + 0.003*\"reduction\" + 0.003*\"numerical\"'),\n",
       " (1,\n",
       "  '0.005*\"data\" + 0.004*\"bounds\" + 0.004*\"local\" + 0.003*\"fake\" + 0.003*\"article\" + 0.003*\"model\" + 0.003*\"piece\" + 0.003*\"our\" + 0.003*\"influence\" + 0.003*\"network\"'),\n",
       " (2,\n",
       "  '0.005*\"wsd\" + 0.004*\"resources\" + 0.004*\"base\" + 0.004*\"word\" + 0.004*\"example\" + 0.004*\"subjectivity\" + 0.003*\"rules\" + 0.003*\"dictionaries\" + 0.003*\"native\" + 0.003*\"phonetic\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5e5SXIACOxM"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "T87K2GyY45rB"
   },
   "outputs": [],
   "source": [
    "def plot_2d_space(corpus, method, use_tsne=False):\n",
    "\n",
    "  if isinstance(method, models.ldamodel.LdaModel):\n",
    "    documents_2d_1=[x[0][0][1] for x in method[corpus]]\n",
    "    documents_2d_2=[x[0][1][1] for x in list(method[corpus])]\n",
    "  else:\n",
    "    documents_2d_1=[x[0][1] for x in method[corpus]]\n",
    "    documents_2d_2=[x[1][1] for x in list(method[corpus])]\n",
    "\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "  # Get topic weights\n",
    "  topic_weights = []\n",
    "  for i, row_list in enumerate(method[corpus]):\n",
    "      if isinstance(method, models.ldamodel.LdaModel):\n",
    "        topic_weights.append([w for i, w in row_list[0]])\n",
    "      else:\n",
    "        topic_weights.append([w for i, w in row_list])\n",
    "\n",
    "  # Array of topic weights    \n",
    "  arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "  # Dominant topic number in each doc\n",
    "  topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "  if use_tsne:\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "    tsne = tsne_model.fit_transform(arr)\n",
    "    documents_2d_1 = tsne[:,0]\n",
    "    documents_2d_2 = tsne[:,1]\n",
    "\n",
    "  ax.scatter(documents_2d_1, documents_2d_2, c=topic_num, s=80 ,alpha=0.8)\n",
    "  print(data_df[\"title\"])\n",
    "  for i,data in enumerate(corpus):\n",
    "      ax.annotate(i, (documents_2d_1[i]+0.01, documents_2d_2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "r9rzndx-4VbD",
    "outputId": "c9cb49f6-697d-4f9e-bc47-96d486cc6b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         A Model of Fake Data in Data-driven Analysis\n",
      "1    Online Sufficient Dimension Reduction Through ...\n",
      "2    Expectation Propagation as a Way of Life: A Fr...\n",
      "3    Generalized Nonbacktracking Bounds on the Infl...\n",
      "4    Cross-language and Cross-encyclopedia Article ...\n",
      "5    Unsupervised Domain Tuning to Improve Word Sen...\n",
      "6    From Words to Senses: A Case Study of Subjecti...\n",
      "7    Rule-based lexical modelling of foreign-accent...\n",
      "8    A Best-Match Algorithm for Broad-Coverage Exam...\n",
      "Name: title, dtype: object\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_2d_space(BoW_corpus, lsi_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "3kNPmCe05KXq",
    "outputId": "a12c88f0-f146-4920-b249-dd4c1c8b46ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         A Model of Fake Data in Data-driven Analysis\n",
      "1    Online Sufficient Dimension Reduction Through ...\n",
      "2    Expectation Propagation as a Way of Life: A Fr...\n",
      "3    Generalized Nonbacktracking Bounds on the Infl...\n",
      "4    Cross-language and Cross-encyclopedia Article ...\n",
      "5    Unsupervised Domain Tuning to Improve Word Sen...\n",
      "6    From Words to Senses: A Case Study of Subjecti...\n",
      "7    Rule-based lexical modelling of foreign-accent...\n",
      "8    A Best-Match Algorithm for Broad-Coverage Exam...\n",
      "Name: title, dtype: object\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_2d_space(corpus_tfidf, lsi_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "OueGZydG5brI",
    "outputId": "dbb4eb73-91e8-4924-e950-42b41abeb9c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         A Model of Fake Data in Data-driven Analysis\n",
      "1    Online Sufficient Dimension Reduction Through ...\n",
      "2    Expectation Propagation as a Way of Life: A Fr...\n",
      "3    Generalized Nonbacktracking Bounds on the Infl...\n",
      "4    Cross-language and Cross-encyclopedia Article ...\n",
      "5    Unsupervised Domain Tuning to Improve Word Sen...\n",
      "6    From Words to Senses: A Case Study of Subjecti...\n",
      "7    Rule-based lexical modelling of foreign-accent...\n",
      "8    A Best-Match Algorithm for Broad-Coverage Exam...\n",
      "Name: title, dtype: object\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_2d_space(corpus_tfidf, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "sKOF40p09vZu",
    "outputId": "daff67d5-1f22-4138-e9f3-e2d04d2bc5ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 8 nearest neighbors...\n",
      "[t-SNE] Indexed 9 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 9 samples in 0.001s...\n",
      "[t-SNE] Computed conditional probabilities for sample 9 / 9\n",
      "[t-SNE] Mean sigma: 1125899906842624.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 46.220062\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.500125\n",
      "0         A Model of Fake Data in Data-driven Analysis\n",
      "1    Online Sufficient Dimension Reduction Through ...\n",
      "2    Expectation Propagation as a Way of Life: A Fr...\n",
      "3    Generalized Nonbacktracking Bounds on the Infl...\n",
      "4    Cross-language and Cross-encyclopedia Article ...\n",
      "5    Unsupervised Domain Tuning to Improve Word Sen...\n",
      "6    From Words to Senses: A Case Study of Subjecti...\n",
      "7    Rule-based lexical modelling of foreign-accent...\n",
      "8    A Best-Match Algorithm for Broad-Coverage Exam...\n",
      "Name: title, dtype: object\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_2d_space(corpus_tfidf, lda_model, use_tsne=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "VbjztoTvqtHG"
   },
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus_tfidf, dictionary=lda_model.id2word, mds='mmds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "id": "NJsrRxeJqxDx",
    "outputId": "daf23108-5903-429f-c113-d9e226d15b88"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el17761402082950566328247247066\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el17761402082950566328247247066_data = {\"mdsDat\": {\"x\": [0.0038588715354039415, -0.0019279141234845648, -0.0019309574119193767], \"y\": [0.003772962981827088, -0.005510834880649005, 0.0017378718988219166], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [49.469236178357704, 35.17135007307942, 15.359413748562883]}, \"tinfo\": {\"Term\": [\"learner\", \"online\", \"regression\", \"sliced\", \"inverse\", \"wsd\", \"step\", \"dimension\", \"through\", \"reduction\", \"resources\", \"base\", \"word\", \"numerical\", \"subjectivity\", \"linear\", \"loss\", \"construct\", \"matrix\", \"cause\", \"confirm\", \"real\", \"high\", \"changing\", \"fashion\", \"theoretical\", \"paradigm\", \"effective\", \"parametric\", \"replacing\", \"data\", \"bounds\", \"local\", \"fake\", \"article\", \"model\", \"piece\", \"our\", \"influence\", \"network\", \"ep\", \"bayesian\", \"inference\", \"receiver\", \"english\", \"among\", \"baidu\", \"cross\", \"baike\", \"wiki\", \"encyclopedias\", \"articles\", \"method\", \"while\", \"sources\", \"sender\", \"various\", \"update\", \"prior\", \"approximations\", \"we\", \"language\", \"in\", \"wsd\", \"resources\", \"base\", \"word\", \"subjectivity\", \"example\", \"rules\", \"dictionaries\", \"native\", \"phonetic\", \"words\", \"system\", \"existing\", \"achieved\", \"sense\", \"useful\", \"available\", \"technique\", \"into\", \"improve\", \"accented\", \"transform\", \"speakers\", \"presented\", \"design\", \"adapted\", \"considerations\", \"transcriptions\", \"rewrite\", \"pair\", \"using\", \"language\", \"different\", \"performance\", \"any\", \"learner\", \"online\", \"regression\", \"sliced\", \"inverse\", \"step\", \"dimension\", \"through\", \"reduction\", \"numerical\", \"linear\", \"loss\", \"construct\", \"matrix\", \"cause\", \"confirm\", \"real\", \"high\", \"changing\", \"fashion\", \"theoretical\", \"paradigm\", \"effective\", \"parametric\", \"goal\", \"replacing\", \"motivated\", \"sufficient\", \"implement\", \"batch\", \"two\", \"as\", \"propose\", \"it\"], \"Freq\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1620465702941727, 0.13170184706765048, 0.11040038093357309, 0.10457640938380451, 0.10055830295550346, 0.10008832619590283, 0.0960285852771402, 0.09507939742282084, 0.09199867053369601, 0.09199755077967382, 0.08717814401041309, 0.08717777318278236, 0.08717754777775193, 0.08716786990370297, 0.08448185630740133, 0.08448115100778995, 0.08448095468727958, 0.08448087470484941, 0.08448106375422978, 0.08448054023286876, 0.08447966042613704, 0.08447920961607615, 0.08591041522129975, 0.08214156312988695, 0.07845675075917877, 0.07844255024226095, 0.07783725048196523, 0.07557286063066952, 0.07557280246162941, 0.07557236619382855, 0.08198289253071613, 0.0854453392033287, 0.07933561224400038, 0.10588881133155702, 0.09813999515819567, 0.08987471668855812, 0.08590538466805875, 0.0833381063420827, 0.08557488788051038, 0.0755295504356199, 0.0755275342971715, 0.07552494433470319, 0.07552127392880997, 0.07289970978271465, 0.06980480802479556, 0.06854792663772767, 0.06854285010450635, 0.06741399867849684, 0.06741229788478012, 0.0657363285189074, 0.06573447263761774, 0.06309272125910971, 0.05974687229939555, 0.05725297624335504, 0.05725250064146465, 0.057251208245023374, 0.05725127028005256, 0.05725102213993583, 0.057250949765735124, 0.057250608573074624, 0.05725057755556003, 0.057250484503016265, 0.05725042763757285, 0.06157252702109663, 0.06403464046385333, 0.059265904378982506, 0.05734248245129192, 0.05737014490472095, 0.04012788435943189, 0.042343947288359135, 0.03147674279533534, 0.03147669312878165, 0.03147548307092805, 0.027153264781254023, 0.027152754570293366, 0.027152284995603907, 0.02715206826882416, 0.02444921441687334, 0.02283612602548038, 0.02283608990435042, 0.02283598605610179, 0.02283593864711872, 0.022835868662429426, 0.022835875435141293, 0.02283585285943507, 0.022835857374576315, 0.0228358257685876, 0.0228358257685876, 0.022835828026158224, 0.022835789647457642, 0.022835789647457642, 0.022835753526327683, 0.02283572869305084, 0.022835717405197724, 0.022835719662768348, 0.02283571289005648, 0.022835717405197724, 0.022835708374915237, 0.02288200663323922, 0.022881672512787107, 0.022858345035545947, 0.02285469454384954], \"Total\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21999025032035024, 0.18942732907907991, 0.16809604354731536, 0.16247108793311407, 0.15826959463448484, 0.15788090994818088, 0.1538711409277437, 0.15281472141811323, 0.14971774108565947, 0.149717202661332, 0.14486993022181746, 0.14486996897104015, 0.14486974297490587, 0.14498954733509786, 0.14218792330179783, 0.14218715078968233, 0.14218732041659088, 0.14218749229048536, 0.14218786313035295, 0.14218737374635113, 0.14218682441246372, 0.14218663584343222, 0.1457563446829565, 0.13985953204967227, 0.1362484899331421, 0.13624481637711625, 0.13557899015942965, 0.1332597908949006, 0.13325970772765763, 0.13325947254856815, 0.15175044100593954, 0.1680800069495207, 0.15248382087421186, 0.17700113710371718, 0.16926520165331996, 0.1609968595744787, 0.1570241357620293, 0.15446497662168232, 0.1644281004606772, 0.14664626448314302, 0.14664752873817577, 0.1466455276946815, 0.14664521943954753, 0.14401948214804997, 0.14091443013882055, 0.13966595390172676, 0.13966582670741415, 0.13852575520652383, 0.13852474794159106, 0.13684074205456015, 0.1368414734201323, 0.1342080925074703, 0.130852164102962, 0.12835773320782615, 0.12835746945110513, 0.1283574008623285, 0.12835771743072344, 0.12835720065489487, 0.1283574134379013, 0.12835764889270462, 0.12835760973917273, 0.1283574786158933, 0.12835796145571174, 0.14058702680839008, 0.1680800069495207, 0.15228845767004387, 0.13140492446834304, 0.13434025737296348, 0.1317814365758479, 0.15556559583829757, 0.12312520494975175, 0.12312522658422614, 0.12312714940625702, 0.11879427203568069, 0.11879485399518802, 0.11879547237080473, 0.11879571573715808, 0.12199633662396753, 0.11445366779745272, 0.11445378592647433, 0.11445390712375622, 0.11445397273198728, 0.11445404587740379, 0.11445412063560484, 0.11445411411377888, 0.11445414143120795, 0.11445414655744258, 0.11445418333446693, 0.11445420677602977, 0.11445421595197268, 0.11445425483054128, 0.11445428210991154, 0.1144543004820405, 0.11445424980996964, 0.1144543343648654, 0.11445431731485636, 0.11445435303846242, 0.11445433631567563, 0.12869732508147044, 0.12869799618620612, 0.13118451518315694, 0.12458752910323458], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.261199951171875, -5.468599796295166, -5.644999980926514, -5.69920015335083, -5.738399982452393, -5.743100166320801, -5.7845001220703125, -5.794400215148926, -5.827300071716309, -5.827300071716309, -5.881199836730957, -5.881199836730957, -5.881199836730957, -5.88129997253418, -5.912600040435791, -5.912600040435791, -5.912600040435791, -5.912600040435791, -5.912600040435791, -5.912600040435791, -5.912600040435791, -5.912600040435791, -5.8958001136779785, -5.940700054168701, -5.986599922180176, -5.986700057983398, -5.994500160217285, -6.02400016784668, -6.02400016784668, -6.02400016784668, -5.942599773406982, -5.901199817657471, -5.975399971008301, -5.345600128173828, -5.421599864959717, -5.5096001625061035, -5.554699897766113, -5.585100173950195, -5.558599948883057, -5.683499813079834, -5.683499813079834, -5.683499813079834, -5.683599948883057, -5.718900203704834, -5.76230001449585, -5.7804999351501465, -5.7804999351501465, -5.797100067138672, -5.7972002029418945, -5.822299957275391, -5.822400093078613, -5.863399982452393, -5.917900085449219, -5.9604997634887695, -5.9604997634887695, -5.9604997634887695, -5.9604997634887695, -5.9604997634887695, -5.9604997634887695, -5.960599899291992, -5.960599899291992, -5.960599899291992, -5.960599899291992, -5.887800216674805, -5.848599910736084, -5.926000118255615, -5.959000110626221, -5.958499908447266, -5.487400054931641, -5.433700084686279, -5.730199813842773, -5.730199813842773, -5.730299949645996, -5.877999782562256, -5.877999782562256, -5.877999782562256, -5.877999782562256, -5.982900142669678, -6.05109977722168, -6.05109977722168, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.051199913024902, -6.049099922180176, -6.049200057983398, -6.05019998550415, -6.050300121307373], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.3981, 0.3404, 0.2834, 0.2632, 0.2503, 0.248, 0.2323, 0.2293, 0.2168, 0.2168, 0.1959, 0.1959, 0.1959, 0.195, 0.1832, 0.1832, 0.1832, 0.1832, 0.1832, 0.1832, 0.1832, 0.1832, 0.1752, 0.1716, 0.1519, 0.1517, 0.1489, 0.1366, 0.1366, 0.1366, 0.0881, 0.0273, 0.0504, 0.5312, 0.4999, 0.462, 0.4418, 0.4279, 0.3919, 0.3814, 0.3814, 0.3814, 0.3813, 0.3641, 0.3425, 0.3332, 0.3331, 0.3247, 0.3247, 0.3118, 0.3117, 0.2902, 0.261, 0.2376, 0.2376, 0.2376, 0.2376, 0.2376, 0.2376, 0.2376, 0.2376, 0.2376, 0.2376, 0.2193, 0.0799, 0.1012, 0.2157, 0.1941, 0.6844, 0.5722, 0.5095, 0.5095, 0.5094, 0.3975, 0.3975, 0.3975, 0.3975, 0.266, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.2616, 0.1463, 0.1463, 0.1262, 0.1776]}, \"token.table\": {\"Topic\": [], \"Freq\": [], \"Term\": []}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el17761402082950566328247247066\", ldavis_el17761402082950566328247247066_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el17761402082950566328247247066\", ldavis_el17761402082950566328247247066_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el17761402082950566328247247066\", ldavis_el17761402082950566328247247066_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1      0.003859  0.003773       1        1  49.469236\n",
       "2     -0.001928 -0.005511       2        1  35.171350\n",
       "0     -0.001931  0.001738       3        1  15.359414, topic_info=           Term      Freq     Total Category  logprob  loglift\n",
       "167     learner  0.000000  0.000000  Default  30.0000  30.0000\n",
       "175      online  0.000000  0.000000  Default  29.0000  29.0000\n",
       "190  regression  0.000000  0.000000  Default  28.0000  28.0000\n",
       "195      sliced  0.000000  0.000000  Default  27.0000  27.0000\n",
       "165     inverse  0.000000  0.000000  Default  26.0000  26.0000\n",
       "..          ...       ...       ...      ...      ...      ...\n",
       "138       batch  0.022836  0.114454   Topic3  -6.0512   0.2616\n",
       "205         two  0.022882  0.128697   Topic3  -6.0491   0.1463\n",
       "136          as  0.022882  0.128698   Topic3  -6.0492   0.1463\n",
       "90      propose  0.022858  0.131185   Topic3  -6.0502   0.1262\n",
       "60           it  0.022855  0.124588   Topic3  -6.0503   0.1776\n",
       "\n",
       "[132 rows x 6 columns], token_table=Empty DataFrame\n",
       "Columns: [Topic, Freq, Term]\n",
       "Index: [], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 3, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
